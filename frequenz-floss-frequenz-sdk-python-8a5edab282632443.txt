Directory structure:
└── frequenz-floss-frequenz-sdk-python/
    ├── README.md
    ├── CODEOWNERS
    ├── CONTRIBUTING.md
    ├── LICENSE
    ├── MANIFEST.in
    ├── mkdocs.yml
    ├── noxfile.py
    ├── pyproject.toml
    ├── RELEASE_NOTES.md
    ├── .cookiecutter-replay.json
    ├── .editorconfig
    ├── benchmarks/
    │   ├── power_distribution/
    │   │   └── power_distributor.py
    │   └── timeseries/
    │       ├── benchmark_datasourcing.py
    │       ├── benchmark_ringbuffer.py
    │       ├── periodic_feature_extractor.py
    │       ├── resampling.py
    │       ├── ringbuffer_memusage.py
    │       └── ringbuffer_serialization.py
    ├── docs/
    │   ├── CONTRIBUTING.md
    │   ├── index.md
    │   ├── SUMMARY.md
    │   ├── _css/
    │   │   ├── mkdocstrings.css
    │   │   └── style.css
    │   ├── _overrides/
    │   │   └── main.html
    │   ├── _scripts/
    │   │   ├── macros.py
    │   │   └── mkdocstrings_autoapi.py
    │   ├── tutorials/
    │   │   └── getting_started.md
    │   └── user-guide/
    │       ├── actors.md
    │       ├── config.md
    │       ├── formula-engine.md
    │       ├── glossary.md
    │       ├── index.md
    │       └── microgrid-concepts.md
    ├── examples/
    │   ├── __init__.py
    │   ├── battery_pool.py
    │   └── load_shedding.py
    ├── src/
    │   └── frequenz/
    │       └── sdk/
    │           ├── __init__.py
    │           ├── conftest.py
    │           ├── py.typed
    │           ├── _internal/
    │           │   ├── __init__.py
    │           │   ├── _asyncio.py
    │           │   ├── _channels.py
    │           │   ├── _constants.py
    │           │   ├── _math.py
    │           │   └── _singleton_meta.py
    │           ├── actor/
    │           │   ├── __init__.py
    │           │   ├── _actor.py
    │           │   ├── _background_service.py
    │           │   └── _run_utils.py
    │           ├── config/
    │           │   ├── __init__.py
    │           │   ├── _base_schema.py
    │           │   ├── _logging_actor.py
    │           │   ├── _manager.py
    │           │   ├── _managing_actor.py
    │           │   └── _util.py
    │           ├── microgrid/
    │           │   ├── __init__.py
    │           │   ├── _data_pipeline.py
    │           │   ├── _power_wrapper.py
    │           │   ├── _resampling.py
    │           │   ├── component_graph.py
    │           │   ├── connection_manager.py
    │           │   ├── _data_sourcing/
    │           │   │   ├── __init__.py
    │           │   │   ├── _component_metric_request.py
    │           │   │   ├── data_sourcing.py
    │           │   │   └── microgrid_api_source.py
    │           │   ├── _power_distributing/
    │           │   │   ├── __init__.py
    │           │   │   ├── _component_pool_status_tracker.py
    │           │   │   ├── power_distributing.py
    │           │   │   ├── request.py
    │           │   │   ├── result.py
    │           │   │   ├── _component_managers/
    │           │   │   │   ├── __init__.py
    │           │   │   │   ├── _battery_manager.py
    │           │   │   │   ├── _component_manager.py
    │           │   │   │   ├── _ev_charger_manager/
    │           │   │   │   │   ├── __init__.py
    │           │   │   │   │   ├── _config.py
    │           │   │   │   │   ├── _ev_charger_manager.py
    │           │   │   │   │   └── _states.py
    │           │   │   │   └── _pv_inverter_manager/
    │           │   │   │       ├── __init__.py
    │           │   │   │       └── _pv_inverter_manager.py
    │           │   │   ├── _component_status/
    │           │   │   │   ├── __init__.py
    │           │   │   │   ├── _battery_status_tracker.py
    │           │   │   │   ├── _blocking_status.py
    │           │   │   │   ├── _component_status.py
    │           │   │   │   ├── _ev_charger_status_tracker.py
    │           │   │   │   └── _pv_inverter_status_tracker.py
    │           │   │   └── _distribution_algorithm/
    │           │   │       ├── __init__.py
    │           │   │       └── _battery_distribution_algorithm.py
    │           │   └── _power_managing/
    │           │       ├── __init__.py
    │           │       ├── _base_classes.py
    │           │       ├── _bounds.py
    │           │       ├── _matryoshka.py
    │           │       ├── _power_managing_actor.py
    │           │       ├── _shifting_matryoshka.py
    │           │       └── _sorted_set.py
    │           └── timeseries/
    │               ├── __init__.py
    │               ├── _base_types.py
    │               ├── _fuse.py
    │               ├── _grid_frequency.py
    │               ├── _moving_window.py
    │               ├── _periodic_feature_extractor.py
    │               ├── _voltage_streamer.py
    │               ├── consumer.py
    │               ├── grid.py
    │               ├── producer.py
    │               ├── _resampling/
    │               │   ├── __init__.py
    │               │   ├── _base_types.py
    │               │   ├── _config.py
    │               │   ├── _exceptions.py
    │               │   ├── _resampler.py
    │               │   └── _wall_clock_timer.py
    │               ├── _ringbuffer/
    │               │   ├── __init__.py
    │               │   ├── buffer.py
    │               │   └── serialization.py
    │               ├── battery_pool/
    │               │   ├── __init__.py
    │               │   ├── _battery_pool.py
    │               │   ├── _battery_pool_reference_store.py
    │               │   ├── _component_metric_fetcher.py
    │               │   ├── _component_metrics.py
    │               │   ├── _methods.py
    │               │   ├── _metric_calculator.py
    │               │   └── messages.py
    │               ├── ev_charger_pool/
    │               │   ├── __init__.py
    │               │   ├── _ev_charger_pool.py
    │               │   ├── _ev_charger_pool_reference_store.py
    │               │   ├── _result_types.py
    │               │   └── _system_bounds_tracker.py
    │               ├── formula_engine/
    │               │   ├── __init__.py
    │               │   ├── _exceptions.py
    │               │   ├── _formula_engine.py
    │               │   ├── _formula_engine_pool.py
    │               │   ├── _formula_evaluator.py
    │               │   ├── _formula_formatter.py
    │               │   ├── _formula_steps.py
    │               │   ├── _resampled_formula_builder.py
    │               │   ├── _tokenizer.py
    │               │   └── _formula_generators/
    │               │       ├── __init__.py
    │               │       ├── _battery_power_formula.py
    │               │       ├── _chp_power_formula.py
    │               │       ├── _consumer_power_formula.py
    │               │       ├── _ev_charger_current_formula.py
    │               │       ├── _ev_charger_power_formula.py
    │               │       ├── _fallback_formula_metric_fetcher.py
    │               │       ├── _formula_generator.py
    │               │       ├── _grid_current_formula.py
    │               │       ├── _grid_power_3_phase_formula.py
    │               │       ├── _grid_power_formula.py
    │               │       ├── _grid_power_formula_base.py
    │               │       ├── _grid_reactive_power_formula.py
    │               │       ├── _producer_power_formula.py
    │               │       ├── _pv_power_formula.py
    │               │       └── _simple_formula.py
    │               ├── logical_meter/
    │               │   ├── __init__.py
    │               │   └── _logical_meter.py
    │               └── pv_pool/
    │                   ├── __init__.py
    │                   ├── _pv_pool.py
    │                   ├── _pv_pool_reference_store.py
    │                   ├── _result_types.py
    │                   └── _system_bounds_tracker.py
    ├── tests/
    │   ├── __init__.py
    │   ├── conftest.py
    │   ├── test_import_all_integration.py
    │   ├── actor/
    │   │   ├── __init__.py
    │   │   ├── test_actor.py
    │   │   ├── test_background_service.py
    │   │   ├── test_channel_registry.py
    │   │   ├── test_resampling.py
    │   │   ├── test_run_utils.py
    │   │   └── _power_managing/
    │   │       ├── __init__.py
    │   │       ├── _sorted_set.py
    │   │       ├── test_matryoshka.py
    │   │       ├── test_report.py
    │   │       └── test_shifting_matryoshka.py
    │   ├── config/
    │   │   ├── test_actor.py
    │   │   ├── test_logging_actor.py
    │   │   ├── test_manager.py
    │   │   └── test_util.py
    │   ├── microgrid/
    │   │   ├── __init__.py
    │   │   ├── fixtures.py
    │   │   ├── test_data_sourcing.py
    │   │   ├── test_datapipeline.py
    │   │   ├── test_grid.py
    │   │   ├── test_microgrid_api.py
    │   │   └── power_distributing/
    │   │       ├── __init__.py
    │   │       └── _component_status/
    │   │           ├── __init__.py
    │   │           ├── test_battery_pool_status.py
    │   │           ├── test_battery_status.py
    │   │           ├── test_ev_charger_status.py
    │   │           └── test_pv_inverter_status.py
    │   ├── timeseries/
    │   │   ├── __init__.py
    │   │   ├── mock_microgrid.py
    │   │   ├── mock_resampler.py
    │   │   ├── test_base_types.py
    │   │   ├── test_consumer.py
    │   │   ├── test_formula_engine.py
    │   │   ├── test_formula_formatter.py
    │   │   ├── test_frequency_streaming.py
    │   │   ├── test_logical_meter.py
    │   │   ├── test_moving_window.py
    │   │   ├── test_periodic_feature_extractor.py
    │   │   ├── test_producer.py
    │   │   ├── test_ringbuffer.py
    │   │   ├── test_ringbuffer_serialization.py
    │   │   ├── test_voltage_streamer.py
    │   │   ├── _battery_pool/
    │   │   │   ├── __init__.py
    │   │   │   └── test_battery_pool_control_methods.py
    │   │   ├── _ev_charger_pool/
    │   │   │   ├── __init__.py
    │   │   │   ├── test_ev_charger_pool.py
    │   │   │   └── test_ev_charger_pool_control_methods.py
    │   │   ├── _formula_engine/
    │   │   │   ├── __init__.py
    │   │   │   ├── test_formula_composition.py
    │   │   │   └── utils.py
    │   │   ├── _pv_pool/
    │   │   │   ├── __init__.py
    │   │   │   └── test_pv_pool_control_methods.py
    │   │   └── _resampling/
    │   │       ├── __init__.py
    │   │       └── wall_clock_timer/
    │   │           ├── __init__.py
    │   │           ├── conftest.py
    │   │           ├── test_clocksinfo.py
    │   │           ├── test_config.py
    │   │           ├── test_timer_basic.py
    │   │           ├── test_timer_ticking.py
    │   │           └── util.py
    │   └── utils/
    │       ├── __init__.py
    │       ├── _a_sequence.py
    │       ├── component_data_streamer.py
    │       ├── component_data_wrapper.py
    │       ├── component_graph_utils.py
    │       ├── graph_generator.py
    │       ├── mock_microgrid_client.py
    │       └── receive_timeout.py
    └── .github/
        ├── dependabot.yml
        ├── keylabeler.yml
        ├── labeler.yml
        ├── RELEASE_NOTES.template.md
        ├── containers/
        │   ├── nox-cross-arch/
        │   │   ├── arm64-ubuntu-20.04-python.Dockerfile
        │   │   └── entrypoint.bash
        │   └── test-installation/
        │       └── Dockerfile
        ├── ISSUE_TEMPLATE/
        │   ├── bug.yml
        │   ├── config.yml
        │   └── feature.yml
        └── workflows/
            ├── ci-pr.yaml
            ├── ci.yaml
            ├── dco-merge-queue.yml
            ├── labeler.yml
            └── release-notes-check.yml

================================================
FILE: README.md
================================================
# Frequenz Python SDK

[![Build Status](https://github.com/frequenz-floss/frequenz-sdk-python/actions/workflows/ci-push.yaml/badge.svg)](https://github.com/frequenz-floss/frequenz-sdk-python/actions/workflows/ci-push.yaml)
[![PyPI Package](https://img.shields.io/pypi/v/frequenz-sdk)](https://pypi.org/project/frequenz-sdk/)
[![Docs](https://img.shields.io/badge/docs-latest-informational)](https://frequenz-floss.github.io/frequenz-sdk-python/)

## Introduction

A development kit to interact with the Frequenz development platform.

## Supported Platforms

The following platforms are officially supported (test):

- **Python:** 3.11 .. 3.13
- **Operating System:** Ubuntu Linux 20.04
- **Architectures:** amd64, arm64

## Quick Start

We assume you are on a system with Python available. If that is not the case,
please [download and install Python](https://www.python.org/downloads/) first.

To install the SDK, you probably want to create a new virtual environment first.
For example, if you use a `sh` compatible shell, you can do this:

```sh
python3 -m venv .venv
. .venv/bin/activate
```

Then, just install using `pip`:

```sh
python3 -m pip install frequenz-sdk
```

## Documentation

For more information, please visit the [documentation
website](https://frequenz-floss.github.io/frequenz-sdk-python/).

## Contributing

If you want to know how to build this project and contribute to it, please
check out the [Contributing Guide](CONTRIBUTING.md).



================================================
FILE: CODEOWNERS
================================================
# Each line is a file pattern followed by one or more owners.
# Owners will be requested for review when someone opens a pull request.

# Fallback owner.
# These are the default owners for everything in the repo, unless a later match
# takes precedence.
* @frequenz-floss/python-sdk-team



================================================
FILE: CONTRIBUTING.md
================================================
# Contributing to Frequenz Python SDK

## Build

You can use `build` to simply build the source and binary distribution:

```sh
python -m pip install build
python -m build
```

## Local development

You can use editable installs to develop the project locally (it will install
all the dependencies too):

```sh
python -m pip install -e .
```

Or you can install all development dependencies (`mypy`, `pylint`, `pytest`,
etc.) in one go too:
```sh
python -m pip install -e .[dev]
```

If you don't want to install all the dependencies, you can also use `nox` to
run the tests and other checks creating its own virtual environments:

```sh
python -m pip install .[dev-noxfile]
nox
```

You can also use `nox -R` to reuse the current testing environment to speed up
test at the expense of a higher chance to end up with a dirty test environment.

### Running tests / checks individually

For a better development test cycle you can install the runtime and test
dependencies and run `pytest` manually.

```sh
python -m pip install .[dev-pytest]  # included in .[dev] too

# And for example
pytest tests/test_*.py
```

Or you can use `nox`:

```sh
nox -R -s pytest -- test/test_*.py
```

The same appliest to `pylint` or `mypy` for example:

```sh
nox -R -s pylint -- test/test_*.py
nox -R -s mypy -- test/test_*.py
```

### Building the documentation

To build the documentation, first install the dependencies (if you didn't
install all `dev` dependencies):

```sh
python -m pip install -e .[dev-mkdocs]
```

Then you can build the documentation (it will be written in the `site/`
directory):

```sh
mkdocs build
```

Or you can just serve the documentation without building it using:

```sh
mkdocs serve
```

Your site will be updated **live** when you change your files (provided that
you used `pip install -e .`, beware of a common pitfall of using `pip install`
without `-e`, in that case the API reference won't change unless you do a new
`pip install`).

To build multi-version documentation, we use
[mike](https://github.com/jimporter/mike). If you want to see how the
multi-version sites looks like locally, you can use:

```sh
mike deploy my-version
mike set-default my-version
mike serve
```

`mike` works in mysterious ways. Some basic information:

* `mike deploy` will do a `mike build` and write the results to your **local**
  `gh-pages` branch. `my-version` is an arbitrary name for the local version
  you want to preview.
* `mike set-default` is needed so when you serve the documentation, it goes to
  your newly produced documentation by default.
* `mike serve` will serve the contents of your **local** `gh-pages` branch. Be
  aware that, unlike `mkdocs serve`, changes to the sources won't be shown
  live, as the `mike deploy` step is needed to refresh them.

Be careful not to use `--push` with `mike deploy`, otherwise it will push your
local `gh-pages` branch to the `origin` remote.

That said, if you want to test the actual website in **your fork**, you can
always use `mike deploy --push --remote your-fork-remote`, and then access the
GitHub pages produced for your fork.

## Releasing

These are the steps to create a new release:

1. Get the latest head you want to create a release from.

2. Update the `RELEASE_NOTES.md` file if it is not complete, up to date, and
   remove template comments (`<!-- ... ->`) and empty sections. Submit a pull
   request if an update is needed, wait until it is merged, and update the
   latest head you want to create a release from to get the new merged pull
   request.

3. Create a new signed tag using the release notes and
   a [semver](https://semver.org/) compatible version number with a `v` prefix,
   for example:

   ```sh
   git tag -s --cleanup=whitespace -F RELEASE_NOTES.md v0.0.1
   ```

4. Push the new tag.

5. A GitHub action will test the tag and if all goes well it will create
   a [GitHub
   Release](https://github.com/frequenz-floss/frequenz-sdk-python/releases),
   and upload a new package to
   [PyPI](https://pypi.org/project/frequenz-sdk/)
   automatically.

6. Once this is done, reset the `RELEASE_NOTES.md` with the template:

   ```sh
   cp .github/RELEASE_NOTES.template.md RELEASE_NOTES.md
   ```

   Commit the new release notes and create a PR (this step should be automated
   eventually too).

7. Celebrate!

##  Cross-Arch Testing

This project has built-in support for testing across multiple architectures.
Currently, our CI conducts tests on `arm64` machines using QEMU emulation. We
also have the flexibility to expand this support to include additional
architectures in the future.

This project contains Dockerfiles that can be used in the CI to test the
python package in non-native machine architectures, e.g., `arm64`. The
Dockerfiles exist in the directory `.github/containers/nox-cross-arch`, and
follow a naming scheme so that they can be easily used in build matrices in the
CI, in `nox-cross-arch` job. The naming scheme is:

```
<arch>-<os>-python-<python-version>.Dockerfile
```

E.g.,

```
arm64-ubuntu-20.04-python-3.11.Dockerfile
```

If a Dockerfile for your desired target architecture, OS, and python version
does not exist here, please add one before proceeding to add your options to
the test matrix.



================================================
FILE: LICENSE
================================================
MIT License

Copyright © 2022 Frequenz Energy-as-a-Service GmbH

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: MANIFEST.in
================================================
exclude .cookiecutter-replay.json
exclude .editorconfig
exclude .gitignore
exclude CODEOWNERS
exclude CONTRIBUTING.md
exclude mkdocs.yml
exclude noxfile.py
exclude src/conftest.py
recursive-exclude .github *
recursive-exclude benchmarks *
recursive-exclude docs *
recursive-exclude tests *
recursive-include py *.pyi



================================================
FILE: mkdocs.yml
================================================
# MkDocs configuration
# For details see: https://www.mkdocs.org/user-guide/configuration/

# Project information
site_name: "Frequenz Python SDK"
site_description: "A development kit to interact with the Frequenz development platform"
site_author: "Frequenz Energy-as-a-Service GmbH"
copyright: "Copyright © 2022 Frequenz Energy-as-a-Service GmbH"
repo_name: "frequenz-sdk-python"
repo_url: "https://github.com/frequenz-floss/frequenz-sdk-python"
edit_uri: "edit/v1.x.x/docs/"
strict: true  # Treat warnings as errors

# Build directories
theme:
  name: "material"
  logo: _img/logo.png
  favicon: _img/logo.png
  language: en
  icon:
    edit: material/file-edit-outline
    repo: fontawesome/brands/github
  custom_dir: docs/_overrides
  features:
    - content.code.annotate
    - content.code.copy
    - navigation.indexes
    - navigation.instant
    - navigation.footer
    - navigation.tabs
    - navigation.top
    - navigation.tracking
    - toc.follow
  palette:
  - media: "(prefers-color-scheme: light)"
    scheme: default
    primary: indigo
    accent: deep purple
    toggle:
      icon: material/weather-sunny
      name: Switch to dark mode
  - media: "(prefers-color-scheme: dark)"
    scheme: slate
    primary: black
    accent: teal
    toggle:
      icon: material/weather-night
      name: Switch to light mode

extra:
  social:
  - icon: fontawesome/brands/github
    link: https://github.com/frequenz-floss
  - icon: fontawesome/brands/linkedin
    link: https://www.linkedin.com/company/frequenz-com
  version:
    provider: mike
    default: latest

extra_css:
  - _css/style.css
  - _css/mkdocstrings.css

# Formatting options
markdown_extensions:
  - admonition
  - attr_list
  - def_list
  - footnotes
  - pymdownx.details
  - pymdownx.highlight:
      anchor_linenums: true
      line_spans: __span
      pygments_lang_class: true
  - pymdownx.keys
  - pymdownx.snippets:
      check_paths: true
  - pymdownx.superfences:
      custom_fences:
        - name: mermaid
          class: mermaid
          format: !!python/name:pymdownx.superfences.fence_code_format
  - pymdownx.tabbed
  - pymdownx.tasklist:
      custom_checkbox: true
  - toc:
      permalink: "¤"

plugins:
  - autorefs:
      resolve_closest: true
  - gen-files:
      scripts:
        - docs/_scripts/mkdocstrings_autoapi.py
  - literate-nav:
      nav_file: SUMMARY.md
  - mike:
      alias_type: symlink
      canonical_version: latest
  - mkdocstrings:
      default_handler: python
      handlers:
        python:
          paths: ["src"]
          options:
            docstring_section_style: spacy
            inherited_members: true
            merge_init_into_class: false
            separate_signature: true
            show_category_heading: true
            show_root_heading: true
            show_root_members_full_path: true
            show_signature_annotations: true
            show_source: true
            show_symbol_type_toc: true
            signature_crossrefs: true
          import:
            # See https://mkdocstrings.github.io/python/usage/#import for details
            - https://docs.python.org/3/objects.inv
            - https://frequenz-floss.github.io/frequenz-channels-python/v1/objects.inv
            - https://frequenz-floss.github.io/frequenz-client-common-python/v0.3/objects.inv
            - https://frequenz-floss.github.io/frequenz-client-microgrid-python/v0.9/objects.inv
            - https://frequenz-floss.github.io/frequenz-core-python/v1/objects.inv
            - https://frequenz-floss.github.io/frequenz-quantities-python/v1/objects.inv
            - https://lovasoa.github.io/marshmallow_dataclass/html/objects.inv
            - https://marshmallow.readthedocs.io/en/stable/objects.inv
            - https://networkx.org/documentation/stable/objects.inv
            - https://numpy.org/doc/stable/objects.inv
            - https://typing-extensions.readthedocs.io/en/stable/objects.inv
  # Note this plugin must be loaded after mkdocstrings to be able to use macros
  # inside docstrings. See the comment in `docs/_scripts/macros.py` for more
  # details
  - macros:
      module_name: docs/_scripts/macros
      on_undefined: strict
      on_error_fail: true
  - search

# Preview controls
watch:
  - "src"
  - CONTRIBUTING.md



================================================
FILE: noxfile.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Configuration file for nox."""

from frequenz.repo.config import RepositoryType, nox

nox.configure(RepositoryType.LIB)



================================================
FILE: pyproject.toml
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

[build-system]
requires = [
  "setuptools == 80.9.0",
  "setuptools_scm[toml] == 9.2.0",
  "frequenz-repo-config[lib] == 0.13.5",
]
build-backend = "setuptools.build_meta"

[project]
name = "frequenz-sdk"
description = "A development kit to interact with the Frequenz development platform"
readme = "README.md"
license = { text = "MIT" }
keywords = ["frequenz", "python", "lib", "library", "sdk", "microgrid"]
classifiers = [
  "Development Status :: 3 - Alpha",
  "Intended Audience :: Developers",
  "License :: OSI Approved :: MIT License",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3 :: Only",
  "Topic :: Software Development :: Libraries",
  "Typing :: Typed",
]
requires-python = ">= 3.11, < 4"
dependencies = [
  # Make sure to update the mkdocs.yml file when
  # changing the version
  # (plugins.mkdocstrings.handlers.python.import)
  "frequenz-client-microgrid >= 0.9.0, < 0.10.0",
  "frequenz-client-common >= 0.3.2, < 0.4.0",
  "frequenz-channels >= 1.6.1, < 2.0.0",
  "frequenz-quantities[marshmallow] >= 1.0.0, < 2.0.0",
  "networkx >= 2.8, < 4",
  "numpy >= 2.1.0, < 3",
  "typing_extensions >= 4.13.0, < 5",
  "marshmallow >= 3.19.0, < 5",
  "marshmallow_dataclass >= 8.7.1, < 9",
]
dynamic = ["version"]

[[project.authors]]
name = "Frequenz Energy-as-a-Service GmbH"
email = "floss@frequenz.com"

[project.optional-dependencies]
dev-flake8 = [
  "flake8 == 7.3.0",
  "flake8-docstrings == 1.7.0",
  "flake8-pyproject == 1.2.3", # For reading the flake8 config from pyproject.toml
  "pydoclint == 0.7.3",
  "pydocstyle == 6.3.0",
]
dev-formatting = ["black == 25.1.0", "isort == 6.0.1"]
dev-mkdocs = [
  "black == 25.1.0",
  "Markdown==3.8.2",
  "mike == 2.1.3",
  "mkdocs-gen-files == 0.5.0",
  "mkdocs-literate-nav == 0.6.2",
  "mkdocs-macros-plugin == 1.3.9",
  "mkdocs-material == 9.6.18",
  "mkdocstrings[python] == 0.30.0",
  "mkdocstrings-python == 1.18.2",
  "frequenz-repo-config[lib] == 0.13.5",
]
dev-mypy = [
  "mypy == 1.17.1",
  "types-Markdown == 3.8.0.20250809",
  "types-protobuf == 6.30.2.20250822",
  "types-setuptools == 80.9.0.20250822",
  # For checking the noxfile, docs/ script, and tests
  "frequenz-sdk[dev-mkdocs,dev-noxfile,dev-pytest]",
]
dev-noxfile = ["nox == 2025.5.1", "frequenz-repo-config[lib] == 0.13.5"]
dev-pylint = [
  "pylint == 3.3.8",
  # For checking the noxfile, docs/ script, and tests
  "frequenz-sdk[dev-mkdocs,dev-noxfile,dev-pytest]",
]
dev-pytest = [
  "pytest == 8.4.1",
  "frequenz-repo-config[extra-lint-examples] == 0.13.5",
  "pytest-mock == 3.14.1",
  "pytest-asyncio == 1.1.0",
  "time-machine == 2.16.0",
  "async-solipsism == 0.8",
  "hypothesis == 6.138.14",
]
dev = [
  "frequenz-sdk[dev-mkdocs,dev-flake8,dev-formatting,dev-mkdocs,dev-mypy,dev-noxfile,dev-pylint,dev-pytest]",
]

[project.urls]
Documentation = "https://frequenz-floss.github.io/frequenz-sdk-python/"
Changelog = "https://github.com/frequenz-floss/frequenz-sdk-python/releases"
Issues = "https://github.com/frequenz-floss/frequenz-sdk-python/issues"
Repository = "https://github.com/frequenz-floss/frequenz-sdk-python"
Support = "https://github.com/frequenz-floss/frequenz-sdk-python/discussions/categories/support"

[tool.black]
line-length = 88
target-version = ['py311']
include = '\.pyi?$'

[tool.isort]
profile = "black"
line_length = 88
src_paths = ["benchmarks", "examples", "src", "tests"]

[tool.flake8]
# We give some flexibility to go over 88, there are cases like long URLs or
# code in documenation that have extra indentation. Black will still take care
# of making everything that can be 88 wide, 88 wide.
max-line-length = 100
extend-ignore = [
  "E203", # Whitespace before ':' (conflicts with black)
  "W503", # Line break before binary operator (conflicts with black)
]
# pydoclint options
style = "google"
check-return-types = false
check-yield-types = false
arg-type-hints-in-docstring = false
arg-type-hints-in-signature = true
allow-init-docstring = true
check-class-attributes = false

[tool.pylint.similarities]
ignore-comments = ['yes']
ignore-docstrings = ['yes']
ignore-imports = ['no']
min-similarity-lines = 40

[tool.pylint.messages_control]
disable = [
  "too-few-public-methods",
  "too-many-return-statements",
  # disabled because it conflicts with isort
  "wrong-import-order",
  "ungrouped-imports",
  # pylint's unsubscriptable check is buggy and is not needed because
  # it is a type-check, for which we already have mypy.
  "unsubscriptable-object",
  # Checked by mypy
  "no-member",
  "possibly-used-before-assignment",
  "no-name-in-module",
  # Checked by flake8
  "f-string-without-interpolation",
  "redefined-outer-name",
  "unused-import",
  "line-too-long",
  "missing-function-docstring",
  "redefined-outer-name",
  "unnecessary-lambda-assignment",
  "unused-import",
  "unused-variable",
  "wrong-import-position",
]

[tool.pylint.design]
max-attributes = 12

[tool.pytest.ini_options]
addopts = "-vv"
testpaths = ["tests", "src"]
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
required_plugins = ["pytest-asyncio", "pytest-mock"]
markers = [
  "integration: integration tests (deselect with '-m \"not integration\"')",
]
filterwarnings = [
  "error",
  "once::DeprecationWarning",
  "once::PendingDeprecationWarning",
  # We need to ignore PytestUnraisableExceptionWarning because we have some
  # cleanup issues, and sometimes exceptions are raised in __del__ methods, for
  # example when trying to do async stuff and the event loop is already closed.
  # This could even be caused by the GC, some time after the test function that
  # has the issue was completed.
  # Please see this issue for more details:
  # https://github.com/frequenz-floss/frequenz-sdk-python/issues/982
  "default::pytest.PytestUnraisableExceptionWarning",
  # We use a raw string (single quote) to avoid the need to escape special
  # chars as this is a regex
  'ignore:Protobuf gencode version .*exactly one major version older.*:UserWarning',
]

[tool.mypy]
explicit_package_bases = true
namespace_packages = true
# This option disables mypy cache, and it is sometimes useful to enable it if
# you are getting weird intermittent error, or error in the CI but not locally
# (or vice versa). In particular errors saying that type: ignore is not
# used but getting the original ignored error when removing the type: ignore.
# See for example: https://github.com/python/mypy/issues/2960
no_incremental = true
mypy_path = "src"
files = ["src", "tests", "examples", "benchmarks", "docs", "noxfile.py"]
strict = true

[[tool.mypy.overrides]]
module = [
  "async_solipsism",
  "mkdocs_macros.*",
  # The available stubs packages are outdated or incomplete (WIP/experimental):
  # https://github.com/frequenz-floss/frequenz-sdk-python/issues/430
  "networkx",
]
ignore_missing_imports = true

[tool.setuptools_scm]
version_scheme = "post-release"



================================================
FILE: RELEASE_NOTES.md
================================================
# Frequenz Python SDK Release Notes

## Summary

This release provides an experimental, opt-in, time-jumps resilient resampler, that can be enabled by using the new `ResamplerConfig2` class.

## Upgrading

* The resampling function now takes plain `float`s as values instead of `Quantity` objects.
* `frequenz.sdk.timeseries.UNIX_EPOCH` was removed, use [`frequenz.core.datetime.UNIX_EPOCH`](https://frequenz-floss.github.io/frequenz-core-python/latest/reference/frequenz/core/datetime/#frequenz.core.datetime.UNIX_EPOCH) instead.

## New Features

- A new configuration mode was added to the resampler (and thus the resampling actor and microgrid high-level interface). When passing a new `ResamplerConfig2` instance to the resampler, it will use a wall clock timer instead of a monotonic clock timer. This timer adjustes sleeps to account for drifts in the monotonic clock, and thus allows for more accurate resampling in cases where the monotonic clock drifts away from the wall clock. The monotonic clock timer option will be deprecated in the future, as it is not really suitable for resampling. The new `ResamplerConfig2` class accepts a `WallClockTimerConfig` to fine-tune the wall clock timer behavior, if necessary.

   Example usage:

   ```python
   from frequenz.sdk import microgrid
   from frequenz.sdk.timeseries import ResamplerConfig2

    await microgrid.initialize(
        MICROGRID_API_URL,
        # Just replace the old `ResamplerConfig` with the new `ResamplerConfig2`
        resampler_config=ResamplerConfig2(resampling_period=timedelta(seconds=1.0)),
    )
    ```

## Bug Fixes

- When using the new wall clock timer in the resampmler, it will now resync to the system time if it drifts away for more than a resample period, and do dynamic adjustments to the timer if the monotonic clock has a small drift compared to the wall clock.

- A power distributor logging issue is fixed, that was causing the power for multiple batteries connected to the same inverter to be reported incorrectly.



================================================
FILE: .cookiecutter-replay.json
================================================
{
  "cookiecutter": {
    "Introduction": "",
    "type": "lib",
    "name": "sdk",
    "description": "A development kit to interact with the Frequenz development platform",
    "title": "Frequenz Python SDK",
    "keywords": "microgrid,actor,ml,machine-learning,ai",
    "github_org": "frequenz-floss",
    "license": "MIT",
    "author_name": "Frequenz Energy-as-a-Service GmbH",
    "author_email": "floss@frequenz.com",
    "python_package": "frequenz.sdk",
    "pypi_package_name": "frequenz-sdk",
    "github_repo_name": "frequenz-sdk-python",
    "default_codeowners": "@frequenz-floss/python-sdk-team"
  }
}



================================================
FILE: .editorconfig
================================================
# EditorConfig is awesome: https://EditorConfig.org

# top-most EditorConfig file
root = true

# Unix-style newlines with a newline ending every file
[*]
end_of_line = lf
insert_final_newline = true

# Set default charset, indent style and trimming of whitespace
[{.editorconfig,CODEOWNERS,LICENSE,*.{in,json,md,proto,py,pyi,toml,yaml,yml}}]
charset = utf-8
indent_style = space
trim_trailing_whitespace = true

# 4 space indentation
[*.{py,pyi}]
indent_size = 4

# 2 space indentation
[{.editorconfig,CODEOWNERS,LICENSE,*.{in,json,proto,toml,yaml,yml}}]
indent_size = 2

# No indentation size specified for *.md because different blocks have
# different indentation rules



================================================
FILE: benchmarks/power_distribution/power_distributor.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Check how long it takes to distribute power."""

import asyncio
import csv
import random
import timeit
from collections.abc import Coroutine
from datetime import timedelta
from typing import Any

from frequenz.channels import Broadcast
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import Component, ComponentCategory
from frequenz.quantities import Power

from frequenz.sdk import microgrid
from frequenz.sdk.microgrid import connection_manager
from frequenz.sdk.microgrid._power_distributing import (
    ComponentPoolStatus,
    Error,
    OutOfBounds,
    PartialFailure,
    PowerDistributingActor,
    Request,
    Result,
    Success,
)
from frequenz.sdk.timeseries import ResamplerConfig2

HOST = "microgrid.sandbox.api.frequenz.io"
PORT = 62060


# TODO: this send_requests function uses the battery pool to # pylint: disable=fixme
# send requests, and those no longer go directly to the power distributing actor, but
# instead through the power managing actor.  So the below function needs to be updated
# to use the PowerDistributingActor directly.
async def send_requests(batteries: set[ComponentId], request_num: int) -> list[Result]:
    """Send requests to the PowerDistributingActor and wait for the response.

    Args:
        batteries: set of batteries where the power should be set
        request_num: number of requests that should be send

    Raises:
        SystemError: If the channel was closed.

    Returns:
        List of the results from the PowerDistributingActor.
    """
    battery_pool = microgrid.new_battery_pool(priority=5, component_ids=batteries)
    results_rx = battery_pool.power_status.new_receiver()
    result: list[Any] = []
    for _ in range(request_num):
        await battery_pool.propose_power(
            Power.from_watts(float(random.randrange(100000, 1000000)))
        )
        try:
            output = await asyncio.wait_for(results_rx.receive(), timeout=3)
            if output is None:
                raise SystemError(f"Power response channel for {battery_pool} closed!")
            result.append(output)
        except asyncio.exceptions.TimeoutError:
            print("TIMEOUT ERROR")

    return result


def parse_result(result: list[list[Result]]) -> dict[str, float]:
    """Parse result.

    Args:
        result: Results from finished tasks.

    Returns:
        Number of each result.
    """
    result_counts = {
        Error: 0,
        Success: 0,
        PartialFailure: 0,
        OutOfBounds: 0,
    }

    for result_list in result:
        for item in result_list:
            result_counts[type(item)] += 1

    return {
        "success_num": result_counts[Success],
        "failed_num": result_counts[PartialFailure],
        "error_num": result_counts[Error],
        "out_of_bounds": result_counts[OutOfBounds],
    }


async def run_test(  # pylint: disable=too-many-locals
    num_requests: int,
    batteries: set[ComponentId],
) -> dict[str, Any]:
    """Run test.

    Args:
        num_requests: Number of requests to send.
        batteries: Set of batteries for each request.

    Returns:
        Dictionary with statistics.
    """
    start = timeit.default_timer()

    power_request_channel = Broadcast[Request](name="power-request")
    battery_status_channel = Broadcast[ComponentPoolStatus](name="battery-status")
    power_result_channel = Broadcast[Result](name="power-result")
    async with PowerDistributingActor(
        component_category=ComponentCategory.BATTERY,
        component_type=None,
        requests_receiver=power_request_channel.new_receiver(),
        results_sender=power_result_channel.new_sender(),
        component_pool_status_sender=battery_status_channel.new_sender(),
        api_power_request_timeout=timedelta(seconds=5.0),
    ):
        tasks: list[Coroutine[Any, Any, list[Result]]] = []
        tasks.append(send_requests(batteries, num_requests))

        result = await asyncio.gather(*tasks)
        exec_time = timeit.default_timer() - start

    summary = parse_result(result)
    summary["num_requests"] = num_requests
    summary["batteries_num"] = len(batteries)
    summary["exec_time"] = exec_time
    return summary


async def run() -> None:
    """Create microgrid api and run tests."""
    await microgrid.initialize(
        "grpc://microgrid.sandbox.api.frequenz.io:62060",
        ResamplerConfig2(resampling_period=timedelta(seconds=1.0)),
    )

    all_batteries: set[Component] = connection_manager.get().component_graph.components(
        component_categories={ComponentCategory.BATTERY}
    )
    batteries_ids = {c.component_id for c in all_batteries}
    # Take some time to get data from components
    await asyncio.sleep(4)
    with open("/dev/stdout", "w", encoding="utf-8") as csvfile:
        fields = await run_test(0, batteries_ids)
        out = csv.DictWriter(csvfile, fields.keys())
        out.writeheader()
        out.writerow(await run_test(1, batteries_ids))
        out.writerow(await run_test(10, batteries_ids))


async def main() -> None:
    """Run the run() function."""
    await run()


asyncio.run(main())



================================================
FILE: benchmarks/timeseries/benchmark_datasourcing.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Benchmark the data sourcing actor.

To be able to access the `tests` package we need to adjust the PYTHONPATH.

Usage:

    PYTHONPATH=. python benchmark_datasourcing.py <num ev chargers> <num messages per battery>
"""
import argparse
import asyncio
import sys
import tracemalloc
from time import perf_counter
from typing import Any

from frequenz.channels import Broadcast, Receiver, ReceiverStoppedError
from frequenz.client.microgrid import ComponentMetricId

from frequenz.sdk import microgrid
from frequenz.sdk._internal._channels import ChannelRegistry
from frequenz.sdk.microgrid._data_sourcing import (
    ComponentMetricRequest,
    DataSourcingActor,
)

try:
    from tests.timeseries.mock_microgrid import MockMicrogrid
    from tests.utils import MockMicrogridClient
except ImportError:
    print(
        "Error: Unable to import the `tests` package. "
        "Please make sure that the PYTHONPATH env variable is set to the root of the repository."
    )
    sys.exit(1)

COMPONENT_METRIC_IDS = [
    ComponentMetricId.CURRENT_PHASE_1,
    ComponentMetricId.CURRENT_PHASE_2,
    ComponentMetricId.CURRENT_PHASE_3,
]


def enable_mock_client(client: MockMicrogridClient) -> None:
    """Enable the mock microgrid client.

    Args:
        client: the mock microgrid client to enable.
    """
    microgrid.connection_manager._CONNECTION_MANAGER = (  # pylint: disable=protected-access
        client.mock_microgrid
    )


async def benchmark_data_sourcing(  # pylint: disable=too-many-locals
    num_ev_chargers: int, num_msgs_per_battery: int
) -> None:
    """Benchmark the data sourcing actor.

    Benchmark the data sourcing actor by sending out a number of requests and
    printing out the number of samples sent and the time taken.

    Args:
        num_ev_chargers: number of EV Chargers to create for the mock microgrid.
        num_msgs_per_battery: number of messages to send out for each battery.
    """
    num_expected_messages = (
        num_ev_chargers * len(COMPONENT_METRIC_IDS) * num_msgs_per_battery
    )
    mock_grid = MockMicrogrid(
        grid_meter=False, num_values=num_msgs_per_battery, sample_rate_s=0.0
    )

    mock_grid.add_ev_chargers(num_ev_chargers)
    mock_grid.start_mock_client(enable_mock_client)

    request_channel = Broadcast[ComponentMetricRequest](
        name="DataSourcingActor Request Channel"
    )

    channel_registry = ChannelRegistry(name="Microgrid Channel Registry")
    request_receiver = request_channel.new_receiver(
        name="datasourcing-benchmark",
        limit=(num_ev_chargers * len(COMPONENT_METRIC_IDS)),
    )
    request_sender = request_channel.new_sender()

    consume_tasks = []

    start_time = perf_counter()
    samples_sent = 0

    async def consume(channel: Receiver[Any]) -> None:
        while True:
            try:
                await channel.ready()
                channel.consume()
            except ReceiverStoppedError:
                return

            nonlocal samples_sent
            samples_sent += 1

    for evc_id in mock_grid.evc_ids:
        for component_metric_id in COMPONENT_METRIC_IDS:
            request = ComponentMetricRequest(
                "current_phase_requests", evc_id, component_metric_id, None
            )

            recv_channel = channel_registry.get_or_create(
                ComponentMetricRequest, request.get_channel_name()
            ).new_receiver()

            await request_sender.send(request)
            consume_tasks.append(asyncio.create_task(consume(recv_channel)))

    async with DataSourcingActor(request_receiver, channel_registry):
        await asyncio.gather(*consume_tasks)

        time_taken = perf_counter() - start_time

        await mock_grid.cleanup()

        print(f"Samples Sent: {samples_sent}, time taken: {time_taken}")
        print(f"Samples per second: {samples_sent / time_taken}")
        print(
            "Expected samples: "
            f"{num_expected_messages}, missing: {num_expected_messages - samples_sent}"
        )
        print(
            f"Missing per EVC: {(num_expected_messages - samples_sent) / num_ev_chargers}"
        )


def parse_args() -> tuple[int, int, bool]:
    """Parse the command line arguments.

    Returns:
        A tuple of (num ev chargers, num messages per battery, record allocations).
    """
    parser = argparse.ArgumentParser(description="Benchmark the data sourcing actor.")
    parser.add_argument(
        "num_ev_chargers",
        type=int,
        help="Number of EV Chargers to create for the mock microgrid.",
    )
    parser.add_argument(
        "num_msgs_per_battery",
        type=int,
        help="Number of messages to send out for each battery.",
    )
    parser.add_argument(
        "--record-allocations",
        action="store_true",
        help="Record memory allocations.",
    )

    args = parser.parse_args()

    return args.num_ev_chargers, args.num_msgs_per_battery, args.record_allocations


def main() -> None:
    """Start everything."""
    (
        num_ev_chargers_pararm,
        num_msgs_per_battery_param,
        record_allocations,
    ) = parse_args()

    if record_allocations:
        tracemalloc.start()

    asyncio.run(
        benchmark_data_sourcing(num_ev_chargers_pararm, num_msgs_per_battery_param)
    )

    if not record_allocations:
        sys.exit(0)

    snapshot = tracemalloc.take_snapshot()
    top_stats = snapshot.statistics("lineno")

    print("\n[ Top 10 ]")
    for stat in top_stats[:10]:
        print(stat)


if __name__ == "__main__":
    main()



================================================
FILE: benchmarks/timeseries/benchmark_ringbuffer.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Performance test for the `Ringbuffer` class."""

import random
import timeit
from datetime import datetime, timedelta, timezone
from typing import Any, TypeVar

import numpy as np

from frequenz.sdk.timeseries import Sample
from frequenz.sdk.timeseries._ringbuffer import OrderedRingBuffer

MINUTES_IN_A_DAY = 24 * 60
MINUTES_IN_29_DAYS = 29 * MINUTES_IN_A_DAY


T = TypeVar("T")


def fill_buffer(days: int, buffer: OrderedRingBuffer[Any]) -> None:
    """Fill the given buffer up to the given amount of days, one sample per minute."""
    random.seed(0)
    basetime = datetime(2022, 1, 1, tzinfo=timezone.utc)
    print("..filling", end="", flush=True)

    for day in range(days):
        # Push in random order
        for i in random.sample(range(MINUTES_IN_A_DAY), MINUTES_IN_A_DAY):
            buffer.update(
                Sample(basetime + timedelta(days=day, minutes=i, seconds=i % 3))
            )


def test_days(days: int, buffer: OrderedRingBuffer[Any]) -> None:
    """Gets the data for each of the 29 days."""
    basetime = datetime(2022, 1, 1, tzinfo=timezone.utc)

    for day in range(days):
        _ = buffer.window(
            basetime + timedelta(days=day), basetime + timedelta(days=day + 1)
        )


def test_slices(days: int, buffer: OrderedRingBuffer[Any], median: bool) -> None:
    """Benchmark slicing.

    Takes a buffer, fills it up and then excessively gets
    the data for each day to calculate the average/median.
    """
    basetime = datetime(2022, 1, 1, tzinfo=timezone.utc)

    total = 0.0

    for _ in range(3):
        for day in range(days):
            minutes = buffer.window(
                basetime + timedelta(days=day), basetime + timedelta(days=day + 1)
            )

            if median:
                total += float(np.median(minutes))
            else:
                total += float(np.average(minutes))


def test_29_days_list(num_runs: int) -> dict[str, float]:
    """Run the 29 day test on the list backend."""
    days = 29
    buffer = OrderedRingBuffer([0.0] * MINUTES_IN_29_DAYS, timedelta(minutes=1))

    fill_time = timeit.Timer(lambda: fill_buffer(days, buffer)).timeit(number=1)
    test_time = timeit.Timer(lambda: test_days(days, buffer)).timeit(number=num_runs)
    return {"fill": fill_time, "test": test_time}


def test_29_days_array(num_runs: int) -> dict[str, float]:
    """Run the 29 day test on the array backend."""
    days = 29
    buffer = OrderedRingBuffer(
        np.empty(
            shape=MINUTES_IN_29_DAYS,
        ),
        timedelta(minutes=1),
    )

    fill_time = timeit.Timer(lambda: fill_buffer(days, buffer)).timeit(number=1)
    test_time = timeit.Timer(lambda: test_days(days, buffer)).timeit(number=num_runs)
    return {"fill": fill_time, "test": test_time}


def test_29_days_slicing_list(num_runs: int) -> dict[str, float]:
    """Run slicing tests on list backend."""
    days = 29
    buffer = OrderedRingBuffer([0.0] * MINUTES_IN_29_DAYS, timedelta(minutes=1))

    fill_time = timeit.Timer(lambda: fill_buffer(days, buffer)).timeit(number=1)
    median_test_time = timeit.Timer(
        lambda: test_slices(days, buffer, median=True)
    ).timeit(number=num_runs)
    avg_test_time = timeit.Timer(
        lambda: test_slices(days, buffer, median=False)
    ).timeit(number=num_runs)

    return {"fill": fill_time, "median": median_test_time, "avg": avg_test_time}


def test_29_days_slicing_array(num_runs: int) -> dict[str, float]:
    """Run slicing tests on array backend."""
    days = 29
    buffer = OrderedRingBuffer(
        np.empty(
            shape=MINUTES_IN_29_DAYS,
        ),
        timedelta(minutes=1),
    )

    fill_time = timeit.Timer(lambda: fill_buffer(days, buffer)).timeit(number=1)
    median_test_time = timeit.Timer(
        lambda: test_slices(days, buffer, median=True)
    ).timeit(number=num_runs)
    avg_test_time = timeit.Timer(
        lambda: test_slices(days, buffer, median=False)
    ).timeit(number=num_runs)

    return {"fill": fill_time, "median": median_test_time, "avg": avg_test_time}


def main() -> None:
    """Run benchmark.

    Result of previous run:

    Date: Mi 1. Feb 17:19:51 CET 2023
    Result:

           =====================
    Array: ..filling
    List:  ..filling
    Time to fill 29 days with data:
            Array: 7.214875044999644 seconds
            List:  7.174421657982748 seconds
            Diff:  0.04045338701689616
    Day-Slices into 29 days with data:
            Array: 0.0001304591482039541 seconds
            List:  0.00019963659869972616 seconds
            Diff:  -6.917745049577205e-05
           =====================
    Array: ..filling
    List:  ..filling
    Avg of windows of 29 days and running average & mean on every day:
            Array: 0.0007829780981410295 seconds
            List:  0.0042931242496706545 seconds
            Diff:  -0.0035101461515296252
    Median of windows of 29 days and running average & mean on every day:
            Array: 0.0021195551002165304 seconds
            List:  0.00501448459981475 seconds
            Diff:  -0.00289492949959822
    """
    num_runs = 20

    print(f"       {''.join(['='] * (num_runs + 1))}")
    print("Array: ", end="")
    array_times = test_29_days_array(num_runs)

    print("\nList:  ", end="")

    list_times = test_29_days_list(num_runs)
    print("")

    print(
        "Time to fill 29 days with data:\n\t"
        + f"Array: {array_times['fill']} seconds\n\t"
        + f"List:  {list_times['fill']} seconds\n\t"
        + f"Diff:  {array_times['fill'] - list_times['fill']}"
    )

    print(
        "Day-Slices into 29 days with data:\n\t"
        + f"Array: {array_times['test']/num_runs} seconds\n\t"
        + f"List:  {list_times['test']/num_runs} seconds\n\t"
        + f"Diff:  {array_times['test']/num_runs - list_times['test']/num_runs}"
    )

    print(f"       {''.join(['='] * (num_runs + 1))}")
    print("Array: ", end="")
    slicing_array_times = test_29_days_slicing_array(num_runs)
    print("\nList:  ", end="")
    slicing_list_times = test_29_days_slicing_list(num_runs)
    print("")

    print(
        "Avg of windows of 29 days and running average & mean on every day:\n\t"
        + f"Array: {slicing_array_times['avg']/num_runs} seconds\n\t"
        + f"List:  {slicing_list_times['avg']/num_runs} seconds\n\t"
        + f"Diff:  {slicing_array_times['avg']/num_runs - slicing_list_times['avg']/num_runs}"
    )

    print(
        "Median of windows of 29 days and running average & mean on every day:\n\t"
        + f"Array: {slicing_array_times['median']/num_runs} seconds\n\t"
        + f"List:  {slicing_list_times['median']/num_runs} seconds\n\t"
        + "Diff:  "
        + f"{slicing_array_times['median']/num_runs - slicing_list_times['median']/num_runs}"
    )


if __name__ == "__main__":
    main()



================================================
FILE: benchmarks/timeseries/periodic_feature_extractor.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""
Benchmarks for the PeriodicFeatureExtractor class.

This module contains benchmarks that are comparing
the performance of a numpy implementation with a python
implementation.
"""


import asyncio
import collections.abc
import contextlib
import logging
from datetime import datetime, timedelta, timezone
from functools import partial
from timeit import timeit

import numpy as np
from frequenz.channels import Broadcast
from frequenz.quantities import Quantity
from numpy.random import default_rng
from numpy.typing import NDArray

from frequenz.sdk.timeseries import MovingWindow, PeriodicFeatureExtractor, Sample


@contextlib.asynccontextmanager
async def init_feature_extractor(
    period: int,
) -> collections.abc.AsyncIterator[PeriodicFeatureExtractor]:
    """Initialize the PeriodicFeatureExtractor class."""
    # We only need the moving window to initialize the PeriodicFeatureExtractor class.
    lm_chan = Broadcast[Sample[Quantity]](name="lm_net_power")
    async with MovingWindow(
        size=timedelta(seconds=1),
        resampled_data_recv=lm_chan.new_receiver(),
        input_sampling_period=timedelta(seconds=1),
    ) as moving_window:
        await lm_chan.new_sender().send(
            Sample(datetime.now(tz=timezone.utc), Quantity(0))
        )

        # Initialize the PeriodicFeatureExtractor class with a period of period seconds.
        # This works since the sampling period is set to 1 second.
        yield PeriodicFeatureExtractor(moving_window, timedelta(seconds=period))


def _calculate_avg_window(
    feature_extractor: PeriodicFeatureExtractor,
    window: NDArray[np.float64],
    window_size: int,
) -> NDArray[np.float64]:
    """
    Reshapes the window and calculates the average.

    This method calculates the average of a window by averaging over all
    windows fully inside the passed numpy array having the period
    `self.period`.

    Args:
        feature_extractor: The instance of the PeriodicFeatureExtractor to use.
        window: The window to calculate the average over.
        window_size: The size of the window to calculate the average over.

    Returns:
        The averaged window.
    """
    reshaped = feature_extractor._reshape_np_array(  # pylint: disable=protected-access
        window, window_size
    )
    # ignoring the type because np.average returns Any
    return np.average(reshaped[:, :window_size], axis=0)  # type: ignore[no-any-return]


def _calculate_avg_window_py(
    feature_extractor: PeriodicFeatureExtractor,
    window: NDArray[np.float64],
    window_size: int,
    weights: list[float] | None = None,
) -> NDArray[np.float64]:
    """
    Plain python version of the average calculator.

    This method avoids copying in any case but is 15 to 600 slower then the
    numpy version.

    This method is only used in these benchmarks.

    Args:
        feature_extractor: The instance of the PeriodicFeatureExtractor to use.
        window: The window to calculate the average over.
        window_size: The size of the window to calculate the average over.
        weights: The weights to use for the average calculation.

    Returns:
        The averaged window.
    """

    def _num_windows(
        window: NDArray[np.float64] | MovingWindow, window_size: int, period: int
    ) -> int:
        """
        Get the number of windows that are fully contained in the MovingWindow.

        This method calculates how often a given window, defined by it size, is
        fully contained in the MovingWindow at its current state or any numpy
        ndarray given the period between two window neighbors.

        Args:
            window: The buffer that is used for the average calculation.
            window_size: The size of the window in samples.
            period: The distance between two succeeding intervals in samples.

        Returns:
            The number of windows that are fully contained in the MovingWindow.
        """

        def length(window: NDArray[np.float64] | MovingWindow) -> int:
            return (
                window.count_valid()
                if isinstance(window, MovingWindow)
                else len(window)
            )

        num_windows = length(window) // period
        if length(window) - num_windows * period >= window_size:
            num_windows += 1

        return num_windows

    period = feature_extractor._period  # pylint: disable=protected-access

    num_windows = _num_windows(
        window,
        window_size,
        period,
    )

    res = np.empty(window_size)

    for i in range(window_size):
        assert num_windows * period - len(window) <= period
        summe = 0
        for j in range(num_windows):
            if weights is None:
                summe += window[i + (j * period)]
            else:
                summe += weights[j] * window[i + (j * period)]

        if not weights:
            res[i] = summe / num_windows
        else:
            res[i] = summe / np.sum(weights)

    return res


def run_benchmark(
    array: NDArray[np.float64],
    window_size: int,
    feature_extractor: PeriodicFeatureExtractor,
) -> None:
    """Run the benchmark for the given ndarray and window size."""

    def run_avg_np(
        array: NDArray[np.float64],
        window_size: int,
        feature_extractor: PeriodicFeatureExtractor,
    ) -> None:
        """
        Run the FeatureExtractor.

        The return value is discarded such that it can be used by timit.

        Args:
            array: The array containing all data.
            window_size: The size of the window.
            feature_extractor: An instance of the PeriodicFeatureExtractor.
        """
        _calculate_avg_window(feature_extractor, array, window_size)

    def run_avg_py(
        array: NDArray[np.float64],
        window_size: int,
        feature_extractor: PeriodicFeatureExtractor,
    ) -> None:
        """
        Run the FeatureExtractor.

        The return value is discarded such that it can be used by timit.

        Args:
            array: The array containing all data.
            window_size: The size of the window.
            feature_extractor: An instance of the PeriodicFeatureExtractor.
        """
        _calculate_avg_window_py(feature_extractor, array, window_size)

    time_np = timeit(
        partial(run_avg_np, array, window_size, feature_extractor), number=10
    )
    time_py = timeit(
        partial(run_avg_py, array, window_size, feature_extractor), number=10
    )
    print(time_np)
    print(time_py)
    print(f"Numpy is {time_py / time_np} times faster!")


DAY_S = 24 * 60 * 60


async def main() -> None:
    """
    Run the benchmarks.

    The benchmark are comparing the numpy
    implementation with the python implementation.
    """
    # initialize random number generator
    rng = default_rng()

    # create a random ndarray with 29 days -5 seconds of data
    days_29_s = 29 * DAY_S
    async with init_feature_extractor(10) as feature_extractor:
        data = rng.standard_normal(days_29_s)
        run_benchmark(data, 4, feature_extractor)

        days_29_s = 29 * DAY_S + 3
        data = rng.standard_normal(days_29_s)
        run_benchmark(data, 4, feature_extractor)

        # create a random ndarray with 29 days +5 seconds of data
        data = rng.standard_normal(29 * DAY_S + 5)

    async with init_feature_extractor(7 * DAY_S) as feature_extractor:
        # TEST one day window and 6 days distance. COPY (Case 3)
        run_benchmark(data, DAY_S, feature_extractor)
        # benchmark one day window and 6 days distance. NO COPY (Case 1)
        run_benchmark(data[: 28 * DAY_S], DAY_S, feature_extractor)


logging.basicConfig(level=logging.DEBUG)
asyncio.run(main())



================================================
FILE: benchmarks/timeseries/resampling.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Benchmark resampling."""

from collections.abc import Sequence
from datetime import datetime, timedelta, timezone
from timeit import timeit

from frequenz.sdk.timeseries import ResamplerConfig, ResamplerConfig2
from frequenz.sdk.timeseries._resampling._base_types import SourceProperties
from frequenz.sdk.timeseries._resampling._resampler import _ResamplingHelper


def nop(  # pylint: disable=unused-argument
    samples: Sequence[tuple[datetime, float]],
    resampler_config: ResamplerConfig,
    source_properties: SourceProperties,
) -> float:
    """Return 0.0."""
    return 0.0


def _benchmark_resampling_helper(resamples: int, samples: int) -> None:
    """Benchmark the resampling helper."""
    helper = _ResamplingHelper(
        "benchmark",
        ResamplerConfig2(
            resampling_period=timedelta(seconds=1.0),
            max_data_age_in_periods=3.0,
            resampling_function=nop,
            initial_buffer_len=samples * 3,
            warn_buffer_len=samples * 3 + 2,
            max_buffer_len=samples * 3 + 3,
        ),
    )
    now = datetime.now(timezone.utc)

    def _do_work() -> None:
        nonlocal now
        delta = timedelta(seconds=1 / samples)
        for _n_resample in range(resamples):
            for _n_sample in range(samples):
                now = now + delta
                helper.add_sample((now, 0.0))
            helper.resample(now)

    print(timeit(_do_work, number=5))


def _benchmark() -> None:
    for resamples in [10, 100, 1000]:
        for samples in [10, 100, 1000]:
            print(f"{resamples=} {samples=}")
            _benchmark_resampling_helper(resamples, samples)


if __name__ == "__main__":
    _benchmark()



================================================
FILE: benchmarks/timeseries/ringbuffer_memusage.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Memory allocation benchmark for the ringbuffer."""


import argparse
import tracemalloc
from datetime import datetime, timedelta, timezone

import numpy as np
from frequenz.quantities import Quantity

from frequenz.sdk.timeseries import Sample
from frequenz.sdk.timeseries._ringbuffer import OrderedRingBuffer

FIVE_MINUTES = timedelta(minutes=5)

RINGBUFFER_LENGTH = 4_000
# Number of iterations to run the benchmark
ITERATIONS = 100


def parse_args() -> tuple[int, int, int]:
    """Parse command line arguments."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--size",
        type=int,
        default=RINGBUFFER_LENGTH,
        help="Size of the ringbuffer to memory benchmark",
    )
    parser.add_argument(
        "--iterations",
        type=int,
        default=ITERATIONS,
        help="Number of iterations to run the benchmark",
    )
    parser.add_argument(
        "--gap-size",
        type=int,
        default=0,
        help="Number of samples to skip between each update",
    )
    args = parser.parse_args()
    return args.size, args.iterations, args.gap_size


def main(ringbuffer_len: int, iterations: int, gap_size: int) -> None:
    """Run the benchmark.

    Args:
        ringbuffer_len: Size of the ringbuffer to dump/load
        iterations: Number of iterations to run the benchmark
        gap_size: Number of samples to skip between each update

    """
    # Trace memory allocations
    tracemalloc.start()

    ringbuffer = OrderedRingBuffer(
        np.arange(0, ringbuffer_len, dtype=np.float64), FIVE_MINUTES
    )

    # Snapshot memory allocations after ringbuffer creation
    snapshot_init = tracemalloc.take_snapshot()

    print(f"size: {ringbuffer_len}")
    print(f"iterations: {iterations}")
    print(f"gap_size: {gap_size}")

    for i in range(0, ringbuffer_len * iterations, gap_size + 1):
        ringbuffer.update(
            Sample(
                datetime.fromtimestamp(
                    200 + i * FIVE_MINUTES.total_seconds(), tz=timezone.utc
                ),
                Quantity(i),
            )
        )

    # Snapshot memory allocations after ringbuffer update
    snapshot_update = tracemalloc.take_snapshot()

    # Allocation diff between ringbuffer creation and update
    top_stats = snapshot_update.compare_to(snapshot_init, "lineno")

    # Print the top 10 memory allocations
    print("Allocations since ringbuffer creation:")
    print("[ Top 10 ]")
    for stat in top_stats[:10]:
        print(stat)


if __name__ == "__main__":
    main(*parse_args())



================================================
FILE: benchmarks/timeseries/ringbuffer_serialization.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Benchmarks the serialization of the `OrderedRingBuffer` class."""


import fnmatch
import os
import time
from datetime import datetime, timedelta, timezone
from typing import Any

import numpy as np
from frequenz.quantities import Quantity

import frequenz.sdk.timeseries._ringbuffer as rb
from frequenz.sdk.timeseries import Sample

FILE_NAME = "ringbuffer.pkl"
FIVE_MINUTES = timedelta(minutes=5)

# Size of the ringbuffer to dump/load
SIZE = 4000_000
# Number of iterations to run the benchmark
ITERATIONS = 100


def delete_files_with_prefix(prefix: str) -> None:
    """Delete all files starting with the given prefix.

    Args:
        prefix: Prefix of the files to delete
    """
    for file in os.listdir():
        if fnmatch.fnmatch(file, prefix + "*"):
            os.remove(file)


def benchmark_serialization(
    ringbuffer: rb.OrderedRingBuffer[Any], iterations: int
) -> float:
    """Benchmark the given buffer `iteration` times.

    Args:
        ringbuffer: Ringbuffer to benchmark to serialize.
        iterations: amount of iterations to run.

    Returns:
        Average time to dump and load the ringbuffer.
    """
    total = 0.0
    for _ in range(iterations):
        start = time.time()
        rb.dump(ringbuffer, FILE_NAME)
        rb.load(FILE_NAME)
        end = time.time()
        total += end - start
        delete_files_with_prefix(FILE_NAME)

    return total / iterations


def main() -> None:
    """Run Benchmark."""
    ringbuffer = rb.OrderedRingBuffer(
        np.arange(0, SIZE, dtype=np.float64), timedelta(minutes=5)
    )

    print("size:", SIZE)
    print("iterations:", ITERATIONS)

    for i in range(0, SIZE, 10000):
        ringbuffer.update(
            Sample(
                datetime.fromtimestamp(
                    200 + i * FIVE_MINUTES.total_seconds(), tz=timezone.utc
                ),
                Quantity(i),
            )
        )

    print(
        "Avg time for Pickle dump/load:  "
        f"{benchmark_serialization(ringbuffer, ITERATIONS)}s"
    )


if __name__ == "__main__":
    main()



================================================
FILE: docs/CONTRIBUTING.md
================================================
--8<-- "CONTRIBUTING.md"



================================================
FILE: docs/index.md
================================================
# Frequenz Python SDK

## Introduction

The Frequenz Python SDK is a development kit for interacting with the Frequenz development platform.

## Installation

First, you need to make sure you have Python installed (at least version 3.11):

```console
$ python3 --version
Python 3.11.4
```

!!! note

    These instructions assume you are using a [POSIX compatible
    `sh`](https://pubs.opengroup.org/onlinepubs/9699919799/utilities/sh.html)
    shell.

If that command doesn't print a version newer than 3.11.0, you'll need to
[download and install Python](https://www.python.org/downloads/) first.

To install the SDK, you probably want to create a new virtual environment first:

```sh
mkdir my-sdk-project
cd my-sdk-project
python3 -m venv .venv
. .venv/bin/activate
```

!!! tip

    Using [`direnv`](https://direnv.net/) can greatly simplify this process as
    it automates the creation, activation, and deactivation of the virtual
    environment. The first time you enable `direnv`, the virtual environment
    will be created, and each time you enter or leave a subdirectory, it will be
    activated and deactivated, respectively.

    ```sh
    sudo apt install direnv # if you use Debian/Ubuntu
    mkdir my-sdk-project
    cd my-sdk-project
    echo "layout python python3" > .envrc
    direnv allow
    ```

    This will create the virtual environment and activate it automatically for you.

Now you can install the SDK by using `pip` (if you don't have `pip` installed
you can follow [the official
instructions](https://pip.pypa.io/en/stable/installation/)):

```sh
python3 -m pip install frequenz-sdk
```

To verify that the installation worked, you can invoke the Python interpreter and
import the SDK:

```console
$ python3
Python 3.11.4 (main, Jun  7 2023, 10:13:09) [GCC 12.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import frequenz.sdk
>>>
```



================================================
FILE: docs/SUMMARY.md
================================================
* [Home](index.md)
* [User Guide](user-guide/)
* [Tutorials](tutorials/)
* [API Reference](reference/)
* [Contributing](CONTRIBUTING.md)



================================================
FILE: docs/_css/mkdocstrings.css
================================================
/* Recommended style from:
 * https://mkdocstrings.github.io/python/customization/#recommended-style-material
 * With some additions from:
 * https://github.com/mkdocstrings/mkdocstrings/blob/master/docs/css/mkdocstrings.css
 */

/* Indentation. */
div.doc-contents:not(.first) {
  padding-left: 25px;
  border-left: .05rem solid var(--md-typeset-table-color);
}

/* Indentation. */
div.doc-contents:not(.first) {
  padding-left: 25px;
  border-left: 4px solid rgba(230, 230, 230);
  margin-bottom: 80px;
}

/* Avoid breaking parameters name, etc. in table cells. */
td code {
  word-break: normal !important;
}

/* Mark external links as such. */
a.autorefs-external::after {
  /* https://primer.style/octicons/arrow-up-right-24 */
  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="rgb(0, 0, 0)" d="M18.25 15.5a.75.75 0 00.75-.75v-9a.75.75 0 00-.75-.75h-9a.75.75 0 000 1.5h7.19L6.22 16.72a.75.75 0 101.06 1.06L17.5 7.56v7.19c0 .414.336.75.75.75z"></path></svg>');
  content: ' ';

  display: inline-block;
  position: relative;
  top: 0.1em;
  margin-left: 0.2em;
  margin-right: 0.1em;

  height: 1em;
  width: 1em;
  border-radius: 100%;
  background-color: var(--md-typeset-a-color);
}
a.autorefs-external:hover::after {
  background-color: var(--md-accent-fg-color);
}



================================================
FILE: docs/_css/style.css
================================================
/* Based on:
 * https://github.com/mkdocstrings/mkdocstrings/blob/master/docs/css/style.css
 */

/* Increase logo size */
.md-header__button.md-logo {
    padding-bottom: 0.2rem;
    padding-right: 0;
}
.md-header__button.md-logo img {
    height: 1.5rem;
}

/* Mark external links as such (also in nav) */
a.external:hover::after, a.md-nav__link[href^="https:"]:hover::after {
  /* https://primer.style/octicons/link-external-16 */
  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path fill="rgb(233, 235, 252)" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>');
  height: 0.8em;
  width: 0.8em;
  margin-left: 0.2em;
  content: ' ';
  display: inline-block;
}

/* More space at the bottom of the page */
.md-main__inner {
  margin-bottom: 1.5rem;
}

/* Code annotations with numbers.
 *
 * Normally annotations are shown with a (+) button that expands the
 * annotation. To be able to explain code step by step, it is good to have
 * annotations with numbers, to be able to follow the notes in a particular
 * order.
 *
 * To do this, we need some custom CSS rules. Before this customization was
 * officially supported and documented, but now they are not officially
 * supported anymore, so it could eventually break (it already did once).
 *
 * If that happens we either need to look into how to fix the CSS ourselves or
 * remove the feature. To do the customization, this is what we should be able
 * to count on:
 *
 * "you can be sure that the data-md-annotation-id attribute will always be
 * present in the source, which means you can always number them in any way you
 * like."
 *
 * Code annotation are described here:
 * https://squidfunk.github.io/mkdocs-material/reference/code-blocks/#code-annotations
 *
 * Here are the original docs on how to enable numbered annotations:
 * https://web.archive.org/web/20230724161216/https://squidfunk.github.io/mkdocs-material/reference/code-blocks/#annotations-with-numbers
 *
 * This is the PR fixing the numbered annotations when they broke:
 * https://github.com/frequenz-floss/frequenz-sdk-python/pull/684
 *
 * And this is the reported regression when it was decided to drop support for
 * numbered annotations officially:
 * https://github.com/squidfunk/mkdocs-material/issues/6042
 */
.md-typeset .md-annotation__index > ::before {
  content: attr(data-md-annotation-id);
}
.md-typeset :focus-within > .md-annotation__index > ::before {
  transform: none;
}
.md-typeset .md-annotation__index {
  width: 4ch;
}



================================================
FILE: docs/_overrides/main.html
================================================
{% extends "base.html" %}

{% block outdated %}
  You're not viewing the latest (stable) version.
  <a href="{{ '../' ~ base_url }}">
    <strong>Click here to go to latest (stable) version</strong>
  </a>
{% endblock %}



================================================
FILE: docs/_scripts/macros.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""This module defines macros for use in Markdown files."""

import os
import pathlib

from frequenz.repo.config.mkdocs.mkdocstrings_macros import (
    hook_env_with_everything,
    slugify,
)
from mkdocs_macros import plugin as macros


def define_env(env: macros.MacrosPlugin) -> None:
    """Define the hook to create macro functions for use in Markdown.

    Args:
        env: The environment to define the macro functions in.
    """

    @env.macro  # type: ignore[misc]
    def glossary(term: str, text: str | None = None) -> str:
        """Create a link to the glossary entry for the given term.

        Args:
            term: The term to link to.
            text: The text to display for the link. Defaults to the term.

        Returns:
            The Markdown link to the glossary entry for the given term.
        """
        current_path = pathlib.Path(env.page.file.src_uri)
        glossary_path = pathlib.Path("user-guide/glossary.md")
        # This needs to use `os.path.relpath` instead of `pathlib.Path.relative_to`
        # because the latter expects one path to be a parent of the other, which is not
        # always the case, for example when referencing the glossary from the API
        # reference.
        link_path = os.path.relpath(glossary_path, current_path.parent)
        return f"[{text or term}]({link_path}#{slugify(term)})"

    # This must be at the end to enable all standard features
    hook_env_with_everything(env)



================================================
FILE: docs/_scripts/mkdocstrings_autoapi.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Generate the code reference pages."""

from frequenz.repo.config.mkdocs import api_pages

api_pages.generate_python_api_pages("src", "reference")



================================================
FILE: docs/tutorials/getting_started.md
================================================
# Getting started

## Prerequisites

1. Python 3.11 or newer installed on your system.
2. Access to a microgrid system supported by the `frequenz.sdk` or you can use
   the sandbox.
3. Basic knowledge of microgrid concepts.
4. [Familiarity with Channels](https://frequenz-floss.github.io/frequenz-channels-python/latest/).
5. [Install the Frequenz SDK](../index.md#installation)

## Create a project

### Create a Python file

You can start by simply creating a Python script (e.g., `pv_optimization.py`)
using your favorite text editor.

### Use Frequenz Repository Configuration

As an alternative and specially for larger projects, it's recommended to set up the
project using the [Frequenz Repository
Configuration](https://frequenz-floss.github.io/frequenz-repo-config-python/latest).

## Import necessary modules

You can now open the app's main file and start adding content. Begin by
importing the necessary libraries.

```python
import asyncio

from datetime import timedelta
from frequenz.sdk import microgrid
from frequenz.sdk.timeseries import ResamplerConfig2
```

## Create the application skeleton

The main logic of your application will run within an async function. Let's
create a skeleton that contains all the necessary code to initialize a
microgrid.

```python
async def run() -> None:
    # This points to the default Frequenz microgrid sandbox
    server_url = "grpc://microgrid.sandbox.api.frequenz.io:62060"

    # Initialize the microgrid
    await microgrid.initialize(
        server_url,
        ResamplerConfig2(resampling_period=timedelta(seconds=1)),
    )

    # Define your application logic here
    # ...
```

## Define the `main()` function

Create a `main()` function that will set up and run the `run()` function using
asyncio.

```python
def main() -> None:
    asyncio.run(run())

if __name__ == "__main__":
    main()
```

## Implement the application logic

Inside the `run()` function, implement the core logic of your application. This
will include creating receivers for data streams, processing the data, making
decisions, and eventually sending control messages to the microgrid system. We
will cover more details in the following tutorials. For now, let's simply read
the power measurements from the microgrid's grid meter and print them on the
screen. The grid meter is a meter that is directly connected to the grid
connection point.

```python
async def run() -> None:
    # This points to the default Frequenz microgrid sandbox
    ...

    # Define your application logic here
    grid_meter = microgrid.grid().power.new_receiver()

    async for power in grid_meter:
        print(power.value)
```

## Putting it all together

Here is the full version of your first Frequenz SDK application.

```python
import asyncio

from datetime import timedelta
from frequenz.sdk import microgrid
from frequenz.sdk.timeseries import ResamplerConfig2

async def run() -> None:
    # This points to the default Frequenz microgrid sandbox
    server_url = "grpc://microgrid.sandbox.api.frequenz.io:62060"

    # Initialize the microgrid
    await microgrid.initialize(
        server_url,
        ResamplerConfig2(resampling_period=timedelta(seconds=1)),
    )

    # Define your application logic here
    grid_meter = microgrid.grid().power.new_receiver()

    async for power in grid_meter:
        print(power.value)

def main() -> None:
    asyncio.run(run())

if __name__ == "__main__":
    main()
```

## Run your application

You're now ready to run your application. When working on an existing
microgrid, make sure to update the `microgrid_host` and `microgrid_port`
variables before running the script.

```bash
# Example usage
python pv_optimization.py
```



================================================
FILE: docs/user-guide/actors.md
================================================
# Actors

::: frequenz.sdk.actor
    options:
        members: []
        show_bases: false
        show_root_heading: false
        show_root_toc_entry: false
        show_source: false



================================================
FILE: docs/user-guide/config.md
================================================
# Configuration

::: frequenz.sdk.config
    options:
        members: []
        show_bases: false
        show_root_heading: false
        show_root_toc_entry: false
        show_source: false



================================================
FILE: docs/user-guide/formula-engine.md
================================================
# Formula Engine

::: frequenz.sdk.timeseries.formula_engine
    options:
        members: None
        show_bases: false
        show_root_full_path: false
        show_root_heading: false
        show_root_toc_entry: false
        show_source: false



================================================
FILE: docs/user-guide/glossary.md
================================================
# Glossary

This glossary provides definitions for common terminology used in the Frequenz SDK,
focusing on microgrid components, metrics, measurements, and power-related terms.

## Common Acronyms

### AC

Alternating current. See the [Wikipedia
article](https://en.wikipedia.org/wiki/Alternating_current) for more details.

### BMS

Battery management system. See the [Wikipedia
article](https://en.wikipedia.org/wiki/Battery_management_system) for more
details.

### CHP

Combined heat and power. See the [Wikipedia
article](https://en.wikipedia.org/wiki/Combined_heat_and_power) for more
details.

### DC

Direct current. See the [Wikipedia
article](https://en.wikipedia.org/wiki/Direct_current) for more details.

### EV

Electric vehicle. See the [Wikipedia
article](https://en.wikipedia.org/wiki/Electric_vehicle) for more details.

### PSC

[Passive sign convention](#passive-sign-convention). See the [Wikipedia
article](https://en.wikipedia.org/wiki/Passive_sign_convention) for more
details.

### PV

Photovoltaic. See the [Wikipedia
article](https://en.wikipedia.org/wiki/Photovoltaics) for more details.

In the SDK it is normally used as a synonym for [solar panel](#solar-panel).

### SoC

[State of charge](#state-of-charge). See the [Wikipedia
article](https://en.wikipedia.org/wiki/State_of_charge) for more details.

### SoP

[State of power](#state-of-power).

## Microgrid

A local electrical grid that connects a set of different [types of
components](#component-category) together. It can be connected to the public
[grid](#grid), or be completely isolated, in which case it is known as an
island.

Components can be grouped into [assets](#assets) and [devices](#devices).
Assets are core components like generators or storage systems that are crucial from a business perspective,
whereas devices are the supporting infrastructure essential for the functionality of these assets.

### Component Category

The category [components](#component) of a [microgrid](#microgrid) belong to.

[Components](#component) of the same category have the same characteristics
(for example offer the same set of [metrics](#metric)).

Examples of categories are meters, inverters or batteries.

A category can be further divided into types.
For instance, PV inverters and battery inverters are types under the inverter category.


### Assets

#### Battery

A storage system for electrical energy.

#### CHP Plant

A generator that produces combined heat and power ([CHP](#chp)). Usually
powered via combustion of some form of fuel.

#### Consumer

Typically used in context with a metric, i.e. consumer power or consumer current.
This aggregates all remaining electrical energy [consumption](#consumption) in a microgrid that are not covered by other asset types and components.
Under normal circumstances this is expected to correspond to the [gross consumption](#gross-consumption) of the site excluding active parts and battery.
In [Passive-sign convention](#passive-sign-convention) consumer power is usually positive, but negative values can occur in
exotic site topologies (e.g. unidentified generators) and or due to sudden short-term effects.

#### EV Charger

A station for charging [EVs](#ev).

#### Electrolyzer

A device that converts water into hydrogen and oxygen.

#### Grid

A point where the local [microgrid](#microgrid) is connected to the public
electricity grid.

#### PV Array

A collection of [PV](#pv) panels.

#### Solar Panel

A panel with [PV](#pv) cells that generates [DC](#dc) electricity from
sunlight.

#### Wind Turbine

A device that converts the wind's kinetic energy into electrical energy.

### Devices

#### Converter

Generally refers to [DC-to-DC converter](#dc-to-dc-converter).

#### DC-to-DC Converter

An electronic circuit or electromechanical device that converts a source of
[DC](#dc) from one voltage level to another.

#### Inverter

A device or circuitry that converts [DC](#dc) electricity to [AC](#ac)
electricity.

#### Meter

A device for measuring electrical [metrics](#metrics) (for example current,
voltage, etc.).

#### Pre-charge module

A device that gradually ramp the [DC](#dc) voltage up to prevent any potential
damage to sensitive electrical components, like capacitors.

While many [inverters](#inverter) and [batteries](#battery) come equipped with
in-built pre-charging mechanisms, some may lack this feature. In such cases,
external pre-charging modules can be used.

#### Relay

A device that generally have two states: open (connected) and closed
(disconnected).

They are generally placed in front of another [component](#component), e.g., an
[inverter](#inverter), to control whether the component is connected to the
[microgrid](#microgrid) or not.

#### Sensor

A device for [measuring](#measurement] ambient [metrics](#metric) (for example
temperature, humidity, etc.).

### Component

A device (of a particular [category](#component-category)) within
a [microgrid](#microgrid).

### Component ID

A numeric identifier uniquely representing an instance of
a [component](#component) in a particular [microgrid](#microgrid). It is always
of type `int`.

For example, a battery with a component ID of **5**.

### Component Graph

A [graph](https://en.wikipedia.org/wiki/Graph_(abstract_data_type))
representation of the configuration in which the electrical components in a
microgrid are connected with each other.  Some of the ways in which the SDK uses
the component graph are:

  - figure out how to calculate high level metrics like
[`grid().power`][frequenz.sdk.timeseries.grid.Grid.power],
[`consumer().power`][frequenz.sdk.timeseries.consumer.Consumer.power],
etc. for a microgrid, using the available components.
  - identify the available {{glossary("battery", "batteries")}} or
    {{glossary("EV charger", "EV chargers")}} at a site that can be controlled.

### Island

A [microgrid](#microgrid) that is not connected to the public electricity
[grid](#grid).

### Passive Sign Convention

A convention for the direction of power flow in a circuit. When the electricity
is flowing into a [component](#component) the value is positive, and when it is
flowing out of a component the value is negative.

In microgrids that have a grid connection, power flowing away from the grid is
positive, and power flowing towards the grid is negative.

## Component Data

### Metric

A quantifiable attribute of a [component](#component).

For example, the metric **capacity** of a battery.

### Measurement

An individual numeric value obtained from a [metric](#metric) of
a [component](#component) instance. It is consistently of type `float`, but it
is often expressed in specific units.

In the context of a sample, this is commonly referred to as a *sample value*.

For example, a measurement of the capacity of a battery with component ID 5 can
be **400**, typically measured in Watt-hours (Wh).

### Timestamp

A specific point in time, always represented as a `datetime` with a `timezone`
attached.

For example, **2022-01-01 22:00:00.000 UTC**.

### Sample

A [measurement](#measurement) recorded at a particular [timestamp](#timestamp),
typically represented as a tuple `(timestamp, value)`.

For example, recording a measurement of 400 from the capacity of a battery at
2022-01-01 22:00:00.000 UTC would constitute a sample **`(2022-01-01
22:00:00.000 UTC, 400)`**.

#### Sample Value

A [measurement](#measurement) stored in a [sample](#sample).

### Time Series

A sequence of [samples](#sample), often organized by [timestamp](#timestamp)
and typically with regular intervals. However, irregular (event-based) time
series are also possible.

For example, a time series representing measurements of a battery's capacity at
2022-01-01 22:00:00.000 UTC every second for 5 seconds would appear as follows:

```
(2022-01-01 22:00:00.000 UTC, 400)
(2022-01-01 22:00:01.000 UTC, 401)
(2022-01-01 22:00:02.000 UTC, 403)
(2022-01-01 22:00:03.000 UTC, 402)
(2022-01-01 22:00:04.000 UTC, 403)
```

### Timeseries

Same as [time series](#time-series).

### Metric ID

An identifier for a [component](#component)'s [metric](#metric), typically
a string (`str`).

Components belonging to the same [category](#component-category) have the same
set of metric IDs.

For example, the metric ID for the capacity of a [battery](#battery) is simply
**`capacity`**.

### Time Series ID

An identifier for a [time series](#time-series) originating from
a [metric](#metric) of a specific component. Typically a string (`str`) derived
from the tuple **([component ID](#component-id), [metric ID](#metric-id))** for
[components](#component).

For example, a time series for the capacity of a battery with component ID
5 has the ID **(component_id, metric_id)** (or
**`f"{component_id}_{metric_id}"`**).

### Timeseries ID

Same as [time series ID](#time-series-id).

## Metrics

All metrics related to power, energy, current, etc. use the [PSC](#psc) to
determine the sign of the value.

### Consumption

The amount of electricity flowing into a [component](#component). It is the
clipped positive value when using the [PSC](#psc), so if the electricity is
flowing out of the component instead, this will be zero.

### Gross Consumption

[Consumption](#consumption) before accounting for any local generation from
[solar](#solar-panel), [wind](#wind-turbine) or [CHP](#chp-plant).

### Instantaneous Power

Same as [power](#power).

### Load

Typically refers to a device that [consumes](#consumption) electricity, but also
to the amount of electricity [consumed](#consumption) by such a device. In
a [microgrid](#microgrid) context, it is often used to refer to all the
electrical devices that are doing active work, for example a light bulb,
a motor, a cooler, etc.

When using the [PSC](#psc), this is the same as [consumption](#consumption) and
it is a positive value. If a *load* [generates](#production) electricity
instead, it is a negative value but it wouldn't typically be called a *load*.

### Net Consumption

This term traditionally refers to the difference between the [gross
consumption](#gross-consumption) and the local generation (like [PV](#pv)
[production](#production)). It is the electricity [consumption](#consumption)
that needs to be provided by the [battery](#battery) or from the public
[grid](#grid).

### Net Load

Same as [net consumption](#net-consumption).

### Power

The rate of energy transfer, i.e. the amount of energy transferred per unit of
time. It is typically measured in Watts (W).

For [AC](#ac) electricity, there are three types of power:
[active](#active-power), [reactive](#reactive-power), and
[apparent](#apparent-power) (P, Q and |S| respectively in [power
triangle](#power-triangle)).

See the [Wikipedia article](https://en.wikipedia.org/wiki/AC_power) for more
information.

#### Power Triangle

The visual representation of the relationship between the three types of
[AC](#ac) [power](#power) and [phase of voltage relative to
current](#phase-of-voltage-relative-to-current).

![Power triangle](../img/power-triangle.svg)

([CC-BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/deed.en),
[Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Cmplxpower.svg))

#### Active Power

The [AC](#ac) power that is actually consumed by the [load](#load). It is the
real part of the [apparent power](#apparent-power), P in the [power
triangle](#power-triangle).

#### Apparent Power

The [AC](#ac) power that is actually supplied to the [load](#load). The
magnitude of the vector sum of the [active](#active-power) and
[reactive](#reactive-power) power, |S| in the
[power-triangle](#power-triangle).

#### Phase of Voltage Relative to Current

The angle of difference (in degrees) between current and voltage in an
[AC](#ac) circuit, φ in the [power triangle](#power-triangle).

#### Power Factor

The ratio of real power to apparent power in an [AC](#ac) circuit. Can be
obtained by computing the cosine of
[φ](#phase-of-voltage-relative-to-current), in the [power
triangle](#power-triangle). See the [Wikipedia
article](https://en.wikipedia.org/wiki/Power_factor) for more details.

#### Reactive Power

The [AC](#ac) power that is not consumed by the [load](#load), but is
alternately stored and returned to the source. It is the imaginary part of the
[apparent power](#apparent-power), Q in the [power triangle](#power-triangle).

#### Real Power

Same as [active power](#active-power).

### Production

The amount of electricity flowing out of a [component](#component). It is the
clipped negative value when using the [PSC](#psc), so if the electricity is
flowing into the component instead, this will be zero.

### Residual Consumption

In [microgrid](#microgrid) context sometimes used as the remaining difference
between the [net consumption](#net-consumption) and the [battery](#battery)
power, i.e. what we define as
[`grid().power`][frequenz.sdk.timeseries.grid.Grid.power].

### Residual Load

Same as [residual consumption](#residual-consumption).

### State of Charge

The level of charge of a [battery](#battery) relative to its capacity,
expressed in percentage points. Calculated as the ratio between the remaining
energy in the battery at a given time and the maximum possible energy under
similar health conditions.
[Source](https://epicpower.es/wp-content/uploads/2020/08/AN028_SoC-SoH-SoP-definitions_v3.pdf)

### State of Power

The ratio of peak power to nominal power. Peak power is the maximum power that
can be sustained for a specific duration without violating preset operational
design limits on battery voltage, SoC, power, or current.

This indicator is crucial to ensure that charge or discharge power remains
within specific limits, optimizing the battery's lifespan. It is particularly
useful in peak power applications to define battery conditions for substantial
charges or discharges.

The state of power depends on the state of charge, battery capacity, initial
characteristics, chemistry, and battery voltage, as well as external factors
like temperature and humidity, which can also have a significant impact on it.
[Source](https://epicpower.es/wp-content/uploads/2020/08/AN028_SoC-SoH-SoP-definitions_v3.pdf)



================================================
FILE: docs/user-guide/index.md
================================================
# Introduction

There are several fundamental concepts that one should comprehend before
diving into the Frequenz SDK. These concepts encompass both general notions
within the energy sector or microgrids and specific details regarding the SDK
itself, such as asynchronous programming, use of channels for communication,
the actor model, data pipelines, and the composition of a components graph.

Given that users may possess diverse backgrounds and varying levels of
familiarity with these subjects, the left menu is organized into sections, each
dedicated to a particular topic.

For individuals without prior knowledge, it is advisable to proceed through the
sections sequentially to establish a solid foundation.



================================================
FILE: docs/user-guide/microgrid-concepts.md
================================================
# Microgrid Concepts

::: frequenz.sdk.microgrid
    options:
        members: []
        show_bases: false
        show_root_heading: false
        show_root_toc_entry: false
        show_source: false



================================================
FILE: examples/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Examples for the sdk."""



================================================
FILE: examples/battery_pool.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Script with an example how to use BatteryPool."""


import asyncio
import logging
from datetime import timedelta

from frequenz.channels import merge

from frequenz.sdk import microgrid
from frequenz.sdk.timeseries import ResamplerConfig2

MICROGRID_API_URL = "grpc://microgrid.sandbox.api.frequenz.io:62060"


async def main() -> None:
    """Create the battery pool, activate all formulas and listen for any update."""
    logging.basicConfig(
        level=logging.DEBUG, format="%(asctime)s %(name)s %(levelname)s:%(message)s"
    )

    await microgrid.initialize(
        MICROGRID_API_URL,
        resampler_config=ResamplerConfig2(resampling_period=timedelta(seconds=1.0)),
    )

    battery_pool = microgrid.new_battery_pool(priority=5)
    receivers = [
        battery_pool.soc.new_receiver(limit=1),
        battery_pool.capacity.new_receiver(limit=1),
        # pylint: disable-next=protected-access
        battery_pool._system_power_bounds.new_receiver(limit=1),
    ]

    async for metric in merge(*receivers):
        print(f"Received new metric: {metric}")


asyncio.run(main())



================================================
FILE: examples/load_shedding.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Load shedding actor."""

import asyncio
import random
import sys
import termios
import tty
from dataclasses import dataclass
from datetime import datetime, timezone
from heapq import heappop, heappush
from typing import AsyncGenerator
from unittest.mock import patch

from frequenz.channels import Broadcast, Receiver, Sender
from frequenz.quantities import Percentage, Power

from frequenz.sdk import microgrid
from frequenz.sdk.actor import Actor, run
from frequenz.sdk.timeseries import Sample

# Mock configuration
CONF_STATE = {}


def mock_set_consumer(name: str, power: float) -> None:
    """Mock setting consumer power by storing the state in a dictionary.

    Args:
        name: Consumer name.
        power: Power value to set.
    """
    CONF_STATE[name] = power


def _log(msg: str) -> None:
    print(msg, end="\n\r")


class PowerMockActor(Actor):
    """Power Mock Actor.

    Asynchronously listens to user key presses 'm' and 'n' to increase and decrease power of a
    static consumer.
    """

    def __init__(self, consumer_name: str) -> None:
        """Initialize the actor."""
        super().__init__()
        self.consumer_name = consumer_name
        self.power_step = Power.from_kilowatts(1.0)

    async def _run(self) -> None:
        _log("Press 'm' to increase power or 'n' to decrease power for the consumer.")

        while True:
            # Call _read_key in a thread to avoid blocking the event loop
            key = await asyncio.to_thread(self._read_key)
            if key == "m":
                CONF_STATE[self.consumer_name] = (
                    CONF_STATE.get(self.consumer_name, 0) + self.power_step.as_watts()
                )
                _log(
                    f"Increased {self.consumer_name} power to "
                    f"{CONF_STATE[self.consumer_name]/1000.0} kW"
                )
            elif key == "n":
                CONF_STATE[self.consumer_name] = max(
                    0,
                    CONF_STATE.get(self.consumer_name, 0) - self.power_step.as_watts(),
                )
                _log(
                    f"Decreased {self.consumer_name} power to "
                    f"{CONF_STATE[self.consumer_name]/1000.0} kW"
                )
            elif key == "q":
                sys.exit()
            else:
                _log("Invalid key. Use 'm' or 'n'.")

    def _read_key(self) -> str:
        """Read a single key press without waiting for Enter."""
        fd = sys.stdin.fileno()
        old_settings = termios.tcgetattr(fd)
        try:
            tty.setraw(fd)
            key = sys.stdin.read(1)
        finally:
            termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)
        return key


@dataclass(order=True)
class Consumer:
    """Consumer dataclass."""

    priority: int
    name: str
    power: Power
    enabled: bool = False


class LoadSheddingActor(Actor):
    """Simple load shedding actor."""

    def __init__(
        self,
        max_peak: Power,
        consumers: list[Consumer],
        grid_meter_receiver: Receiver[Sample[Power]],
    ):
        """Initialize the actor."""
        super().__init__()
        self.max_peak = max_peak
        self.disable_tolerance = self.max_peak * 0.9
        self.enable_tolerance = self.max_peak * 0.8
        self.grid_meter_receiver = grid_meter_receiver

        self.enabled_consumers: list[Consumer] = []
        self.disabled_consumers: list[Consumer] = []

        for c in consumers:
            heappush(self.disabled_consumers, c)

    async def _enable_consumer(self, consumer: Consumer) -> None:
        if not consumer.enabled:
            consumer.enabled = True
            heappush(self.enabled_consumers, consumer)
            _log(f"+++{consumer.name}, +{consumer.power}")
            # This is a mock function to set the consumer power,
            # in a real system this would be replaced with the actual implementation
            mock_set_consumer(consumer.name, consumer.power.as_watts())

    async def _disable_consumer(self, consumer: Consumer) -> None:
        if consumer.enabled:
            consumer.enabled = False
            heappush(self.disabled_consumers, consumer)
            _log(f"---{consumer.name}, -{consumer.power}")
            # This is a mock function to set the consumer power,
            # in a real system this would be replaced with the actual implementation
            mock_set_consumer(consumer.name, 0)

    async def _adjust_loads(self, current_load: Power) -> None:
        while current_load > self.disable_tolerance and self.enabled_consumers:
            enabled_consumer: Consumer = heappop(self.enabled_consumers)
            await self._disable_consumer(enabled_consumer)
            current_load -= enabled_consumer.power

        temp_disabled: list[Consumer] = []
        while self.disabled_consumers:
            disabled_consumer: Consumer = heappop(self.disabled_consumers)
            if current_load + disabled_consumer.power <= self.enable_tolerance:
                await self._enable_consumer(disabled_consumer)
                current_load += disabled_consumer.power
            else:
                heappush(temp_disabled, disabled_consumer)
                break

        while temp_disabled:
            heappush(self.disabled_consumers, heappop(temp_disabled))

    async def _run(self) -> None:
        async for power_sample in self.grid_meter_receiver:
            if power_sample.value:
                _log(
                    f"Power: {power_sample.value}, "
                    f"Peak: {self.max_peak} ({self.disable_tolerance} / {self.enable_tolerance})"
                    f", Enabled: {', '.join(c.name for c in self.enabled_consumers)}\r"
                )
                await self._adjust_loads(power_sample.value)


async def mock_sender(
    sender: Sender[Sample[Power]],
) -> AsyncGenerator[Sample[Power], None]:
    """Mock implementation of a grid meter receiver.

    It sends power values every second.
    """
    current_load = Power.from_kilowatts(0.0)

    def compute_power() -> Power:
        """Compute current grid power based on mock state."""
        return Power.from_watts(sum(CONF_STATE.values()))

    while True:
        current_load = compute_power()
        # Add +- 8% noise to the current load
        current_load += current_load * Percentage.from_fraction(
            random.uniform(-0.08, 0.08)
        )
        await sender.send(
            Sample(timestamp=datetime.now(tz=timezone.utc), value=current_load)
        )
        await asyncio.sleep(1)


async def main() -> None:
    """Program entry point."""
    consumers = [
        Consumer(priority=1, name="Fan2", power=Power.from_kilowatts(2.5)),
        Consumer(priority=2, name="Drier1", power=Power.from_kilowatts(3.0)),
        Consumer(priority=2, name="Drier2", power=Power.from_kilowatts(2.0)),
        Consumer(priority=3, name="Conveyor1", power=Power.from_kilowatts(1.5)),
        Consumer(priority=3, name="Conveyor2", power=Power.from_kilowatts(1.0)),
        Consumer(priority=4, name="Auger", power=Power.from_kilowatts(2.0)),
        Consumer(priority=4, name="HopperMixer", power=Power.from_kilowatts(2.5)),
        Consumer(priority=5, name="SiloVentilation", power=Power.from_kilowatts(1.0)),
        Consumer(priority=5, name="LoaderArm", power=Power.from_kilowatts(3.0)),
        Consumer(priority=6, name="SeedCleaner", power=Power.from_kilowatts(2.5)),
        Consumer(priority=6, name="Sprayer", power=Power.from_kilowatts(2.0)),
        Consumer(priority=7, name="Grinder", power=Power.from_kilowatts(3.0)),
        Consumer(priority=7, name="Shaker", power=Power.from_kilowatts(1.5)),
        Consumer(priority=8, name="Sorter", power=Power.from_kilowatts(2.0)),
    ]

    for consumer in consumers:
        mock_set_consumer(consumer.name, 0)

    grid_meter_receiver = microgrid.grid().power.new_receiver()

    actor_instance = LoadSheddingActor(
        max_peak=Power.from_kilowatts(30),
        consumers=consumers,
        grid_meter_receiver=grid_meter_receiver,
    )

    user_input_actor = PowerMockActor(consumer_name="static_consumer")

    await run(actor_instance, user_input_actor)


if __name__ == "__main__":
    with patch("frequenz.sdk.microgrid.grid") as mock_grid:
        chan = Broadcast[Sample[Power]](name="grid_power")
        mock_grid.return_value.power.new_receiver = chan.new_receiver

        async def begin() -> None:
            """Start main & mock sender."""
            await asyncio.gather(
                main(),
                mock_sender(chan.new_sender()),
            )

        asyncio.run(begin())



================================================
FILE: src/frequenz/sdk/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Frequenz Python SDK."""



================================================
FILE: src/frequenz/sdk/conftest.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Validate docstring code examples.

Code examples are often wrapped in triple backticks (```) within docstrings.
This plugin extracts these code examples and validates them using pylint.
"""

from frequenz.repo.config.pytest import examples
from sybil import Sybil

pytest_collect_file = Sybil(**examples.get_sybil_arguments()).pytest()



================================================
FILE: src/frequenz/sdk/py.typed
================================================
[Empty file]


================================================
FILE: src/frequenz/sdk/_internal/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Utility types and functions for internal use."""

from ._singleton_meta import SingletonMeta

# Explicitly declare the public API.
__all__ = ["SingletonMeta"]



================================================
FILE: src/frequenz/sdk/_internal/_asyncio.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""General purpose async tools."""


import asyncio
import logging
import sys
from abc import ABC
from datetime import timedelta
from typing import Any, Callable, Coroutine

_logger = logging.getLogger(__name__)


async def cancel_and_await(task: asyncio.Task[Any]) -> None:
    """Cancel a task and wait for it to finish.

    Exits immediately if the task is already done.

    The `CancelledError` is suppressed, but any other exception will be propagated.

    Args:
        task: The task to be cancelled and waited for.

    Raises:
        asyncio.CancelledError: when our task was cancelled
    """
    if task.done():
        return
    task.cancel()
    try:
        await task
    except asyncio.CancelledError:
        if not task.cancelled():
            raise


def is_loop_running() -> bool:
    """Check if the event loop is running."""
    try:
        asyncio.get_running_loop()
        return True
    except RuntimeError:
        return False


async def run_forever(
    async_callable: Callable[[], Coroutine[Any, Any, None]],
    interval: timedelta = timedelta(seconds=1),
) -> None:
    """Run a given function forever, restarting it after any exception.

    Args:
        async_callable: The async callable to run.
        interval: The interval between restarts.
    """
    interval_s = interval.total_seconds()
    while True:
        try:
            await async_callable()
        except Exception:  # pylint: disable=broad-except
            if not is_loop_running():
                _logger.exception("There is no running event loop, aborting...")
                sys.exit(-1)
            _logger.exception("Restarting after exception")
        await asyncio.sleep(interval_s)


class NotSyncConstructible(AssertionError):
    """Raised when object with async constructor is created in sync way."""


class AsyncConstructible(ABC):
    """Parent class for classes where part of the constructor is async."""

    def __init__(self) -> None:
        """Raise error when object is created in sync way.

        Raises:
            NotSyncConstructible: If this method is called.
        """
        raise NotSyncConstructible(
            "This object shouldn't be created with default constructor. ",
            "Check class documentation for more information.",
        )



================================================
FILE: src/frequenz/sdk/_internal/_channels.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""General purpose classes for use with channels."""

import abc
import dataclasses
import logging
import traceback
import typing

from frequenz.channels import Broadcast, Receiver

_logger = logging.getLogger(__name__)

T_co = typing.TypeVar("T_co", covariant=True)
U_co = typing.TypeVar("U_co", covariant=True)
T = typing.TypeVar("T")


class ReceiverFetcher(typing.Generic[T_co], typing.Protocol):
    """An interface that just exposes a `new_receiver` method."""

    @abc.abstractmethod
    def new_receiver(self, *, limit: int = 50) -> Receiver[T_co]:
        """Get a receiver from the channel.

        Args:
            limit: The maximum size of the receiver.

        Returns:
            A receiver instance.
        """


class MappingReceiverFetcher(typing.Generic[T_co, U_co]):
    """A receiver fetcher that can manipulate receivers before returning them."""

    def __init__(
        self,
        fetcher: ReceiverFetcher[T_co],
        mapping_function: typing.Callable[[Receiver[T_co]], Receiver[U_co]],
    ) -> None:
        """Initialize this instance.

        Args:
            fetcher: The underlying fetcher to get receivers from.
            mapping_function: The method to be applied on new receivers before returning
                them.
        """
        self._fetcher = fetcher
        self._mapping_function = mapping_function

    def new_receiver(self, *, limit: int = 50) -> Receiver[U_co]:
        """Get a receiver from the channel.

        Args:
            limit: The maximum size of the receiver.

        Returns:
            A receiver instance.
        """
        return self._mapping_function(self._fetcher.new_receiver(limit=limit))


class ChannelRegistry:
    """Dynamically creates, own and provide access to broadcast channels.

    It can be used by actors to dynamically establish a communication channel
    between each other.

    The registry is responsible for creating channels when they are first requested via
    the [`get_or_create()`][frequenz.sdk.actor.ChannelRegistry.get_or_create] method.

    The registry also stores type information to make sure that the same channel is not
    used for different message types.

    Since the registry owns the channels, it is also responsible for closing them when
    they are no longer needed. There is no way to remove a channel without closing it.

    Note:
        This registry stores [`Broadcast`][frequenz.channels.Broadcast] channels.
    """

    def __init__(self, *, name: str) -> None:
        """Initialize this registry.

        Args:
            name: A name to identify the registry in the logs. This name is also used as
                a prefix for the channel names.
        """
        self._name = name
        self._channels: dict[str, _Entry] = {}

    @property
    def name(self) -> str:
        """The name of this registry."""
        return self._name

    def message_type(self, key: str) -> type:
        """Get the message type of the channel for the given key.

        Args:
            key: The key to identify the channel.

        Returns:
            The message type of the channel.

        Raises:
            KeyError: If the channel does not exist.
        """
        entry = self._channels.get(key)
        if entry is None:
            raise KeyError(f"No channel for key {key!r} exists.")
        return entry.message_type

    def __contains__(self, key: str) -> bool:
        """Check whether the channel for the given `key` exists."""
        return key in self._channels

    def get_or_create(self, message_type: type[T], key: str) -> Broadcast[T]:
        """Get or create a channel for the given key.

        If a channel for the given key already exists, the message type of the existing
        channel is checked against the requested message type. If they do not match,
        a `ValueError` is raised.

        Note:
            The types have to match exactly, it doesn't do a subtype check due to
            technical limitations. In the future subtype checks might be supported.

        Args:
            message_type: The type of the message that is sent through the channel.
            key: The key to identify the channel.

        Returns:
            The channel for the given key.

        Raises:
            ValueError: If the channel exists and the message type does not match.
        """
        if key not in self._channels:
            if _logger.isEnabledFor(logging.DEBUG):
                _logger.debug(
                    "Creating a new channel for key %r with type %s at:\n%s",
                    key,
                    message_type,
                    "".join(traceback.format_stack(limit=10)[:9]),
                )
            self._channels[key] = _Entry(
                message_type, Broadcast(name=f"{self._name}-{key}")
            )

        entry = self._channels[key]
        if entry.message_type is not message_type:
            error_message = (
                f"Type mismatch, a channel for key {key!r} exists and the requested "
                f"message type {message_type} is not the same as the existing "
                f"message type {entry.message_type}."
            )
            if _logger.isEnabledFor(logging.DEBUG):
                _logger.debug(
                    "%s at:\n%s",
                    error_message,
                    # We skip the last frame because it's this method, and limit the
                    # stack to 9 frames to avoid adding too much noise.
                    "".join(traceback.format_stack(limit=10)[:9]),
                )
            raise ValueError(error_message)

        return typing.cast(Broadcast[T], entry.channel)

    async def close_and_remove(self, key: str) -> None:
        """Remove the channel for the given key.

        Args:
            key: The key to identify the channel.

        Raises:
            KeyError: If the channel does not exist.
        """
        entry = self._channels.pop(key, None)
        if entry is None:
            raise KeyError(f"No channel for key {key!r} exists.")
        await entry.channel.close()


@dataclasses.dataclass(frozen=True)
class _Entry:
    """An entry in a channel registry."""

    message_type: type
    """The type of the message that is sent through the channel in this entry."""

    # We use object instead of Any to minimize the chances of hindering type checking.
    # If for some reason the channel is not casted to the proper underlaying type, when
    # using object at least accessing any member that's not part of the object base
    # class will yield a type error, while if we used Any, it would not and the issue
    # would be much harder to find.
    channel: Broadcast[object]
    """The channel in this entry."""



================================================
FILE: src/frequenz/sdk/_internal/_constants.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Module with constants shared between instances of the sdk.

To be replaced by ConfigManager.
"""

RECEIVER_MAX_SIZE = 50
"""Default buffer size of the receiver."""

WAIT_FOR_COMPONENT_DATA_SEC: float = 2
"""Delay the start of the application to wait for the data."""

MAX_BATTERY_DATA_AGE_SEC: float = 2
"""Max time difference for the battery or inverter data to be considered as reliable.

If battery or inverter stopped sending data, then this is the maximum time when its
last message should be considered as valid. After that time, component data
should not be used.
"""



================================================
FILE: src/frequenz/sdk/_internal/_math.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Internal math tools."""

import math


def is_close_to_zero(value: float, abs_tol: float = 1e-9) -> bool:
    """Check if a floating point value is close to zero.

    A value of 1e-9 is a commonly used absolute tolerance to balance precision
    and robustness for floating-point numbers comparisons close to zero. Note
    that this is also the default value for the relative tolerance.
    For more technical details, see https://peps.python.org/pep-0485/#behavior-near-zero

    Args:
        value: the floating point value to compare to.
        abs_tol: the minimum absolute tolerance. Defaults to 1e-9.

    Returns:
        whether the floating point value is close to zero.
    """
    zero: float = 0.0
    return math.isclose(a=value, b=zero, abs_tol=abs_tol)



================================================
FILE: src/frequenz/sdk/_internal/_singleton_meta.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Definition of Singleton metaclass."""

from threading import Lock
from typing import Any


class SingletonMeta(type):
    """This is a thread-safe implementation of Singleton."""

    _instances: dict[Any, type] = {}
    """The dictionary of instances of the singleton classes."""

    _lock: Lock = Lock()
    """The lock to ensure thread safety.

    The lock to acquire when creating a singleton instance, preventing
    multiple threads from creating instances simultaneously.
    """

    def __call__(cls, *args: Any, **kwargs: Any) -> type:
        """Overload function call operator to return the singleton instance.

        Args:
            *args: positional args
            **kwargs: and keyword args for the super class.

        Returns:
            The singleton instance.
        """
        with cls._lock:
            if cls not in cls._instances:
                instance = super().__call__(*args, **kwargs)
                cls._instances[cls] = instance
        return cls._instances[cls]



================================================
FILE: src/frequenz/sdk/actor/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

# Please note this docstring is included in the getting started documentation in docs/.
"""Actors are a primitive unit of computation that runs autonomously.

## Actor Programming Model

!!! quote "From [Wikipedia](https://en.wikipedia.org/wiki/Actor_model)"

    The actor model in computer science is a mathematical model of concurrent
    computation that treats an actor as the basic building block of concurrent
    computation. In response to a message it receives, an actor can: make local
    decisions, create more actors, send more messages, and determine how to
    respond to the next message received. Actors may modify their own private
    state, but can only affect each other indirectly through messaging (removing
    the need for lock-based synchronization).

We won't get into much more detail here because it is outside the scope of this
documentation. However, if you are interested in learning more about the actor
programming model, here are some useful resources:

- [Actor Model (Wikipedia)](https://en.wikipedia.org/wiki/Actor_model)
- [How the Actor Model Meets the Needs of Modern, Distributed Systems
  (Akka)](https://doc.akka.io/docs/akka/current/typed/guide/actors-intro.html)

## Frequenz SDK Actors

The [`Actor`][frequenz.sdk.actor.Actor] class serves as the foundation for
creating concurrent tasks and all actors in the SDK inherit from it. This class
provides a straightforward way to implement actors. It shares similarities with
the traditional actor programming model but also has some unique features:

- **Message Passing:** Like traditional actors, our [`Actor`][frequenz.sdk.actor.Actor]
  class communicates through message passing. Even when no particular message passing
  mechanism is enforced, the SDK actors use [`frequenz.channels`][] for communication.

- **Automatic Restart:** If an unhandled exception occurs in an actor's logic
  ([`_run()`][_run] method), the actor will be automatically restarted. This ensures
  robustness in the face of errors.

- **Simplified Task Management:** Actors manage asynchronous tasks using
  [`asyncio`][]. You can create and manage tasks within the actor, and the
  [`Actor`][frequenz.sdk.actor.Actor] class handles task cancellation and cleanup.

- **Simplified lifecycle management:** Actors are [async context managers] and also
  a [`run()`][frequenz.sdk.actor.run] function is provided to easily run a group of
  actors and wait for them to finish.

## Lifecycle


Actors are not started when they are created. There are 3 main ways to start an actor
(from most to least recommended):

1. By using the [`run()`][frequenz.sdk.actor.run] function.
2. By using the actor as an [async context manager].
3. By using the [`start()`][frequenz.sdk.actor.Actor.start] method.

!!! warning

    1. If an actor raises an unhandled exception, it will be restarted automatically.

    2. Actors manage [`asyncio.Task`][] objects, so a reference to the actor object must
       be held for as long as the actor is expected to be running, otherwise its tasks
       will be cancelled and the actor will stop. For more information, please refer to
       the [Python `asyncio`
       documentation](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task).


### The `run()` Function

The [`run()`][frequenz.sdk.actor.run] function can start many actors at once and waits
for them to finish. If any of the actors is stopped with errors, the errors will be
logged.

???+ example

    ```python
    import asyncio
    from frequenz.sdk.actor import Actor, run

    class MyActor(Actor):
        async def _run(self) -> None:
            while True:
                print("Hello World!")
                await asyncio.sleep(1)

    await asyncio.run(MyActor()) # (1)!
    ```

    1. This line will block until the actor completes its execution or is manually stopped.

### Async Context Manager

When an actor is used as an [async context manager], it is started when the
`async with` block is entered and stopped automatically when the block is exited
(even if an unhandled exception is raised).

???+ example

    ```python
    from frequenz.sdk.actor import Actor

    class MyActor(Actor):
        async def _run(self) -> None:
            print("Hello World!")

    async with MyActor() as actor: # (1)!
        print("The actor is running")
    # (2)!
    ```

    1. [`start()`][frequenz.sdk.actor.Actor.start] is called automatically when entering
        the `async with` block.
    2. [`stop()`][frequenz.sdk.actor.Actor.stop] is called automatically when exiting
        the `async with` block.

### The `start()` Method

When using the [`start()`][frequenz.sdk.actor.Actor.start] method, the actor is
started in the background and the method returns immediately. The actor will
continue to run until it is **manually** stopped or until it completes its execution.

???+ example

    ```python
    from frequenz.sdk.actor import Actor

    class MyActor(Actor):
        async def _run(self) -> None:
            print("Hello World!")

    actor = MyActor() # (1)!
    actor.start() # (2)!
    print("The actor is running") # (3)!
    await actor.stop() # (4)!
    ```

    1. The actor is created but not started yet.
    2. The actor is started manually, it keeps running in the background.
    3. !!! danger

        If this function would raise an exception, the actor will never be stopped!

    4. Until the actor is stopped manually.

!!! warning

    This method is not recommended because it is easy to forget to stop the actor
    manually, especially in error conditions.

## Communication

The [`Actor`][frequenz.sdk.actor.Actor] class doesn't impose any specific way to
communicate between actors. However, [`frequenz.channels`][] are always used as the
communication mechanism between actors in the SDK.

## Implementing an Actor

To implement an actor, you must inherit from the [`Actor`][frequenz.sdk.actor.Actor]
class and implement an *initializer* and the abstract [`_run()`][_run] method.

### Initialization

The [initializer][object.__init__] is called when the actor is created. The
[`Actor`][frequenz.sdk.actor.Actor] class initializer
([`__init__`][frequenz.sdk.actor.Actor.__init__]) should be always called first in the
class we are implementing to make sure actors are properly initialized.

The [`Actor.__init__()`][frequenz.sdk.actor.Actor.__init__] takes one optional argument,
a [`name`][frequenz.sdk.actor.Actor.name] that will be used to identify the actor in
logs. If no name is provided, a default name will be generated, but it is recommended
that [`Actor`][frequenz.sdk.actor.Actor] subclasses can also receive a name as an
argument to make it easier to identify individual instances in logs more easily.

The actor initializer normally also accepts as arguments the input channels receivers
and output channels senders that will be used for communication. These channels should
be created outside the actor and passed to it as arguments to ensure actors can be
composed easily.

???+ example "Example echo actor"

    ```python
    from frequenz.channels import Receiver, Sender
    from frequenz.sdk.actor import Actor

    class EchoActor(Actor):  # (1)!
        def __init__(
                self,
                receiver: Receiver[int],  # (2)!
                output: Sender[int],  # (3)!
                name: str | None = None,  # (4)!
        ) -> None:
            super().__init__(name=name) # (5)!
            self._input: Receiver[int] = receiver  # (6)!
            self._output: Sender[int] = output  # (7)!
    ```

    1. We define a new actor class called `EchoActor` that inherits from
        [`Actor`][frequenz.sdk.actor.Actor].

    2. We accept an `receiver` argument that will be used to receive messages from
        a channel.
    3. We accept an `output` argument that will be used to send messages to a channel.
    4. We accept an optional `name` argument that will be used to identify the actor in
        logs.
    5. We call [`Actor.__init__()`][frequenz.sdk.actor.Actor.__init__] to make sure the
        actor is properly initialized.
    6. We store the `receiver` argument in a *private* attribute to use it later.
    7. We store the `output` argument in a *private* attribute to use it later.

### The `_run()` Method

The abstract `_run()` method is called automatically by the base class when the actor is
[started][frequenz.sdk.actor.Actor.start].

Normally an actor should run forever (or until it is
[stopped][frequenz.sdk.actor.Actor.stop]), so it is very common to have an infinite loop
in the `_run()` method, typically receiving messages from one or more channels
([receivers][frequenz.channels.Receiver]), processing them and sending the results to
other channels ([senders][frequenz.channels.Sender]).
However, it is worth noting that an actor can also be designed for a one-time execution
or a limited number of runs, terminating upon completion.

???+ example "Example echo actor"

    ```python
    from frequenz.channels import Receiver, Sender
    from frequenz.sdk.actor import Actor

    class EchoActor(Actor):
        def __init__(
                self,
                receiver: Receiver[int],
                output: Sender[int],
                name: str | None = None,
        ) -> None:
            super().__init__(name=name)
            self._input: Receiver[int] = receiver
            self._output: Sender[int] = output

        async def _run(self) -> None:  # (1)!
            async for msg in self._input:  # (2)!
                await self._output.send(msg)  # (3)!
    ```

    1. We implement the abstract [`_run()`][_run] method.
    2. We receive messages from the `receiver` one by one.
    3. We send the received message to the `output` channel.

### Stopping

By default, the [`stop()`][frequenz.sdk.actor.Actor.stop] method will call the
[`cancel()`][frequenz.sdk.actor.Actor.cancel] method (which will cancel all the tasks
created by the actor) and will wait for them to finish.

This means that when an actor is stopped, the [`_run()`][_run] method will receive
a [`CancelledError`][asyncio.CancelledError] exception. You should have this in mind
when implementing your actor and make sure to handle this exception properly if you need
to do any cleanup.

The actor will handle the [`CancelledError`][asyncio.CancelledError] exception
automatically if it is not handled in the [`_run()`][_run] method, so if there is no
need for extra cleanup, you don't need to worry about it.

If an unhandled exception is raised in the [`_run()`][_run] method, the actor will
re-run the [`_run()`][_run] method automatically. This ensures robustness in the face of
errors, but you should also have this in mind if you need to do any cleanup to make sure
the re-run doesn't cause any problems.

???+ tip

    You could implement your own [`stop()`][frequenz.sdk.actor.Actor.stop] method to,
    for example, send a message to a channel to notify that the actor should stop, or
    any other more graceful way to stop the actor if you need to make sure it can't be
    interrupted at any `await` point.

### Spawning Extra Tasks

Actors run at least one background task, created automatically by the
[`Actor`][frequenz.sdk.actor.Actor] class. But [`Actor`][frequenz.sdk.actor.Actor]
inherits from [`BackgroundService`][frequenz.sdk.actor.BackgroundService], which
provides a few methods to create and manage extra tasks.

If your actor needs to spawn extra tasks, you can use
[`BackgroundService`][frequenz.sdk.actor.BackgroundService] facilities to manage the
tasks, so they are also automatically stopped when the actor is stopped.

All you need to do is add the newly spawned tasks to the actor's
[`tasks`][frequenz.sdk.actor.Actor.tasks] set.

???+ example

    ```python
    import asyncio
    from frequenz.sdk.actor import Actor

    class MyActor(Actor):
        async def _run(self) -> None:
            extra_task = asyncio.create_task(self._extra_task())  # (1)!
            self.tasks.add(extra_task)  # (2)!
            while True:  # (3)!
                print("_run() running")
                await asyncio.sleep(1)

        async def _extra_task(self) -> None:
            while True:  # (4)!
                print("_extra_task() running")
                await asyncio.sleep(1.1)

    async with MyActor() as actor:  # (5)!
        await asyncio.sleep(3)  # (6)!
    # (7)!
    ```

    1. We create a new task using [`asyncio.create_task()`][asyncio.create_task].
    2. We add the task to the actor's [`tasks`][frequenz.sdk.actor.Actor.tasks] set.
        This ensures the task will be cancelled and cleaned up when the actor is stopped.
    3. We leave the actor running forever.
    4. The extra task will also run forever.
    5. The actor is started.
    6. We wait for 3 seconds, the actor should print a bunch of "_run() running" and
        "_extra_task() running" messages while it's running.
    7. The actor is stopped and the extra task is cancelled automatically.

### Examples

Here are a few simple but complete examples to demonstrate how to create actors and
connect them using [channels][frequenz.channels].

!!! tip

    The code examples are annotated with markers (like {{code_annotation_marker}}), you
    can click on them to see the step-by-step explanation of what's going on. The
    annotations are numbered according to the order of execution.

#### Composing actors

This example shows how to create two actors and connect them using
[broadcast][frequenz.channels.Broadcast] channels.

```python title="compose.py"
import asyncio

from frequenz.channels import Broadcast, Receiver, Sender
from frequenz.sdk.actor import Actor

class Actor1(Actor):  # (1)!
    def __init__(
        self,
        receiver: Receiver[str],
        output: Sender[str],
        name: str | None = None,
    ) -> None:
        super().__init__(name=name)
        self._receiver = receiver
        self._output = output

    async def _run(self) -> None:
        async for msg in self._receiver:
            await self._output.send(f"Actor1 forwarding: {msg!r}")  # (8)!


class Actor2(Actor):
    def __init__(
        self,
        receiver: Receiver[str],
        output: Sender[str],
        name: str | None = None,
    ) -> None:
        super().__init__(name=name)
        self._receiver = receiver
        self._output = output

    async def _run(self) -> None:
        async for msg in self._receiver:
            await self._output.send(f"Actor2 forwarding: {msg!r}")  # (9)!


async def main() -> None:  # (2)!
    # (4)!
    input_channel: Broadcast[str] = Broadcast("Input to Actor1")
    middle_channel: Broadcast[str] = Broadcast("Actor1 -> Actor2 stream")
    output_channel: Broadcast[str] = Broadcast("Actor2 output")

    input_sender = input_channel.new_sender()
    output_receiver = output_channel.new_receiver()

    async with (  # (5)!
        Actor1(input_channel.new_receiver(), middle_channel.new_sender(), "actor1"),
        Actor2(middle_channel.new_receiver(), output_channel.new_sender(), "actor2"),
    ):
        await input_sender.send("Hello")  # (6)!
        msg = await output_receiver.receive()  # (7)!
        print(msg)  # (10)!
    # (11)!

if __name__ == "__main__":  # (3)!
    asyncio.run(main())
```

1. We define 2 actors: `Actor1` and `Actor2` that will just forward a message
   from an input channel to an output channel, adding some text.

2. We define an async `main()` function within the main logic of our [asyncio][] program.

3. We start the `main()` function in the async loop using [`asyncio.run()`][asyncio.run].

4. We create a bunch of [broadcast][frequenz.channels.Broadcast]
   [channels][frequenz.channels] to connect our actors.

    * `input_channel` is the input channel for `Actor1`.
    * `middle_channel` is the channel that connects `Actor1` and `Actor2`.
    * `output_channel` is the output channel for `Actor2`.

5. We create two actors and use them as async context managers, `Actor1` and
    `Actor2`, and connect them by creating new
    [senders][frequenz.channels.Sender] and
    [receivers][frequenz.channels.Receiver] from the channels.

    !!! note

        We don't use the [`run()`][frequenz.sdk.actor.run] function here because we
        want to stop the actors when we are done with them, but the actors will run
        forever (as long as the channel is not closed). So the async context manager
        is a better fit for this example.

6. We schedule the [sending][frequenz.channels.Sender.send] of the message
   `Hello` to `Actor1` via `input_channel`.

7. We [receive][frequenz.channels.Receiver.receive] (await) the response from
   `Actor2` via `output_channel`. Between this and the previous steps the
   `async` calls in the actors will be executed.

8. `Actor1` sends the re-formatted message (`Actor1 forwarding: Hello`) to
   `Actor2` via the `middle_channel`.

9. `Actor2` sends the re-formatted message (`Actor2 forwarding: "Actor1
   forwarding: 'Hello'"`) to the `output_channel`.

10. Finally, we print the received message, which will still be `Actor2
    forwarding: "Actor1 forwarding: 'Hello'"`.

11. The actors are stopped and cleaned up automatically when the `async with`
    block ends.

The expected output is:

```
Actor2 forwarding: "Actor1 forwarding: 'Hello'"
```

#### Receiving from multiple channels

This example shows how to create an actor that receives messages from multiple
[broadcast][frequenz.channels.Broadcast] channels using
[`select()`][frequenz.channels.select].

```python title="select.py"
import asyncio

from frequenz.channels import Broadcast, Receiver, Sender
from frequenz.channels.util import select, selected_from
from frequenz.sdk.actor import Actor, run


class EchoActor(Actor):  # (1)!
    def __init__(
        self,
        receiver_1: Receiver[bool],
        receiver_2: Receiver[bool],
        output: Sender[bool],
        name: str | None = None,
    ) -> None:
        super().__init__(name=name)
        self._receiver_1 = receiver_1
        self._receiver_2 = receiver_2
        self._output = output

    async def _run(self) -> None:  # (2)!
        async for selected in select(self._receiver_1, self._receiver_2):  # (10)!
            if selected_from(selected, self._receiver_1):  # (11)!
                print(f"Received from receiver_1: {selected.value}")
                await self._output.send(selected.value)
                if not selected.value:  # (12)!
                    break
            elif selected_from(selected, self._receiver_2):  # (13)!
                print(f"Received from receiver_2: {selected.value}")
                await self._output.send(selected.value)
                if not selected.value:  # (14)!
                    break
            else:
                assert False, "Unknown selected channel"
        print("EchoActor finished")
    # (15)!


# (3)!
input_channel_1 = Broadcast[bool]("input_channel_1")
input_channel_2 = Broadcast[bool]("input_channel_2")
echo_channel = Broadcast[bool]("echo_channel")

echo_actor = EchoActor(  # (4)!
    input_channel_1.new_receiver(),
    input_channel_2.new_receiver(),
    echo_channel.new_sender(),
    "echo-actor",
)

echo_receiver = echo_channel.new_receiver()  # (5)!

async def main() -> None:  # (6)!
    # (8)!
    await input_channel_1.new_sender().send(True)
    await input_channel_2.new_sender().send(False)

    await run(echo_actor)  # (9)!

    await echo_channel.close()  # (16)!

    async for message in echo_receiver:  # (17)!
        print(f"Received {message=}")


if __name__ == "__main__":  # (7)!
    asyncio.run(main())
```

1. We define an `EchoActor` that receives messages from two channels and sends
    them to another channel.

2. We implement the [`_run()`][_run] method that will receive messages from the two
    channels using [`select()`][frequenz.channels.select] and send them to the
    output channel. The `run()` method will stop if a `False` message is received.

3. We create the channels that will be used with the actor.

4. We create the actor and connect it to the channels by creating new receivers and
    senders from the channels.

5. We create a receiver for the `echo_channel` to eventually receive the messages sent
    by the actor.

6. We define the `main()` function that will run the actor.

7. We start the `main()` function in the async loop using [`asyncio.run()`][asyncio.run].

8. We send a message to each of the input channels. These messages will be queued in
    the channels until they are consumed by the actor.

9. We start the actor and wait for it to finish using the
    [`run()`][frequenz.sdk.actor.run] function.

10. The [`select()`][frequenz.channels.select] function will get the first message
    available from the two channels. The order in which they will be handled is
    unknown, but in this example we assume that the first message will be from
    `input_channel_1` (`True`) and the second from `input_channel_1` (`False`).

11. The [`selected_from()`][frequenz.channels.selected_from] function will return
    `True` for the `input_channel_1` receiver. `selected.value` holds the received
    message, so `"Received from receiver_1: True"` will be printed and `True` will be
    sent to the `output` channel.

12. Since `selected.value` is `True`, the loop will continue, going back to the
    [`select()`][frequenz.channels.select] function.

13. The [`selected_from()`][frequenz.channels.selected_from] function will return
    `False` for the `input_channel_1` receiver and `True` for the `input_channel_2`
    receiver. The message stored in `selected.value` will now be `False`, so
    `"Received from receiver_2: False"` will be printed and `False` will be sent to the
    `output` channel.

14. Since `selected.value` is `False`, the loop will break.

15. The [`_run()`][_run] method will finish normally and the actor will be stopped, so
    the [`run()`][frequenz.sdk.actor.run] function will return.

16. We close the `echo_channel` to make sure the `echo_receiver` will stop receiving
    messages after all the queued messages are consumed (otherwise the step 17 will
    never end!).

17. We receive the messages sent by the actor to the `echo_channel` one by one and print
    them, it should print first `Received message=True` and then `Received
    message=False`.

The expected output is:

```
Received from receiver_1: True
Received from receiver_2: False
Received message=True
Received message=False
```

[async context manager]: https://docs.python.org/3/reference/datamodel.html#async-context-managers
[_run]: #the-_run-method
"""

from ..timeseries._resampling._config import ResamplerConfig
from ._actor import Actor
from ._background_service import BackgroundService
from ._run_utils import run

__all__ = [
    "Actor",
    "BackgroundService",
    "ResamplerConfig",
    "run",
]



================================================
FILE: src/frequenz/sdk/actor/_actor.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Actor model implementation."""

import abc
import asyncio
import logging
from datetime import timedelta

from .._internal._asyncio import is_loop_running
from ._background_service import BackgroundService

_logger = logging.getLogger(__name__)


class Actor(BackgroundService, abc.ABC):
    """A primitive unit of computation that runs autonomously.

    To implement an actor, subclasses must implement the
    [`_run()`][frequenz.sdk.actor--the-_run-method] method, which should run the actor's
    logic. The [`_run()`][frequenz.sdk.actor--the-_run-method] method is called by the
    base class when the actor is started, and is expected to run until the actor is
    stopped.

    !!! info

        Please read the [`actor` module documentation][frequenz.sdk.actor] for more
        comprehensive guide on how to use and implement actors properly.
    """

    RESTART_DELAY: timedelta = timedelta(seconds=2)
    """The delay to wait between restarts of this actor."""

    _restart_limit: int | None = None
    """The number of times actors can be restarted when they are stopped by unhandled exceptions.

    If this is bigger than 0 or `None`, the actor will be restarted when there is an
    unhanded exception in the `_run()` method.

    If `None`, the actor will be restarted an unlimited number of times.

    !!! note

        This is mostly used for testing purposes and shouldn't be set in production.
    """

    def __init__(self, *, name: str | None = None) -> None:
        """Create actor instance.

        Args:
            name: The name of this background service.
        """
        super().__init__(name=name)
        self._is_cancelled = False

    def start(self) -> None:
        """Start this actor.

        If this actor is already running, this method does nothing.
        """
        if self.is_running:
            return

        self._is_cancelled = False
        self._tasks.clear()
        self._tasks.add(asyncio.create_task(self._run_loop()))

    @abc.abstractmethod
    async def _run(self) -> None:
        """Run this actor's logic."""

    async def _delay_if_restart(self, iteration: int) -> None:
        """Delay the restart of this actor's n'th iteration.

        Args:
            iteration: The current iteration of the restart.
        """
        # NB: I think it makes sense (in the future) to think about deminishing returns
        # the longer the actor has been running.
        # Not just for the restart-delay but actually for the n_restarts counter as well.
        if iteration > 0:
            delay = self.RESTART_DELAY.total_seconds()
            _logger.info("Actor %s: Waiting %s seconds...", self, delay)
            await asyncio.sleep(delay)

    async def _run_loop(self) -> None:
        """Run the actor's task continuously, managing restarts, cancellation, and termination.

        This method handles the execution of the actor's task, including
        restarts for unhandled exceptions, cancellation, or normal termination.

        Raises:
            asyncio.CancelledError: If the actor's `_run()` method is cancelled.
            Exception: If the actor's `_run()` method raises any other exception.
            BaseException: If the actor's `_run()` method raises any base exception.
        """
        _logger.info("Actor %s: Started.", self)
        n_restarts = 0
        while True:
            try:
                await self._delay_if_restart(n_restarts)
                await self._run()
                _logger.info("Actor %s: _run() returned without error.", self)
            except asyncio.CancelledError:
                _logger.info("Actor %s: Cancelled.", self)
                raise
            except Exception:  # pylint: disable=broad-except
                if not is_loop_running():
                    _logger.exception(
                        "Something went wrong, no running event loop,"
                        " not trying to restart %s again.",
                        self,
                    )
                    raise

                if self._is_cancelled:
                    # If actor was cancelled, but any tasks have failed with an exception
                    # other than asyncio.CancelledError, those exceptions are combined
                    # in an ExceptionGroup or BaseExceptionGroup.
                    # We have to handle that case separately to stop actor instead
                    # of restarting it.
                    _logger.exception(
                        "Actor %s: Raised an unhandled exception during stop.", self
                    )
                    break

                _logger.exception("Actor %s: Raised an unhandled exception.", self)
                limit_str = "∞" if self._restart_limit is None else self._restart_limit
                limit_str = f"({n_restarts}/{limit_str})"
                if self._restart_limit is None or n_restarts < self._restart_limit:
                    n_restarts += 1
                    _logger.info("Actor %s: Restarting %s...", self, limit_str)
                    continue
                _logger.info(
                    "Actor %s: Maximum restarts attempted %s, bailing out...",
                    self,
                    limit_str,
                )
                raise
            except BaseException:  # pylint: disable=broad-except
                _logger.exception("Actor %s: Raised a BaseException.", self)
                raise
            break

        _logger.info("Actor %s: Stopped.", self)

    def cancel(self, msg: str | None = None) -> None:
        """Cancel actor.

        Cancelled actor can't be started again.

        Args:
            msg: The message to be passed to the tasks being cancelled.
        """
        self._is_cancelled = True
        return super().cancel(msg)



================================================
FILE: src/frequenz/sdk/actor/_background_service.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Background service implementation."""

import abc
import asyncio
import collections.abc
from types import TracebackType
from typing import Any, Self


class BackgroundService(abc.ABC):
    """A background service that can be started and stopped.

    A background service is a service that runs in the background spawning one or more
    tasks. The service can be [started][frequenz.sdk.actor.BackgroundService.start]
    and [stopped][frequenz.sdk.actor.BackgroundService.stop] and can work as an
    async context manager to provide deterministic cleanup.

    To implement a background service, subclasses must implement the
    [`start()`][frequenz.sdk.actor.BackgroundService.start] method, which should
    start the background tasks needed by the service, and add them to the `_tasks`
    protected attribute.

    If you need to collect results or handle exceptions of the tasks when stopping the
    service, then you need to also override the
    [`stop()`][frequenz.sdk.actor.BackgroundService.stop] method, as the base
    implementation does not collect any results and re-raises all exceptions.

    !!! warning

        As background services manage [`asyncio.Task`][] objects, a reference to them
        must be held for as long as the background service is expected to be running,
        otherwise its tasks will be cancelled and the service will stop. For more
        information, please refer to the [Python `asyncio`
        documentation](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task).

    Example: Simple background service example
        ```python
        from typing_extensions import override
        import datetime
        import asyncio

        class Clock(BackgroundService):
            def __init__(self, resolution_s: float, *, name: str | None = None) -> None:
                super().__init__(name=name)
                self._resolution_s = resolution_s

            @override
            def start(self) -> None:
                self._tasks.add(asyncio.create_task(self._tick()))

            async def _tick(self) -> None:
                while True:
                    await asyncio.sleep(self._resolution_s)
                    print(datetime.datetime.now())

        async def main() -> None:
            # As an async context manager
            async with Clock(resolution_s=1):
                await asyncio.sleep(5)

            # Manual start/stop (only use if necessary, as cleanup is more complicated)
            clock = Clock(resolution_s=1)
            clock.start()
            await asyncio.sleep(5)
            await clock.stop()

        asyncio.run(main())
        ```

    Example: Background service example using custom stopping logic
        If you need to implement custom stopping logic, you can override the
        [`cancel()`][frequenz.sdk.actor.BackgroundService.cancel] and
        [`wait()`][frequenz.sdk.actor.BackgroundService.wait] methods, and the
        [`is_running`][frequenz.sdk.actor.BackgroundService.is_running] property.

        For example, if you are using an external library that uses tasks internally and
        you don't have access to them.

        ```python
        from typing_extensions import override
        import asyncio

        class SomeService(BackgroundService):
            def __init__(self, somelib, *, name: str | None = None) -> None:
                self.somelib = somelib
                super().__init__(name=name)

            @override
            def start(self) -> None:
                self.somelib.start()

            @property
            @override
            def is_running(self) -> bool:
                return self.somelib.is_running()

            @override
            def cancel(self, msg: str | None = None) -> None:
                self.somelib.cancel()

            @override
            async def wait(self) -> None:
                try:
                    await self.somelib.wait()
                except BaseExceptionGroup as exc:
                    raise BaseExceptionGroup("Error while stopping SomeService", [exc]) from exc
        ```
    """

    def __init__(self, *, name: str | None = None) -> None:
        """Initialize this BackgroundService.

        Args:
            name: The name of this background service. If `None`, `str(id(self))` will
                be used. This is used mostly for debugging purposes.
        """
        self._name: str = str(id(self)) if name is None else name
        self._tasks: set[asyncio.Task[Any]] = set()

    @abc.abstractmethod
    def start(self) -> None:
        """Start this background service."""

    @property
    def name(self) -> str:
        """The name of this background service.

        Returns:
            The name of this background service.
        """
        return self._name

    @property
    def tasks(self) -> collections.abc.Set[asyncio.Task[Any]]:
        """Return the set of running tasks spawned by this background service.

        Users typically should not modify the tasks in the returned set and only use
        them for informational purposes.

        !!! danger

            Changing the returned tasks may lead to unexpected behavior, don't do it
            unless the class explicitly documents it is safe to do so.

        Returns:
            The set of running tasks spawned by this background service.
        """
        return self._tasks

    @property
    def is_running(self) -> bool:
        """Return whether this background service is running.

        A service is considered running when at least one task is running.

        Returns:
            Whether this background service is running.
        """
        return any(not task.done() for task in self._tasks)

    def cancel(self, msg: str | None = None) -> None:
        """Cancel all running tasks spawned by this background service.

        Args:
            msg: The message to be passed to the tasks being cancelled.
        """
        for task in self._tasks:
            task.cancel(msg)

    # We need the noqa because pydoclint can't figure out `rest` is
    # a `BaseExceptionGroup` instance.
    async def stop(self, msg: str | None = None) -> None:  # noqa: DOC503
        """Stop this background service.

        This method cancels all running tasks spawned by this service and waits for them
        to finish.

        Args:
            msg: The message to be passed to the tasks being cancelled.

        Raises:
            BaseExceptionGroup: If any of the tasks spawned by this service raised an
                exception.
        """
        self.cancel(msg)
        try:
            await self.wait()
        except BaseExceptionGroup as exc_group:
            # We want to ignore CancelledError here as we explicitly cancelled all the
            # tasks.
            _, rest = exc_group.split(asyncio.CancelledError)
            if rest is not None:
                # We are filtering out from an exception group, we really don't want to
                # add the exceptions we just filtered by adding a from clause here.
                raise rest  # pylint: disable=raise-missing-from

    async def __aenter__(self) -> Self:
        """Enter an async context.

        Start this background service.

        Returns:
            This background service.
        """
        self.start()
        return self

    async def __aexit__(
        self,
        exc_type: type[BaseException] | None,
        exc_val: BaseException | None,
        exc_tb: TracebackType | None,
    ) -> None:
        """Exit an async context.

        Stop this background service.

        Args:
            exc_type: The type of the exception raised, if any.
            exc_val: The exception raised, if any.
            exc_tb: The traceback of the exception raised, if any.
        """
        await self.stop()

    async def wait(self) -> None:
        """Wait this background service to finish.

        Wait until all background service tasks are finished.

        Raises:
            BaseExceptionGroup: If any of the tasks spawned by this service raised an
                exception (`CancelError` is not considered an error and not returned in
                the exception group).
        """
        # We need to account for tasks that were created between when we started
        # awaiting and we finished awaiting.
        while self._tasks:
            done, pending = await asyncio.wait(self._tasks)
            assert not pending

            # We remove the done tasks, but there might be new ones created after we
            # started waiting.
            self._tasks = self._tasks - done

            exceptions: list[BaseException] = []
            for task in done:
                try:
                    # This will raise a CancelledError if the task was cancelled or any
                    # other exception if the task raised one.
                    _ = task.result()
                except BaseException as error:  # pylint: disable=broad-except
                    exceptions.append(error)
            if exceptions:
                raise BaseExceptionGroup(
                    f"Error while stopping background service {self}", exceptions
                )

    def __await__(self) -> collections.abc.Generator[None, None, None]:
        """Await this background service.

        An awaited background service will wait for all its tasks to finish.

        Returns:
            An implementation-specific generator for the awaitable.
        """
        return self.wait().__await__()

    def __repr__(self) -> str:
        """Return a string representation of this instance.

        Returns:
            A string representation of this instance.
        """
        return f"{type(self).__name__}(name={self._name!r}, tasks={self._tasks!r})"

    def __str__(self) -> str:
        """Return a string representation of this instance.

        Returns:
            A string representation of this instance.
        """
        return f"{type(self).__name__}[{self._name}]"



================================================
FILE: src/frequenz/sdk/actor/_run_utils.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Utility functions to run and synchronize the execution of actors."""


import asyncio
import logging

from ._actor import Actor

_logger = logging.getLogger(__name__)


async def run(*actors: Actor) -> None:
    """Await the completion of all actors.

    !!! info

        Please read the [`actor` module documentation][frequenz.sdk.actor] for more
        comprehensive guide on how to use and implement actors properly.

    Args:
        *actors: the actors to be awaited.
    """
    _logger.info("Starting %s actor(s)...", len(actors))

    for actor in actors:
        if actor.is_running:
            _logger.info("Actor %s: Already running, skipping start.", actor)
        else:
            _logger.info("Actor %s: Starting...", actor)
            actor.start()

    # Wait until all actors are done
    pending_tasks = {asyncio.create_task(a.wait(), name=str(a)) for a in actors}
    while pending_tasks:
        done_tasks, pending_tasks = await asyncio.wait(
            pending_tasks, return_when=asyncio.FIRST_COMPLETED
        )
        # This should always be only one task, but we handle many for extra safety
        for task in done_tasks:
            # BackgroundService returns a BaseExceptionGroup containing multiple
            # exceptions. The 'task.result()' statement raises these exceptions,
            # and 'except*' is used to handle them as a group. If the task raises
            # multiple different exceptions, 'except*' will be invoked multiple times,
            # once for each exception group.
            try:
                task.result()
            except* asyncio.CancelledError:
                _logger.info("Actor %s: Cancelled while running.", task.get_name())
            except* Exception:  # pylint: disable=broad-exception-caught
                _logger.exception(
                    "Actor %s: Raised an exception while running.",
                    task.get_name(),
                )
            else:
                _logger.info("Actor %s: Finished normally.", task.get_name())

    _logger.info("All %s actor(s) finished.", len(actors))



================================================
FILE: src/frequenz/sdk/config/__init__.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Configuration management.

# Overview

To provide dynamic configurations to an application, you can use the
[`ConfigManager`][frequenz.sdk.config.ConfigManager] class. This class provides
a convenient interface to manage configurations from multiple config files and receive
updates when the configurations change.  Users can create a receiver to receive
configurations from the manager.

# Setup

To use the `ConfigManager`, you need to create an instance of it and pass the
paths to the configuration files. The configuration files must be in the TOML
format.

When specifying multiple files order matters, as the configuration will be read and
updated in the order of the paths, so the last path will override the configuration set
by the previous paths. Dict keys will be merged recursively, but other objects (like
lists) will be replaced by the value in the last path.

```python
from frequenz.sdk.config import ConfigManager

async with ConfigManager(["base-config.toml", "overrides.toml"]) as config_manager:
    ...
```

# Logging

The `ConfigManager` can also instantiate
a [`LoggingConfigUpdatingActor`][frequenz.sdk.config.LoggingConfigUpdatingActor] to
monitor logging configurations. This actor will listen for logging configuration changes
and update the logging configuration accordingly.

This feature is enabled by default using the key `logging` in the configuration file. To
disable it you can pass `logging_config_key=None` to the `ConfigManager`.

# Receiving configurations

To receive configurations, you can create a receiver using the [`new_receiver()`][
frequenz.sdk.config.ConfigManager.new_receiver] method. The receiver will receive
configurations from the manager for a particular key, and validate and load the
configurations to a dataclass using [`marshmallow_dataclass`][].

If the key is a sequence of strings, it will be treated as a nested key and the
receiver will receive the configuration under the nested key. For example
`["key", "subkey"]` will get only `config["key"]["subkey"]`.

Besides a configuration instance, the receiver can also receive exceptions if there are
errors loading the configuration (typically
a [`ValidationError`][marshmallow.ValidationError]), or `None` if there is no
configuration for the key.

The value under `key` must be another mapping, otherwise
a [`InvalidValueForKeyError`][frequenz.sdk.config.InvalidValueForKeyError] instance will
be sent to the receiver.

If there were any errors loading the configuration, the error will be logged too.

```python
from dataclasses import dataclass
from frequenz.sdk.config import ConfigManager

@dataclass(frozen=True, kw_only=True)
class AppConfig:
    test: int

async with ConfigManager("config.toml") as config_manager:
    receiver = config_manager.new_receiver("app", AppConfig)
    app_config = await receiver.receive()
    match app_config:
        case AppConfig(test=42):
            print("App configured with 42")
        case Exception() as error:
            print(f"Error loading configuration: {error}")
        case None:
            print("There is no configuration for the app key")
```

## Validation and loading

The configuration class used to create the configuration instance is expected to be
a [`dataclasses.dataclass`][], which is used to create a [`marshmallow.Schema`][] via
the [`marshmallow_dataclass.class_schema`][] function.

This means you can customize the schema derived from the configuration
dataclass using [`marshmallow_dataclass`][] to specify extra validation and
options via field metadata.

Customization can also be done via a `base_schema`. By default
[`BaseConfigSchema`][frequenz.sdk.config.BaseConfigSchema] is used to provide support
for some extra commonly used fields (like [quantities][frequenz.quantities]) and to
exclude unknown fields by default.

```python
import marshmallow.validate
from dataclasses import dataclass, field

@dataclass(frozen=True, kw_only=True)
class Config:
    test: int = field(
        metadata={"validate": marshmallow.validate.Range(min=0)},
    )
```

Additional arguments can be passed to [`marshmallow.Schema.load`][] using
the `marshmallow_load_kwargs` keyword arguments.

When [`marshmallow.EXCLUDE`][] is used, a warning will be logged if there are extra
fields in the configuration that are excluded. This is useful, for example, to catch
typos in the configuration file.

## Skipping superfluous updates

If there is a burst of configuration updates, the receiver will only receive the
last configuration, older configurations will be ignored.

If `skip_unchanged` is set to `True`, then a configuration that didn't change
compared to the last one received will be ignored and not sent to the receiver.
The comparison is done using the *raw* `dict` to determine if the configuration
has changed.

## Error handling

The value under `key` must be another mapping, otherwise an error
will be logged and a [`frequenz.sdk.config.InvalidValueForKeyError`][] instance
will be sent to the receiver.

Configurations that don't pass the validation will be logged as an error and
the [`ValidationError`][marshmallow.ValidationError] sent to the receiver.

Any other unexpected error raised during the configuration loading will be
logged as an error and the error instance sent to the receiver.

## Further customization

If you have special needs for receiving the configurations (for example validating using
`marshmallow` doesn't fit your needs), you can create a custom receiver using
[`config_channel.new_receiver()`][frequenz.sdk.config.ConfigManager.config_channel]
directly. Please bear in mind that this provides a low-level access to the whole config
in the file as a raw Python mapping.

# Recommended usage

Actors that need to be reconfigured should take a configuration manager and a key to
receive configurations updates, and instantiate the new receiver themselves. This allows
actors to have full control over how the configuration is loaded (for example providing
a custom base schema or marshmallow options).

Passing the key explicitly too allows application to structure the configuration in
whatever way is most convenient for the application.

Actors can use the [`wait_for_first()`][frequenz.sdk.config.wait_for_first] function to
wait for the first configuration to be received, and cache the configuration for later
use and in case the actor is restarted. If the configuration is not received after some
timeout, a [`asyncio.TimeoutError`][] will be raised (and if uncaught, the actor will
be automatically restarted after some delay).

Example: Actor that can run without a configuration (using a default configuration)
    ```python title="actor.py" hl_lines="18 34 42 62 64"
    import dataclasses
    import logging
    from collections.abc import Sequence
    from datetime import timedelta
    from typing import assert_never

    from frequenz.channels import select, selected_from
    from frequenz.channels.event import Event

    from frequenz.sdk.actor import Actor
    from frequenz.sdk.config import ConfigManager, wait_for_first

    _logger = logging.getLogger(__name__)

    @dataclasses.dataclass(frozen=True, kw_only=True)
    class MyActorConfig:
        some_config: timedelta = dataclasses.field(
            default=timedelta(seconds=42), # (1)!
            metadata={"metadata": {"description": "Some optional configuration"}},
        )

    class MyActor(Actor):
        def __init__(
            self,
            config_manager: ConfigManager,
            /,
            *,
            config_key: str | Sequence[str],
            name: str | None = None,
        ) -> None:
            super().__init__(name=name)
            self._config_manager = config_manager
            self._config_key = config_key
            self._config: MyActorConfig = MyActorConfig() # (2)!

        async def _run(self) -> None:
            config_receiver = self._config_manager.new_receiver(
                self._config_key, MyActorConfig
            )
            self._update_config(
                await wait_for_first(
                    config_receiver, receiver_name=str(self), allow_none=True # (3)!
                )
            )

            other_receiver = Event()

            async for selected in select(config_receiver, other_receiver):
                if selected_from(selected, config_receiver):
                    self._update_config(selected.message)
                elif selected_from(selected, other_receiver):
                    # Do something else
                    ...

        def _update_config(self, config_update: MyActorConfig | Exception | None) -> None:
            match config_update:
                case MyActorConfig() as config:
                    _logger.info("New configuration received, updating.")
                    self._reconfigure(config)
                case None:
                    _logger.info("Configuration was unset, resetting to the default")
                    self._reconfigure(MyActorConfig()) # (4)!
                case Exception():
                    _logger.info( # (5)!
                        "New configuration has errors, keeping the old configuration."
                    )
                case unexpected:
                    assert_never(unexpected)

        def _reconfigure(self, config: MyActorConfig) -> None:
            self._config = config
            # Do something with the new configuration
    ```

    1. This is different when the actor requires a configuration to run. Here, the
        config has a default value.
    2. This is different when the actor requires a configuration to run. Here, the actor
        can just instantiate a default configuration.
    3. This is different when the actor requires a configuration to run. Here, the actor
        can accept a `None` configuration.
    4. This is different when the actor requires a configuration to run. Here, the actor
        can reset to a default configuration.
    5. There is no need to log the error itself, the configuration manager will log it
        automatically.

Example: Actor that requires a configuration to run
    ```python title="actor.py" hl_lines="17 33 40 58 60"
    import dataclasses
    import logging
    from collections.abc import Sequence
    from datetime import timedelta
    from typing import assert_never

    from frequenz.channels import select, selected_from
    from frequenz.channels.event import Event

    from frequenz.sdk.actor import Actor
    from frequenz.sdk.config import ConfigManager, wait_for_first

    _logger = logging.getLogger(__name__)

    @dataclasses.dataclass(frozen=True, kw_only=True)
    class MyActorConfig:
        some_config: timedelta = dataclasses.field( # (1)!
            metadata={"metadata": {"description": "Some required configuration"}},
        )

    class MyActor(Actor):
        def __init__(
            self,
            config_manager: ConfigManager,
            /,
            *,
            config_key: str | Sequence[str],
            name: str | None = None,
        ) -> None:
            super().__init__(name=name)
            self._config_manager = config_manager
            self._config_key = config_key
            self._config: MyActorConfig # (2)!

        async def _run(self) -> None:
            config_receiver = self._config_manager.new_receiver(
                self._config_key, MyActorConfig
            )
            self._update_config(
                await wait_for_first(config_receiver, receiver_name=str(self)) # (3)!
            )

            other_receiver = Event()

            async for selected in select(config_receiver, other_receiver):
                if selected_from(selected, config_receiver):
                    self._update_config(selected.message)
                elif selected_from(selected, other_receiver):
                    # Do something else
                    ...

        def _update_config(self, config_update: MyActorConfig | Exception | None) -> None:
            match config_update:
                case MyActorConfig() as config:
                    _logger.info("New configuration received, updating.")
                    self._reconfigure(config)
                case None:
                    _logger.info("Configuration was unset, keeping the old configuration.") # (4)!
                case Exception():
                    _logger.info( # (5)!
                        "New configuration has errors, keeping the old configuration."
                    )
                case unexpected:
                    assert_never(unexpected)

        def _reconfigure(self, config: MyActorConfig) -> None:
            self._config = config
            # Do something with the new configuration
    ```

    1. This is different when the actor can use a default configuration. Here, the
        field is required, so there is no default configuration possible.
    2. This is different when the actor can use a default configuration. Here, the
       assignment of the configuration is delayed to the `_run()` method.
    3. This is different when the actor can use a default configuration. Here, the actor
        doesn't accept `None` as a valid configuration as it can't create a default
        configuration.
    4. This is different when the actor can use a default configuration. Here, the actor
        doesn't accept `None` as a valid configuration as it can't create a default
        configuration, so it needs to keep the old configuration.
    5. There is no need to log the error itself, the configuration manager will log it
        automatically.


Example: Application
    The pattern used by the application is very similar to the one used by actors. In
    this case the application requires a configuration to run, but if it could also use
    a default configuration, the changes would be the same as in the actor examples.

    ```python title="app.py" hl_lines="14"
    import asyncio
    import dataclasses
    import logging
    import pathlib
    from collections.abc import Sequence
    from datetime import timedelta
    from typing import Sequence, assert_never

    from frequenz.sdk.actor import Actor
    from frequenz.sdk.config import ConfigManager, wait_for_first

    _logger = logging.getLogger(__name__)

    class MyActor(Actor): # (1)!
        def __init__(
            self, config_manager: ConfigManager, /, *, config_key: str | Sequence[str]
        ) -> None:
            super().__init__()
            self._config_manager = config_manager
            self._config_key = config_key
        async def _run(self) -> None: ...

    @dataclasses.dataclass(frozen=True, kw_only=True)
    class AppConfig:
        enable_actor: bool = dataclasses.field(
            metadata={"metadata": {"description": "Whether to enable the actor"}},
        )

    class App:
        def __init__(self, *, config_paths: Sequence[pathlib.Path]):
            self._config_manager = ConfigManager(config_paths)
            self._config_receiver = self._config_manager.new_receiver("app", AppConfig)
            self._actor = MyActor(self._config_manager, config_key="actor")

        async def _update_config(self, config_update: AppConfig | Exception | None) -> None:
            match config_update:
                case AppConfig() as config:
                    _logger.info("New configuration received, updating.")
                    await self._reconfigure(config)
                case None:
                    _logger.info("Configuration was unset, keeping the old configuration.")
                case Exception():
                    _logger.info("New configuration has errors, keeping the old configuration.")
                case unexpected:
                    assert_never(unexpected)

        async def _reconfigure(self, config: AppConfig) -> None:
            if config.enable_actor:
                self._actor.start()
            else:
                await self._actor.stop()

        async def run(self) -> None:
            _logger.info("Starting App...")

            async with self._config_manager:
                await self._update_config(
                    await wait_for_first(self._config_receiver, receiver_name="app")
                )

                _logger.info("Waiting for configuration updates...")
                async for config_update in self._config_receiver:
                    await self._reconfigure(config_update)

    if __name__ == "__main__":
        asyncio.run(App(config_paths="config.toml").run())
    ```

    1. Look for the actor examples for a proper implementation of the actor.

    Example configuration file:

    ```toml title="config.toml"
    [app]
    enable_actor = true

    [actor]
    some_config = 10

    [logging.root_logger]
    level = "DEBUG"
    ```
"""

from ._base_schema import BaseConfigSchema
from ._logging_actor import (
    LoggerConfig,
    LoggingConfig,
    LoggingConfigUpdatingActor,
    RootLoggerConfig,
)
from ._manager import ConfigManager, InvalidValueForKeyError, wait_for_first
from ._managing_actor import ConfigManagingActor
from ._util import load_config

__all__ = [
    "BaseConfigSchema",
    "ConfigManager",
    "ConfigManagingActor",
    "InvalidValueForKeyError",
    "LoggerConfig",
    "LoggingConfig",
    "LoggingConfigUpdatingActor",
    "RootLoggerConfig",
    "load_config",
    "wait_for_first",
]



================================================
FILE: src/frequenz/sdk/config/_base_schema.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Base schema for configuration classes."""

import marshmallow
from frequenz.quantities.experimental.marshmallow import QuantitySchema


class BaseConfigSchema(QuantitySchema):
    """A base schema for configuration classes.

    This schema provides validation for quantities and ignores unknown fields by
    default.
    """

    class Meta:
        """Meta options for the schema."""

        unknown = marshmallow.EXCLUDE



================================================
FILE: src/frequenz/sdk/config/_logging_actor.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Read and update logging severity from config."""

import logging
from dataclasses import dataclass, field
from typing import Annotated, Sequence, assert_never

import marshmallow
import marshmallow.validate

from ..actor import Actor
from ._manager import ConfigManager, wait_for_first

_logger = logging.getLogger(__name__)

LogLevel = Annotated[
    str,
    marshmallow.fields.String(
        validate=marshmallow.validate.OneOf(choices=logging.getLevelNamesMapping())
    ),
]
"""A marshmallow field for validating log levels."""


@dataclass(frozen=True, kw_only=True)
class RootLoggerConfig:
    """A configuration for the root logger."""

    level: LogLevel = field(
        default="NOTSET",
        metadata={
            "metadata": {
                "description": "Log level for the logger. Uses standard logging levels."
            },
        },
    )
    """The log level for the root logger."""


@dataclass(frozen=True, kw_only=True)
class LoggerConfig(RootLoggerConfig):
    """A configuration for a logger."""

    name: str = field(
        metadata={
            "metadata": {
                "description": "The name of the logger that will be affected by this "
                "configuration."
            },
        },
    )


@dataclass(frozen=True, kw_only=True)
class LoggingConfig:
    """A configuration for the logging system."""

    root_logger: RootLoggerConfig = field(
        default_factory=lambda: RootLoggerConfig(level="INFO"),
        metadata={
            "metadata": {
                "description": "Default default configuration for all loggers.",
            },
        },
    )
    """The default log level."""

    loggers: dict[str, LoggerConfig] = field(
        default_factory=dict,
        metadata={
            "metadata": {
                "description": "Configuration for a logger (the key is the logger name)."
            },
        },
    )
    """The list of loggers configurations."""


class LoggingConfigUpdatingActor(Actor):
    """Actor that listens for logging configuration changes and sets them.

    Example:
        `config.toml` file:
        ```toml
        [logging.root_logger]
        level = "INFO"

        [logging.loggers.power_dist]
        name = "frequenz.sdk.actor.power_distributing"
        level = "DEBUG"

        [logging.loggers.chan]
        name = "frequenz.channels"
        level = "DEBUG"
        ```

        ```python
        import asyncio

        from frequenz.sdk.config import LoggingConfigUpdatingActor
        from frequenz.sdk.actor import run as run_actors

        async def run() -> None:
            config_manager: ConfigManager = ...
            await run_actors(LoggingConfigUpdatingActor(config_manager))

        asyncio.run(run())
        ```

        Now whenever the `config.toml` file is updated, the logging configuration
        will be updated as well.
    """

    # pylint: disable-next=too-many-arguments
    def __init__(
        self,
        config_manager: ConfigManager,
        /,
        *,
        config_key: str | Sequence[str] = "logging",
        log_datefmt: str = "%Y-%m-%dT%H:%M:%S%z",
        log_format: str = "%(asctime)s %(levelname)-8s %(name)s:%(lineno)s: %(message)s",
        name: str | None = None,
    ):
        """Initialize this instance.

        Args:
            config_manager: The configuration manager to use.
            config_key: The key to use to retrieve the configuration from the
                configuration manager.  If `None`, the whole configuration will be used.
            log_datefmt: Use the specified date/time format in logs.
            log_format: Use the specified format string in logs.
            name: The name of this actor. If `None`, `str(id(self))` will be used. This
                is used mostly for debugging purposes.

        Note:
            The `log_format` and `log_datefmt` parameters are used in a call to
            `logging.basicConfig()`. If logging has already been configured elsewhere
            in the application (through a previous `basicConfig()` call), then the format
            settings specified here will be ignored.
        """
        self._config_receiver = config_manager.new_receiver(
            config_key, LoggingConfig, base_schema=None
        )

        # Setup default configuration.
        # This ensures logging is configured even if actor fails to start or
        # if the configuration cannot be loaded.
        self._current_config: LoggingConfig = LoggingConfig()

        super().__init__(name=name)

        logging.basicConfig(
            format=log_format,
            datefmt=log_datefmt,
            level=logging.INFO,
        )
        _logger.info("Applying initial default logging configuration...")
        self._reconfigure(self._current_config)

    async def _run(self) -> None:
        """Listen for configuration changes and update logging."""
        self._reconfigure(
            await wait_for_first(
                self._config_receiver, receiver_name=str(self), allow_none=True
            )
        )
        async for config_update in self._config_receiver:
            self._reconfigure(config_update)

    def _reconfigure(self, config_update: LoggingConfig | Exception | None) -> None:
        """Update the logging configuration.

        Args:
            config_update: The new configuration, or an exception if there was an error
                parsing the configuration, or `None` if the configuration was unset.
        """
        match config_update:
            case LoggingConfig():
                _logger.info(
                    "New configuration received, updating logging configuration."
                )
                self._update_logging(config_update)
            case None:
                _logger.info(
                    "Configuration was unset, resetting to the default "
                    "logging configuration."
                )
                self._update_logging(LoggingConfig())
            case Exception():
                _logger.info(
                    "New configuration has errors, keeping the old logging "
                    "configuration."
                )
            case unexpected:
                assert_never(unexpected)

    def _update_logging(self, config: LoggingConfig) -> None:
        """Configure the logging level."""
        # If the logger is not in the new config, set it to NOTSET
        old_names = {old.name for old in self._current_config.loggers.values()}
        new_names = {new.name for new in config.loggers.values()}
        loggers_to_unset = old_names - new_names
        for logger_name in loggers_to_unset:
            _logger.debug("Unsetting log level for logger '%s'", logger_name)
            logging.getLogger(logger_name).setLevel(logging.NOTSET)

        self._current_config = config
        _logger.info(
            "Setting root logger level to '%s'", self._current_config.root_logger.level
        )
        logging.getLogger().setLevel(self._current_config.root_logger.level)

        # For each logger in the new config, set the log level
        for logger_config in self._current_config.loggers.values():
            _logger.info(
                "Setting log level for logger '%s' to '%s'",
                logger_config.name,
                logger_config.level,
            )
            logging.getLogger(logger_config.name).setLevel(logger_config.level)

        _logger.info("Logging config update completed.")



================================================
FILE: src/frequenz/sdk/config/_manager.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Management of configuration."""

import asyncio
import logging
import pathlib
from collections.abc import Mapping, Sequence
from dataclasses import is_dataclass
from datetime import timedelta
from typing import Any, Final, Literal, TypeGuard, overload

import marshmallow
from frequenz.channels import Broadcast, Receiver, ReceiverStoppedError
from frequenz.channels.experimental import WithPrevious
from marshmallow import Schema, ValidationError
from typing_extensions import override

from ..actor._background_service import BackgroundService
from ._base_schema import BaseConfigSchema
from ._managing_actor import ConfigManagingActor
from ._util import DataclassT, _validate_load_kwargs, load_config

_logger = logging.getLogger(__name__)


class InvalidValueForKeyError(ValueError):
    """An error indicating that the value under the specified key is invalid."""

    def __init__(self, msg: str, *, key: Sequence[str], value: Any) -> None:
        """Initialize this error.

        Args:
            msg: The error message.
            key: The key that has an invalid value.
            value: The actual value that was found that is not a mapping.
        """
        super().__init__(msg)

        self.key: Final[Sequence[str]] = key
        """The key that has an invalid value."""

        self.value: Final[Any] = value
        """The actual value that was found that is not a mapping."""


class ConfigManager(BackgroundService):
    """A manager for configuration files.

    This class reads configuration files and sends the configuration to the receivers,
    providing configuration key filtering and value validation.

    For a more in-depth introduction and examples, please read the [module
    documentation][frequenz.sdk.config].
    """

    def __init__(  # pylint: disable=too-many-arguments
        self,
        config_paths: str | pathlib.Path | Sequence[pathlib.Path | str],
        /,
        *,
        force_polling: bool = True,
        logging_config_key: str | Sequence[str] | None = "logging",
        name: str | None = None,
        polling_interval: timedelta = timedelta(seconds=1),
    ) -> None:
        """Initialize this config manager.

        Args:
            config_paths: The paths to the TOML files with the configuration. Order
                matters, as the configuration will be read and updated in the order
                of the paths, so the last path will override the configuration set by
                the previous paths. Dict keys will be merged recursively, but other
                objects (like lists) will be replaced by the value in the last path.
            force_polling: Whether to force file polling to check for changes.
            logging_config_key: The key to use for the logging configuration. If `None`,
                logging configuration will not be managed.  If a key is provided, the
                manager update the logging configuration whenever the configuration
                changes.
            name: A name to use when creating actors. If `None`, `str(id(self))` will
                be used. This is used mostly for debugging purposes.
            polling_interval: The interval to poll for changes. Only relevant if
                polling is enabled.
        """
        super().__init__(name=name)

        self.config_channel: Final[Broadcast[Mapping[str, Any]]] = Broadcast(
            name=f"{self}_config", resend_latest=True
        )
        """The channel used for sending configuration updates (resends the latest value).

        This is the channel used to communicate with the
        [`ConfigManagingActor`][frequenz.sdk.config.ConfigManager.config_actor] and will
        receive the complete raw configuration as a mapping.
        """

        self.config_actor: Final[ConfigManagingActor] = ConfigManagingActor(
            config_paths,
            self.config_channel.new_sender(),
            name=self.name,
            force_polling=force_polling,
            polling_interval=polling_interval,
        )
        """The actor that manages the configuration for this manager."""

        # pylint: disable-next=import-outside-toplevel,cyclic-import
        from ._logging_actor import LoggingConfigUpdatingActor

        self.logging_actor: Final[LoggingConfigUpdatingActor | None] = (
            None
            if logging_config_key is None
            else LoggingConfigUpdatingActor(
                self, config_key=logging_config_key, name=self.name
            )
        )
        """The actor that manages the logging configuration for this manager."""

    @override
    def start(self) -> None:
        """Start this config manager."""
        self.config_actor.start()
        if self.logging_actor:
            self.logging_actor.start()

    @property
    @override
    def is_running(self) -> bool:
        """Whether this config manager is running."""
        return self.config_actor.is_running or (
            self.logging_actor is not None and self.logging_actor.is_running
        )

    @override
    def cancel(self, msg: str | None = None) -> None:
        """Cancel all running tasks and actors spawned by this config manager.

        Args:
            msg: The message to be passed to the tasks being cancelled.
        """
        if self.logging_actor:
            self.logging_actor.cancel(msg)
        self.config_actor.cancel(msg)

    @override
    async def wait(self) -> None:
        """Wait this config manager to finish.

        Wait until all tasks and actors are finished.

        Raises:
            BaseExceptionGroup: If any of the tasks spawned by this service raised an
                exception (`CancelError` is not considered an error and not returned in
                the exception group).
        """
        exceptions: list[BaseException] = []
        if self.logging_actor:
            try:
                await self.logging_actor
            except BaseExceptionGroup as err:  # pylint: disable=try-except-raise
                exceptions.append(err)

        try:
            await self.config_actor
        except BaseExceptionGroup as err:  # pylint: disable=try-except-raise
            exceptions.append(err)

        if exceptions:
            raise BaseExceptionGroup(f"Error while stopping {self!r}", exceptions)

    @override
    def __repr__(self) -> str:
        """Return a string representation of this config manager."""
        logging_actor = (
            f"logging_actor={self.logging_actor!r}, " if self.logging_actor else ""
        )
        return (
            f"<{self.__class__.__name__}: "
            f"name={self.name!r}, "
            f"config_channel={self.config_channel!r}, "
            + logging_actor
            + f"config_actor={self.config_actor!r}>"
        )

    def new_receiver(  # pylint: disable=too-many-arguments
        self,
        # This is tricky, because a str is also a Sequence[str], if we would use only
        # Sequence[str], then a regular string would also be accepted and taken as
        # a sequence, like "key" -> ["k", "e", "y"]. We should never remove the str from
        # the allowed types without changing Sequence[str] to something more specific,
        # like list[str] or tuple[str] (but both have their own problems).
        key: str | Sequence[str],
        config_class: type[DataclassT],
        /,
        *,
        skip_unchanged: bool = True,
        base_schema: type[Schema] | None = BaseConfigSchema,
        marshmallow_load_kwargs: dict[str, Any] | None = None,
    ) -> Receiver[DataclassT | Exception | None]:
        """Create a new receiver for receiving the configuration for a particular key.

        This method has a lot of features and functionalities to make it easier to
        receive configurations, but it also imposes some restrictions on how the
        configurations are received. If you need more control over the configuration
        receiver, you can create a receiver directly using
        [`config_channel.new_receiver()`][frequenz.sdk.config.ConfigManager.config_channel].

        For a more in-depth introduction and examples, please read the [module
        documentation][frequenz.sdk.config].

        Args:
            key: The configuration key to be read by the receiver. If a sequence of
                strings is used, it is used as a sub-key.
            config_class: The class object to use to instantiate a configuration. The
                configuration will be validated against this type too using
                [`marshmallow_dataclass`][].
            skip_unchanged: Whether to skip sending the configuration if it hasn't
                changed compared to the last one received.
            base_schema: An optional class to be used as a base schema for the
                configuration class. This allow using custom fields for example. Will be
                passed to [`marshmallow_dataclass.class_schema`][].
            marshmallow_load_kwargs: Additional arguments to be passed to
                [`marshmallow.Schema.load`][].

        Returns:
            The receiver for the configuration.
        """
        _validate_load_kwargs(marshmallow_load_kwargs)

        # We disable warning on overflow, because we are only interested in the latest
        # configuration, it is completely fine to drop old configuration updates.
        receiver = self.config_channel.new_receiver(
            name=f"{self}:{key}", limit=1, warn_on_overflow=False
        ).map(
            lambda config: _load_config_with_logging_and_errors(
                config,
                config_class,
                key=key,
                base_schema=base_schema,
                marshmallow_load_kwargs=marshmallow_load_kwargs,
            )
        )

        if skip_unchanged:
            # For some reason the type argument for WithPrevious is not inferred
            # correctly, so we need to specify it explicitly.
            return receiver.filter(
                WithPrevious[DataclassT | Exception | None](
                    lambda old, new: _not_equal_with_logging(
                        key=key, old_value=old, new_value=new
                    )
                )
            )

        return receiver


@overload
async def wait_for_first(
    receiver: Receiver[DataclassT | Exception | None],
    /,
    *,
    receiver_name: str | None = None,
    allow_none: Literal[False] = False,
    timeout: timedelta = timedelta(minutes=1),
) -> DataclassT: ...


@overload
async def wait_for_first(
    receiver: Receiver[DataclassT | Exception | None],
    /,
    *,
    receiver_name: str | None = None,
    allow_none: Literal[True] = True,
    timeout: timedelta = timedelta(minutes=1),
) -> DataclassT | None: ...


async def wait_for_first(
    receiver: Receiver[DataclassT | Exception | None],
    /,
    *,
    receiver_name: str | None = None,
    allow_none: bool = False,
    timeout: timedelta = timedelta(minutes=1),
) -> DataclassT | None:
    """Wait for and receive the the first configuration.

    For a more in-depth introduction and examples, please read the [module
    documentation][frequenz.sdk.config].

    Args:
        receiver: The receiver to receive the first configuration from.
        receiver_name: The name of the receiver, used for logging. If `None`, the
            string representation of the receiver will be used.
        allow_none: Whether consider a `None` value as a valid configuration.
        timeout: The timeout in seconds to wait for the first configuration.

    Returns:
        The first configuration received.

    Raises:
        asyncio.TimeoutError: If the first configuration is not received within the
            timeout.
        ReceiverStoppedError: If the receiver is stopped before the first configuration
            is received.
    """
    if receiver_name is None:
        receiver_name = str(receiver)

    # We need this type guard because we can't use a TypeVar for isinstance checks or
    # match cases.
    def is_config_class(value: DataclassT | Exception | None) -> TypeGuard[DataclassT]:
        return is_dataclass(value) if value is not None else False

    _logger.info(
        "%s: Waiting %s seconds for the first configuration to arrive...",
        receiver_name,
        timeout.total_seconds(),
    )
    try:
        async with asyncio.timeout(timeout.total_seconds()):
            async for config in receiver:
                match config:
                    case None:
                        if allow_none:
                            return None
                        _logger.error(
                            "%s: Received empty configuration, waiting again for "
                            "a first configuration to be set.",
                            receiver_name,
                        )
                    case Exception() as error:
                        _logger.error(
                            "%s: Error while receiving the first configuration, "
                            "will keep waiting for an update: %s.",
                            receiver_name,
                            error,
                        )
                    case config if is_config_class(config):
                        _logger.info("%s: Received first configuration.", receiver_name)
                        return config
                    case unexpected:
                        assert (
                            False
                        ), f"{receiver_name}: Unexpected value received: {unexpected!r}."
    except asyncio.TimeoutError:
        _logger.error("%s: No configuration received in time.", receiver_name)
        raise
    raise ReceiverStoppedError(receiver)


def _not_equal_with_logging(
    *,
    key: str | Sequence[str],
    old_value: DataclassT | Exception | None,
    new_value: DataclassT | Exception | None,
) -> bool:
    """Return whether the two mappings are not equal, logging if they are the same."""
    if old_value == new_value:
        _logger.info("Configuration has not changed for key %r, skipping update.", key)
        return False

    if isinstance(new_value, InvalidValueForKeyError) and not isinstance(
        old_value, InvalidValueForKeyError
    ):
        subkey_str = ""
        if key != new_value.key:
            subkey_str = f"When looking for sub-key {key!r}: "
        _logger.error(
            "%sConfiguration for key %r has an invalid value: %r",
            subkey_str,
            new_value.key,
            new_value.value,
        )
    return True


def _load_config_with_logging_and_errors(
    config: Mapping[str, Any],
    config_class: type[DataclassT],
    *,
    key: str | Sequence[str],
    base_schema: type[Schema] | None = None,
    marshmallow_load_kwargs: dict[str, Any] | None = None,
) -> DataclassT | Exception | None:
    """Load the configuration for the specified key, logging errors and returning them."""
    try:
        sub_config = _get_key(config, key)
        if sub_config is None:
            _logger.debug("Configuration key %r not found, sending None", key)
            return None

        loaded_config = _load_config(
            sub_config,
            config_class,
            key=key,
            base_schema=base_schema,
            marshmallow_load_kwargs=marshmallow_load_kwargs,
        )
        _logger.debug("Received new configuration: %s", loaded_config)
        return loaded_config
    except InvalidValueForKeyError as error:
        if len(key) > 1 and key != error.key:
            _logger.error("Error when looking for sub-key %r: %s", key, error)
        else:
            _logger.error(str(error))
        return error
    except ValidationError as error:
        _logger.error("The configuration for key %r is invalid: %s", key, error)
        return error
    except Exception as error:  # pylint: disable=broad-except
        _logger.exception(
            "An unexpected error occurred while loading the configuration for key %r: %s",
            key,
            error,
        )
        return error


def _get_key(
    config: Mapping[str, Any],
    # This is tricky, because a str is also a Sequence[str], if we would use only
    # Sequence[str], then a regular string would also be accepted and taken as
    # a sequence, like "key" -> ["k", "e", "y"]. We should never remove the str from
    # the allowed types without changing Sequence[str] to something more specific,
    # like list[str] or tuple[str].
    key: str | Sequence[str],
) -> Mapping[str, Any] | None:
    """Get the value from the configuration under the specified key.

    Args:
        config: The configuration to get the value from.
        key: The key to get the value for.

    Returns:
        The value under the key, or `None` if the key is not found.

    Raises:
        InvalidValueForKeyError: If the value under the key is not a mapping.
    """
    # We first normalize to a Sequence[str] to make it easier to work with.
    if isinstance(key, str):
        key = (key,)
    value = config
    current_path = []
    for subkey in key:
        current_path.append(subkey)
        if value is None:
            return None
        match value.get(subkey):
            case None:
                return None
            case Mapping() as new_value:
                value = new_value
            case invalid_value:
                raise InvalidValueForKeyError(
                    f"Value for key {current_path!r} is not a mapping: {invalid_value!r}",
                    key=current_path,
                    value=invalid_value,
                )
        value = new_value
    return value


def _load_config(
    config: Mapping[str, Any],
    config_class: type[DataclassT],
    *,
    key: str | Sequence[str],
    base_schema: type[Schema] | None = BaseConfigSchema,
    marshmallow_load_kwargs: dict[str, Any] | None = None,
) -> DataclassT | InvalidValueForKeyError | ValidationError | None:
    """Try to load a configuration and log any validation errors."""
    loaded_config = load_config(
        config_class,
        config,
        base_schema=base_schema,
        marshmallow_load_kwargs=marshmallow_load_kwargs,
    )

    marshmallow_load_kwargs = (
        {} if marshmallow_load_kwargs is None else marshmallow_load_kwargs.copy()
    )

    # When excluding unknown fields we still want to notify the user, as
    # this could mean there is a typo in the configuration and some value is
    # not being loaded as desired.
    marshmallow_load_kwargs["unknown"] = marshmallow.RAISE
    try:
        load_config(
            config_class,
            config,
            base_schema=base_schema,
            marshmallow_load_kwargs=marshmallow_load_kwargs,
        )
    except ValidationError as err:
        _logger.warning(
            "The configuration for key %r has extra fields that will be ignored: %s",
            key,
            err,
        )

    return loaded_config



================================================
FILE: src/frequenz/sdk/config/_managing_actor.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Read and update config variables."""

import logging
import pathlib
import tomllib
from collections import abc
from collections.abc import Mapping, MutableMapping
from datetime import timedelta
from typing import Any, assert_never

from frequenz.channels import Sender
from frequenz.channels.file_watcher import EventType, FileWatcher

from ..actor._actor import Actor

_logger = logging.getLogger(__name__)


class ConfigManagingActor(Actor):
    """An actor that monitors a TOML configuration files for updates.

    When the actor is started the configuration files will be read and sent to the
    output sender. Then the actor will start monitoring the files for updates. If any
    file is updated, all the configuration files will be re-read and sent to the output
    sender.

    If no configuration file could be read, the actor will raise an exception.

    The configuration files are read in the order of the paths, so the last path will
    override the configuration set by the previous paths. Dict keys will be merged
    recursively, but other objects (like lists) will be replaced by the value in the
    last path.

    Example:
        If `config1.toml` contains:

        ```toml
        var1 = [1, 2]
        var2 = 2
        [section]
        var3 = [1, 3]
        ```

        And `config2.toml` contains:

        ```toml
        var2 = "hello" # Can override with a different type too
        var3 = 4
        [section]
        var3 = 5
        var4 = 5
        ```

        Then the final configuration will be:

        ```py
        {
            "var1": [1, 2],
            "var2": "hello",
            "var3": 4,
            "section": {
                "var3": 5,
                "var4": 5,
            },
        }
        ```
    """

    # pylint: disable-next=too-many-arguments
    def __init__(
        self,
        config_paths: str | pathlib.Path | abc.Sequence[pathlib.Path | str],
        output: Sender[abc.Mapping[str, Any]],
        *,
        name: str | None = None,
        force_polling: bool = True,
        polling_interval: timedelta = timedelta(seconds=1),
    ) -> None:
        """Initialize this instance.

        Args:
            config_paths: The paths to the TOML files with the configuration. Order
                matters, as the configuration will be read and updated in the order
                of the paths, so the last path will override the configuration set by
                the previous paths. Dict keys will be merged recursively, but other
                objects (like lists) will be replaced by the value in the last path.
            output: The sender to send the configuration to.
            name: The name of the actor. If `None`, `str(id(self))` will
                be used. This is used mostly for debugging purposes.
            force_polling: Whether to force file polling to check for changes.
            polling_interval: The interval to poll for changes. Only relevant if
                polling is enabled.

        Raises:
            ValueError: If no configuration path is provided.
        """
        super().__init__(name=name)
        match config_paths:
            case str():
                self._config_paths = [pathlib.Path(config_paths)]
            case pathlib.Path():
                self._config_paths = [config_paths]
            case abc.Sequence() as seq if len(seq) == 0:
                raise ValueError("At least one config path is required.")
            case abc.Sequence():
                self._config_paths = [
                    (
                        config_path
                        if isinstance(config_path, pathlib.Path)
                        else pathlib.Path(config_path)
                    )
                    for config_path in config_paths
                ]
            case unexpected:
                assert_never(unexpected)
        self._output: Sender[abc.Mapping[str, Any]] = output
        self._force_polling: bool = force_polling
        self._polling_interval: timedelta = polling_interval

    def _read_config(self) -> abc.Mapping[str, Any] | None:
        """Read the contents of the configuration file.

        Returns:
            A dictionary containing configuration variables.
        """
        error_count = 0
        config: dict[str, Any] = {}

        for config_path in self._config_paths:
            _logger.info(
                "[%s] Reading configuration file %r...", self.name, str(config_path)
            )
            try:
                with config_path.open("rb") as toml_file:
                    data = tomllib.load(toml_file)
                    _logger.info(
                        "[%s] Configuration file %r read successfully.",
                        self.name,
                        str(config_path),
                    )
                    config = _recursive_update(config, data)
            except ValueError as err:
                _logger.error("[%s] Can't read config file, err: %s", self.name, err)
                error_count += 1
            except OSError as err:
                # It is ok for config file to don't exist.
                _logger.error(
                    "[%s] Error reading config file %r (%s). Ignoring it.",
                    self.name,
                    str(config_path),
                    err,
                )
                error_count += 1

        if error_count == len(self._config_paths):
            _logger.error(
                "[%s] Can't read any of the config files, ignoring config update.", self
            )
            return None

        _logger.info(
            "[%s] Read %s/%s configuration files successfully.",
            self.name,
            len(self._config_paths) - error_count,
            len(self._config_paths),
        )
        return config

    async def send_config(self) -> None:
        """Send the configuration to the output sender."""
        config = self._read_config()
        if config is not None:
            await self._output.send(config)

    async def _run(self) -> None:
        """Monitor for and send configuration file updates.

        At startup, the Config Manager sends the current config so that it
        can be cache in the Broadcast channel and served to receivers even if
        there hasn't been any change to the config file itself.
        """
        await self.send_config()

        parent_paths = {p.parent for p in self._config_paths}

        # FileWatcher can't watch for non-existing files, so we need to watch for the
        # parent directories instead just in case a configuration file doesn't exist yet
        # or it is deleted and recreated again.
        file_watcher = FileWatcher(
            paths=list(parent_paths),
            event_types={EventType.CREATE, EventType.MODIFY},
            force_polling=self._force_polling,
            polling_interval=self._polling_interval,
        )

        try:
            async for event in file_watcher:
                if not event.path.exists():
                    _logger.error(
                        "[%s] Received event %s, but the watched path %s doesn't exist.",
                        self.name,
                        event,
                        event.path,
                    )
                    continue
                # Since we are watching the whole parent directories, we need to make
                # sure we only react to events related to the configuration files we
                # are interested in.
                #
                # pathlib.Path.samefile raises error if any path doesn't exist so we need to
                # make sure the paths exists before calling it. This could happen as it is not
                # required that all config files exist, only one is required but we don't know
                # which.
                if not any(
                    event.path.samefile(p) for p in self._config_paths if p.exists()
                ):
                    continue

                match event.type:
                    case EventType.CREATE:
                        _logger.info(
                            "[%s] The configuration file %s was created, sending new config...",
                            self.name,
                            event.path,
                        )
                        await self.send_config()
                    case EventType.MODIFY:
                        _logger.info(
                            "[%s] The configuration file %s was modified, sending update...",
                            self.name,
                            event.path,
                        )
                        await self.send_config()
                    case EventType.DELETE:
                        _logger.error(
                            "[%s] Unexpected DELETE event for path %s. Please report this "
                            "issue to Frequenz.",
                            self.name,
                            event.path,
                        )
                    case _:
                        assert_never(event.type)
        finally:
            del file_watcher


def _recursive_update(
    target: dict[str, Any], overrides: Mapping[str, Any]
) -> dict[str, Any]:
    """Recursively updates dictionary d1 with values from dictionary d2.

    Args:
        target: The original dictionary to be updated.
        overrides: The dictionary with updates.

    Returns:
        The updated dictionary.
    """
    for key, value in overrides.items():
        if (
            key in target
            and isinstance(target[key], MutableMapping)
            and isinstance(value, MutableMapping)
        ):
            _recursive_update(target[key], value)
        else:
            target[key] = value
    return target



================================================
FILE: src/frequenz/sdk/config/_util.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Utilities to deal with configuration."""

from collections.abc import Mapping
from typing import Any, ClassVar, Protocol, TypeVar, cast

import marshmallow
from marshmallow import Schema
from marshmallow_dataclass import class_schema

from ._base_schema import BaseConfigSchema


# This is a hack that relies on identifying dataclasses by looking into an undocumented
# property of dataclasses[1], so it might break in the future. Nevertheless, it seems to
# be widely used in the community, for example `mypy` and `pyright` seem to rely on
# it[2].
#
# [1]: https://github.com/python/mypy/issues/15974#issuecomment-1694781006
# [2]: https://github.com/python/mypy/issues/15974#issuecomment-1694993493
class Dataclass(Protocol):
    """A protocol for dataclasses."""

    __dataclass_fields__: ClassVar[dict[str, Any]]
    """The fields of the dataclass."""


DataclassT = TypeVar("DataclassT", bound=Dataclass)
"""Type variable for configuration classes."""


def load_config(
    cls: type[DataclassT],
    config: Mapping[str, Any],
    /,
    *,
    base_schema: type[Schema] | None = BaseConfigSchema,
    marshmallow_load_kwargs: dict[str, Any] | None = None,
) -> DataclassT:
    """Load a configuration from a dictionary into an instance of a configuration class.

    The configuration class is expected to be a [`dataclasses.dataclass`][], which is
    used to create a [`marshmallow.Schema`][] schema to validate the configuration
    dictionary using [`marshmallow_dataclass.class_schema`][] (which in turn uses the
    [`marshmallow.Schema.load`][] method to do the validation and deserialization).

    To customize the schema derived from the configuration dataclass, you can use the
    `metadata` key in [`dataclasses.field`][] to pass extra options to
    [`marshmallow_dataclass`][] to be used during validation and deserialization.

    Additional arguments can be passed to [`marshmallow.Schema.load`][] using keyword
    arguments `marshmallow_load_kwargs`.

    Note:
        This method will raise [`marshmallow.ValidationError`][] if the configuration
        dictionary is invalid and you have to have in mind all of the gotchas of
        [`marshmallow`][] and [`marshmallow_dataclass`][] applies when using this
        function.  It is recommended to carefully read the documentation of these
        libraries.

    Args:
        cls: The configuration class.
        config: The configuration dictionary.
        base_schema: An optional class to be used as a base schema for the configuration
            class. This allow using custom fields for example. Will be passed to
            [`marshmallow_dataclass.class_schema`][].
        marshmallow_load_kwargs: Additional arguments to be passed to
            [`marshmallow.Schema.load`][].

    Returns:
        The loaded configuration as an instance of the configuration class.
    """
    _validate_load_kwargs(marshmallow_load_kwargs)

    instance = class_schema(cls, base_schema)().load(
        config, **(marshmallow_load_kwargs or {})
    )
    # We need to cast because `.load()` comes from marshmallow and doesn't know which
    # type is returned.
    return cast(DataclassT, instance)


def _validate_load_kwargs(marshmallow_load_kwargs: dict[str, Any] | None) -> None:
    """Validate the marshmallow load kwargs.

    This function validates the `unknown` option of the marshmallow load kwargs to
    prevent loading unknown fields when loading to a dataclass.

    Args:
        marshmallow_load_kwargs: The dictionary to get the marshmallow load kwargs from.

    Raises:
        ValueError: If the `unknown` option is set to [`marshmallow.INCLUDE`][].
    """
    if (
        marshmallow_load_kwargs
        and marshmallow_load_kwargs.get("unknown") == marshmallow.INCLUDE
    ):
        raise ValueError(
            "The 'unknown' option can't be 'INCLUDE' when loading to a dataclass"
        )



================================================
FILE: src/frequenz/sdk/microgrid/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""A {{glossary("microgrid")}} is a local electrical grid that connects a set of
electrical components together.  They are often built around a passive power consumer,
to supplement the electricity consumed from the {{glossary("grid", "public grid")}} with
on-site power generation or storage systems.

Microgrids can also function in {{glossary("island", "island-mode")}}, without a grid
connection, or without a local power consumer, but they have to have at least one of the
two, to be meaningful.

## Frequenz SDK Microgrid Model

The SDK aims to provide an abstract model of the microgrid that enables high-level
interactions with {{glossary("component", "microgrid components")}}, without having to
worry about (or even be aware of) location-specific details such as:

- where the {{glossary("meter", "meters")}} are placed,
- how many {{glossary("battery", "batteries")}},
- whether there's a grid connection or a passive consumer,
- what models the {{glossary("inverter", "inverters")}} are, etc.
- whether components are having downtimes, because {{glossary("metric", "metrics")}} and
  limits get adjusted automatically when components are having downtimes.

Users of the SDK can develop applications around this interface once and deploy
anywhere, and the SDK will take care of translating the requests and instructions to
correspond to the specific microgrid configurations.

``` mermaid
flowchart LR

subgraph Left[Measurements only]
direction LR
  grid["Grid Connection"]
  consumer["Consumer"]
  pv["PV Arrays"]
  chp["CHP"]
end

junction(( ))

subgraph Right[Measurements and control]
direction LR
  bat["Batteries"]
  ev["EV Chargers"]
end

grid --- junction
consumer --- junction
pv --- junction
chp --- junction

junction --- bat
junction --- ev
```

## Grid

This refers to a microgrid's connection to the external Grid.  The power flowing through
this connection can be streamed through
[`grid_power`][frequenz.sdk.timeseries.grid.Grid.power].

In locations without a grid connection, this method remains accessible, and streams zero
values.

## Consumer

This is the main power consumer at the site of a microgrid, and often the
{{glossary("load")}} the microgrid is built to support.  The power drawn by the consumer
is available through [`consumer_power`][frequenz.sdk.timeseries.consumer.Consumer.power]

In locations without a consumer, this method streams zero values.

## Producers: PV Arrays, CHP

The total CHP production in a site can be streamed through
[`chp_power`][frequenz.sdk.timeseries.logical_meter.LogicalMeter.chp_power].  PV Power
is available through the PV pool described below.  And total producer power is available
through [`microgrid.producer().power`][frequenz.sdk.timeseries.producer.Producer.power].

As is the case with the other methods, if PV Arrays or CHPs are not available in a
microgrid, the corresponding methods stream zero values.

## PV Arrays

The total PV power production is available through
[`pv_pool`][frequenz.sdk.microgrid.new_pv_pool]'s
[`power`][frequenz.sdk.timeseries.pv_pool.PVPool.power].  The PV pool by default uses
all PV inverters available at a location, but PV pool instances can be created for
subsets of PV inverters if necessary, by specifying the inverter ids.

The `pv_pool` also provides available power bounds through the
[`power_status`][frequenz.sdk.timeseries.pv_pool.PVPool.power_status] method.

The `pv_pool` also provides a control method
[`propose_power`][frequenz.sdk.timeseries.pv_pool.PVPool.propose_power], which accepts
values in the {{glossary("psc", "Passive Sign Convention")}} and supports only
production.


## Batteries

The total Battery power is available through the
[`battery_pool`][frequenz.sdk.microgrid.new_battery_pool]'s
[`power`][frequenz.sdk.timeseries.battery_pool.BatteryPool.power].  The battery pool by
default uses all batteries available at a location, but battery pool instances can be
created for subsets of batteries if necessary, by specifying the battery ids.

The `battery_pool` also provides
[`soc`][frequenz.sdk.timeseries.battery_pool.BatteryPool.soc],
[`capacity`][frequenz.sdk.timeseries.battery_pool.BatteryPool.capacity],
[`temperature`][frequenz.sdk.timeseries.battery_pool.BatteryPool.temperature] and
available power bounds through the
[`power_status`][frequenz.sdk.timeseries.battery_pool.BatteryPool.power_status] method.

The `battery_pool` also provides control methods
[`propose_power`][frequenz.sdk.timeseries.battery_pool.BatteryPool.propose_power] (which
accepts values in the {{glossary("psc", "Passive Sign Convention")}} and supports both
charging and discharging), or through
[`propose_charge`][frequenz.sdk.timeseries.battery_pool.BatteryPool.propose_charge], or
[`propose_discharge`][frequenz.sdk.timeseries.battery_pool.BatteryPool.propose_discharge].

## EV Chargers

The [`ev_charger_pool`][frequenz.sdk.microgrid.new_ev_charger_pool] offers a
[`power`][frequenz.sdk.timeseries.ev_charger_pool.EVChargerPool.power] method that
streams the total power measured for all the {{glossary("ev-charger", "EV Chargers")}}
at a site.

The `ev_charger_pool` also provides available power bounds through the
[`power_status`][frequenz.sdk.timeseries.ev_charger_pool.EVChargerPool.power_status]
method.


The `ev_charger_pool` also provides a control method
[`propose_power`][frequenz.sdk.timeseries.ev_charger_pool.EVChargerPool.propose_power],
which accepts values in the {{glossary("psc", "Passive Sign Convention")}} and supports
only charging.

# Component pools

The SDK provides a unified interface for interacting with sets of Batteries, EV
chargers and PV arrays, through their corresponding `Pool`s.

* [Battery pool][frequenz.sdk.microgrid.new_battery_pool]
* [EV charger pool][frequenz.sdk.microgrid.new_ev_charger_pool]
* [PV pool][frequenz.sdk.microgrid.new_pv_pool]

All of them provide support for streaming aggregated data and for setting the
power values of the components.

## Streaming component data

All pools have a `power` property, which is a
[`FormulaEngine`][frequenz.sdk.timeseries.formula_engine.FormulaEngine] that can

- provide a stream of resampled power values, which correspond to the sum of the
power measured from all the components in the pool together.

- be composed with other power streams to for composite formulas.

In addition, the battery pool has some additional properties that can be used as
streams for metrics specific to batteries:
[`soc`][frequenz.sdk.timeseries.battery_pool.BatteryPool.soc],
[`capacity`][frequenz.sdk.timeseries.battery_pool.BatteryPool.capacity] and
[`temperature`][frequenz.sdk.timeseries.battery_pool.BatteryPool.temperature].

## Setting power

All pools provide a `propose_power` method for setting power for the pool.  This
would then be distributed to the individual components in the pool, using an
algorithm that's suitable for the category of the components.  For example, when
controlling batteries, power could be distributed based on the `SoC` of the
individual batteries, to keep the batteries in balance.

### How to work with other actors

If multiple actors are trying to control (by proposing power values) the same
set of components, the power manager will aggregate their desired power values,
while considering the priority of the actors and the bounds they set, to
calculate the target power for the components.

The final target power can be accessed using the receiver returned from the
[`power_status`][frequenz.sdk.timeseries.battery_pool.BatteryPool.power_status]
method available for all pools, which also streams the bounds that an actor
should comply with, based on its priority.

#### Adding the power proposals of individual actors

When an actor A calls the `propose_power` method with a power, the proposed
power of the lower priority actor will get added to actor A's power.  This works
as follows:

 - the lower priority actor would see bounds shifted by the power proposed by
   actor A.
 - After lower priority actor B sets a power in its shifted bounds, it will get
   shifted back by the power set by actor A.

This has the effect of adding the powers set by actors A and B.

*Example 1*: Battery bounds available for use: -100kW to 100kW

| Actor | Priority | System Bounds   | Requested Bounds | Requested | Adjusted     | Aggregate |
|       |          |                 |                  | Power     | Power        | Power     |
|-------|----------|-----------------|------------------|-----------|--------------|-----------|
| A     | 3        | -100kW .. 100kW | None             | 20kW      | 20kW         | 20kW      |
| B     | 2        | -120kW .. 80kW  | None             | 50kW      | 50kW         | 70kW      |
| C     | 1        | -170kW .. 30kW  | None             | 50kW      | 30kW         | 100kW     |
|       |          |                 |                  |           | target power | 100kW     |

Actor A proposes a power of `20kW`, but no bounds.  In this case, actor B sees
bounds shifted by A's proposal.  Actor B proposes a power of `50kW` on this
shifted range, and if this is applied on to the original bounds (aka shift the
bounds back to the original range), it would be `20kW + 50kW = 70kW`.

So Actor C sees bounds shifted by `70kW` from the original bounds, and sets
`50kW` on this shifted range, but it can't exceed `30kW`, so its request gets
limited to 30kW.  Shifting this back by `70kW`, the target power is calculated
to be `100kW`.

Irrespective of what any actor sets, the final power won't exceed the available
battery bounds.

*Example 2*:

| Actor | Priority | System Bounds   | Requested Bounds | Requested | Adjusted     | Aggregate |
|       |          |                 |                  | Power     | Power        | Power     |
|-------|----------|-----------------|------------------|-----------|--------------|-----------|
| A     | 3        | -100kW .. 100kW | None             | 20kW      | 20kW         | 20kW      |
| B     | 2        | -120kW .. 80kW  | None             | -20kW     | -20kW        | 0kW       |
|       |          |                 |                  |           | target power | 0kW       |

Actors with exactly opposite requests cancel each other out.

#### Limiting bounds for lower priority actors

When an actor A calls the `propose_power` method with bounds (either both lower
and upper bounds or at least one of them), lower priority actors will see their
(shifted) bounds restricted and can only propose power values within that range.

*Example 1*: Battery bounds available for use: -100kW to 100kW

| Actor | Priority | System Bounds   | Requested Bounds | Requested | Adjusted     | Aggregate |
|       |          |                 |                  | Power     | Power        | Power     |
|-------|----------|-----------------|------------------|-----------|--------------|-----------|
| A     | 3        | -100kW .. 100kW | -20kW .. 100kW   | 50kW      | 40kW         | 50kW      |
| B     | 2        | -70kW .. 50kW   | -90kW .. 0kW     | -10kW     | -10kW        | 40kW      |
| C     | 1        | -60kW .. 10kW   | None             | -20kW     | -20kW        | 20kW      |
|       |          |                 |                  |           | target power | 20kW      |

Actor A with the highest priority has the entire battery bounds available to it.
It sets limited bounds of -20kW .. 100kW, and proposes a power of 50kW.

Actor B sees Actor A's limit of -20kW..100kW shifted by 50kW as -70kW..50kW, and
can only propose powers within this range, which will get added (shifted back)
to Actor A's proposed power.

Actor B tries to limit the bounds of actor C to -90kW .. 0kW, but it can only
operate in the -70kW .. 50kW range because of bounds set by actor A, so its
requested bounds get restricted to -70kW .. 0kW.

Actor C sees this as -60kW .. 10kW, because it gets shifted by Actor B's
proposed power of -10kW.

Actor C proposes a power within its bounds and the proposals of all the actors
are added to get the target power.

*Example 2*:

| Actor | Priority | System Bounds   | Requested Bounds | Requested | Adjusted     | Aggregate |
|       |          |                 |                  | Power     | Power        | Power     |
|-------|----------|-----------------|------------------|-----------|--------------|-----------|
| A     | 3        | -100kW .. 100kW | -20kW .. 100kW   | 50kW      | 50kW         | 50kW      |
| B     | 2        | -70kW .. 50kW   | -90kW .. 0kW     | -90kW     | -70kW        | -20kW     |
|       |          |                 |                  |           | target power | -20kW     |

When an actor requests a power that's outside its available bounds, the closest
available power is used.

#### Comprehensive example

Battery bounds available for use: -100kW to 100kW

| Priority | System Bounds     | Requested Bounds | Requested | Adjusted     | Aggregate |
|          |                   |                  | Power     | Power        | Power     |
|----------|-------------------|------------------|-----------|--------------|-----------|
| 7        | -100 kW .. 100 kW | None             | 10 kW     | 10 kW        | 10 kW     |
| 6        | -110 kW .. 90 kW  | -110 kW .. 80 kW | 10 kW     | 10 kW        | 20 kW     |
| 5        | -120 kW .. 70 kW  | -100 kW .. 80 kW | 80 kW     | 70 kW        | 90 kW     |
| 4        | -170 kW .. 0 kW   | None             | -120 kW   | -120 kW      | -30 kW    |
| 3        | -50 kW .. 120 kW  | None             | 60 kW     | 60 kW        | 30 kW     |
| 2        | -110 kW .. 60 kW  | -40 kW .. 30 kW  | 20 kW     | 20 kW        | 50 kW     |
| 1        | -60 kW .. 10 kW   | -50 kW .. 40 kW  | 25 kW     | 10 kW        | 60 kW     |
| 0        | -60 kW .. 0 kW    | None             | 12 kW     | 0 kW         | 60 kW     |
| -1       | -60 kW .. 0 kW    | -40 kW .. -10 kW | -10 kW    | -10 kW       | 50 kW     |
|          |                   |                  |           | Target Power | 50 kW     |

## Withdrawing power proposals

An actor can withdraw its power proposal by calling `propose_power` with `None`
target_power and `None` bounds (which are the default anyway).  As soon as an actor
calls `pool.propose_power(None)`, its proposal is dropped and the target power is
recalculated and the component powers are updated.

When all the proposals for a pool are withdrawn, the components get reset to their
default powers immediately.  These are:

| component category | default power (according to Passive Sign Convention) |
|--------------------|------------------------------------------------------|
| Batteries          | Zero                                                 |
| PV                 | Max production (Min power according to PSC)          |
| EV Chargers        | Max consumption (Max power according to PSC)         |
"""  # noqa: D205, D400

from datetime import timedelta

from ..timeseries._resampling._config import ResamplerConfig
from . import _data_pipeline, connection_manager
from ._data_pipeline import (
    consumer,
    frequency,
    grid,
    logical_meter,
    new_battery_pool,
    new_ev_charger_pool,
    new_pv_pool,
    producer,
    voltage_per_phase,
)


async def initialize(
    server_url: str,
    resampler_config: ResamplerConfig,
    *,
    api_power_request_timeout: timedelta = timedelta(seconds=5.0),
) -> None:
    """Initialize the microgrid connection manager and the data pipeline.

    Args:
        server_url: The location of the microgrid API server in the form of a URL.
            The following format is expected: `grpc://hostname{:port}{?ssl=ssl}`,
            where the port should be an int between `0` and `65535` (defaulting to
            `9090`) and ssl should be a boolean (defaulting to false). For example:
            `grpc://localhost:1090?ssl=true`.
        resampler_config: Configuration for the resampling actor.
        api_power_request_timeout: Timeout to use when making power requests to
            the microgrid API.  When requests to components timeout, they will
            be marked as blocked for a short duration, during which time they
            will be unavailable from the corresponding component pools.
    """
    await connection_manager.initialize(server_url)
    await _data_pipeline.initialize(
        resampler_config,
        api_power_request_timeout=api_power_request_timeout,
    )


__all__ = [
    "initialize",
    "consumer",
    "grid",
    "frequency",
    "logical_meter",
    "new_battery_pool",
    "new_ev_charger_pool",
    "new_pv_pool",
    "producer",
    "voltage_per_phase",
]



================================================
FILE: src/frequenz/sdk/microgrid/_data_pipeline.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Create, connect and own instances of data pipeline components.

Provides SDK users direct access to higher level components of the data pipeline,
eliminating the boiler plate code required to setup the DataSourcingActor and the
ResamplingActor.
"""

from __future__ import annotations

import logging
import typing
from collections import abc
from dataclasses import dataclass
from datetime import timedelta

from frequenz.channels import Broadcast, Sender
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentCategory, InverterType

from frequenz.sdk.microgrid._power_managing._base_classes import Algorithm, DefaultPower

from .._internal._channels import ChannelRegistry
from ..actor._actor import Actor
from ..timeseries import ResamplerConfig
from ..timeseries._voltage_streamer import VoltageStreamer
from ..timeseries.grid import Grid
from ..timeseries.grid import get as get_grid
from ..timeseries.grid import initialize as initialize_grid
from ._data_sourcing import ComponentMetricRequest, DataSourcingActor
from ._power_wrapper import PowerWrapper

# A number of imports had to be done inside functions where they are used, to break
# import cycles.
#
# pylint: disable=import-outside-toplevel
if typing.TYPE_CHECKING:
    from ..timeseries._grid_frequency import GridFrequency
    from ..timeseries.battery_pool import BatteryPool
    from ..timeseries.battery_pool._battery_pool_reference_store import (
        BatteryPoolReferenceStore,
    )
    from ..timeseries.consumer import Consumer
    from ..timeseries.ev_charger_pool import EVChargerPool
    from ..timeseries.ev_charger_pool._ev_charger_pool_reference_store import (
        EVChargerPoolReferenceStore,
    )
    from ..timeseries.logical_meter import LogicalMeter
    from ..timeseries.producer import Producer
    from ..timeseries.pv_pool import PVPool
    from ..timeseries.pv_pool._pv_pool_reference_store import PVPoolReferenceStore

_logger = logging.getLogger(__name__)


_REQUEST_RECV_BUFFER_SIZE = 500
"""The maximum number of requests that can be queued in the request receiver.

A larger buffer size means that the DataSourcing and Resampling actors don't drop
requests and will be able to keep up with higher request rates in larger installations.
"""


@dataclass
class _ActorInfo:
    """Holds instances of core data pipeline actors and their request channels."""

    actor: Actor
    """The actor instance."""

    channel: Broadcast[ComponentMetricRequest]
    """The request channel for the actor."""


class _DataPipeline:  # pylint: disable=too-many-instance-attributes
    """Create, connect and own instances of data pipeline components.

    Provides SDK users direct access to higher level components of the data pipeline,
    eliminating the boiler plate code required to setup the DataSourcingActor and the
    ResamplingActor.
    """

    def __init__(
        self,
        resampler_config: ResamplerConfig,
        api_power_request_timeout: timedelta = timedelta(seconds=5.0),
    ) -> None:
        """Create a `DataPipeline` instance.

        Args:
            resampler_config: Config to pass on to the resampler.
            api_power_request_timeout: Timeout to use when making power requests to
                the microgrid API.
        """
        self._resampler_config: ResamplerConfig = resampler_config

        self._channel_registry: ChannelRegistry = ChannelRegistry(
            name="Data Pipeline Registry"
        )

        self._data_sourcing_actor: _ActorInfo | None = None
        self._resampling_actor: _ActorInfo | None = None

        self._battery_power_wrapper = PowerWrapper(
            self._channel_registry,
            api_power_request_timeout=api_power_request_timeout,
            power_manager_algorithm=Algorithm.SHIFTING_MATRYOSHKA,
            component_category=ComponentCategory.BATTERY,
            default_power=DefaultPower.ZERO,
        )
        self._ev_power_wrapper = PowerWrapper(
            self._channel_registry,
            api_power_request_timeout=api_power_request_timeout,
            power_manager_algorithm=Algorithm.MATRYOSHKA,
            component_category=ComponentCategory.EV_CHARGER,
            default_power=DefaultPower.MAX,
        )
        self._pv_power_wrapper = PowerWrapper(
            self._channel_registry,
            api_power_request_timeout=api_power_request_timeout,
            power_manager_algorithm=Algorithm.MATRYOSHKA,
            component_category=ComponentCategory.INVERTER,
            component_type=InverterType.SOLAR,
            default_power=DefaultPower.MIN,
        )

        self._logical_meter: LogicalMeter | None = None
        self._consumer: Consumer | None = None
        self._producer: Producer | None = None
        self._grid: Grid | None = None
        self._ev_charger_pool_reference_stores: dict[
            frozenset[ComponentId], EVChargerPoolReferenceStore
        ] = {}
        self._battery_pool_reference_stores: dict[
            frozenset[ComponentId], BatteryPoolReferenceStore
        ] = {}
        self._pv_pool_reference_stores: dict[
            frozenset[ComponentId], PVPoolReferenceStore
        ] = {}
        self._frequency_instance: GridFrequency | None = None
        self._voltage_instance: VoltageStreamer | None = None

        self._known_pool_keys: set[str] = set()
        """A set of keys for corresponding to created EVChargerPool instances.

        This is used to warn the user if they try to create a new EVChargerPool instance
        for the same set of component IDs, and with the same priority.
        """

    def frequency(self) -> GridFrequency:
        """Return the grid frequency measuring point."""
        from ..timeseries._grid_frequency import GridFrequency

        if self._frequency_instance is None:
            self._frequency_instance = GridFrequency(
                self._data_sourcing_request_sender(),
                self._channel_registry,
            )

        return self._frequency_instance

    def voltage_per_phase(self) -> VoltageStreamer:
        """Return the per-phase voltage measuring point."""
        if not self._voltage_instance:
            self._voltage_instance = VoltageStreamer(
                self._resampling_request_sender(),
                self._channel_registry,
            )

        return self._voltage_instance

    def logical_meter(self) -> LogicalMeter:
        """Return the logical meter of the microgrid."""
        from ..timeseries.logical_meter import LogicalMeter

        if self._logical_meter is None:
            self._logical_meter = LogicalMeter(
                channel_registry=self._channel_registry,
                resampler_subscription_sender=self._resampling_request_sender(),
            )
        return self._logical_meter

    def consumer(self) -> Consumer:
        """Return the consumption measuring point of the microgrid."""
        from ..timeseries.consumer import Consumer

        if self._consumer is None:
            self._consumer = Consumer(
                channel_registry=self._channel_registry,
                resampler_subscription_sender=self._resampling_request_sender(),
            )
        return self._consumer

    def producer(self) -> Producer:
        """Return the production measuring point of the microgrid."""
        from ..timeseries.producer import Producer

        if self._producer is None:
            self._producer = Producer(
                channel_registry=self._channel_registry,
                resampler_subscription_sender=self._resampling_request_sender(),
            )
        return self._producer

    def grid(self) -> Grid:
        """Return the grid measuring point."""
        if self._grid is None:
            initialize_grid(
                channel_registry=self._channel_registry,
                resampler_subscription_sender=self._resampling_request_sender(),
            )
            self._grid = get_grid()

        return self._grid

    def new_ev_charger_pool(
        self,
        *,
        priority: int,
        component_ids: abc.Set[ComponentId] | None = None,
        name: str | None = None,
    ) -> EVChargerPool:
        """Return the corresponding EVChargerPool instance for the given ids.

        If an EVChargerPool instance for the given ids doesn't exist, a new one is
        created and returned.

        Args:
            priority: The priority of the actor making the call.
            component_ids: Optional set of IDs of EV Chargers to be managed by the
                EVChargerPool.
            name: An optional name used to identify this instance of the pool or a
                corresponding actor in the logs.

        Returns:
            An EVChargerPool instance.
        """
        from ..timeseries.ev_charger_pool import EVChargerPool
        from ..timeseries.ev_charger_pool._ev_charger_pool_reference_store import (
            EVChargerPoolReferenceStore,
        )

        if not self._ev_power_wrapper.started:
            self._ev_power_wrapper.start()

        # We use frozenset to make a hashable key from the input set.
        ref_store_key: frozenset[ComponentId] = frozenset()
        if component_ids is not None:
            ref_store_key = frozenset(component_ids)

        pool_key = f"{ref_store_key}-{priority}"
        if pool_key in self._known_pool_keys:
            _logger.warning(
                "An EVChargerPool instance was already created for ev_charger_ids=%s "
                "and priority=%s using `microgrid.ev_charger_pool(...)`."
                "\n  Hint: If the multiple instances are created from the same actor, "
                "consider reusing the same instance."
                "\n  Hint: If the instances are created from different actors, "
                "consider using different priorities to distinguish them.",
                component_ids,
                priority,
            )
        else:
            self._known_pool_keys.add(pool_key)

        if ref_store_key not in self._ev_charger_pool_reference_stores:
            self._ev_charger_pool_reference_stores[ref_store_key] = (
                EVChargerPoolReferenceStore(
                    channel_registry=self._channel_registry,
                    resampler_subscription_sender=self._resampling_request_sender(),
                    status_receiver=self._ev_power_wrapper.status_channel.new_receiver(
                        limit=1
                    ),
                    power_manager_requests_sender=(
                        self._ev_power_wrapper.proposal_channel.new_sender()
                    ),
                    power_manager_bounds_subs_sender=(
                        self._ev_power_wrapper.bounds_subscription_channel.new_sender()
                    ),
                    power_distribution_results_fetcher=(
                        self._ev_power_wrapper.distribution_results_fetcher()
                    ),
                    component_ids=component_ids,
                )
            )
        return EVChargerPool(
            pool_ref_store=self._ev_charger_pool_reference_stores[ref_store_key],
            name=name,
            priority=priority,
        )

    def new_pv_pool(
        self,
        *,
        priority: int,
        component_ids: abc.Set[ComponentId] | None = None,
        name: str | None = None,
    ) -> PVPool:
        """Return a new `PVPool` instance for the given ids.

        If a `PVPoolReferenceStore` instance for the given PV inverter ids doesn't
        exist, a new one is created and used for creating the `PVPool`.

        Args:
            priority: The priority of the actor making the call.
            component_ids: Optional set of IDs of PV inverters to be managed by the
                `PVPool`.
            name: An optional name used to identify this instance of the pool or a
                corresponding actor in the logs.

        Returns:
            A `PVPool` instance.
        """
        from ..timeseries.pv_pool import PVPool
        from ..timeseries.pv_pool._pv_pool_reference_store import PVPoolReferenceStore

        if not self._pv_power_wrapper.started:
            self._pv_power_wrapper.start()

        # We use frozenset to make a hashable key from the input set.
        ref_store_key: frozenset[ComponentId] = frozenset()
        if component_ids is not None:
            ref_store_key = frozenset(component_ids)

        pool_key = f"{ref_store_key}-{priority}"
        if pool_key in self._known_pool_keys:
            _logger.warning(
                "A PVPool instance was already created for pv_inverter_ids=%s and "
                "priority=%s using `microgrid.pv_pool(...)`."
                "\n  Hint: If the multiple instances are created from the same actor, "
                "consider reusing the same instance."
                "\n  Hint: If the instances are created from different actors, "
                "consider using different priorities to distinguish them.",
                component_ids,
                priority,
            )
        else:
            self._known_pool_keys.add(pool_key)

        if ref_store_key not in self._pv_pool_reference_stores:
            self._pv_pool_reference_stores[ref_store_key] = PVPoolReferenceStore(
                channel_registry=self._channel_registry,
                resampler_subscription_sender=self._resampling_request_sender(),
                status_receiver=(
                    self._pv_power_wrapper.status_channel.new_receiver(limit=1)
                ),
                power_manager_requests_sender=(
                    self._pv_power_wrapper.proposal_channel.new_sender()
                ),
                power_manager_bounds_subs_sender=(
                    self._pv_power_wrapper.bounds_subscription_channel.new_sender()
                ),
                power_distribution_results_fetcher=(
                    self._pv_power_wrapper.distribution_results_fetcher()
                ),
                component_ids=component_ids,
            )

        return PVPool(
            pool_ref_store=self._pv_pool_reference_stores[ref_store_key],
            name=name,
            priority=priority,
        )

    def new_battery_pool(
        self,
        *,
        priority: int,
        component_ids: abc.Set[ComponentId] | None = None,
        name: str | None = None,
    ) -> BatteryPool:
        """Return a new `BatteryPool` instance for the given ids.

        If a `BatteryPoolReferenceStore` instance for the given battery ids doesn't
        exist, a new one is created and used for creating the `BatteryPool`.

        Args:
            priority: The priority of the actor making the call.
            component_ids: Optional set of IDs of batteries to be managed by the
                `BatteryPool`.
            name: An optional name used to identify this instance of the pool or a
                corresponding actor in the logs.

        Returns:
            A `BatteryPool` instance.
        """
        from ..timeseries.battery_pool import BatteryPool
        from ..timeseries.battery_pool._battery_pool_reference_store import (
            BatteryPoolReferenceStore,
        )

        if not self._battery_power_wrapper.started:
            self._battery_power_wrapper.start()

        # We use frozenset to make a hashable key from the input set.
        ref_store_key: frozenset[ComponentId] = frozenset()
        if component_ids is not None:
            ref_store_key = frozenset(component_ids)

        pool_key = f"{ref_store_key}-{priority}"
        if pool_key in self._known_pool_keys:
            _logger.warning(
                "A BatteryPool instance was already created for battery_ids=%s and "
                "priority=%s using `microgrid.battery_pool(...)`."
                "\n  Hint: If the multiple instances are created from the same actor, "
                "consider reusing the same instance."
                "\n  Hint: If the instances are created from different actors, "
                "consider using different priorities to distinguish them.",
                component_ids,
                priority,
            )
        else:
            self._known_pool_keys.add(pool_key)

        if ref_store_key not in self._battery_pool_reference_stores:
            self._battery_pool_reference_stores[ref_store_key] = (
                BatteryPoolReferenceStore(
                    channel_registry=self._channel_registry,
                    resampler_subscription_sender=self._resampling_request_sender(),
                    batteries_status_receiver=(
                        self._battery_power_wrapper.status_channel.new_receiver(limit=1)
                    ),
                    power_manager_requests_sender=(
                        self._battery_power_wrapper.proposal_channel.new_sender()
                    ),
                    power_manager_bounds_subscription_sender=(
                        self._battery_power_wrapper.bounds_subscription_channel.new_sender()
                    ),
                    power_distribution_results_fetcher=(
                        self._battery_power_wrapper.distribution_results_fetcher()
                    ),
                    min_update_interval=self._resampler_config.resampling_period,
                    batteries_id=component_ids,
                )
            )

        return BatteryPool(
            pool_ref_store=self._battery_pool_reference_stores[ref_store_key],
            name=name,
            priority=priority,
        )

    def _data_sourcing_request_sender(self) -> Sender[ComponentMetricRequest]:
        """Return a Sender for sending requests to the data sourcing actor.

        If the data sourcing actor is not already running, this function also starts it.

        Returns:
            A Sender for sending requests to the data sourcing actor.
        """
        if self._data_sourcing_actor is None:
            channel: Broadcast[ComponentMetricRequest] = Broadcast(
                name="Data Pipeline: Data Sourcing Actor Request Channel"
            )
            actor = DataSourcingActor(
                request_receiver=channel.new_receiver(limit=_REQUEST_RECV_BUFFER_SIZE),
                registry=self._channel_registry,
            )
            self._data_sourcing_actor = _ActorInfo(actor, channel)
            self._data_sourcing_actor.actor.start()
        return self._data_sourcing_actor.channel.new_sender()

    def _resampling_request_sender(self) -> Sender[ComponentMetricRequest]:
        """Return a Sender for sending requests to the resampling actor.

        If the resampling actor is not already running, this function also starts it.

        Returns:
            A Sender for sending requests to the resampling actor.
        """
        from ._resampling import ComponentMetricsResamplingActor

        if self._resampling_actor is None:
            channel: Broadcast[ComponentMetricRequest] = Broadcast(
                name="Data Pipeline: Component Metric Resampling Actor Request Channel"
            )
            actor = ComponentMetricsResamplingActor(
                channel_registry=self._channel_registry,
                data_sourcing_request_sender=self._data_sourcing_request_sender(),
                resampling_request_receiver=channel.new_receiver(
                    limit=_REQUEST_RECV_BUFFER_SIZE,
                    name=channel.name + " Receiver",
                ),
                config=self._resampler_config,
            )
            self._resampling_actor = _ActorInfo(actor, channel)
            self._resampling_actor.actor.start()
        return self._resampling_actor.channel.new_sender()

    async def _stop(self) -> None:
        """Stop the data pipeline actors."""
        if self._data_sourcing_actor:
            await self._data_sourcing_actor.actor.stop()
        if self._resampling_actor:
            await self._resampling_actor.actor.stop()
        await self._battery_power_wrapper.stop()
        await self._ev_power_wrapper.stop()
        await self._pv_power_wrapper.stop()
        for pool in self._battery_pool_reference_stores.values():
            await pool.stop()
        for evpool in self._ev_charger_pool_reference_stores.values():
            await evpool.stop()
        for pvpool in self._pv_pool_reference_stores.values():
            await pvpool.stop()


_DATA_PIPELINE: _DataPipeline | None = None


async def initialize(
    resampler_config: ResamplerConfig,
    api_power_request_timeout: timedelta = timedelta(seconds=5.0),
) -> None:
    """Initialize a `DataPipeline` instance.

    Args:
        resampler_config: Config to pass on to the resampler.
        api_power_request_timeout: Timeout to use when making power requests to
            the microgrid API.  When requests to components timeout, they will
            be marked as blocked for a short duration, during which time they
            will be unavailable from the corresponding component pools.

    Raises:
        RuntimeError: if the DataPipeline is already initialized.
    """
    global _DATA_PIPELINE  # pylint: disable=global-statement

    if _DATA_PIPELINE is not None:
        raise RuntimeError("DataPipeline is already initialized.")
    _DATA_PIPELINE = _DataPipeline(resampler_config, api_power_request_timeout)


def frequency() -> GridFrequency:
    """Return the grid frequency measuring point."""
    return _get().frequency()


def voltage_per_phase() -> VoltageStreamer:
    """Return the per-phase voltage measuring point."""
    return _get().voltage_per_phase()


def logical_meter() -> LogicalMeter:
    """Return the logical meter of the microgrid."""
    return _get().logical_meter()


def consumer() -> Consumer:
    """Return the [`Consumption`][frequenz.sdk.timeseries.consumer.Consumer] measuring point."""
    return _get().consumer()


def producer() -> Producer:
    """Return the [`Production`][frequenz.sdk.timeseries.producer.Producer] measuring point."""
    return _get().producer()


def new_ev_charger_pool(
    *,
    priority: int,
    component_ids: abc.Set[ComponentId] | None = None,
    name: str | None = None,
) -> EVChargerPool:
    """Return a new `EVChargerPool` instance for the given parameters.

    The priority value is used to resolve conflicts when multiple actors are trying to
    propose different power values for the same set of EV chargers.

    !!! note
        When specifying priority, bigger values indicate higher priority.

        It is recommended to reuse the same instance of the `EVChargerPool` within the
        same actor, unless they are managing different sets of EV chargers.

        In deployments with multiple actors managing the same set of EV chargers, it is
        recommended to use different priorities to distinguish between them.  If not,
        a random prioritization will be imposed on them to resolve conflicts, which may
        lead to unexpected behavior like longer duration to converge on the desired
        power.

    Args:
        priority: The priority of the actor making the call.
        component_ids: Optional set of IDs of EV Chargers to be managed by the
            EVChargerPool.  If not specified, all EV Chargers available in the
            component graph are used.
        name: An optional name used to identify this instance of the pool or a
            corresponding actor in the logs.

    Returns:
        An `EVChargerPool` instance.
    """
    return _get().new_ev_charger_pool(
        priority=priority, component_ids=component_ids, name=name
    )


def new_battery_pool(
    *,
    priority: int,
    component_ids: abc.Set[ComponentId] | None = None,
    name: str | None = None,
) -> BatteryPool:
    """Return a new `BatteryPool` instance for the given parameters.

    The priority value is used to resolve conflicts when multiple actors are trying to
    propose different power values for the same set of batteries.

    !!! note
        When specifying priority, bigger values indicate higher priority.

        It is recommended to reuse the same instance of the `BatteryPool` within the
        same actor, unless they are managing different sets of batteries.

        In deployments with multiple actors managing the same set of batteries, it is
        recommended to use different priorities to distinguish between them.  If not,
        a random prioritization will be imposed on them to resolve conflicts, which may
        lead to unexpected behavior like longer duration to converge on the desired
        power.

    Args:
        priority: The priority of the actor making the call.
        component_ids: Optional set of IDs of batteries to be managed by the
            `BatteryPool`.  If not specified, all batteries available in the component
            graph are used.
        name: An optional name used to identify this instance of the pool or a
            corresponding actor in the logs.

    Returns:
        A `BatteryPool` instance.
    """
    return _get().new_battery_pool(
        priority=priority, component_ids=component_ids, name=name
    )


def new_pv_pool(
    *,
    priority: int,
    component_ids: abc.Set[ComponentId] | None = None,
    name: str | None = None,
) -> PVPool:
    """Return a new `PVPool` instance for the given parameters.

    The priority value is used to resolve conflicts when multiple actors are trying to
    propose different power values for the same set of PV inverters.

    !!! note
        When specifying priority, bigger values indicate higher priority.

        It is recommended to reuse the same instance of the `PVPool` within the same
        actor, unless they are managing different sets of PV inverters.

        In deployments with multiple actors managing the same set of PV inverters, it is
        recommended to use different priorities to distinguish between them.  If not,
        a random prioritization will be imposed on them to resolve conflicts, which may
        lead to unexpected behavior like longer duration to converge on the desired
        power.

    Args:
        priority: The priority of the actor making the call.
        component_ids: Optional set of IDs of PV inverters to be managed by the
            `PVPool`. If not specified, all PV inverters available in the component
            graph are used.
        name: An optional name used to identify this instance of the pool or a
            corresponding actor in the logs.

    Returns:
        A `PVPool` instance.
    """
    return _get().new_pv_pool(priority=priority, component_ids=component_ids, name=name)


def grid() -> Grid:
    """Return the grid measuring point."""
    return _get().grid()


def _get() -> _DataPipeline:
    if _DATA_PIPELINE is None:
        raise RuntimeError(
            "DataPipeline is not initialized. "
            "Call `await microgrid.initialize()` first."
        )
    return _DATA_PIPELINE



================================================
FILE: src/frequenz/sdk/microgrid/_power_wrapper.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Wrapper around the power managing and power distributing actors."""

from __future__ import annotations

import logging
from datetime import timedelta

from frequenz.channels import Broadcast
from frequenz.client.microgrid import ComponentCategory, ComponentType

from .._internal._channels import ChannelRegistry, ReceiverFetcher

# pylint seems to think this is a cyclic import, but it is not.
#
# pylint: disable-next=cyclic-import
from . import _power_managing, connection_manager

# pylint: disable-next=cyclic-import
from ._power_distributing import (
    ComponentPoolStatus,
    PowerDistributingActor,
    Request,
    Result,
)
from ._power_managing._base_classes import Algorithm, DefaultPower

_logger = logging.getLogger(__name__)


class PowerWrapper:  # pylint: disable=too-many-instance-attributes
    """Wrapper around the power managing and power distributing actors."""

    def __init__(  # pylint: disable=too-many-arguments
        self,
        channel_registry: ChannelRegistry,
        *,
        api_power_request_timeout: timedelta,
        power_manager_algorithm: Algorithm,
        default_power: DefaultPower,
        component_category: ComponentCategory,
        component_type: ComponentType | None = None,
    ):
        """Initialize the power control.

        Args:
            channel_registry: A channel registry for use in the actors.
            api_power_request_timeout: Timeout to use when making power requests to
                the microgrid API.
            power_manager_algorithm: The power management algorithm to use.
            default_power: The default power to use for the components.
            component_category: The category of the components that actors started by
                this instance of the PowerWrapper will be responsible for.
            component_type: The type of the component of the given category that this
                actor is responsible for.  This is used only when the component category
                is not enough to uniquely identify the component.  For example, when the
                category is `ComponentCategory.INVERTER`, the type is needed to identify
                the inverter as a solar inverter or a battery inverter.  This can be
                `None` when the component category is enough to uniquely identify the
                component.
        """
        self._default_power = default_power
        self._component_category = component_category
        self._component_type = component_type
        self._power_manager_algorithm = power_manager_algorithm
        self._channel_registry = channel_registry
        self._api_power_request_timeout = api_power_request_timeout

        self.status_channel: Broadcast[ComponentPoolStatus] = Broadcast(
            name="Component Status Channel", resend_latest=True
        )
        self._power_distribution_requests_channel: Broadcast[Request] = Broadcast(
            name="Power Distributing Actor, Requests Broadcast Channel"
        )
        self._power_distribution_results_channel: Broadcast[Result] = Broadcast(
            name="Power Distributing Actor, Results Broadcast Channel"
        )

        self.proposal_channel: Broadcast[_power_managing.Proposal] = Broadcast(
            name="Power Managing Actor, Requests Broadcast Channel"
        )
        self.bounds_subscription_channel: Broadcast[_power_managing.ReportRequest] = (
            Broadcast(name="Power Managing Actor, Bounds Subscription Channel")
        )

        self._power_distributing_actor: PowerDistributingActor | None = None
        self._power_managing_actor: _power_managing.PowerManagingActor | None = None

    def _start_power_managing_actor(self) -> None:
        """Start the power managing actor if it is not already running."""
        if self._power_managing_actor:
            return

        component_graph = connection_manager.get().component_graph
        # Currently the power managing actor only supports batteries.  The below
        # constraint needs to be relaxed if the actor is extended to support other
        # components.
        if not component_graph.components(
            component_categories={self._component_category}
        ):
            _logger.warning(
                "No %s found in the component graph. "
                "The power managing actor will not be started.",
                self._component_category,
            )
            return

        self._power_managing_actor = _power_managing.PowerManagingActor(
            default_power=self._default_power,
            component_category=self._component_category,
            component_type=self._component_type,
            algorithm=self._power_manager_algorithm,
            proposals_receiver=self.proposal_channel.new_receiver(),
            bounds_subscription_receiver=(
                self.bounds_subscription_channel.new_receiver()
            ),
            power_distributing_requests_sender=(
                self._power_distribution_requests_channel.new_sender()
            ),
            power_distributing_results_receiver=(
                self._power_distribution_results_channel.new_receiver()
            ),
            channel_registry=self._channel_registry,
        )
        self._power_managing_actor.start()

    def _start_power_distributing_actor(self) -> None:
        """Start the power distributing actor if it is not already running."""
        if self._power_distributing_actor:
            return

        component_graph = connection_manager.get().component_graph
        if not component_graph.components(
            component_categories={self._component_category}
        ):
            _logger.warning(
                "No %s found in the component graph. "
                "The power distributing actor will not be started.",
                self._component_category,
            )
            return

        # The PowerDistributingActor is started with only a single default user channel.
        # Until the PowerManager is implemented, support for multiple use-case actors
        # will not be available in the high level interface.
        self._power_distributing_actor = PowerDistributingActor(
            component_category=self._component_category,
            component_type=self._component_type,
            api_power_request_timeout=self._api_power_request_timeout,
            requests_receiver=self._power_distribution_requests_channel.new_receiver(),
            results_sender=self._power_distribution_results_channel.new_sender(),
            component_pool_status_sender=self.status_channel.new_sender(),
        )
        self._power_distributing_actor.start()

    @property
    def started(self) -> bool:
        """Return True if power managing and power distributing actors are started."""
        return (
            self._power_managing_actor is not None
            and self._power_distributing_actor is not None
        )

    def start(self) -> None:
        """Start the power managing and power distributing actors."""
        if self.started:
            return
        self._start_power_distributing_actor()
        self._start_power_managing_actor()

    async def stop(self) -> None:
        """Stop the power managing and power distributing actors."""
        if self._power_distributing_actor:
            await self._power_distributing_actor.stop()
        if self._power_managing_actor:
            await self._power_managing_actor.stop()

    def distribution_results_fetcher(self) -> ReceiverFetcher[Result]:
        """Return a fetcher for the power distribution results."""
        return self._power_distribution_results_channel



================================================
FILE: src/frequenz/sdk/microgrid/_resampling.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""An actor to resample microgrid component metrics."""


import asyncio
import dataclasses
import logging

from frequenz.channels import Receiver, Sender
from frequenz.quantities import Quantity

from .._internal._asyncio import cancel_and_await
from .._internal._channels import ChannelRegistry
from ..actor._actor import Actor
from ..timeseries import Sample
from ..timeseries._resampling._config import ResamplerConfig
from ..timeseries._resampling._exceptions import ResamplingError
from ..timeseries._resampling._resampler import Resampler
from ._data_sourcing import ComponentMetricRequest

_logger = logging.getLogger(__name__)


class ComponentMetricsResamplingActor(Actor):
    """An actor to resample microgrid component metrics."""

    def __init__(  # pylint: disable=too-many-arguments
        self,
        *,
        channel_registry: ChannelRegistry,
        data_sourcing_request_sender: Sender[ComponentMetricRequest],
        resampling_request_receiver: Receiver[ComponentMetricRequest],
        config: ResamplerConfig,
        name: str | None = None,
    ) -> None:
        """Initialize an instance.

        Args:
            channel_registry: The channel registry used to get senders and
                receivers for data sourcing subscriptions.
            data_sourcing_request_sender: The sender used to send requests to
                the [`DataSourcingActor`][frequenz.sdk.actor.DataSourcingActor]
                to subscribe to component metrics.
            resampling_request_receiver: The receiver to use to receive new
                resampling subscription requests.
            config: The configuration for the resampler.
            name: The name of the actor. If `None`, `str(id(self))` will be used. This
                is used mostly for debugging purposes.
        """
        super().__init__(name=name)
        self._channel_registry: ChannelRegistry = channel_registry
        self._data_sourcing_request_sender: Sender[ComponentMetricRequest] = (
            data_sourcing_request_sender
        )
        self._resampling_request_receiver: Receiver[ComponentMetricRequest] = (
            resampling_request_receiver
        )
        self._resampler: Resampler = Resampler(config)
        self._active_req_channels: set[str] = set()

    async def _subscribe(self, request: ComponentMetricRequest) -> None:
        """Request data for a component metric.

        Args:
            request: The request for component metric data.
        """
        request_channel_name = request.get_channel_name()

        # If we are already handling this request, there is nothing to do.
        if request_channel_name in self._active_req_channels:
            return

        self._active_req_channels.add(request_channel_name)

        data_source_request = dataclasses.replace(
            request, namespace=request.namespace + ":Source"
        )
        data_source_channel_name = data_source_request.get_channel_name()
        await self._data_sourcing_request_sender.send(data_source_request)
        receiver = self._channel_registry.get_or_create(
            Sample[Quantity], data_source_channel_name
        ).new_receiver()

        # This is a temporary hack until the Sender implementation uses
        # exceptions to report errors.
        sender = self._channel_registry.get_or_create(
            Sample[Quantity], request_channel_name
        ).new_sender()

        self._resampler.add_timeseries(request_channel_name, receiver, sender.send)

    async def _process_resampling_requests(self) -> None:
        """Process resampling data requests."""
        async for request in self._resampling_request_receiver:
            await self._subscribe(request)

    async def _run(self) -> None:
        """Resample known component metrics and process resampling requests.

        If there is a resampling error while resampling some component metric,
        then that metric will be discarded and not resampled any more. Any
        other error will be propagated (most likely ending in the actor being
        restarted).

        This method creates 2 main tasks:

        - One task to process incoming subscription requests to resample new metrics.
        - One task to run the resampler.
        """
        tasks_to_cancel: set[asyncio.Task[None]] = set()
        subscriptions_task: asyncio.Task[None] | None = None
        resampling_task: asyncio.Task[None] | None = None

        try:
            while True:
                if subscriptions_task is None or subscriptions_task.done():
                    subscriptions_task = asyncio.create_task(
                        self._process_resampling_requests()
                    )
                    tasks_to_cancel.add(subscriptions_task)

                if resampling_task is None or resampling_task.done():
                    resampling_task = asyncio.create_task(self._resampler.resample())
                    tasks_to_cancel.add(resampling_task)

                done, _ = await asyncio.wait(
                    [resampling_task, subscriptions_task],
                    return_when=asyncio.FIRST_COMPLETED,
                )

                if subscriptions_task in done:
                    tasks_to_cancel.remove(subscriptions_task)
                    self._log_subscriptions_task_error(subscriptions_task)

                if resampling_task in done:
                    tasks_to_cancel.remove(resampling_task)
                    self._log_resampling_task_error(resampling_task)

        finally:
            await asyncio.gather(*[cancel_and_await(t) for t in tasks_to_cancel])

            # XXX: Here we should probably do a:  pylint: disable=fixme
            # await self._resampler.stop()
            # But since the actor will be restarted, the internal state would
            # be broken if we stop the resampler.
            #
            # We have an even bigger problem with this naive restarting
            # approach, as restarting this actor without really resetting its
            # state would be mostly the same as not really leaving the run()
            # method and just swallow any exception, which doesn't look super
            # smart.

    def _log_subscriptions_task_error(
        self, subscriptions_task: asyncio.Task[None]
    ) -> None:
        """Log an error from a stopped subscriptions task.

        Args:
            subscriptions_task: The subscriptions task.
        """
        try:
            subscriptions_task.result()
        # pylint: disable-next=broad-except
        except (Exception, asyncio.CancelledError):
            _logger.exception(
                "The subscriptions task ended with an exception, restarting..."
            )
        else:
            _logger.error("The subscriptions task ended unexpectedly, restarting...")

    def _log_resampling_task_error(self, resampling_task: asyncio.Task[None]) -> None:
        """Log an error from a stopped resampling task.

        Args:
            resampling_task: The resampling task.
        """
        # The resampler shouldn't be cancelled or end without an exception
        try:
            resampling_task.result()
        except ResamplingError as error:
            for source, source_error in error.exceptions.items():
                _logger.error(
                    "Error resampling source %s, removing source",
                    source,
                    exc_info=source_error,
                )
                removed = self._resampler.remove_timeseries(source)
                if not removed:
                    _logger.error(
                        "Got an exception from an unknown source: "
                        "source=%r, exception=%r",
                        source,
                        source_error,
                    )
        # pylint: disable-next=broad-except
        except (Exception, asyncio.CancelledError):
            # We don't know what to do with something other than
            # ResamplingError, so we log it, restart, and hope for the best.
            _logger.exception(
                "The resample() function got an unexpected error, restarting..."
            )
        else:
            # The resample function should not end normally, so we log it,
            # restart, and hope for the best.
            _logger.error(
                "The resample() function ended without an exception, restarting..."
            )



================================================
FILE: src/frequenz/sdk/microgrid/component_graph.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Defines a graph representation of how microgrid components are connected.

The component graph is an approximate representation of the microgrid circuit,
abstracted to a level appropriate for higher-level monitoring and control.
Common use cases include:

* Combining component measurements to compute grid power or onsite load by using
  the graph structure to determine which measurements to aggregate

* Identifying which inverter(s) need to be engaged to charge or discharge
  a particular battery based on their connectivity in the graph

* Understanding which power flows in the microgrid are derived from green vs
  grey sources based on the component connectivity

The graph deliberately does not include all pieces of hardware placed in the microgrid,
instead limiting itself to just those that are needed to monitor and control the
flow of power.
"""

import asyncio
import logging
from abc import ABC, abstractmethod
from collections.abc import Callable, Iterable

import networkx as nx
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    Component,
    ComponentCategory,
    Connection,
    InverterType,
    MicrogridApiClient,
)

_logger = logging.getLogger(__name__)

# pylint: disable=too-many-lines


# Constant to store the actual objects as data attached to the graph nodes and edges
_DATA_KEY = "data"


class InvalidGraphError(Exception):
    """Exception type that will be thrown if graph data is not valid."""


class ComponentGraph(ABC):
    """Interface for component graph implementations."""

    @abstractmethod
    def components(
        self,
        component_ids: set[ComponentId] | None = None,
        component_categories: set[ComponentCategory] | None = None,
    ) -> set[Component]:
        """Fetch the components of the microgrid.

        Args:
            component_ids: The component IDs that the components must match.
            component_categories: The component categories that the components must match.

        Returns:
            The set of components currently connected to the microgrid, filtered by
                the provided `component_ids` and `component_categories` values.
        """

    @abstractmethod
    def connections(
        self,
        start: set[ComponentId] | None = None,
        end: set[ComponentId] | None = None,
    ) -> set[Connection]:
        """Fetch the connections between microgrid components.

        Args:
            start: The component IDs that the connections' start must match.
            end: The component IDs that the connections' end must match.

        Returns:
            The set of connections between components in the microgrid, filtered by
                the provided `start`/`end` choices.
        """

    @abstractmethod
    def predecessors(self, component_id: ComponentId) -> set[Component]:
        """Fetch the graph predecessors of the specified component.

        Args:
            component_id: The IDs of the components whose predecessors should be
                fetched.

        Returns:
            The set of components that are predecessors of `component_id`, i.e. for
                which there is a connection from each of these components to
                `component_id`.

        Raises:
            KeyError: If the specified `component_id` is not in the graph.
        """

    @abstractmethod
    def successors(self, component_id: ComponentId) -> set[Component]:
        """Fetch the graph successors of the specified component.

        Args:
            component_id: The IDs of the components whose successors should be fetched.

        Returns:
            The set of components that are successors of `component_id`, i.e. for
                which there is a connection from `component_id` to each of these
                components.

        Raises:
            KeyError: If the specified `component_id` is not in the graph
        """

    @abstractmethod
    def is_grid_meter(self, component: Component) -> bool:
        """Check if the specified component is a grid meter.

        This is done by checking if the component is the only successor to the `Grid`
        component.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is a grid meter.
        """

    @abstractmethod
    def is_pv_inverter(self, component: Component) -> bool:
        """Check if the specified component is a PV inverter.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is a PV inverter.
        """

    @abstractmethod
    def is_pv_meter(self, component: Component) -> bool:
        """Check if the specified component is a PV meter.

        This is done by checking if the component has only PV inverters as its
        successors.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is a PV meter.
        """

    @abstractmethod
    def is_pv_chain(self, component: Component) -> bool:
        """Check if the specified component is part of a PV chain.

        A component is part of a PV chain if it is a PV meter or a PV inverter.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is part of a PV chain.
        """

    @abstractmethod
    def is_battery_inverter(self, component: Component) -> bool:
        """Check if the specified component is a battery inverter.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is a battery inverter.
        """

    @abstractmethod
    def is_battery_meter(self, component: Component) -> bool:
        """Check if the specified component is a battery meter.

        This is done by checking if the component has only battery inverters as its
        predecessors.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is a battery meter.
        """

    @abstractmethod
    def is_battery_chain(self, component: Component) -> bool:
        """Check if the specified component is part of a battery chain.

        A component is part of a battery chain if it is a battery meter or a battery
        inverter.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is part of a battery chain.
        """

    @abstractmethod
    def is_ev_charger(self, component: Component) -> bool:
        """Check if the specified component is an EV charger.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is an EV charger.
        """

    @abstractmethod
    def is_ev_charger_meter(self, component: Component) -> bool:
        """Check if the specified component is an EV charger meter.

        This is done by checking if the component has only EV chargers as its
        successors.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is an EV charger meter.
        """

    @abstractmethod
    def is_ev_charger_chain(self, component: Component) -> bool:
        """Check if the specified component is part of an EV charger chain.

        A component is part of an EV charger chain if it is an EV charger meter or an
        EV charger.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is part of an EV charger chain.
        """

    @abstractmethod
    def is_chp(self, component: Component) -> bool:
        """Check if the specified component is a CHP.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is a CHP.
        """

    @abstractmethod
    def is_chp_meter(self, component: Component) -> bool:
        """Check if the specified component is a CHP meter.

        This is done by checking if the component has only CHPs as its successors.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is a CHP meter.
        """

    @abstractmethod
    def is_chp_chain(self, component: Component) -> bool:
        """Check if the specified component is part of a CHP chain.

        A component is part of a CHP chain if it is a CHP meter or a CHP.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is part of a CHP chain.
        """

    @abstractmethod
    def dfs(
        self,
        current_node: Component,
        visited: set[Component],
        condition: Callable[[Component], bool],
    ) -> set[Component]:
        """Search for components that fulfill the condition in the Graph.

        DFS is used for searching the graph. The graph traversal is stopped
        once a component fulfills the condition.

        Args:
            current_node: The current node to search from.
            visited: The set of visited nodes.
            condition: The condition function to check for.

        Returns:
            A set of component IDs where the corresponding components fulfill
                the `condition` function.
        """

    @abstractmethod
    def find_first_descendant_component(
        self,
        *,
        descendant_categories: Iterable[ComponentCategory],
    ) -> Component:
        """Find the first descendant component given root and descendant categories.

        This method looks for the first descendant component from the GRID
        component, considering only the immediate descendants.

        The priority of the component to search for is determined by the order
        of the descendant categories, with the first category having the
        highest priority.

        Args:
            descendant_categories: The descendant classes to search for the first
                descendant component in.

        Returns:
            The first descendant component found in the component graph,
                considering the specified `descendants` categories.
        """


class _MicrogridComponentGraph(
    ComponentGraph
):  # pylint: disable=too-many-public-methods
    """ComponentGraph implementation designed to work with the microgrid API.

    For internal-only use of the `microgrid` package.
    """

    def __init__(
        self,
        components: set[Component] | None = None,
        connections: set[Connection] | None = None,
    ) -> None:
        """Initialize the component graph.

        Args:
            components: The components to initialize the graph with. If set, must
                provide `connections` as well.
            connections: The connections to initialize the graph with. If set, must
                provide `components` as well.

        Raises:
            InvalidGraphError: If `components` and `connections` are not both `None`
                and either of them is either `None` or empty.
        """
        self._graph: nx.DiGraph = nx.DiGraph()

        if components is None and connections is None:
            return

        if components is None or len(components) == 0:
            raise InvalidGraphError("Must provide components as well as connections")

        if connections is None or len(connections) == 0:
            raise InvalidGraphError("Must provide connections as well as components")

        self.refresh_from(components, connections)
        self.validate()

    def components(
        self,
        component_ids: set[ComponentId] | None = None,
        component_categories: set[ComponentCategory] | None = None,
    ) -> set[Component]:
        """Fetch the components of the microgrid.

        Args:
            component_ids: The component IDs that the components must match.
            component_categories: The component categories that the components must match.

        Returns:
            The set of components currently connected to the microgrid, filtered by
                the provided `component_ids` and `component_categories` values.
        """
        selection_ids = (
            self._graph.nodes
            if component_ids is None
            else component_ids & self._graph.nodes
        )
        selection: Iterable[Component] = (
            self._graph.nodes[i][_DATA_KEY] for i in selection_ids
        )

        if component_categories is not None:
            selection = filter(lambda c: c.category in component_categories, selection)

        return set(selection)

    def connections(
        self,
        start: set[ComponentId] | None = None,
        end: set[ComponentId] | None = None,
    ) -> set[Connection]:
        """Fetch the connections between microgrid components.

        Args:
            start: The component IDs that the connections' start must match.
            end: The component IDs that the connections' end must match.

        Returns:
            The set of connections between components in the microgrid, filtered by
                the provided `start`/`end` choices.
        """
        match (start, end):
            case (None, None):
                selection_ids = self._graph.edges
            case (None, _):
                selection_ids = self._graph.in_edges(end)
            case (_, None):
                selection_ids = self._graph.out_edges(start)
            case (_, _):
                start_edges = self._graph.out_edges(start)
                end_edges = self._graph.in_edges(end)
                selection_ids = set(start_edges).intersection(end_edges)

        return set(self._graph.edges[i][_DATA_KEY] for i in selection_ids)

    def predecessors(self, component_id: ComponentId) -> set[Component]:
        """Fetch the graph predecessors of the specified component.

        Args:
            component_id: The IDs of the components whose predecessors should be
                fetched.

        Returns:
            The set of components that are predecessors of `component_id`, i.e. for
                which there is a connection from each of these components to
                `component_id`.

        Raises:
            KeyError: If the specified `component_id` is not in the graph.
        """
        if component_id not in self._graph:
            raise KeyError(
                f"Component with {component_id} not in graph, cannot get predecessors!"
            )

        predecessors_ids = self._graph.predecessors(component_id)

        return set(map(lambda idx: self._graph.nodes[idx][_DATA_KEY], predecessors_ids))

    def successors(self, component_id: ComponentId) -> set[Component]:
        """Fetch the graph successors of the specified component.

        Args:
            component_id: The IDs of the components whose successors should be fetched.

        Returns:
            The set of components that are successors of `component_id`, i.e. for
                which there is a connection from `component_id` to each of these
                components.

        Raises:
            KeyError: If the specified `component_id` is not in the graph
        """
        if component_id not in self._graph:
            raise KeyError(
                f"Component with {component_id} not in graph, cannot get successors!"
            )

        successors_ids = self._graph.successors(component_id)

        return set(map(lambda idx: self._graph.nodes[idx][_DATA_KEY], successors_ids))

    def refresh_from(
        self,
        components: set[Component],
        connections: set[Connection],
        correct_errors: Callable[["_MicrogridComponentGraph"], None] | None = None,
    ) -> None:
        """Refresh the graph from the provided list of components and connections.

        This will completely overwrite the current graph data with the provided
        components and connections.

        Args:
            components: The components to include in the graph.
            connections: The connections to include in the graph.
            correct_errors: The callback that, if set, will be invoked if the
                provided graph data is in any way invalid (it will attempt to
                correct the errors by inferring what the correct data should be).

        Raises:
            InvalidGraphError: If the provided `components` and `connections`
                do not form a valid component graph and `correct_errors` does
                not fix it.
        """
        if not all(component.is_valid() for component in components):
            raise InvalidGraphError(f"Invalid components in input: {components}")
        if not all(connection.is_valid() for connection in connections):
            raise InvalidGraphError(f"Invalid connections in input: {connections}")

        new_graph = nx.DiGraph()
        for component in components:
            new_graph.add_node(component.component_id, **{_DATA_KEY: component})

        # Store the original connection object in the edge data (third item in the
        # tuple) so that we can retrieve it later.
        for connection in connections:
            new_graph.add_edge(
                connection.start, connection.end, **{_DATA_KEY: connection}
            )

        # check if we can construct a valid ComponentGraph
        # from the new NetworkX graph data
        _provisional = _MicrogridComponentGraph()
        _provisional._graph = new_graph  # pylint: disable=protected-access
        if correct_errors is not None:
            try:
                _provisional.validate()
            except InvalidGraphError as err:
                _logger.warning("Attempting to fix invalid component data: %s", err)
                correct_errors(_provisional)

        try:
            _provisional.validate()
        except Exception as err:
            _logger.error("Failed to parse component graph: %s", err)
            raise InvalidGraphError(
                "Cannot populate component graph from provided input!"
            ) from err

        old_graph = self._graph
        self._graph = new_graph
        old_graph.clear()  # just in case any references remain, but should not

    async def refresh_from_api(
        self,
        api: MicrogridApiClient,
        correct_errors: Callable[["_MicrogridComponentGraph"], None] | None = None,
    ) -> None:
        """Refresh the contents of a component graph from the remote API.

        Args:
            api: The API client from which to fetch graph data.
            correct_errors: The callback that, if set, will be invoked if the
                provided graph data is in any way invalid (it will attempt to
                correct the errors by inferring what the correct data should be).
        """
        components, connections = await asyncio.gather(
            api.components(),
            api.connections(),
        )

        self.refresh_from(set(components), set(connections), correct_errors)

    def validate(self) -> None:
        """Check that the component graph contains valid microgrid data."""
        self._validate_graph()
        self._validate_graph_root()
        self._validate_grid_endpoint()
        self._validate_intermediary_components()
        self._validate_leaf_components()

    def is_grid_meter(self, component: Component) -> bool:
        """Check if the specified component is a grid meter.

        This is done by checking if the component is the only successor to the `Grid`
        component.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is a grid meter.
        """
        if component.category != ComponentCategory.METER:
            return False

        predecessors = self.predecessors(component.component_id)
        if len(predecessors) != 1:
            return False

        predecessor = next(iter(predecessors))
        if predecessor.category != ComponentCategory.GRID:
            return False

        grid_successors = self.successors(predecessor.component_id)
        return len(grid_successors) == 1

    def is_pv_inverter(self, component: Component) -> bool:
        """Check if the specified component is a PV inverter.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is a PV inverter.
        """
        return (
            component.category == ComponentCategory.INVERTER
            and component.type == InverterType.SOLAR
        )

    def is_pv_meter(self, component: Component) -> bool:
        """Check if the specified component is a PV meter.

        This is done by checking if the component has only PV inverters as its
        successors.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is a PV meter.
        """
        successors = self.successors(component.component_id)
        return (
            component.category == ComponentCategory.METER
            and not self.is_grid_meter(component)
            and len(successors) > 0
            and all(
                self.is_pv_inverter(successor)
                for successor in self.successors(component.component_id)
            )
        )

    def is_pv_chain(self, component: Component) -> bool:
        """Check if the specified component is part of a PV chain.

        A component is part of a PV chain if it is either a PV inverter or a PV
        meter.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is part of a PV chain.
        """
        return self.is_pv_inverter(component) or self.is_pv_meter(component)

    def is_ev_charger(self, component: Component) -> bool:
        """Check if the specified component is an EV charger.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is an EV charger.
        """
        return component.category == ComponentCategory.EV_CHARGER

    def is_ev_charger_meter(self, component: Component) -> bool:
        """Check if the specified component is an EV charger meter.

        This is done by checking if the component has only EV chargers as its
        successors.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is an EV charger meter.
        """
        successors = self.successors(component.component_id)
        return (
            component.category == ComponentCategory.METER
            and not self.is_grid_meter(component)
            and len(successors) > 0
            and all(self.is_ev_charger(successor) for successor in successors)
        )

    def is_ev_charger_chain(self, component: Component) -> bool:
        """Check if the specified component is part of an EV charger chain.

        A component is part of an EV charger chain if it is either an EV charger or an
        EV charger meter.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is part of an EV charger chain.
        """
        return self.is_ev_charger(component) or self.is_ev_charger_meter(component)

    def is_battery_inverter(self, component: Component) -> bool:
        """Check if the specified component is a battery inverter.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is a battery inverter.
        """
        return (
            component.category == ComponentCategory.INVERTER
            and component.type == InverterType.BATTERY
        )

    def is_battery_meter(self, component: Component) -> bool:
        """Check if the specified component is a battery meter.

        This is done by checking if the component has only battery inverters as
        its successors.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is a battery meter.
        """
        successors = self.successors(component.component_id)
        return (
            component.category == ComponentCategory.METER
            and not self.is_grid_meter(component)
            and len(successors) > 0
            and all(self.is_battery_inverter(successor) for successor in successors)
        )

    def is_battery_chain(self, component: Component) -> bool:
        """Check if the specified component is part of a battery chain.

        A component is part of a battery chain if it is either a battery inverter or a
        battery meter.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is part of a battery chain.
        """
        return self.is_battery_inverter(component) or self.is_battery_meter(component)

    def is_chp(self, component: Component) -> bool:
        """Check if the specified component is a CHP.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is a CHP.
        """
        return component.category == ComponentCategory.CHP

    def is_chp_meter(self, component: Component) -> bool:
        """Check if the specified component is a CHP meter.

        This is done by checking if the component has only CHPs as its
        successors.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is a CHP meter.
        """
        successors = self.successors(component.component_id)
        return (
            component.category == ComponentCategory.METER
            and not self.is_grid_meter(component)
            and len(successors) > 0
            and all(self.is_chp(successor) for successor in successors)
        )

    def is_chp_chain(self, component: Component) -> bool:
        """Check if the specified component is part of a CHP chain.

        A component is part of a CHP chain if it is either a CHP or a CHP meter.

        Args:
            component: The component to check.

        Returns:
            Whether the specified component is part of a CHP chain.
        """
        return self.is_chp(component) or self.is_chp_meter(component)

    def dfs(
        self,
        current_node: Component,
        visited: set[Component],
        condition: Callable[[Component], bool],
    ) -> set[Component]:
        """Search for components that fulfill the condition in the Graph.

        DFS is used for searching the graph. The graph traversal is stopped
        once a component fulfills the condition.

        Args:
            current_node: The current node to search from.
            visited: The set of visited nodes.
            condition: The condition function to check for.

        Returns:
            A set of component IDs where the corresponding components fulfill
            the condition function.
        """
        if current_node in visited:
            return set()

        visited.add(current_node)

        if condition(current_node):
            return {current_node}

        component: set[Component] = set()

        for successor in self.successors(current_node.component_id):
            component.update(self.dfs(successor, visited, condition))

        return component

    def find_first_descendant_component(
        self,
        *,
        descendant_categories: Iterable[ComponentCategory],
    ) -> Component:
        """Find the first descendant component given root and descendant categories.

        This method looks for the first descendant component from the GRID
        component, considering only the immediate descendants.

        The priority of the component to search for is determined by the order
        of the descendant categories, with the first category having the
        highest priority.

        Args:
            descendant_categories: The descendant classes to search for the first
                descendant component in.

        Returns:
            The first descendant component found in the component graph,
                considering the specified `descendants` categories.

        Raises:
            InvalidGraphError: When no GRID component is found in the graph.
            ValueError: When no component is found in the given categories.
        """
        root_component = next(
            (
                comp
                for comp in self.components(
                    component_categories={ComponentCategory.GRID}
                )
            ),
            None,
        )
        if root_component is None:
            raise InvalidGraphError("No GRID component found in the component graph!")

        # Sort by component ID to ensure consistent results.
        successors = sorted(
            self.successors(root_component.component_id),
            key=lambda comp: comp.component_id,
        )

        def find_component(component_category: ComponentCategory) -> Component | None:
            return next(
                (comp for comp in successors if comp.category == component_category),
                None,
            )

        # Find the first component that matches the given descendant categories
        # in the order of the categories list.
        component = next(filter(None, map(find_component, descendant_categories)), None)

        if component is None:
            raise ValueError("Component not found in any of the descendant categories.")

        return component

    def _validate_graph(self) -> None:
        """Check that the underlying graph data is valid.

        Raises:
            InvalidGraphError: If:
                - There are no components.
                - There are no connections.
                - The graph is not a tree.
                - Any node lacks its associated component data.
        """
        if self._graph.number_of_nodes() == 0:
            raise InvalidGraphError("No components in graph!")

        if self._graph.number_of_edges() == 0:
            raise InvalidGraphError("No connections in component graph!")

        if not nx.is_directed_acyclic_graph(self._graph):
            raise InvalidGraphError("Component graph is not a tree!")

        # This check doesn't seem to have much sense, it only search for nodes without
        # data associated with them. We leave it here for now, but we should consider
        # removing it in the future.
        if undefined := [
            node[0] for node in self._graph.nodes(data=True) if len(node[1]) == 0
        ]:
            undefined_str = ", ".join(map(str, sorted(undefined)))
            raise InvalidGraphError(
                f"Missing definition for graph components: {undefined_str}"
            )

        # should be true as a consequence of checks above
        if sum(1 for _ in self.components()) <= 0:
            raise InvalidGraphError("Graph must have a least one component!")
        if sum(1 for _ in self.connections()) <= 0:
            raise InvalidGraphError("Graph must have a least one connection!")

        # should be true as a consequence of the tree property:
        # there should be no unconnected components
        unconnected = filter(
            lambda c: self._graph.degree(c.component_id) == 0, self.components()
        )
        if sum(1 for _ in unconnected) != 0:
            raise InvalidGraphError(
                "Every component must have at least one connection!"
            )

    def _validate_graph_root(self) -> None:
        """Check that there is exactly one node without predecessors, of valid type.

        Raises:
            InvalidGraphError: If there is more than one node without predecessors,
                or if there is a single such node that is not one of NONE or GRID.
        """
        no_predecessors = filter(
            lambda c: self._graph.in_degree(c.component_id) == 0,
            self.components(),
        )

        valid_root_types = {
            ComponentCategory.NONE,
            ComponentCategory.GRID,
        }

        valid_roots = list(
            filter(lambda c: c.category in valid_root_types, no_predecessors)
        )

        if len(valid_roots) == 0:
            raise InvalidGraphError("No valid root nodes of component graph!")

        if len(valid_roots) > 1:
            raise InvalidGraphError(f"Multiple potential root nodes: {valid_roots}")

        root = valid_roots[0]
        if self._graph.out_degree(root.component_id) == 0:
            raise InvalidGraphError(f"Graph root {root} has no successors!")

    def _validate_grid_endpoint(self) -> None:
        """Check that the grid endpoint is configured correctly in the graph.

        Raises:
            InvalidGraphError: If there is more than one grid endpoint in the
                graph, or if the grid endpoint has predecessors (if it exists,
                then it should be the root of the component-graph tree), or if
                it has no successors in the graph (i.e. it is not connected to
                anything).
        """
        grid = list(self.components(component_categories={ComponentCategory.GRID}))

        if len(grid) == 0:
            # it's OK to not have a grid endpoint as long as other properties
            # (checked by other `_validate...` methods) hold
            return

        if len(grid) > 1:
            raise InvalidGraphError(
                f"Multiple grid endpoints in component graph: {grid}"
            )

        grid_id = grid[0].component_id
        if self._graph.in_degree(grid_id) > 0:
            grid_predecessors = list(self.predecessors(grid_id))
            raise InvalidGraphError(
                f"Grid endpoint with {grid_id} has graph predecessors: {grid_predecessors}"
            )

        if self._graph.out_degree(grid_id) == 0:
            raise InvalidGraphError(
                f"Grid endpoint with {grid_id} has no graph successors!"
            )

    def _validate_intermediary_components(self) -> None:
        """Check that intermediary components (e.g. meters) are configured correctly.

        Intermediary components are components that should have both predecessors and
        successors in the component graph, such as METER, or INVERTER.

        Raises:
            InvalidGraphError: If any intermediary component has zero predecessors
                or zero successors.
        """
        intermediary_components = list(
            self.components(component_categories={ComponentCategory.INVERTER})
        )

        missing_predecessors = list(
            filter(
                lambda c: sum(1 for _ in self.predecessors(c.component_id)) == 0,
                intermediary_components,
            )
        )
        if len(missing_predecessors) > 0:
            raise InvalidGraphError(
                "Intermediary components without graph predecessors: "
                f"{missing_predecessors}"
            )

    def _validate_leaf_components(self) -> None:
        """Check that leaf components (e.g. batteries) are configured correctly.

        Leaf components are components that should be leaves of the component-graph
        tree, such as LOAD, BATTERY or EV_CHARGER.  These should have only incoming
        connections and no outgoing connections.

        Raises:
            InvalidGraphError: If any leaf component in the graph has 0 predecessors,
                or has > 0 successors.
        """
        leaf_components = list(
            self.components(
                component_categories={
                    ComponentCategory.BATTERY,
                    ComponentCategory.EV_CHARGER,
                }
            )
        )

        missing_predecessors = list(
            filter(
                lambda c: sum(1 for _ in self.predecessors(c.component_id)) == 0,
                leaf_components,
            )
        )
        if len(missing_predecessors) > 0:
            raise InvalidGraphError(
                f"Leaf components without graph predecessors: {missing_predecessors}"
            )

        with_successors = list(
            filter(
                lambda c: sum(1 for _ in self.successors(c.component_id)) > 0,
                leaf_components,
            )
        )
        if len(with_successors) > 0:
            raise InvalidGraphError(
                f"Leaf components with graph successors: {with_successors}"
            )



================================================
FILE: src/frequenz/sdk/microgrid/connection_manager.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Microgrid Connection Manager singleton abstraction.

This module provides a singleton abstraction over the microgrid. The main
purpose is to provide the connection the microgrid API client and the microgrid
component graph.
"""

import logging
from abc import ABC, abstractmethod

from frequenz.client.common.microgrid import MicrogridId
from frequenz.client.microgrid import Location, Metadata, MicrogridApiClient

from .component_graph import ComponentGraph, _MicrogridComponentGraph

_logger = logging.getLogger(__name__)


class ConnectionManager(ABC):
    """Creates and stores core features."""

    def __init__(self, server_url: str) -> None:
        """Create object instance.

        Args:
            server_url: The location of the microgrid API server in the form of a URL.
                The following format is expected: `grpc://hostname{:port}{?ssl=ssl}`,
                where the port should be an int between `0` and `65535` (defaulting to
                `9090`) and ssl should be a boolean (defaulting to false). For example:
                `grpc://localhost:1090?ssl=true`.
        """
        super().__init__()
        self._server_url = server_url

    @property
    def server_url(self) -> str:
        """The location of the microgrid API server in the form of a URL."""
        return self._server_url

    @property
    @abstractmethod
    def api_client(self) -> MicrogridApiClient:
        """Get the MicrogridApiClient.

        Returns:
            api client
        """

    @property
    @abstractmethod
    def component_graph(self) -> ComponentGraph:
        """Get component graph.

        Returns:
            component graph
        """

    @property
    @abstractmethod
    def microgrid_id(self) -> MicrogridId | None:
        """Get the ID of the microgrid if available.

        Returns:
            the ID of the microgrid if available, None otherwise.
        """

    @property
    @abstractmethod
    def location(self) -> Location | None:
        """Get the location of the microgrid if available.

        Returns:
            the location of the microgrid if available, None otherwise.
        """

    async def _update_api(self, server_url: str) -> None:
        self._server_url = server_url

    @abstractmethod
    async def _initialize(self) -> None:
        """Initialize the object. This function should be called only once."""


class _InsecureConnectionManager(ConnectionManager):
    """Microgrid Api with insecure channel implementation."""

    def __init__(self, server_url: str) -> None:
        """Create and stores core features.

        Args:
            server_url: The location of the microgrid API server in the form of a URL.
                The following format is expected: `grpc://hostname{:port}{?ssl=ssl}`,
                where the port should be an int between `0` and `65535` (defaulting to
                `9090`) and ssl should be a boolean (defaulting to false). For example:
                `grpc://localhost:1090?ssl=true`.
        """
        super().__init__(server_url)
        self._api = MicrogridApiClient(server_url)
        # To create graph from the api we need await.
        # So create empty graph here, and update it in `run` method.
        self._graph = _MicrogridComponentGraph()

        self._metadata: Metadata
        """The metadata of the microgrid."""

    @property
    def api_client(self) -> MicrogridApiClient:
        """Get the MicrogridApiClient.

        Returns:
            api client
        """
        return self._api

    @property
    def microgrid_id(self) -> MicrogridId | None:
        """Get the ID of the microgrid if available.

        Returns:
            the ID of the microgrid if available, None otherwise.
        """
        return self._metadata.microgrid_id

    @property
    def location(self) -> Location | None:
        """Get the location of the microgrid if available.

        Returns:
            the location of the microgrid if available, None otherwise.
        """
        return self._metadata.location

    @property
    def component_graph(self) -> ComponentGraph:
        """Get component graph.

        Returns:
            component graph
        """
        return self._graph

    async def _update_api(self, server_url: str) -> None:
        """Update api with new host and port.

        Args:
            server_url: The new location of the microgrid API server in the form of a
                URL. The following format is expected:
                `grpc://hostname{:port}{?ssl=ssl}`, where the port should be an int
                between `0` and `65535` (defaulting to `9090`) and ssl should be
                a boolean (defaulting to false). For example:
                `grpc://localhost:1090?ssl=true`.
        """
        await super()._update_api(server_url)  # pylint: disable=protected-access

        self._api = MicrogridApiClient(server_url)
        await self._initialize()

    async def _initialize(self) -> None:
        self._metadata = await self._api.metadata()
        await self._graph.refresh_from_api(self._api)


_CONNECTION_MANAGER: ConnectionManager | None = None
"""The ConnectionManager singleton instance."""


async def initialize(server_url: str) -> None:
    """Initialize the MicrogridApi. This function should be called only once.

    Args:
        server_url: The location of the microgrid API server in the form of a URL.
            The following format is expected: `grpc://hostname{:port}{?ssl=ssl}`,
            where the port should be an int between `0` and `65535` (defaulting to
            `9090`) and ssl should be a boolean (defaulting to false). For example:
            `grpc://localhost:1090?ssl=true`.

    Raises:
        AssertionError: If method was called more then once.
    """
    # From Doc: pylint just try to discourage this usage.
    # That doesn't mean you cannot use it.
    global _CONNECTION_MANAGER  # pylint: disable=global-statement

    if _CONNECTION_MANAGER is not None:
        raise AssertionError("MicrogridApi was already initialized.")

    _logger.info("Connecting to microgrid at %s", server_url)

    microgrid_api = _InsecureConnectionManager(server_url)
    await microgrid_api._initialize()  # pylint: disable=protected-access

    # Check again that _MICROGRID_API is None in case somebody had the great idea of
    # calling initialize() twice and in parallel.
    if _CONNECTION_MANAGER is not None:
        raise AssertionError("MicrogridApi was already initialized.")

    _CONNECTION_MANAGER = microgrid_api


def get() -> ConnectionManager:
    """Get the MicrogridApi instance created by initialize().

    This function should be only called after initialize().

    Raises:
        RuntimeError: Raised when:
            * If `initialize()` method was not called before this call.
            * If `initialize()` methods was called but was not awaited and instance was
                not created yet.

    Returns:
        MicrogridApi instance.
    """
    if _CONNECTION_MANAGER is None:
        raise RuntimeError(
            "ConnectionManager is not initialized. "
            "Call `await microgrid.initialize()` first."
        )

    return _CONNECTION_MANAGER



================================================
FILE: src/frequenz/sdk/microgrid/_data_sourcing/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""The DataSourcingActor."""

from ._component_metric_request import ComponentMetricId, ComponentMetricRequest
from .data_sourcing import DataSourcingActor

__all__ = [
    "ComponentMetricId",
    "ComponentMetricRequest",
    "DataSourcingActor",
]



================================================
FILE: src/frequenz/sdk/microgrid/_data_sourcing/_component_metric_request.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""The ComponentMetricRequest class."""

from dataclasses import dataclass
from datetime import datetime

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentMetricId

__all__ = ["ComponentMetricRequest", "ComponentMetricId"]


@dataclass
class ComponentMetricRequest:
    """A request to start streaming a component's metric.

    Requesters use this class to specify which component's metric they want to subscribe
    to, including the component ID, metric ID, and an optional start time. The
    `namespace` is defined by the requester and influences the construction of the
    channel name via the `get_channel_name()` method.

    The `namespace` allows differentiation of data streams for the same component and
    metric. For example, requesters can use different `namespace` values to subscribe to
    raw or resampled data streams separately. This ensures that each requester receives
    the appropriate type of data without interference. Requests with the same
    `namespace`, `component_id`, and `metric_id` will use the same channel, preventing
    unnecessary duplication of data streams.

    The requester and provider must use the same channel name so that they can
    independently retrieve the same channel from the `ChannelRegistry`.  This is
    achieved by using the `get_channel_name` method to generate the name on both sides
    based on parameters set by the requesters.
    """

    namespace: str
    """A client-defined identifier influencing the channel name."""

    component_id: ComponentId
    """The ID of the requested component."""

    metric_id: ComponentMetricId
    """The ID of the requested component's metric."""

    start_time: datetime | None
    """The start time from which data is required.

    If None, only live data is streamed.
    """

    def get_channel_name(self) -> str:
        """Construct the channel name based on the request parameters.

        Returns:
            A string representing the channel name.
        """
        start = f",start={self.start_time}" if self.start_time else ""
        return (
            "component_metric_request<"
            f"namespace={self.namespace},"
            f"component_id={self.component_id},"
            f"metric_id={self.metric_id.name}"
            f"{start}"
            ">"
        )



================================================
FILE: src/frequenz/sdk/microgrid/_data_sourcing/data_sourcing.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""The DataSourcing Actor."""

from frequenz.channels import Receiver

from ..._internal._channels import ChannelRegistry
from ...actor import Actor
from ._component_metric_request import ComponentMetricRequest
from .microgrid_api_source import MicrogridApiSource


class DataSourcingActor(Actor):
    """An actor that provides data streams of metrics as time series."""

    def __init__(
        self,
        request_receiver: Receiver[ComponentMetricRequest],
        registry: ChannelRegistry,
        *,
        name: str | None = None,
    ) -> None:
        """Create a `DataSourcingActor` instance.

        Args:
            request_receiver: A channel receiver to accept metric requests from.
            registry: A channel registry.  To be replaced by a singleton
                instance.
            name: The name of the actor. If `None`, `str(id(self))` will be used. This
                is used mostly for debugging purposes.
        """
        super().__init__(name=name)
        self._request_receiver = request_receiver
        self._microgrid_api_source = MicrogridApiSource(registry)

    async def _run(self) -> None:
        """Run the actor."""
        async for request in self._request_receiver:
            await self._microgrid_api_source.add_metric(request)



================================================
FILE: src/frequenz/sdk/microgrid/_data_sourcing/microgrid_api_source.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""The Microgrid API data source for the DataSourcingActor."""

import asyncio
import logging
from collections.abc import Callable
from typing import Any

from frequenz.channels import Receiver, Sender
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    BatteryData,
    ComponentCategory,
    ComponentMetricId,
    EVChargerData,
    InverterData,
    MeterData,
)
from frequenz.quantities import Quantity

from ..._internal._asyncio import run_forever
from ..._internal._channels import ChannelRegistry
from ...microgrid import connection_manager
from ...timeseries import Sample
from ._component_metric_request import ComponentMetricRequest

_logger = logging.getLogger(__name__)

_MeterDataMethods: dict[ComponentMetricId, Callable[[MeterData], float]] = {
    ComponentMetricId.ACTIVE_POWER: lambda msg: msg.active_power,
    ComponentMetricId.ACTIVE_POWER_PHASE_1: lambda msg: msg.active_power_per_phase[0],
    ComponentMetricId.ACTIVE_POWER_PHASE_2: lambda msg: msg.active_power_per_phase[1],
    ComponentMetricId.ACTIVE_POWER_PHASE_3: lambda msg: msg.active_power_per_phase[2],
    ComponentMetricId.CURRENT_PHASE_1: lambda msg: msg.current_per_phase[0],
    ComponentMetricId.CURRENT_PHASE_2: lambda msg: msg.current_per_phase[1],
    ComponentMetricId.CURRENT_PHASE_3: lambda msg: msg.current_per_phase[2],
    ComponentMetricId.VOLTAGE_PHASE_1: lambda msg: msg.voltage_per_phase[0],
    ComponentMetricId.VOLTAGE_PHASE_2: lambda msg: msg.voltage_per_phase[1],
    ComponentMetricId.VOLTAGE_PHASE_3: lambda msg: msg.voltage_per_phase[2],
    ComponentMetricId.FREQUENCY: lambda msg: msg.frequency,
    ComponentMetricId.REACTIVE_POWER: lambda msg: msg.reactive_power,
    ComponentMetricId.REACTIVE_POWER_PHASE_1: lambda msg: msg.reactive_power_per_phase[
        0
    ],
    ComponentMetricId.REACTIVE_POWER_PHASE_2: lambda msg: msg.reactive_power_per_phase[
        1
    ],
    ComponentMetricId.REACTIVE_POWER_PHASE_3: lambda msg: msg.reactive_power_per_phase[
        2
    ],
}

_BatteryDataMethods: dict[ComponentMetricId, Callable[[BatteryData], float]] = {
    ComponentMetricId.SOC: lambda msg: msg.soc,
    ComponentMetricId.SOC_LOWER_BOUND: lambda msg: msg.soc_lower_bound,
    ComponentMetricId.SOC_UPPER_BOUND: lambda msg: msg.soc_upper_bound,
    ComponentMetricId.CAPACITY: lambda msg: msg.capacity,
    ComponentMetricId.POWER_INCLUSION_LOWER_BOUND: lambda msg: (
        msg.power_inclusion_lower_bound
    ),
    ComponentMetricId.POWER_EXCLUSION_LOWER_BOUND: lambda msg: (
        msg.power_exclusion_lower_bound
    ),
    ComponentMetricId.POWER_EXCLUSION_UPPER_BOUND: lambda msg: (
        msg.power_exclusion_upper_bound
    ),
    ComponentMetricId.POWER_INCLUSION_UPPER_BOUND: lambda msg: (
        msg.power_inclusion_upper_bound
    ),
    ComponentMetricId.TEMPERATURE: lambda msg: msg.temperature,
}

_InverterDataMethods: dict[ComponentMetricId, Callable[[InverterData], float]] = {
    ComponentMetricId.ACTIVE_POWER: lambda msg: msg.active_power,
    ComponentMetricId.ACTIVE_POWER_PHASE_1: lambda msg: msg.active_power_per_phase[0],
    ComponentMetricId.ACTIVE_POWER_PHASE_2: lambda msg: msg.active_power_per_phase[1],
    ComponentMetricId.ACTIVE_POWER_PHASE_3: lambda msg: msg.active_power_per_phase[2],
    ComponentMetricId.ACTIVE_POWER_INCLUSION_LOWER_BOUND: lambda msg: (
        msg.active_power_inclusion_lower_bound
    ),
    ComponentMetricId.ACTIVE_POWER_EXCLUSION_LOWER_BOUND: lambda msg: (
        msg.active_power_exclusion_lower_bound
    ),
    ComponentMetricId.ACTIVE_POWER_EXCLUSION_UPPER_BOUND: lambda msg: (
        msg.active_power_exclusion_upper_bound
    ),
    ComponentMetricId.ACTIVE_POWER_INCLUSION_UPPER_BOUND: lambda msg: (
        msg.active_power_inclusion_upper_bound
    ),
    ComponentMetricId.CURRENT_PHASE_1: lambda msg: msg.current_per_phase[0],
    ComponentMetricId.CURRENT_PHASE_2: lambda msg: msg.current_per_phase[1],
    ComponentMetricId.CURRENT_PHASE_3: lambda msg: msg.current_per_phase[2],
    ComponentMetricId.VOLTAGE_PHASE_1: lambda msg: msg.voltage_per_phase[0],
    ComponentMetricId.VOLTAGE_PHASE_2: lambda msg: msg.voltage_per_phase[1],
    ComponentMetricId.VOLTAGE_PHASE_3: lambda msg: msg.voltage_per_phase[2],
    ComponentMetricId.FREQUENCY: lambda msg: msg.frequency,
    ComponentMetricId.REACTIVE_POWER: lambda msg: msg.reactive_power,
    ComponentMetricId.REACTIVE_POWER_PHASE_1: lambda msg: msg.reactive_power_per_phase[
        0
    ],
    ComponentMetricId.REACTIVE_POWER_PHASE_2: lambda msg: msg.reactive_power_per_phase[
        1
    ],
    ComponentMetricId.REACTIVE_POWER_PHASE_3: lambda msg: msg.reactive_power_per_phase[
        2
    ],
}

_EVChargerDataMethods: dict[ComponentMetricId, Callable[[EVChargerData], float]] = {
    ComponentMetricId.ACTIVE_POWER: lambda msg: msg.active_power,
    ComponentMetricId.ACTIVE_POWER_PHASE_1: lambda msg: msg.active_power_per_phase[0],
    ComponentMetricId.ACTIVE_POWER_PHASE_2: lambda msg: msg.active_power_per_phase[1],
    ComponentMetricId.ACTIVE_POWER_PHASE_3: lambda msg: msg.active_power_per_phase[2],
    ComponentMetricId.CURRENT_PHASE_1: lambda msg: msg.current_per_phase[0],
    ComponentMetricId.CURRENT_PHASE_2: lambda msg: msg.current_per_phase[1],
    ComponentMetricId.CURRENT_PHASE_3: lambda msg: msg.current_per_phase[2],
    ComponentMetricId.VOLTAGE_PHASE_1: lambda msg: msg.voltage_per_phase[0],
    ComponentMetricId.VOLTAGE_PHASE_2: lambda msg: msg.voltage_per_phase[1],
    ComponentMetricId.VOLTAGE_PHASE_3: lambda msg: msg.voltage_per_phase[2],
    ComponentMetricId.FREQUENCY: lambda msg: msg.frequency,
    ComponentMetricId.REACTIVE_POWER: lambda msg: msg.reactive_power,
    ComponentMetricId.REACTIVE_POWER_PHASE_1: lambda msg: msg.reactive_power_per_phase[
        0
    ],
    ComponentMetricId.REACTIVE_POWER_PHASE_2: lambda msg: msg.reactive_power_per_phase[
        1
    ],
    ComponentMetricId.REACTIVE_POWER_PHASE_3: lambda msg: msg.reactive_power_per_phase[
        2
    ],
}


class MicrogridApiSource:
    """Fetches requested metrics from the Microgrid API.

    Used by the DataSourcingActor.
    """

    def __init__(
        self,
        registry: ChannelRegistry,
    ) -> None:
        """Create a `MicrogridApiSource` instance.

        Args:
            registry: A channel registry.  To be replaced by a singleton
                instance.
        """
        self._comp_categories_cache: dict[ComponentId, ComponentCategory] = {}

        self.comp_data_receivers: dict[ComponentId, Receiver[Any]] = {}
        """The dictionary of component IDs to data receivers."""

        self.comp_data_tasks: dict[ComponentId, asyncio.Task[None]] = {}
        """The dictionary of component IDs to asyncio tasks."""

        self._registry = registry
        self._req_streaming_metrics: dict[
            ComponentId, dict[ComponentMetricId, list[ComponentMetricRequest]]
        ] = {}

    async def _get_component_category(
        self, comp_id: ComponentId
    ) -> ComponentCategory | None:
        """Get the component category of the given component.

        Args:
            comp_id: Id of the requested component.

        Returns:
            The category of the given component, if it is a valid component, or None
                otherwise.
        """
        if comp_id in self._comp_categories_cache:
            return self._comp_categories_cache[comp_id]

        api = connection_manager.get().api_client
        for comp in await api.components():
            self._comp_categories_cache[comp.component_id] = comp.category

        if comp_id in self._comp_categories_cache:
            return self._comp_categories_cache[comp_id]

        return None

    async def _check_battery_request(
        self,
        comp_id: ComponentId,
        requests: dict[ComponentMetricId, list[ComponentMetricRequest]],
    ) -> None:
        """Check if the requests are valid Battery metrics.

        Raises:
            ValueError: if the requested metric is not available for batteries.

        Args:
            comp_id: The id of the requested component.
            requests: A list of metric requests received from external actors
                for the given battery.
        """
        for metric in requests:
            if metric not in _BatteryDataMethods:
                err = f"Unknown metric {metric} for Battery id {comp_id}"
                _logger.error(err)
                raise ValueError(err)
        if comp_id not in self.comp_data_receivers:
            self.comp_data_receivers[comp_id] = (
                await connection_manager.get().api_client.battery_data(comp_id)
            )

    async def _check_ev_charger_request(
        self,
        comp_id: ComponentId,
        requests: dict[ComponentMetricId, list[ComponentMetricRequest]],
    ) -> None:
        """Check if the requests are valid EV Charger metrics.

        Raises:
            ValueError: if the requested metric is not available for ev charger.

        Args:
            comp_id: The id of the requested component.
            requests: A list of metric requests received from external actors
                for the given EV Charger.
        """
        for metric in requests:
            if metric not in _EVChargerDataMethods:
                err = f"Unknown metric {metric} for EvCharger id {comp_id}"
                _logger.error(err)
                raise ValueError(err)
        if comp_id not in self.comp_data_receivers:
            self.comp_data_receivers[comp_id] = (
                await connection_manager.get().api_client.ev_charger_data(comp_id)
            )

    async def _check_inverter_request(
        self,
        comp_id: ComponentId,
        requests: dict[ComponentMetricId, list[ComponentMetricRequest]],
    ) -> None:
        """Check if the requests are valid Inverter metrics.

        Raises:
            ValueError: if the requested metric is not available for inverters.

        Args:
            comp_id: The id of the requested component.
            requests: A list of metric requests received from external actors
                for the given inverter.
        """
        for metric in requests:
            if metric not in _InverterDataMethods:
                err = f"Unknown metric {metric} for Inverter id {comp_id}"
                _logger.error(err)
                raise ValueError(err)
        if comp_id not in self.comp_data_receivers:
            self.comp_data_receivers[comp_id] = (
                await connection_manager.get().api_client.inverter_data(comp_id)
            )

    async def _check_meter_request(
        self,
        comp_id: ComponentId,
        requests: dict[ComponentMetricId, list[ComponentMetricRequest]],
    ) -> None:
        """Check if the requests are valid Meter metrics.

        Raises:
            ValueError: if the requested metric is not available for meters.

        Args:
            comp_id: The id of the requested component.
            requests: A list of metric requests received from external actors
                for the given meter.
        """
        for metric in requests:
            if metric not in _MeterDataMethods:
                err = f"Unknown metric {metric} for Meter id {comp_id}"
                _logger.error(err)
                raise ValueError(err)
        if comp_id not in self.comp_data_receivers:
            self.comp_data_receivers[comp_id] = (
                await connection_manager.get().api_client.meter_data(comp_id)
            )

    async def _check_requested_component_and_metrics(
        self,
        comp_id: ComponentId,
        category: ComponentCategory,
        requests: dict[ComponentMetricId, list[ComponentMetricRequest]],
    ) -> None:
        """Check if the requested component and metrics are valid.

        Raises:
            ValueError: if the category is unknown or if the requested metric
                is unavailable to the given category.

        Args:
            comp_id: The id of the requested component.
            category: The category of the requested component.
            requests: A list of metric requests received from external actors
                for the given component.
        """
        if comp_id in self.comp_data_receivers:
            return

        if category == ComponentCategory.BATTERY:
            await self._check_battery_request(comp_id, requests)
        elif category == ComponentCategory.EV_CHARGER:
            await self._check_ev_charger_request(comp_id, requests)
        elif category == ComponentCategory.INVERTER:
            await self._check_inverter_request(comp_id, requests)
        elif category == ComponentCategory.METER:
            await self._check_meter_request(comp_id, requests)
        else:
            err = f"Unknown component category {category}"
            _logger.error(err)
            raise ValueError(err)

    def _get_data_extraction_method(
        self, category: ComponentCategory, metric: ComponentMetricId
    ) -> Callable[[Any], float]:
        """Get the data extraction method for the given metric.

        Raises:
            ValueError: if the category is unknown.

        Args:
            category: The category of the component.
            metric: The metric for which we need an extraction method.

        Returns:
            A method that accepts a `ComponentData` object and returns a float
                representing the given metric.
        """
        if category == ComponentCategory.BATTERY:
            return _BatteryDataMethods[metric]
        if category == ComponentCategory.INVERTER:
            return _InverterDataMethods[metric]
        if category == ComponentCategory.METER:
            return _MeterDataMethods[metric]
        if category == ComponentCategory.EV_CHARGER:
            return _EVChargerDataMethods[metric]
        err = f"Unknown component category {category}"
        _logger.error(err)
        raise ValueError(err)

    def _get_metric_senders(
        self,
        category: ComponentCategory,
        requests: dict[ComponentMetricId, list[ComponentMetricRequest]],
    ) -> list[tuple[Callable[[Any], float], list[Sender[Sample[Quantity]]]]]:
        """Get channel senders from the channel registry for each requested metric.

        Args:
            category: The category of the component.
            requests: A list of metric requests received from external actors for a
                certain component.

        Returns:
            A dictionary of output metric names to channel senders from the channel
                registry.
        """
        return [
            (
                self._get_data_extraction_method(category, metric),
                [
                    self._registry.get_or_create(
                        Sample[Quantity], request.get_channel_name()
                    ).new_sender()
                    for request in req_list
                ],
            )
            for (metric, req_list) in requests.items()
        ]

    async def _handle_data_stream(
        self,
        comp_id: ComponentId,
        category: ComponentCategory,
    ) -> None:
        """Stream component data and send the requested metrics out.

        Args:
            comp_id: Id of the requested component.
            category: The category of the component.

        Raises:
            Exception: if an error occurs while handling the data stream.
        """
        try:
            stream_senders = []
            if comp_id in self._req_streaming_metrics:
                await self._check_requested_component_and_metrics(
                    comp_id, category, self._req_streaming_metrics[comp_id]
                )
                stream_senders = self._get_metric_senders(
                    category, self._req_streaming_metrics[comp_id]
                )
            api_data_receiver: Receiver[Any] = self.comp_data_receivers[comp_id]

            async def process_msg(data: Any) -> None:
                async with asyncio.TaskGroup() as tg:
                    for extractor, senders in stream_senders:
                        for sender in senders:
                            sample = Sample(data.timestamp, Quantity(extractor(data)))
                            name = f"send:ts={sample.timestamp}:cid={comp_id}"
                            tg.create_task(sender.send(sample), name=name)

            sending_tasks: set[asyncio.Task[None]] = set()

            async def clean_tasks(
                sending_tasks: set[asyncio.Task[None]],
            ) -> set[asyncio.Task[None]]:
                done, pending = await asyncio.wait(sending_tasks, timeout=0)
                for task in done:
                    try:
                        task.result()
                    # pylint: disable-next=broad-except
                    except (asyncio.CancelledError, Exception):
                        _logger.exception(
                            "Error while processing message in task %s",
                            task.get_name(),
                        )
                return pending

            async for data in api_data_receiver:
                name = f"process_msg:cid={comp_id}"
                sending_tasks.add(asyncio.create_task(process_msg(data), name=name))
                sending_tasks = await clean_tasks(sending_tasks)

            await asyncio.gather(*sending_tasks)
            await asyncio.gather(
                *[
                    self._registry.close_and_remove(r.get_channel_name())
                    for requests in self._req_streaming_metrics[comp_id].values()
                    for r in requests
                ]
            )
        except Exception:
            _logger.exception(
                "Unexpected error while handling data stream for component %d (%s), "
                "component data is not being streamed anymore",
                comp_id,
                category.name,
            )
            raise

    async def _update_streams(
        self,
        comp_id: ComponentId,
        category: ComponentCategory,
    ) -> None:
        """Update the requested metric streams for the given component.

        Args:
            comp_id: Id of the requested component.
            category: Category of the requested component.
        """
        if comp_id in self.comp_data_tasks:
            self.comp_data_tasks[comp_id].cancel()

        self.comp_data_tasks[comp_id] = asyncio.create_task(
            run_forever(lambda: self._handle_data_stream(comp_id, category)),
            name=f"{type(self).__name__}._update_stream({comp_id=}, {category.name})",
        )

    async def add_metric(self, request: ComponentMetricRequest) -> None:
        """Add a metric to be streamed from the microgrid API.

        Args:
            request: A request object for a metric, received from a downstream
                actor.
        """
        comp_id = request.component_id

        category = await self._get_component_category(comp_id)

        if category is None:
            _logger.error("Unknown component ID: %d in request %s", comp_id, request)
            return

        self._req_streaming_metrics.setdefault(comp_id, {}).setdefault(
            request.metric_id, []
        )

        for existing_request in self._req_streaming_metrics[comp_id][request.metric_id]:
            if existing_request.get_channel_name() == request.get_channel_name():
                # the requested metric is already being handled, so nothing to do.
                return

        self._req_streaming_metrics[comp_id][request.metric_id].append(request)

        await self._update_streams(
            comp_id,
            category,
        )



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""This module provides feature to set power between many batteries.

Distributing power is very important to keep the microgrid ready
for the power requirements.
This module provides PowerDistributingActor that knows how to distribute power.
It also provides all the secondary features that should be used to communicate with
PowerDistributingActor and send requests for charging or discharging power.
"""

from ._component_status import ComponentPoolStatus
from .power_distributing import PowerDistributingActor
from .request import Request
from .result import Error, OutOfBounds, PartialFailure, Result, Success

__all__ = [
    "PowerDistributingActor",
    "Request",
    "Result",
    "Error",
    "Success",
    "OutOfBounds",
    "PartialFailure",
    "ComponentPoolStatus",
]



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_pool_status_tracker.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Class that tracks the status of pool of components."""


import asyncio
import contextlib
from collections import abc
from datetime import timedelta

from frequenz.channels import Broadcast, Merger, Receiver, Sender, merge
from frequenz.client.common.microgrid.components import ComponentId

from ..._internal._asyncio import cancel_and_await, run_forever
from ._component_status import (
    ComponentPoolStatus,
    ComponentStatus,
    ComponentStatusEnum,
    ComponentStatusTracker,
    SetPowerResult,
)


class ComponentPoolStatusTracker:
    """Track status of components of a given category.

    Send set of working and uncertain components, when the status of any of the tracked
    components changes.
    """

    def __init__(  # pylint: disable=too-many-arguments
        self,
        *,
        component_ids: abc.Set[ComponentId],
        component_status_sender: Sender[ComponentPoolStatus],
        max_data_age: timedelta,
        max_blocking_duration: timedelta,
        component_status_tracker_type: type[ComponentStatusTracker],
    ) -> None:
        """Create ComponentPoolStatusTracker instance.

        Args:
            component_ids: set of component ids whose status is to be tracked.
            component_status_sender: The sender used for sending the status of the
                tracked components.
            max_data_age: If a component stops sending data, then this is the maximum
                time for which its last message should be considered as valid. After
                that time, the component won't be used until it starts sending data.
            max_blocking_duration: This value tell what should be the maximum timeout
                used for blocking failing component.
            component_status_tracker_type: component status tracker to use for tracking
                the status of the components.
        """
        self._component_ids = component_ids
        self._max_data_age = max_data_age
        self._max_blocking_duration = max_blocking_duration
        self._component_status_sender = component_status_sender
        self._component_status_tracker_type = component_status_tracker_type

        # At first no component is working, we will get notification when they start
        # working.
        self._current_status = ComponentPoolStatus(working=set(), uncertain=set())

        # Channel for sending results of requests to the components.
        self._set_power_result_channel = Broadcast[SetPowerResult](
            name="component_request_status"
        )
        self._set_power_result_sender = self._set_power_result_channel.new_sender()
        self._component_status_trackers: list[ComponentStatusTracker] = []
        self._merged_status_receiver = self._make_merged_status_receiver()

        self._task = asyncio.create_task(self._run())

    async def join(self) -> None:
        """Wait and return when the instance's task completes.

        It will not terminate the instance, which can be done with the `stop` method.
        """
        await self._task

    async def stop(self) -> None:
        """Stop the ComponentPoolStatusTracker instance."""
        await cancel_and_await(self._task)
        await self._merged_status_receiver.stop()

    def _make_merged_status_receiver(
        self,
    ) -> Merger[ComponentStatus]:
        status_receivers: list[Receiver[ComponentStatus]] = []

        for component_id in self._component_ids:
            channel: Broadcast[ComponentStatus] = Broadcast(
                name=f"component_{component_id}_status"
            )
            tracker = self._component_status_tracker_type(
                component_id=component_id,
                max_data_age=self._max_data_age,
                max_blocking_duration=self._max_blocking_duration,
                status_sender=channel.new_sender(),
                set_power_result_receiver=self._set_power_result_channel.new_receiver(),
            )
            self._component_status_trackers.append(tracker)
            status_receivers.append(channel.new_receiver())
        return merge(*status_receivers)

    async def _run(self) -> None:
        """Start tracking component status."""
        async with contextlib.AsyncExitStack() as stack:
            for tracker in self._component_status_trackers:
                await stack.enter_async_context(tracker)
            await run_forever(self._update_status)

    async def _update_status(self) -> None:
        async for status in self._merged_status_receiver:
            component_id = status.component_id
            if status.value == ComponentStatusEnum.WORKING:
                self._current_status.working.add(component_id)
                self._current_status.uncertain.discard(component_id)
            elif status.value == ComponentStatusEnum.UNCERTAIN:
                self._current_status.working.discard(component_id)
                self._current_status.uncertain.add(component_id)
            elif status.value == ComponentStatusEnum.NOT_WORKING:
                self._current_status.working.discard(component_id)
                self._current_status.uncertain.discard(component_id)

            await self._component_status_sender.send(self._current_status)

    async def update_status(
        self,
        succeeded_components: set[ComponentId],
        failed_components: set[ComponentId],
    ) -> None:
        """Notify which components succeeded or failed in the request.

        Components that failed will be considered as broken and will be temporarily
        blocked.

        Components that succeeded will be unblocked.

        Args:
            succeeded_components: Component IDs for which the power request succeeded.
            failed_components: Component IDs for which the power request failed.
        """
        await self._set_power_result_sender.send(
            SetPowerResult(succeeded=succeeded_components, failed=failed_components)
        )

    def get_working_components(
        self, components: abc.Set[ComponentId]
    ) -> abc.Set[ComponentId]:
        """From the given set of components, return only working ones.

        Args:
            components: Set of component IDs.

        Returns:
            IDs of subset with working components.
        """
        return self._current_status.get_working_components(components)



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/power_distributing.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Actor to distribute power between components.

The purpose of this actor is to distribute power between components in a microgrid.

The actor receives power requests from the power manager, process them by
distributing the power between the components and sends the results back to it.
"""


import asyncio
import logging
from datetime import timedelta

from frequenz.channels import Receiver, Sender
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentCategory, ComponentType, InverterType
from typing_extensions import override

from ...actor._actor import Actor
from ._component_managers import (
    BatteryManager,
    ComponentManager,
    EVChargerManager,
    PVManager,
)
from ._component_status import ComponentPoolStatus
from .request import Request
from .result import Result

_logger = logging.getLogger(__name__)


class PowerDistributingActor(Actor):  # pylint: disable=too-many-instance-attributes
    """Actor to distribute the power between components in a microgrid.

    One instance of the actor can handle only one component category and type,
    which needs to be specified at actor startup and it will setup the correct
    component manager based on the given category and type.

    Only one power request is processed at a time to prevent from sending
    multiple requests for the same components to the microgrid API at the
    same time.

    Edge cases:
    * If a new power request is received while a power request with the same
    set of components is being processed, the new request will be added to
    the pending requests. Then the pending request will be processed after the
    request with the same set of components being processed is done. Only one
    pending request is kept for each set of components, the latest request will
    overwrite the previous one if there is any.

    * If there are 2 requests and their set of components is different but they
    overlap (they have at least one common component), then both requests will
    be processed concurrently. Though, the power manager will make sure this
    doesn't happen as overlapping component IDs are not possible at the moment.
    """

    def __init__(  # pylint: disable=too-many-arguments
        self,
        requests_receiver: Receiver[Request],
        results_sender: Sender[Result],
        component_pool_status_sender: Sender[ComponentPoolStatus],
        *,
        api_power_request_timeout: timedelta,
        component_category: ComponentCategory,
        component_type: ComponentType | None = None,
        name: str | None = None,
    ) -> None:
        """Create actor instance.

        Args:
            requests_receiver: Receiver for receiving power requests from the power
                manager.
            results_sender: Sender for sending results to the power manager.
            component_pool_status_sender: Channel for sending information about which
                components are expected to be working.
            api_power_request_timeout: Timeout to use when making power requests to
                the microgrid API.
            component_category: The category of the components that this actor is
                responsible for.
            component_type: The type of the component of the given category that this
                actor is responsible for.  This is used only when the component category
                is not enough to uniquely identify the component.  For example, when the
                category is `ComponentCategory.INVERTER`, the type is needed to identify
                the inverter as a solar inverter or a battery inverter.  This can be
                `None` when the component category is enough to uniquely identify the
                component.
            name: The name of the actor. If `None`, `str(id(self))` will be used. This
                is used mostly for debugging purposes.

        Raises:
            ValueError: If the given component category is not supported.
        """
        super().__init__(name=name)
        self._component_category = component_category
        self._component_type = component_type
        self._requests_receiver = requests_receiver
        self._result_sender = results_sender
        self._api_power_request_timeout = api_power_request_timeout

        self._processing_tasks: dict[frozenset[ComponentId], asyncio.Task[None]] = {}
        """Track the power request tasks currently being processed."""

        self._pending_requests: dict[frozenset[ComponentId], Request] = {}
        """Track the power requests that are waiting to be processed.

        Only one pending power request is kept for each set of components, the
        latest request will overwrite the previous one.
        """

        self._component_manager: ComponentManager
        if component_category == ComponentCategory.BATTERY:
            self._component_manager = BatteryManager(
                component_pool_status_sender, results_sender, api_power_request_timeout
            )
        elif component_category == ComponentCategory.EV_CHARGER:
            self._component_manager = EVChargerManager(
                component_pool_status_sender, results_sender, api_power_request_timeout
            )
        elif (
            component_category == ComponentCategory.INVERTER
            and component_type == InverterType.SOLAR
        ):
            self._component_manager = PVManager(
                component_pool_status_sender, results_sender, api_power_request_timeout
            )
        else:
            raise ValueError(
                f"PowerDistributor doesn't support controlling: {component_category}"
            )

    @override
    async def _run(self) -> None:
        """Run this actor's logic.

        It waits for new power requests and process them. Only one power request
        can be processed at a time to prevent from sending multiple requests for
        the same components to the microgrid API at the same time.

        A new power request will be ignored if a power request with the same
        components is currently being processed.

        Every component that failed or didn't respond in time will be marked
        as broken for some time.
        """
        await self._component_manager.start()

        async for request in self._requests_receiver:
            req_id = frozenset(request.component_ids)

            if req_id in self._processing_tasks:
                if pending_request := self._pending_requests.get(req_id):
                    _logger.debug(
                        "Pending request: %s, overwritten with request: %s",
                        pending_request,
                        request,
                    )
                self._pending_requests[req_id] = request
            else:
                self._process_request(req_id, request)

    @override
    async def stop(self, msg: str | None = None) -> None:
        """Stop this actor.

        Args:
            msg: The message to be passed to the tasks being cancelled.
        """
        await self._component_manager.stop()
        await super().stop(msg)

    def _handle_task_completion(
        self,
        req_id: frozenset[ComponentId],
        request: Request,
        task: asyncio.Task[None],
    ) -> None:
        """Handle the completion of a power request task.

        Args:
            req_id: The id to identify the power request.
            request: The power request that has been processed.
            task: The task that has completed.
        """
        try:
            task.result()
        except Exception:  # pylint: disable=broad-except
            _logger.exception("Failed power request: %s", request)

        if req_id in self._pending_requests:
            self._process_request(req_id, self._pending_requests.pop(req_id))
        elif req_id in self._processing_tasks:
            del self._processing_tasks[req_id]
        else:
            _logger.error("Request id not found in processing tasks: %s", req_id)

    def _process_request(
        self, req_id: frozenset[ComponentId], request: Request
    ) -> None:
        """Process a power request.

        Args:
            req_id: The id to identify the power request.
            request: The power request to process.
        """
        task = asyncio.create_task(
            self._component_manager.distribute_power(request),
            name=f"{type(self).__name__}:{request}",
        )
        task.add_done_callback(
            lambda t: self._handle_task_completion(req_id, request, t)
        )
        self._processing_tasks[req_id] = task



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/request.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH
"""Definition of the user request."""


import dataclasses
from collections import abc

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.quantities import Power


@dataclasses.dataclass
class Request:
    """Request to set power to the `PowerDistributingActor`."""

    power: Power
    """The requested power."""

    component_ids: abc.Set[ComponentId]
    """The component ids of the components to be used for this request."""

    adjust_power: bool = True
    """Whether to adjust the power to match the bounds.

    If `True`, the power will be adjusted (lowered) to match the bounds, so
    only the reduced power will be set.

    If `False` and the power is outside the available bounds, the request will
    fail and be replied to with an `OutOfBound` result.
    """



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/result.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Results from PowerDistributingActor."""


import dataclasses
from collections import abc

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.quantities import Power

from .request import Request


@dataclasses.dataclass
class _BaseResultMixin:
    """Base mixin class for reporting power distribution results."""

    request: Request
    """The user's request to which this message responds."""


@dataclasses.dataclass
class _BaseSuccessMixin:
    """Result returned when setting the power succeed for all components."""

    succeeded_power: Power
    """The part of the requested power that was successfully set."""

    succeeded_components: abc.Set[ComponentId]
    """The subset of components for which power was set successfully."""

    excess_power: Power
    """The part of the requested power that could not be fulfilled.

    This happens when the requested power is outside the available power bounds.
    """


# We need to put the _BaseSuccessMixin before _BaseResultMixin in the
# inheritance list to make sure that the _BaseResultMixin attributes appear
# before the _BaseSuccessMixin, otherwise the request attribute will be last
# in the dataclass constructor because of how MRO works.


@dataclasses.dataclass
class Success(_BaseSuccessMixin, _BaseResultMixin):  # Order matters here. See above.
    """Result returned when setting the power was successful for all components."""


@dataclasses.dataclass
class PartialFailure(_BaseSuccessMixin, _BaseResultMixin):
    """Result returned when some of the components had an error setting the power."""

    failed_power: Power
    """The part of the requested power that failed to be set."""

    failed_components: abc.Set[ComponentId]
    """The subset of batteries for which the request failed."""


@dataclasses.dataclass
class Error(_BaseResultMixin):
    """Result returned when an error occurred and power was not set at all."""

    msg: str
    """The error message explaining why error happened."""


@dataclasses.dataclass
class PowerBounds:
    """Inclusion and exclusion power bounds for the requested components."""

    inclusion_lower: Power
    """The lower value of the inclusion power bounds for the requested components."""

    exclusion_lower: Power
    """The lower value of the exclusion power bounds for the requested components."""

    exclusion_upper: Power
    """The upper value of the exclusion power bounds for the requested components."""

    inclusion_upper: Power
    """The upper value of the inclusion power bounds for the requested components."""


@dataclasses.dataclass
class OutOfBounds(_BaseResultMixin):
    """Result returned when the power was not set because it was out of bounds.

    This result happens when the originating request was done with
    `adjust_power = False` and the requested power is not within the available bounds.
    """

    bounds: PowerBounds
    """The power bounds for the requested components.

    If the requested power negative, then this value is the lower bound.
    Otherwise it is upper bound.
    """


Result = Success | PartialFailure | Error | OutOfBounds
"""Power distribution result.

Example: Handling power distribution results

    ```python
    from typing import assert_never

    from frequenz.sdk.actor.power_distributing import (
        Error,
        OutOfBounds,
        PartialFailure,
        Result,
        Success,
    )
    from frequenz.sdk.actor.power_distributing.request import Request
    from frequenz.sdk.actor.power_distributing.result import PowerBounds
    from frequenz.quantities import Power

    def handle_power_request_result(result: Result) -> None:
        match result:
            case Success() as success:
                print(f"Power request was successful: {success}")
            case PartialFailure() as partial_failure:
                print(f"Power request was partially successful: {partial_failure}")
            case OutOfBounds() as out_of_bounds:
                print(f"Power request was out of bounds: {out_of_bounds}")
            case Error() as error:
                print(f"Power request failed: {error}")
            case _ as unreachable:
                assert_never(unreachable)

    request = Request(
        namespace="TestChannel",
        power=Power.from_watts(123.4),
        component_ids={ComponentId(8), ComponentId(18)},
    )

    results: list[Result] = [
        Success(
            request,
            succeeded_power=Power.from_watts(123.4),
            succeeded_components={ComponentId(8), ComponentId(18)},
            excess_power=Power.zero(),
        ),
        PartialFailure(
            request,
            succeeded_power=Power.from_watts(103.4),
            succeeded_components={ComponentId(8)},
            excess_power=Power.zero(),
            failed_components={ComponentId(18)},
            failed_power=Power.from_watts(20.0),
        ),
        OutOfBounds(request, bounds=PowerBounds(0, 0, 0, 800)),
        Error(request, msg="The components are not available"),
    ]

    for r in results:
        handle_power_request_result(r)
    ```
"""



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_managers/__init__.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Interfaces for the power distributing actor with different component types."""

from ._battery_manager import BatteryManager
from ._component_manager import ComponentManager
from ._ev_charger_manager import EVChargerManager
from ._pv_inverter_manager import PVManager

__all__ = [
    "BatteryManager",
    "ComponentManager",
    "EVChargerManager",
    "PVManager",
]



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_managers/_battery_manager.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Manage batteries and inverters for the power distributor."""

import asyncio
import collections.abc
import logging
import math
import typing
from datetime import timedelta

from frequenz.channels import LatestValueCache, Receiver, Sender
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    ApiClientError,
    BatteryData,
    ComponentCategory,
    InverterData,
    OperationOutOfRange,
)
from frequenz.quantities import Power
from typing_extensions import override

from ... import connection_manager
from .._component_pool_status_tracker import ComponentPoolStatusTracker
from .._component_status import BatteryStatusTracker, ComponentPoolStatus
from .._distribution_algorithm import (
    AggregatedBatteryData,
    BatteryDistributionAlgorithm,
    DistributionResult,
    InvBatPair,
)
from ..request import Request
from ..result import Error, OutOfBounds, PartialFailure, PowerBounds, Result, Success
from ._component_manager import ComponentManager

_logger = logging.getLogger(__name__)


def _get_all_from_map(
    source: dict[ComponentId, frozenset[ComponentId]],
    keys: collections.abc.Set[ComponentId],
) -> set[ComponentId]:
    """Get all values for the given keys from the given map.

    Args:
        source: map to get values from.
        keys: keys to get values for.

    Returns:
        Set of values for the given keys.
    """
    return set().union(*[source[key] for key in keys])


def _get_battery_inverter_mappings(
    battery_ids: collections.abc.Set[ComponentId],
    *,  # force keyword arguments
    inv_bats: bool = True,
    bat_bats: bool = True,
    inv_invs: bool = True,
) -> dict[str, dict[ComponentId, frozenset[ComponentId]]]:
    """Create maps between battery and adjacent inverters.

    Args:
        battery_ids: set of battery ids
        inv_bats: whether to create the inverter to batteries map
        bat_bats: whether to create the battery to batteries map
        inv_invs: whether to create the inverter to inverters map

    Returns:
        a dict of the requested maps, using the following keys:
            * "bat_invs": battery to inverters map
            * "inv_bats": inverter to batteries map
            * "bat_bats": battery to batteries map
            * "inv_invs": inverter to inverters map
    """
    bat_invs_map: dict[ComponentId, set[ComponentId]] = {}
    inv_bats_map: dict[ComponentId, set[ComponentId]] | None = {} if inv_bats else None
    bat_bats_map: dict[ComponentId, set[ComponentId]] | None = {} if bat_bats else None
    inv_invs_map: dict[ComponentId, set[ComponentId]] | None = {} if inv_invs else None
    component_graph = connection_manager.get().component_graph

    for battery_id in battery_ids:
        inverters: set[ComponentId] = set(
            component.component_id
            for component in component_graph.predecessors(battery_id)
            if component.category == ComponentCategory.INVERTER
        )

        if len(inverters) == 0:
            _logger.error("No inverters for battery %d", battery_id)
            continue

        bat_invs_map[battery_id] = inverters
        if bat_bats_map is not None:
            bat_bats_map.setdefault(battery_id, set()).update(
                set(
                    component.component_id
                    for inverter in inverters
                    for component in component_graph.successors(inverter)
                )
            )

        for inverter in inverters:
            if inv_bats_map is not None:
                inv_bats_map.setdefault(inverter, set()).add(battery_id)
            if inv_invs_map is not None:
                inv_invs_map.setdefault(inverter, set()).update(bat_invs_map)

    mapping: dict[str, dict[ComponentId, frozenset[ComponentId]]] = {}

    # Convert sets to frozensets to make them hashable.
    def _add(key: str, value: dict[ComponentId, set[ComponentId]] | None) -> None:
        if value is not None:
            mapping[key] = {k: frozenset(v) for k, v in value.items()}

    _add("bat_invs", bat_invs_map)
    _add("inv_bats", inv_bats_map)
    _add("bat_bats", bat_bats_map)
    _add("inv_invs", inv_invs_map)

    return mapping


class BatteryManager(ComponentManager):  # pylint: disable=too-many-instance-attributes
    """Class to manage the data streams for batteries."""

    @override
    def __init__(
        self,
        component_pool_status_sender: Sender[ComponentPoolStatus],
        results_sender: Sender[Result],
        api_power_request_timeout: timedelta,
    ):
        """Initialize this instance.

        Args:
            component_pool_status_sender: Channel sender to send the status of the
                battery pool to.  This status is used by the battery pool metric
                streams, to dynamically adjust the values based on the health of the
                individual batteries.
            results_sender: Channel sender to send the power distribution results to.
            api_power_request_timeout: Timeout to use when making power requests to
                the microgrid API.
        """
        self._results_sender = results_sender
        self._api_power_request_timeout = api_power_request_timeout
        self._batteries = connection_manager.get().component_graph.components(
            component_categories={ComponentCategory.BATTERY}
        )
        self._battery_ids = {battery.component_id for battery in self._batteries}

        maps = _get_battery_inverter_mappings(self._battery_ids)

        self._bat_invs_map = maps["bat_invs"]
        self._inv_bats_map = maps["inv_bats"]
        self._bat_bats_map = maps["bat_bats"]
        self._inv_invs_map = maps["inv_invs"]

        self._battery_caches: dict[ComponentId, LatestValueCache[BatteryData]] = {}
        self._inverter_caches: dict[ComponentId, LatestValueCache[InverterData]] = {}

        self._component_pool_status_tracker = ComponentPoolStatusTracker(
            component_ids=set(self._battery_ids),
            component_status_sender=component_pool_status_sender,
            max_blocking_duration=timedelta(seconds=30.0),
            max_data_age=timedelta(seconds=10.0),
            component_status_tracker_type=BatteryStatusTracker,
        )

        # NOTE: power_distributor_exponent should be received from ConfigManager
        self._power_distributor_exponent: float = 1.0
        """The exponent for the power distribution algorithm.

        The exponent determines how fast the batteries should strive to the
        equal SoC level.
        """

        self._distribution_algorithm = BatteryDistributionAlgorithm(
            self._power_distributor_exponent
        )
        """The distribution algorithm used to distribute power between batteries."""

    @override
    def component_ids(self) -> collections.abc.Set[ComponentId]:
        """Return the set of component ids."""
        return self._battery_ids

    @override
    async def start(self) -> None:
        """Start the battery data manager."""
        await self._create_channels()

    @override
    async def stop(self) -> None:
        """Stop the battery data manager."""
        for bat_cache in self._battery_caches.values():
            await bat_cache.stop()
        for inv_cache in self._inverter_caches.values():
            await inv_cache.stop()
        await self._component_pool_status_tracker.stop()

    @override
    async def distribute_power(self, request: Request) -> None:
        """Distribute the requested power to the components.

        Args:
            request: Request to get the distribution for.
        """
        distribution_result = await self._get_distribution(request)
        if not isinstance(distribution_result, DistributionResult):
            result = distribution_result
        else:
            result = await self._distribute_power(request, distribution_result)
        await self._results_sender.send(result)

    async def _get_distribution(self, request: Request) -> DistributionResult | Result:
        """Get the distribution of the batteries.

        Args:
            request: Request to get the distribution for.

        Returns:
            Distribution of the batteries.
        """
        match self._get_components_data(request.component_ids):
            case str() as err:
                return Error(request=request, msg=err)
            case list() as pairs_data:
                pass
            case unexpected:
                typing.assert_never(unexpected)

        if not pairs_data:
            error_msg = (
                "No data for at least one of the given batteries: "
                + self._str_ids(request.component_ids)
            )
            return Error(request=request, msg=str(error_msg))

        error = self._check_request(request, pairs_data)
        if error:
            return error

        try:
            distribution = self._get_power_distribution(request, pairs_data)
        except ValueError as err:
            _logger.exception("Couldn't distribute power")
            error_msg = f"Couldn't distribute power, error: {str(err)}"
            return Error(request=request, msg=str(error_msg))

        return distribution

    async def _distribute_power(
        self, request: Request, distribution: DistributionResult
    ) -> Result:
        """Set the distributed power to the batteries.

        Args:
            request: Request to set the power for.
            distribution: Distribution to set.

        Returns:
            Result from the microgrid API.
        """
        distributed_power_value = request.power - distribution.remaining_power
        battery_distribution: dict[frozenset[ComponentId], Power] = {}
        battery_ids: set[ComponentId] = set()
        for inverter_id, dist in distribution.distribution.items():
            for battery_id in self._inv_bats_map[inverter_id]:
                battery_ids.add(battery_id)
            battery_distribution[self._inv_bats_map[inverter_id]] = dist
        if _logger.isEnabledFor(logging.DEBUG):
            _logger.debug(
                "Distributing power %s between the batteries: %s",
                distributed_power_value,
                ", ".join(
                    (
                        str(next(iter(cids)))
                        if len(cids) == 1
                        else f"({', '.join(str(cid) for cid in cids)})"
                    )
                    + f": {power}"
                    for cids, power in battery_distribution.items()
                ),
            )

        failed_power, failed_batteries = await self._set_distributed_power(
            distribution, self._api_power_request_timeout
        )

        response: Success | PartialFailure
        if len(failed_batteries) > 0:
            succeed_batteries = battery_ids - failed_batteries
            response = PartialFailure(
                request=request,
                succeeded_power=distributed_power_value - failed_power,
                succeeded_components=succeed_batteries,
                failed_power=failed_power,
                failed_components=failed_batteries,
                excess_power=distribution.remaining_power,
            )
        else:
            succeed_batteries = battery_ids
            response = Success(
                request=request,
                succeeded_power=distributed_power_value,
                succeeded_components=succeed_batteries,
                excess_power=distribution.remaining_power,
            )

        await asyncio.gather(
            *[
                self._component_pool_status_tracker.update_status(
                    succeed_batteries, failed_batteries
                ),
            ]
        )

        return response

    async def _create_channels(self) -> None:
        """Create channels to get data of components in microgrid."""
        api = connection_manager.get().api_client
        manager_id = f"{type(self).__name__}«{hex(id(self))}»"
        for battery_id, inverter_ids in self._bat_invs_map.items():
            bat_recv: Receiver[BatteryData] = await api.battery_data(battery_id)
            self._battery_caches[battery_id] = LatestValueCache(
                bat_recv,
                unique_id=f"{manager_id}:battery«{battery_id}»",
            )

            for inverter_id in inverter_ids:
                inv_recv: Receiver[InverterData] = await api.inverter_data(inverter_id)
                self._inverter_caches[inverter_id] = LatestValueCache(
                    inv_recv, unique_id=f"{manager_id}:inverter«{inverter_id}»"
                )

    def _get_bounds(
        self,
        pairs_data: list[InvBatPair],
    ) -> PowerBounds:
        """Get power bounds for given batteries.

        Args:
            pairs_data: list of battery and adjacent inverter data pairs.

        Returns:
            Power bounds for given batteries.
        """
        return PowerBounds(
            inclusion_lower=sum(
                (
                    max(
                        battery.power_bounds.inclusion_lower,
                        Power.from_watts(
                            sum(
                                inverter.active_power_inclusion_lower_bound
                                for inverter in inverters
                            )
                        ),
                    )
                    for battery, inverters in pairs_data
                ),
                start=Power.zero(),
            ),
            inclusion_upper=sum(
                (
                    min(
                        battery.power_bounds.inclusion_upper,
                        Power.from_watts(
                            sum(
                                inverter.active_power_inclusion_upper_bound
                                for inverter in inverters
                            )
                        ),
                    )
                    for battery, inverters in pairs_data
                ),
                start=Power.zero(),
            ),
            exclusion_lower=min(
                sum(
                    (battery.power_bounds.exclusion_lower for battery, _ in pairs_data),
                    start=Power.zero(),
                ),
                Power.from_watts(
                    sum(
                        inverter.active_power_exclusion_lower_bound
                        for _, inverters in pairs_data
                        for inverter in inverters
                    )
                ),
            ),
            exclusion_upper=max(
                sum(
                    (battery.power_bounds.exclusion_upper for battery, _ in pairs_data),
                    start=Power.zero(),
                ),
                Power.from_watts(
                    sum(
                        inverter.active_power_exclusion_upper_bound
                        for _, inverters in pairs_data
                        for inverter in inverters
                    )
                ),
            ),
        )

    def _check_request(
        self,
        request: Request,
        pairs_data: list[InvBatPair],
    ) -> Result | None:
        """Check whether the given request if correct.

        Args:
            request: request to check
            pairs_data: list of battery and adjacent inverter data pairs.

        Returns:
            Result for the user if the request is wrong, None otherwise.
        """
        if not request.component_ids:
            return Error(request=request, msg="Empty battery IDs in the request")

        for battery in request.component_ids:
            if battery not in self._battery_caches:
                msg = (
                    f"No battery {battery}, available batteries: "
                    f"{list(self._battery_caches.keys())}"
                )
                return Error(request=request, msg=msg)

        bounds = self._get_bounds(pairs_data)

        power = request.power

        # Zero power requests are always forwarded to the microgrid API, even if they
        # are outside the exclusion bounds.
        if power.isclose(Power.zero()):
            return None

        if request.adjust_power:
            # Automatic power adjustments can only bring down the requested power down
            # to the inclusion bounds.
            #
            # If the requested power is in the exclusion bounds, it is NOT possible to
            # increase it so that it is outside the exclusion bounds.
            if bounds.exclusion_lower < power < bounds.exclusion_upper:
                return OutOfBounds(request=request, bounds=bounds)
        else:
            in_lower_range = bounds.inclusion_lower <= power <= bounds.exclusion_lower
            in_upper_range = bounds.exclusion_upper <= power <= bounds.inclusion_upper
            if not (in_lower_range or in_upper_range):
                return OutOfBounds(request=request, bounds=bounds)

        return None

    def _get_battery_inverter_data(
        self, battery_ids: frozenset[ComponentId], inverter_ids: frozenset[ComponentId]
    ) -> InvBatPair | None:
        """Get battery and inverter data if they are correct.

        Each float data from the microgrid can be "NaN".
        We can't do math operations on "NaN".
        So check all the metrics and if any are "NaN" then return None.

        Args:
            battery_ids: battery ids
            inverter_ids: inverter ids

        Returns:
            Data for the battery and adjacent inverter without NaN values.
                Return None if we could not replace NaN values.
        """
        # It means that nothing has been send on these channels, yet.
        # This should be handled by BatteryStatus. BatteryStatus should not return
        # this batteries as working.
        if not all(
            self._battery_caches[bat_id].has_value() for bat_id in battery_ids
        ) or not all(
            self._inverter_caches[inv_id].has_value() for inv_id in inverter_ids
        ):
            _logger.error(
                "Battery %s or inverter %s send no data, yet. They should be not used.",
                battery_ids,
                inverter_ids,
            )
            return None

        battery_data = [
            self._battery_caches[battery_id].get() for battery_id in battery_ids
        ]
        inverter_data = [
            self._inverter_caches[inverter_id].get() for inverter_id in inverter_ids
        ]

        DataType = typing.TypeVar("DataType", BatteryData, InverterData)

        def metric_is_nan(data: DataType, metrics: list[str]) -> bool:
            """Check if non-replaceable metrics are NaN."""
            assert data is not None
            return any(map(lambda metric: math.isnan(getattr(data, metric)), metrics))

        def nan_metric_in_list(data: list[DataType], metrics: list[str]) -> bool:
            """Check if any metric is NaN."""
            return any(map(lambda datum: metric_is_nan(datum, metrics), data))

        crucial_metrics_bat = [
            "soc",
            "soc_lower_bound",
            "soc_upper_bound",
            "capacity",
            "power_inclusion_lower_bound",
            "power_inclusion_upper_bound",
        ]

        crucial_metrics_inv = [
            "active_power_inclusion_lower_bound",
            "active_power_inclusion_upper_bound",
        ]

        if nan_metric_in_list(battery_data, crucial_metrics_bat):
            _logger.debug("Some metrics for battery set %s are NaN", list(battery_ids))
            return None

        if nan_metric_in_list(inverter_data, crucial_metrics_inv):
            _logger.debug(
                "Some metrics for inverter set %s are NaN", list(inverter_ids)
            )
            return None

        return InvBatPair(AggregatedBatteryData(battery_data), inverter_data)

    def _get_components_data(
        self, batteries: collections.abc.Set[ComponentId]
    ) -> list[InvBatPair] | str:
        """Get data for the given batteries and adjacent inverters.

        Args:
            batteries: Batteries that needs data.

        Returns:
            Pairs of battery and adjacent inverter data or an error message if there was
                an error while getting the data.
        """
        inverter_ids: collections.abc.Set[ComponentId]
        pairs_data: list[InvBatPair] = []

        working_batteries = self._component_pool_status_tracker.get_working_components(
            batteries
        )

        for battery_id in working_batteries:
            if battery_id not in self._battery_caches:
                return (
                    f"No battery {battery_id}, "
                    f"available batteries: {self._str_ids(self._battery_caches.keys())}"
                )

        connected_inverters = _get_all_from_map(self._bat_invs_map, batteries)

        # Check to see if inverters are involved that are connected to batteries
        # that were not requested.
        batteries_from_inverters = _get_all_from_map(
            self._inv_bats_map, connected_inverters
        )

        if batteries_from_inverters != batteries:
            extra_batteries = batteries_from_inverters - batteries
            inverter_ids = _get_all_from_map(self._bat_invs_map, extra_batteries)
            return (
                f"Inverter(s) ({self._str_ids(inverter_ids)}) are connected to "
                f"battery(ies) ({self._str_ids(extra_batteries)}) that were not requested"
            )

        # set of set of batteries one for each working_battery
        battery_sets: frozenset[frozenset[ComponentId]] = frozenset(
            self._bat_bats_map[working_battery] for working_battery in working_batteries
        )

        for battery_ids in battery_sets:
            inverter_ids = self._bat_invs_map[next(iter(battery_ids))]

            data = self._get_battery_inverter_data(battery_ids, inverter_ids)
            if data is None:
                _logger.warning(
                    "Skipping battery set %s because at least one of its messages isn't correct.",
                    list(battery_ids),
                )
                continue

            assert len(data.inverter) > 0
            pairs_data.append(data)
        return pairs_data

    def _str_ids(self, ids: collections.abc.Set[ComponentId]) -> str:
        return ", ".join(str(cid) for cid in sorted(ids))

    def _get_power_distribution(
        self, request: Request, inv_bat_pairs: list[InvBatPair]
    ) -> DistributionResult:
        """Get power distribution result for the batteries in the request.

        Args:
            request: the power request to process.
            inv_bat_pairs: the battery and adjacent inverter data pairs.

        Returns:
            the power distribution result.
        """
        available_bat_ids = _get_all_from_map(
            self._bat_bats_map, {pair.battery.component_id for pair in inv_bat_pairs}
        )

        unavailable_bat_ids = request.component_ids - available_bat_ids
        unavailable_inv_ids: set[ComponentId] = set()

        for inverter_ids in [
            self._bat_invs_map[battery_id_set] for battery_id_set in unavailable_bat_ids
        ]:
            unavailable_inv_ids = unavailable_inv_ids.union(inverter_ids)

        result = self._distribution_algorithm.distribute_power(
            request.power, inv_bat_pairs
        )

        return result

    async def _set_distributed_power(
        self,
        distribution: DistributionResult,
        timeout: timedelta,
    ) -> tuple[Power, set[ComponentId]]:
        """Send distributed power to the inverters.

        Args:
            distribution: Distribution result
            timeout: How long wait for the response

        Returns:
            Tuple where first element is total failed power, and the second element
            set of batteries that failed.
        """
        api = connection_manager.get().api_client

        tasks = {
            inverter_id: asyncio.create_task(
                api.set_power(inverter_id, power.as_watts())
            )
            for inverter_id, power in distribution.distribution.items()
        }

        _, pending = await asyncio.wait(
            tasks.values(),
            timeout=timeout.total_seconds(),
            return_when=asyncio.ALL_COMPLETED,
        )

        await self._cancel_tasks(pending)

        return self._parse_result(tasks, distribution.distribution, timeout)

    def _parse_result(
        self,
        tasks: dict[ComponentId, asyncio.Task[None]],
        distribution: dict[ComponentId, Power],
        request_timeout: timedelta,
    ) -> tuple[Power, set[ComponentId]]:
        """Parse the results of `set_power` requests.

        Check if any task has failed and determine the reason for failure.
        If any task did not succeed, then the corresponding battery is marked as broken.

        Args:
            tasks: A dictionary where the key is the inverter ID and the value is the task that
                set the power for this inverter. Each task should be finished or cancelled.
            distribution: A dictionary where the key is the inverter ID and the value is how much
                power was set to the corresponding inverter.
            request_timeout: The timeout that was used for the request.

        Returns:
            A tuple where the first element is the total failed power, and the second element is
            the set of batteries that failed.
        """
        failed_power: Power = Power.zero()
        failed_batteries: set[ComponentId] = set()

        for inverter_id, aws in tasks.items():
            battery_ids = self._inv_bats_map[inverter_id]
            failed = True
            try:
                aws.result()
                failed = False
            except OperationOutOfRange as err:
                _logger.debug(
                    "Set power for battery %s failed due to out of range error: %s",
                    battery_ids,
                    err,
                )
            except ApiClientError as err:
                _logger.warning(
                    "Set power for battery %s failed, mark it as broken. Error: %s",
                    battery_ids,
                    err,
                )
            except asyncio.exceptions.CancelledError:
                _logger.warning(
                    "Battery %s didn't respond in %f sec. Mark it as broken.",
                    battery_ids,
                    request_timeout.total_seconds(),
                )
            except Exception:  # pylint: disable=broad-except
                _logger.exception(
                    "Unknown error while setting power to batteries: %s",
                    battery_ids,
                )

            if failed:
                failed_power += distribution[inverter_id]
                failed_batteries.update(battery_ids)

        return failed_power, failed_batteries

    async def _cancel_tasks(
        self, tasks: collections.abc.Iterable[asyncio.Task[typing.Any]]
    ) -> None:
        """Cancel given asyncio tasks and wait for them.

        Args:
            tasks: tasks to cancel.
        """
        for aws in tasks:
            aws.cancel()

        await asyncio.gather(*tasks, return_exceptions=True)



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_managers/_component_manager.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Manage batteries and inverters for the power distributor."""

import abc
import collections.abc

from frequenz.channels import Sender
from frequenz.client.common.microgrid.components import ComponentId

from .._component_status import ComponentPoolStatus
from ..request import Request
from ..result import Result


class ComponentManager(abc.ABC):
    """Abstract class to manage the data streams for components."""

    @abc.abstractmethod
    def __init__(
        self,
        component_pool_status_sender: Sender[ComponentPoolStatus],
        results_sender: Sender[Result],
    ):
        """Initialize the component data manager.

        Args:
            component_pool_status_sender: Channel for sending information about which
                components are expected to be working.
            results_sender: Channel for sending the results of power distribution.
        """

    @abc.abstractmethod
    def component_ids(self) -> collections.abc.Set[ComponentId]:
        """Return the set of component ids."""

    @abc.abstractmethod
    async def start(self) -> None:
        """Start the component data manager."""

    @abc.abstractmethod
    async def distribute_power(self, request: Request) -> None:
        """Distribute the requested power to the components.

        Args:
            request: Request to get the distribution for.

        Returns:
            Result of the distribution.
        """

    @abc.abstractmethod
    async def stop(self) -> None:
        """Stop the component data manager."""



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_managers/_ev_charger_manager/__init__.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Manage ev chargers for the power distributor."""

from ._ev_charger_manager import EVChargerManager

__all__ = [
    "EVChargerManager",
]



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_managers/_ev_charger_manager/_config.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Configuration for the power distributor's EV charger manager."""

from collections import abc
from dataclasses import dataclass, field
from datetime import timedelta

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.quantities import Current


@dataclass(frozen=True)
class EVDistributionConfig:
    """Configuration for the power distributor's EV charger manager."""

    component_ids: abc.Set[ComponentId]
    """The component ids of the EV chargers."""

    min_current: Current = field(default_factory=lambda: Current.from_amperes(6.0))
    """The minimum current that can be allocated to an EV charger."""

    initial_current: Current = field(default_factory=lambda: Current.from_amperes(10.0))
    """The initial current that can be allocated to an EV charger."""

    increase_power_interval: timedelta = timedelta(seconds=60)
    """The interval at which the power can be increased for an EV charger."""



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_managers/_ev_charger_manager/_ev_charger_manager.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Manage EV chargers for the power distributor."""

import asyncio
import collections.abc
import logging
from datetime import datetime, timedelta, timezone

from frequenz.channels import (
    Broadcast,
    LatestValueCache,
    Sender,
    merge,
    select,
    selected_from,
)
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    ApiClientError,
    ComponentCategory,
    EVChargerData,
    MicrogridApiClient,
)
from frequenz.quantities import Power, Voltage
from typing_extensions import override

from ....._internal._asyncio import run_forever
from ....._internal._math import is_close_to_zero
from .....timeseries import Sample3Phase
from .... import _data_pipeline, connection_manager
from ..._component_pool_status_tracker import ComponentPoolStatusTracker
from ..._component_status import ComponentPoolStatus, EVChargerStatusTracker
from ...request import Request
from ...result import PartialFailure, Result, Success
from .._component_manager import ComponentManager
from ._config import EVDistributionConfig
from ._states import EvcState, EvcStates

_logger = logging.getLogger(__name__)


class EVChargerManager(ComponentManager):
    """Manage ev chargers for the power distributor."""

    @override
    def __init__(
        self,
        component_pool_status_sender: Sender[ComponentPoolStatus],
        results_sender: Sender[Result],
        api_power_request_timeout: timedelta,
    ):
        """Initialize the ev charger data manager.

        Args:
            component_pool_status_sender: Channel for sending information about which
                components are expected to be working.
            results_sender: Channel for sending results of power distribution.
            api_power_request_timeout: Timeout to use when making power requests to
                the microgrid API.
        """
        self._results_sender = results_sender
        self._api_power_request_timeout = api_power_request_timeout
        self._ev_charger_ids = self._get_ev_charger_ids()
        self._evc_states = EvcStates()
        self._voltage_cache: LatestValueCache[Sample3Phase[Voltage]] = LatestValueCache(
            _data_pipeline.voltage_per_phase().new_receiver(),
            unique_id=f"{type(self).__name__}«{hex(id(self))}»:voltage_cache",
        )
        self._config = EVDistributionConfig(component_ids=self._ev_charger_ids)
        self._component_pool_status_tracker = ComponentPoolStatusTracker(
            component_ids=self._ev_charger_ids,
            component_status_sender=component_pool_status_sender,
            max_data_age=timedelta(seconds=10.0),
            max_blocking_duration=timedelta(seconds=30.0),
            component_status_tracker_type=EVChargerStatusTracker,
        )
        self._target_power = Power.zero()
        self._target_power_channel = Broadcast[Request](name="target_power")
        self._target_power_tx = self._target_power_channel.new_sender()
        self._task: asyncio.Task[None] | None = None
        self._latest_request: Request = Request(Power.zero(), set())

    @override
    def component_ids(self) -> collections.abc.Set[ComponentId]:
        """Return the set of ev charger ids."""
        return self._ev_charger_ids

    @override
    async def start(self) -> None:
        """Start the ev charger data manager."""
        # Need to start a task only if there are EV chargers in the component graph.
        if self._ev_charger_ids:
            self._task = asyncio.create_task(run_forever(self._run))

    @override
    async def distribute_power(self, request: Request) -> None:
        """Distribute the requested power to the ev chargers.

        Args:
            request: Request to get the distribution for.
        """
        if self._ev_charger_ids:
            await self._target_power_tx.send(request)

    @override
    async def stop(self) -> None:
        """Stop the ev charger manager."""
        await self._voltage_cache.stop()
        await self._component_pool_status_tracker.stop()

    def _get_ev_charger_ids(self) -> collections.abc.Set[ComponentId]:
        """Return the IDs of all EV chargers present in the component graph."""
        return {
            evc.component_id
            for evc in connection_manager.get().component_graph.components(
                component_categories={ComponentCategory.EV_CHARGER}
            )
        }

    def _allocate_new_ev(self, component_id: ComponentId) -> dict[ComponentId, Power]:
        """Allocate power to a newly connected EV charger.

        Args:
            component_id: ID of the EV charger to allocate power to.

        Returns:
            A dictionary containing updated power allocations for the EV chargers.
        """
        available_power = (
            self._target_power - self._evc_states.get_total_allocated_power()
        )
        voltage = self._voltage_cache.get().min()
        if voltage is None:
            _logger.warning(
                "Voltage data is not available. Cannot allocate power to EV charger %s",
                component_id,
            )
            return {}
        initial_power = voltage * self._config.initial_current * 3.0
        if available_power > initial_power:
            return {component_id: initial_power}

        min_power = voltage * self._config.min_current * 3.0
        if available_power > min_power:
            return {component_id: min_power}

        return {}

    def _act_on_new_data(self, ev_data: EVChargerData) -> dict[ComponentId, Power]:
        """Act on new data from an EV charger.

        Args:
            ev_data: New data from the EV charger.

        Returns:
            A dictionary containing updated power allocations for the EV chargers.
        """
        component_id = ev_data.component_id
        ev_connected = ev_data.is_ev_connected()
        ev_previously_connected = self._evc_states.get(
            component_id
        ).last_data.is_ev_connected()

        # if EV is just connected, try to set config.initial_current, throttle other
        # EVs if necessary
        ev_newly_connected = ev_connected and not ev_previously_connected
        if ev_newly_connected:
            _logger.info("New EV connected to EV charger %s", component_id)
            return self._allocate_new_ev(component_id)

        # if EV is disconnected, set limit to 0.0.  redistribution to other EVs will
        # happen separately, when possible.
        if not ev_connected:
            if ev_previously_connected:
                _logger.info("EV disconnected from EV charger %s", component_id)
            if self._evc_states.get(component_id).last_allocation > Power.zero():
                return {component_id: Power.zero()}

        # else if last throttling was less than 'increase_power_interval', do nothing.
        now = datetime.now(tz=timezone.utc)
        last_throttling_time = self._evc_states.get(component_id).last_reallocation_time
        if last_throttling_time is not None:
            dur = now - last_throttling_time
            if dur < self._config.increase_power_interval:
                return {}

        # if ev's target power was previously set to zero, treat it like it is newly
        # connected
        evc = self._evc_states.get(component_id)
        if is_close_to_zero(evc.last_allocation.as_watts()):
            return self._allocate_new_ev(component_id)

        # if the ev charger is already allocated the max power, do nothing
        allottable_power = Power.from_watts(
            evc.last_data.active_power_inclusion_upper_bound
            - evc.last_allocation.as_watts()
        )
        available_power = (
            self._target_power - self._evc_states.get_total_allocated_power()
        )
        allottable_power = min(allottable_power, available_power)

        if (
            is_close_to_zero(allottable_power.as_watts())
            or allottable_power < Power.zero()
        ):
            return {}

        target_power = min(
            evc.last_allocation + allottable_power,
            Power.from_watts(evc.last_data.active_power_inclusion_upper_bound),
        )
        _logger.debug(
            "Increasing power to EV charger %s from %s to %s",
            component_id,
            evc.last_allocation,
            target_power,
        )
        return {component_id: target_power}

    async def _run(self) -> None:  # pylint: disable=too-many-locals
        """Run the main event loop of the EV charger manager."""
        api = connection_manager.get().api_client
        ev_charger_data_rx = merge(
            *[await api.ev_charger_data(evc_id) for evc_id in self._ev_charger_ids]
        )
        target_power_rx = self._target_power_channel.new_receiver()
        latest_target_powers: dict[ComponentId, Power] = {}
        async for selected in select(ev_charger_data_rx, target_power_rx):
            target_power_changes = {}
            now = datetime.now(tz=timezone.utc)

            if selected_from(selected, ev_charger_data_rx):
                evc_data = selected.message
                # If a new ev charger is added, add it to the state tracker, with
                # now as the last reallocation time and last charging time.
                #
                # This means it won't be assigned any power until the reallocation
                # duration has passed.
                if evc_data.component_id not in self._evc_states:
                    self._evc_states.add_evc(
                        EvcState(
                            component_id=evc_data.component_id,
                            last_data=evc_data,
                            power=Power.zero(),
                            last_allocation=Power.zero(),
                            last_reallocation_time=now,
                            last_charging_time=now,
                        )
                    )
                    target_power_changes = {evc_data.component_id: Power.zero()}

                # See if the ev charger has room for more power, and if the last
                # allocation was not in the last reallocation duration.
                else:
                    target_power_changes = self._act_on_new_data(evc_data)
                    self._evc_states.get(evc_data.component_id).update_state(evc_data)

            elif selected_from(selected, target_power_rx):
                self._latest_request = selected.message
                self._target_power = selected.message.power
                _logger.debug("New target power: %s", self._target_power)
                used_power = self._evc_states.get_ev_total_used_power()
                allocated_power = self._evc_states.get_total_allocated_power()
                if self._target_power < used_power:
                    diff_power = used_power - self._target_power
                    target_power_changes = self._throttle_ev_chargers(diff_power)
                    _logger.warning(
                        "Throttling EV chargers by %s-%s=%s: %s",
                        used_power,
                        self._target_power,
                        diff_power,
                        target_power_changes,
                    )
                elif self._target_power < allocated_power:
                    diff_power = allocated_power - self._target_power
                    target_power_changes = self._deallocate_unused_power(diff_power)

            if target_power_changes:
                _logger.debug("Setting power to EV chargers: %s", target_power_changes)
            else:
                continue
            for component_id, power in target_power_changes.items():
                self._evc_states.get(component_id).update_last_allocation(power, now)

            latest_target_powers.update(target_power_changes)
            result = await self._set_api_power(
                api, target_power_changes, self._api_power_request_timeout
            )
            await self._results_sender.send(result)

    async def _set_api_power(
        self,
        api: MicrogridApiClient,
        target_power_changes: dict[ComponentId, Power],
        api_request_timeout: timedelta,
    ) -> Result:
        """Send the EV charger power changes to the microgrid API.

        Args:
            api: The microgrid API client to use for setting the power.
            target_power_changes: A dictionary containing the new power allocations for
                the EV chargers.
            api_request_timeout: The timeout for the API request.

        Returns:
            Power distribution result, corresponding to the result of the API
                request.
        """
        tasks: dict[ComponentId, asyncio.Task[None]] = {}
        for component_id, power in target_power_changes.items():
            tasks[component_id] = asyncio.create_task(
                api.set_power(component_id, power.as_watts())
            )
        _, pending = await asyncio.wait(
            tasks.values(),
            timeout=api_request_timeout.total_seconds(),
            return_when=asyncio.ALL_COMPLETED,
        )
        for task in pending:
            task.cancel()
        await asyncio.gather(*pending, return_exceptions=True)

        failed_components: set[ComponentId] = set()
        succeeded_components: set[ComponentId] = set()
        failed_power = Power.zero()
        for component_id, task in tasks.items():
            try:
                task.result()
            except asyncio.CancelledError:
                _logger.warning(
                    "Timeout while setting power to EV charger %s", component_id
                )
            except ApiClientError as exc:
                _logger.warning(
                    "Got a client error while setting power to EV charger %s: %s",
                    component_id,
                    exc,
                )
            except Exception:  # pylint: disable=broad-except
                _logger.exception(
                    "Unknown error while setting power to EV charger: %s", component_id
                )
            else:
                succeeded_components.add(component_id)
                continue

            failed_components.add(component_id)
            failed_power += target_power_changes[component_id]

        if failed_components:
            return PartialFailure(
                failed_components=failed_components,
                succeeded_components=succeeded_components,
                failed_power=failed_power,
                succeeded_power=self._target_power - failed_power,
                excess_power=Power.zero(),
                request=self._latest_request,
            )
        return Success(
            succeeded_components=succeeded_components,
            succeeded_power=self._target_power,
            excess_power=Power.zero(),
            request=self._latest_request,
        )

    def _deallocate_unused_power(
        self, to_deallocate: Power
    ) -> dict[ComponentId, Power]:
        """Reduce the power allocated to the EV chargers to meet the target power.

        This prioritizes reducing power to EV chargers that aren't consuming the
        allocated power.

        Args:
            to_deallocate: The amount of power to reduce the total allocated power by.

        Returns:
            A list of new (reduced) charging current limits for a subset of ev
                chargers, required to bring down the consumption by the given value.
        """
        voltage = self._voltage_cache.get().min()
        if voltage is None:
            _logger.warning(
                "Voltage data is unavailable. Can't deallocate power from EV chargers",
            )
            return {}
        min_power = voltage * self._config.min_current * 3.0

        evc_list = list(self._evc_states.values())
        evc_list.sort(key=lambda st: st.last_allocation - st.power, reverse=True)

        deallocated_power = Power.zero()
        target_power_changes = {}

        for evc in evc_list:
            if deallocated_power >= to_deallocate:
                break
            ev_to_deallocate = evc.last_allocation - evc.power
            if ev_to_deallocate <= Power.zero():
                continue
            ev_to_deallocate = min(ev_to_deallocate, to_deallocate - deallocated_power)
            tgt_power = evc.last_allocation - ev_to_deallocate
            if tgt_power < min_power:
                tgt_power = Power.zero()
            deallocated_power += evc.last_allocation - tgt_power
            target_power_changes[evc.component_id] = tgt_power
        return target_power_changes

    def _throttle_ev_chargers(  # pylint: disable=too-many-locals
        self,
        throttle_by: Power,
    ) -> dict[ComponentId, Power]:
        """Reduce EV charging power to meet the target power.

        This targets EV chargers that are currently consuming the most.

        Level 1 throttling is done by reducing the power to the minimum current required
        to charge the EV.  When the consumption is still above the target power, level 2
        throttling is done by reducing the power to 0.

        Args:
            throttle_by: The amount of power to reduce the total EV charging power by.

        Returns:
            A list of new (reduced) charging current limits for a subset of ev
                chargers, required to bring down the consumption by the given value.
        """
        min_power = Power.zero()
        voltage = self._voltage_cache.get().min()
        if voltage is None:
            _logger.warning(
                "Voltage data is not available. Cannot perform level 1 throttling.",
            )
        else:
            min_power = voltage * self._config.min_current * 3.0

        evc_list = list(self._evc_states.values())
        evc_list.sort(key=lambda st: (st.power, st.last_allocation), reverse=True)

        level1_throttling_count = 0
        level1_amps_achieved = Power.zero()

        level2_throttling_count = 0
        level2_amps_achieved = Power.zero()

        for evc in evc_list:
            evc_power = evc.power
            evc_level1_power = Power.zero()
            if evc_power > min_power:
                evc_level1_power = evc_power - min_power

            if evc_power == Power.zero():
                evc_power = evc.last_allocation

            if evc_power == Power.zero():
                break

            if level1_amps_achieved < throttle_by:
                level1_amps_achieved += evc_level1_power
                level1_throttling_count += 1
            else:
                break
            if level2_amps_achieved < throttle_by:
                level2_amps_achieved += evc_power
                level2_throttling_count += 1

        if level1_amps_achieved >= throttle_by:
            throttled_powers = {
                evc.component_id: min_power
                for evc in evc_list[:level1_throttling_count]
            }
        else:
            throttled_powers = {
                evc.component_id: Power.zero()
                for evc in evc_list[:level2_throttling_count]
            }
        _logger.debug("Throttling: %s", throttled_powers)
        return throttled_powers



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_managers/_ev_charger_manager/_states.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Power distribution state tracking for ev chargers."""


from dataclasses import dataclass
from datetime import datetime
from typing import Iterable

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import EVChargerData
from frequenz.quantities import Power


@dataclass
class EvcState:
    """A class for tracking state of an ev charger."""

    component_id: ComponentId
    """The component id of the ev charger."""

    last_data: EVChargerData
    """The last data received from the EV charger."""

    power: Power
    """The power currently used by the EV charger."""

    last_allocation: Power
    """The last allocation made for the EV."""

    last_reallocation_time: datetime
    """The last time the ev charger was allocated power.

    Used to make sure we don't allocate power to the ev charger too often.
    """

    last_charging_time: datetime
    """The last time the ev charger was charging.

    Used to de-allocate power from the ev charger if it has not been charging
    for a while.
    """

    def update_last_allocation(self, allocation: Power, alloc_time: datetime) -> None:
        """Update the last allocation and related timestamps.

        Args:
            allocation: The most allocation allocation made for the EV.
            alloc_time: The time at which the allocation was made.
        """
        self.last_allocation = allocation
        self.last_reallocation_time = alloc_time
        self.last_charging_time = alloc_time

    def update_state(
        self,
        latest_ev_data: EVChargerData,
    ) -> None:
        """Update EvcState from component data.

        Args:
            latest_ev_data: latest ev data from component data stream.
        """
        self.power = Power.from_watts(latest_ev_data.active_power)
        self.last_data = latest_ev_data

        if self.power > Power.zero():
            self.last_charging_time = latest_ev_data.timestamp


class EvcStates:
    """Tracks states of all ev chargers."""

    _states: dict[ComponentId, EvcState]

    def __init__(self) -> None:
        """Initialize this instance."""
        self._states = {}

    def get_ev_total_used_power(self) -> Power:
        """Return the total power consumed by all EV Chargers."""
        total_used = Power.zero()
        for evc in self._states.values():
            total_used += evc.power
        return total_used

    def get_total_allocated_power(self) -> Power:
        """Return the total power allocated to all EV Chargers."""
        total_allocated = Power.zero()
        for evc in self._states.values():
            total_allocated += evc.last_allocation
        return total_allocated

    def get(self, component_id: ComponentId) -> EvcState:
        """Return a reference to the EvcState object with the given component_id.

        Args:
            component_id: identifies the object to retrieve.

        Returns:
            The EvcState object with the given component_id.
        """
        return self._states[component_id]

    def add_evc(self, state: EvcState) -> None:
        """Add the given EvcState object to the list.

        Args:
            state: The EvcState object to add to the list.
        """
        self._states[state.component_id] = state

    def values(self) -> Iterable[EvcState]:
        """Return an iterator over all EvcState objects.

        Returns:
            An iterator over all EvcState objects.
        """
        return self._states.values()

    def __contains__(self, component_id: ComponentId) -> bool:
        """Check if the given component_id has an associated EvcState object.

        Args:
            component_id: The component id to test.

        Returns:
            Boolean indicating whether the given component_id is a known
            EvCharger.
        """
        return component_id in self._states



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_managers/_pv_inverter_manager/__init__.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Manage PV inverters for the power distributor."""

from ._pv_inverter_manager import PVManager

__all__ = ["PVManager"]



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_managers/_pv_inverter_manager/_pv_inverter_manager.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Manage PV inverters for the power distributor."""

import asyncio
import collections.abc
import logging
from datetime import timedelta

from frequenz.channels import LatestValueCache, Sender
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    ApiClientError,
    ComponentCategory,
    InverterData,
    InverterType,
)
from frequenz.quantities import Power
from typing_extensions import override

from ....._internal._math import is_close_to_zero
from .... import connection_manager
from ..._component_pool_status_tracker import ComponentPoolStatusTracker
from ..._component_status import ComponentPoolStatus, PVInverterStatusTracker
from ...request import Request
from ...result import PartialFailure, Result, Success
from .._component_manager import ComponentManager

_logger = logging.getLogger(__name__)


class PVManager(ComponentManager):
    """Manage PV inverters for the power distributor."""

    @override
    def __init__(
        self,
        component_pool_status_sender: Sender[ComponentPoolStatus],
        results_sender: Sender[Result],
        api_power_request_timeout: timedelta,
    ) -> None:
        """Initialize this instance.

        Args:
            component_pool_status_sender: Channel for sending information about which
                components are expected to be working.
            results_sender: Channel for sending results of power distribution.
            api_power_request_timeout: Timeout to use when making power requests to
                the microgrid API.
        """
        self._results_sender = results_sender
        self._api_power_request_timeout = api_power_request_timeout
        self._pv_inverter_ids = self._get_pv_inverter_ids()

        self._component_pool_status_tracker = (
            ComponentPoolStatusTracker(
                component_ids=self._pv_inverter_ids,
                component_status_sender=component_pool_status_sender,
                max_data_age=timedelta(seconds=10.0),
                max_blocking_duration=timedelta(seconds=30.0),
                component_status_tracker_type=PVInverterStatusTracker,
            )
            if self._pv_inverter_ids
            else None
        )
        self._component_data_caches: dict[
            ComponentId, LatestValueCache[InverterData]
        ] = {}
        self._task: asyncio.Task[None] | None = None

    @override
    def component_ids(self) -> collections.abc.Set[ComponentId]:
        """Return the set of PV inverter ids."""
        return self._pv_inverter_ids

    @override
    async def start(self) -> None:
        """Start the PV inverter manager."""
        self._component_data_caches = {
            inv_id: LatestValueCache(
                await connection_manager.get().api_client.inverter_data(inv_id),
                unique_id=f"{type(self).__name__}«{hex(id(self))}»:inverter«{inv_id}»",
            )
            for inv_id in self._pv_inverter_ids
        }

    @override
    async def stop(self) -> None:
        """Stop the PV inverter manager."""
        await asyncio.gather(
            *[tracker.stop() for tracker in self._component_data_caches.values()]
        )
        if self._component_pool_status_tracker:
            await self._component_pool_status_tracker.stop()

    @override
    async def distribute_power(self, request: Request) -> None:
        """Distribute the requested power to the PV inverters.

        Args:
            request: Request to get the distribution for.

        Raises:
            ValueError: If no PV inverters are present in the component graph, but
                component_ids are provided in the request.
        """
        remaining_power = request.power
        allocations: dict[ComponentId, Power] = {}
        if not self._component_pool_status_tracker:
            if not request.component_ids:
                await self._results_sender.send(
                    Success(
                        succeeded_components=set(),
                        succeeded_power=Power.zero(),
                        excess_power=remaining_power,
                        request=request,
                    )
                )
                return
            raise ValueError(
                "Cannot distribute power to PV inverters without any inverters"
            )

        working_components: list[ComponentId] = []
        for inv_id in self._component_pool_status_tracker.get_working_components(
            request.component_ids
        ):
            if self._component_data_caches[inv_id].has_value():
                working_components.append(inv_id)
            else:
                _logger.warning(
                    "Excluding PV inverter %s from power distribution due to "
                    "lack of data since startup.",
                    inv_id,
                )

        # When sorting by lower bounds, which are negative for PV inverters, we have to
        # reverse the order, so that the inverters with the higher bounds i.e., the
        # least absolute value are first.
        working_components.sort(
            key=lambda inv_id: self._component_data_caches[inv_id]
            .get()
            .active_power_inclusion_lower_bound,
            reverse=True,
        )

        num_components = len(working_components)
        if num_components == 0:
            _logger.error("No PV inverters available for power distribution. Aborting.")
            return

        for idx, inv_id in enumerate(working_components):
            # Request powers are negative for PV inverters.  When remaining power is
            # greater than or equal to 0.0, we can stop allocating further, and set 0
            # power for all inverters for which no allocations were made.
            if remaining_power > Power.zero() or is_close_to_zero(
                remaining_power.as_watts()
            ):
                allocations[inv_id] = Power.zero()
                continue
            distribution = remaining_power / float(num_components - idx)
            inv_data = self._component_data_caches[inv_id]
            if not inv_data.has_value():
                allocations[inv_id] = Power.zero()
                # Can't get device bounds, so can't use inverter.
                continue
            discharge_bounds = Power.from_watts(
                inv_data.get().active_power_inclusion_lower_bound
            )
            # Because all 3 values are negative or 0, we use max, to get the value
            # with the least absolute value.
            allocated_power = max(remaining_power, discharge_bounds, distribution)
            allocations[inv_id] = allocated_power
            remaining_power -= allocated_power

        _logger.debug(
            "Distributing %s to PV inverters %s",
            request.power,
            allocations,
        )
        await self._set_api_power(request, allocations, remaining_power)

    async def _set_api_power(  # pylint: disable=too-many-locals
        self,
        request: Request,
        allocations: dict[ComponentId, Power],
        remaining_power: Power,
    ) -> None:
        api_client = connection_manager.get().api_client
        tasks: dict[ComponentId, asyncio.Task[None]] = {}
        for component_id, power in allocations.items():
            tasks[component_id] = asyncio.create_task(
                api_client.set_power(component_id, power.as_watts())
            )
        _, pending = await asyncio.wait(
            tasks.values(),
            timeout=self._api_power_request_timeout.total_seconds(),
            return_when=asyncio.ALL_COMPLETED,
        )
        # collect the timed out tasks and cancel them while keeping the
        # exceptions, so that they can be processed later.
        for task in pending:
            task.cancel()
        await asyncio.gather(*pending, return_exceptions=True)

        failed_components: set[ComponentId] = set()
        succeeded_components: set[ComponentId] = set()
        failed_power = Power.zero()
        for component_id, task in tasks.items():
            try:
                task.result()
            except asyncio.CancelledError:
                _logger.warning(
                    "Timeout while setting power to PV inverter %s", component_id
                )
            except ApiClientError as exc:
                _logger.warning(
                    "Got a client error while setting power to PV inverter %s: %s",
                    component_id,
                    exc,
                )
            except Exception:  # pylint: disable=broad-except
                _logger.exception(
                    "Unknown error while setting power to PV inverter: %s",
                    component_id,
                )
            else:
                succeeded_components.add(component_id)
                continue

            failed_components.add(component_id)
            failed_power += allocations[component_id]

        if failed_components:
            await self._results_sender.send(
                PartialFailure(
                    failed_components=failed_components,
                    succeeded_components=succeeded_components,
                    failed_power=failed_power,
                    succeeded_power=request.power - failed_power - remaining_power,
                    excess_power=remaining_power,
                    request=request,
                )
            )
            return
        await self._results_sender.send(
            Success(
                succeeded_components=succeeded_components,
                succeeded_power=request.power - remaining_power,
                excess_power=remaining_power,
                request=request,
            )
        )

    def _get_pv_inverter_ids(self) -> collections.abc.Set[ComponentId]:
        """Return the IDs of all PV inverters present in the component graph."""
        return {
            inv.component_id
            for inv in connection_manager.get().component_graph.components(
                component_categories={ComponentCategory.INVERTER}
            )
            if inv.type == InverterType.SOLAR
        }



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_status/__init__.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Status tracking for components."""

from ._battery_status_tracker import BatteryStatusTracker
from ._component_status import (
    ComponentPoolStatus,
    ComponentStatus,
    ComponentStatusEnum,
    ComponentStatusTracker,
    SetPowerResult,
)
from ._ev_charger_status_tracker import EVChargerStatusTracker
from ._pv_inverter_status_tracker import PVInverterStatusTracker

__all__ = [
    "BatteryStatusTracker",
    "ComponentPoolStatus",
    "ComponentStatus",
    "ComponentStatusEnum",
    "ComponentStatusTracker",
    "EVChargerStatusTracker",
    "PVInverterStatusTracker",
    "SetPowerResult",
]



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_status/_battery_status_tracker.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Background service that tracks the status of a battery.

A battery is consider to be WORKING if both the battery and the adjacent inverter are
sending data that shows that they are working.

If either of them stops sending data, or if the data shows that they are not working,
then the battery is considered to be NOT_WORKING.

If a battery and its adjacent inverter are WORKING, but the last request to the battery
failed, then the battery's status is considered to be UNCERTAIN. In this case, the
battery is blocked for a short time, and it is not recommended to use it unless it is
necessary.
"""

import asyncio
import logging
import math
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone

from frequenz.channels import Receiver, Sender, select, selected_from
from frequenz.channels.timer import SkipMissedAndDrift, Timer
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    BatteryComponentState,
    BatteryData,
    BatteryRelayState,
    ComponentCategory,
    ComponentData,
    ErrorLevel,
    InverterComponentState,
    InverterData,
)
from typing_extensions import override

from frequenz.sdk._internal._asyncio import run_forever

from ....actor._background_service import BackgroundService
from ... import connection_manager
from ._blocking_status import BlockingStatus
from ._component_status import (
    ComponentStatus,
    ComponentStatusEnum,
    ComponentStatusTracker,
    SetPowerResult,
)

_logger = logging.getLogger(__name__)


@dataclass
class _ComponentStreamStatus:
    component_id: ComponentId
    """Component id."""

    data_recv_timer: Timer
    """Timer that is set when no component data has been received for some time."""

    last_msg_timestamp: datetime = datetime.now(tz=timezone.utc)
    """Timestamp of the last message from the component."""

    last_msg_correct: bool = False
    """Flag whether last message was correct or not."""


class BatteryStatusTracker(ComponentStatusTracker, BackgroundService):
    """Class for tracking if battery is working.

    Status updates are sent out only when there is a status change.
    """

    _battery_valid_relay: set[BatteryRelayState] = {BatteryRelayState.CLOSED}
    """The list of valid relay states of a battery.

    A working battery in any other battery relay state will be reported as failing.
    """

    _battery_valid_state: set[BatteryComponentState] = {
        BatteryComponentState.IDLE,
        BatteryComponentState.CHARGING,
        BatteryComponentState.DISCHARGING,
    }
    """The list of valid states of a battery.

    A working battery in any other battery state will be reported as failing.
    """

    _inverter_valid_state: set[InverterComponentState] = {
        InverterComponentState.STANDBY,
        InverterComponentState.IDLE,
        InverterComponentState.CHARGING,
        InverterComponentState.DISCHARGING,
    }
    """The list of valid states of an inverter.

    A working inverter in any other inverter state will be reported as failing.
    """

    @override
    def __init__(  # pylint: disable=too-many-arguments
        self,
        *,
        component_id: ComponentId,
        max_data_age: timedelta,
        max_blocking_duration: timedelta,
        status_sender: Sender[ComponentStatus],
        set_power_result_receiver: Receiver[SetPowerResult],
    ) -> None:
        """Create class instance.

        Args:
            component_id: Id of this battery
            max_data_age: If component stopped sending data, then this is the maximum
                time when its last message should be considered as valid. After that
                time, component won't be used until it starts sending data.
            max_blocking_duration: This value tell what should be the maximum
                timeout used for blocking failing component.
            status_sender: Channel to send status updates.
            set_power_result_receiver: Channel to receive results of the requests to the
                components.

        Raises:
            RuntimeError: If battery has no adjacent inverter.
        """
        BackgroundService.__init__(self, name=f"BatteryStatusTracker({component_id})")
        self._max_data_age = max_data_age
        self._status_sender = status_sender
        self._set_power_result_receiver = set_power_result_receiver

        # First battery is considered as not working.
        # Change status after first messages are received.
        self._last_status: ComponentStatusEnum = ComponentStatusEnum.NOT_WORKING
        self._blocking_status: BlockingStatus = BlockingStatus(
            min_duration=timedelta(seconds=1.0), max_duration=max_blocking_duration
        )
        self._timedelta_zero = timedelta(seconds=0.0)

        inverter_id = self._find_adjacent_inverter_id(component_id)
        if inverter_id is None:
            raise RuntimeError(
                f"Can't find inverter adjacent to battery: {component_id}"
            )

        self._battery: _ComponentStreamStatus = _ComponentStreamStatus(
            component_id,
            data_recv_timer=Timer(max_data_age, SkipMissedAndDrift()),
        )
        self._inverter: _ComponentStreamStatus = _ComponentStreamStatus(
            inverter_id,
            data_recv_timer=Timer(max_data_age, SkipMissedAndDrift()),
        )

        # Select needs receivers that can be get in async way only.

    @override
    def start(self) -> None:
        """Start the BatteryStatusTracker instance."""
        self._tasks.add(
            asyncio.create_task(
                self._run(self._status_sender, self._set_power_result_receiver)
            )
        )

    @property
    def battery_id(self) -> ComponentId:
        """Get battery id.

        Returns:
            Battery id
        """
        return self._battery.component_id

    def _handle_status_battery(self, bat_data: BatteryData) -> None:
        self._battery.last_msg_correct = (
            self._is_message_reliable(bat_data)
            and self._is_battery_state_correct(bat_data)
            and self._no_critical_error(bat_data)
            and self._is_capacity_present(bat_data)
        )
        self._battery.last_msg_timestamp = bat_data.timestamp
        self._battery.data_recv_timer.reset()

    def _handle_status_inverter(self, inv_data: InverterData) -> None:
        self._inverter.last_msg_correct = (
            self._is_message_reliable(inv_data)
            and self._is_inverter_state_correct(inv_data)
            and self._no_critical_error(inv_data)
        )
        self._inverter.last_msg_timestamp = inv_data.timestamp
        self._inverter.data_recv_timer.reset()

    def _handle_status_set_power_result(self, result: SetPowerResult) -> None:
        if self.battery_id in result.succeeded:
            self._blocking_status.unblock()

        elif (
            self.battery_id in result.failed
            and self._last_status != ComponentStatusEnum.NOT_WORKING
        ):
            duration = self._blocking_status.block()

            if duration > self._timedelta_zero:
                _logger.warning(
                    "battery %d failed last response. block it for %s",
                    self.battery_id,
                    duration,
                )

    def _handle_status_battery_timer(self) -> None:
        if self._battery.last_msg_correct:
            self._battery.last_msg_correct = False
            _logger.warning(
                "Battery %d stopped sending data, last timestamp: %s",
                self._battery.component_id,
                self._battery.last_msg_timestamp,
            )

    def _handle_status_inverter_timer(self) -> None:
        if self._inverter.last_msg_correct:
            self._inverter.last_msg_correct = False
            _logger.warning(
                "Inverter %d stopped sending data, last timestamp: %s",
                self._inverter.component_id,
                self._inverter.last_msg_timestamp,
            )

    def _get_new_status_if_changed(self) -> ComponentStatusEnum | None:
        current_status = self._get_current_status()
        if self._last_status != current_status:
            self._last_status = current_status
            _logger.info(
                "battery %d changed status %s",
                self.battery_id,
                str(self._last_status),
            )
            return current_status
        return None

    async def _run(
        self,
        status_sender: Sender[ComponentStatus],
        set_power_result_receiver: Receiver[SetPowerResult],
    ) -> None:
        """Process data from the components and set_power_result_receiver.

        New status is send only when it change.

        Args:
            status_sender: Channel to send status updates.
            set_power_result_receiver: Channel to receive results of the requests to the
                components.
        """
        api_client = connection_manager.get().api_client

        battery_receiver = await api_client.battery_data(self._battery.component_id)
        inverter_receiver = await api_client.inverter_data(self._inverter.component_id)

        battery = battery_receiver
        battery_timer = self._battery.data_recv_timer
        inverter_timer = self._inverter.data_recv_timer
        inverter = inverter_receiver
        set_power_result = set_power_result_receiver

        async def _loop() -> None:
            try:
                async for selected in select(
                    battery,
                    battery_timer,
                    inverter_timer,
                    inverter,
                    set_power_result,
                ):
                    new_status = None

                    if selected_from(selected, battery):
                        self._handle_status_battery(selected.message)

                    elif selected_from(selected, inverter):
                        self._handle_status_inverter(selected.message)

                    elif selected_from(selected, set_power_result):
                        self._handle_status_set_power_result(selected.message)

                    elif selected_from(selected, battery_timer):
                        if (
                            datetime.now(tz=timezone.utc)
                            - self._battery.last_msg_timestamp
                        ) < self._max_data_age:
                            # This means that we have received data from the battery
                            # since the timer triggered, but the timer event arrived
                            # late, so we can ignore it.
                            continue
                        self._handle_status_battery_timer()

                    elif selected_from(selected, inverter_timer):
                        if (
                            datetime.now(tz=timezone.utc)
                            - self._inverter.last_msg_timestamp
                        ) < self._max_data_age:
                            # This means that we have received data from the inverter
                            # since the timer triggered, but the timer event arrived
                            # late, so we can ignore it.
                            continue
                        self._handle_status_inverter_timer()

                    else:
                        _logger.error("Unknown message returned from select")

                    new_status = self._get_new_status_if_changed()

                    if new_status is not None:
                        await status_sender.send(
                            ComponentStatus(self.battery_id, new_status)
                        )

            except Exception as err:  # pylint: disable=broad-except
                _logger.exception("BatteryStatusTracker crashed with error: %s", err)

        await run_forever(_loop)

    def _get_current_status(self) -> ComponentStatusEnum:
        """Get current battery status.

        Returns:
            Battery status.
        """
        is_msg_correct = (
            self._battery.last_msg_correct and self._inverter.last_msg_correct
        )

        if not is_msg_correct:
            return ComponentStatusEnum.NOT_WORKING
        if self._last_status == ComponentStatusEnum.NOT_WORKING:
            # If message just become correct, then try to use it
            self._blocking_status.unblock()
            return ComponentStatusEnum.WORKING
        if self._blocking_status.is_blocked():
            return ComponentStatusEnum.UNCERTAIN

        return ComponentStatusEnum.WORKING

    def _is_capacity_present(self, msg: BatteryData) -> bool:
        """Check whether the battery capacity is NaN or not.

        If battery capacity is missing, then we can't work with it.

        Args:
            msg: battery message

        Returns:
            True if battery capacity is present, false otherwise.
        """
        if math.isnan(msg.capacity):
            if self._last_status == ComponentStatusEnum.WORKING:
                _logger.warning(
                    "Battery %d capacity is NaN",
                    msg.component_id,
                )
            return False
        return True

    def _no_critical_error(self, msg: BatteryData | InverterData) -> bool:
        """Check if battery or inverter message has any critical error.

        Args:
            msg: message.

        Returns:
            True if message has no critical error, False otherwise.
        """
        critical = ErrorLevel.CRITICAL
        critical_err = next((err for err in msg.errors if err.level == critical), None)
        if critical_err is not None:
            last_status = self._last_status  # pylint: disable=protected-access
            if last_status == ComponentStatusEnum.WORKING:
                _logger.warning(
                    "Component %d has critical error: %s",
                    msg.component_id,
                    str(critical_err),
                )
            return False
        return True

    def _is_inverter_state_correct(self, msg: InverterData) -> bool:
        """Check if inverter is in correct state from message.

        Args:
            msg: message

        Returns:
            True if inverter is in correct state. False otherwise.
        """
        # Component state is not exposed to the user.
        state = msg.component_state
        # pylint: disable-next=protected-access
        if state not in BatteryStatusTracker._inverter_valid_state:
            if self._last_status == ComponentStatusEnum.WORKING:
                _logger.warning(
                    "Inverter %d has invalid state: %s",
                    msg.component_id,
                    state.name,
                )
            return False
        return True

    def _is_battery_state_correct(self, msg: BatteryData) -> bool:
        """Check if battery is in correct state from message.

        Args:
            msg: message

        Returns:
            True if battery is in correct state. False otherwise.
        """
        # Component state is not exposed to the user.
        state = msg.component_state
        # pylint: disable-next=protected-access
        if state not in BatteryStatusTracker._battery_valid_state:
            if self._last_status == ComponentStatusEnum.WORKING:
                _logger.warning(
                    "Battery %d has invalid state: %s",
                    self.battery_id,
                    state.name,
                )
            return False

        # Component state is not exposed to the user.
        relay_state = msg.relay_state
        if relay_state not in BatteryStatusTracker._battery_valid_relay:
            if self._last_status == ComponentStatusEnum.WORKING:
                _logger.warning(
                    "Battery %d has invalid relay state: %s",
                    self.battery_id,
                    relay_state.name,
                )
            return False
        return True

    def _is_timestamp_outdated(self, timestamp: datetime) -> bool:
        """Return if timestamp is to old.

        Args:
            timestamp: timestamp

        Returns:
            _True if timestamp is to old, False otherwise
        """
        now = datetime.now(tz=timezone.utc)
        diff = now - timestamp
        return diff > self._max_data_age

    def _is_message_reliable(self, message: ComponentData) -> bool:
        """Check if message is too old to be considered as reliable.

        Args:
            message: message to check

        Returns:
            True if message is reliable, False otherwise.
        """
        is_outdated = self._is_timestamp_outdated(message.timestamp)

        if is_outdated and self._last_status == ComponentStatusEnum.WORKING:
            _logger.warning(
                "Component %d stopped sending data. Last timestamp: %s.",
                message.component_id,
                str(message.timestamp),
            )

        return not is_outdated

    def _find_adjacent_inverter_id(self, battery_id: ComponentId) -> ComponentId | None:
        """Find inverter adjacent to this battery.

        Args:
            battery_id: battery id adjacent to the wanted inverter

        Returns:
            Id of the inverter. If battery hasn't adjacent inverter, then return None.
        """
        graph = connection_manager.get().component_graph
        return next(
            (
                comp.component_id
                for comp in graph.predecessors(battery_id)
                if comp.category == ComponentCategory.INVERTER
            ),
            None,
        )



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_status/_blocking_status.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Tracking of the blocking status of a component."""

from dataclasses import dataclass
from datetime import datetime, timedelta, timezone


@dataclass(kw_only=True)
class BlockingStatus:
    """Tracking of the blocking status of a component."""

    min_duration: timedelta
    """The minimum blocking duration."""

    max_duration: timedelta
    """The maximum blocking duration."""

    last_blocking_duration: timedelta = timedelta(seconds=0.0)
    """Last blocking duration."""

    blocked_until: datetime | None = None
    """Time until which the component is blocked."""

    def __post_init__(self) -> None:
        """Validate the blocking duration."""
        assert self.min_duration <= self.max_duration, (
            f"Minimum blocking duration ({self.min_duration}) cannot be greater "
            f"than maximum blocking duration ({self.max_duration})"
        )
        self.last_blocking_duration = self.min_duration
        self._timedelta_zero = timedelta(seconds=0.0)

    def block(self) -> timedelta:
        """Set the component as blocked.

        Returns:
            The duration for which the component is blocked.
        """
        now = datetime.now(tz=timezone.utc)

        # If is not blocked
        if self.blocked_until is None:
            self.last_blocking_duration = self.min_duration
            self.blocked_until = now + self.last_blocking_duration
            return self.last_blocking_duration

        # If still blocked, then do nothing
        if self.blocked_until > now:
            return self._timedelta_zero

        # If previous blocking time expired, then blocked it once again.
        # Increase last blocking time, unless it reach the maximum.
        self.last_blocking_duration = min(
            2 * self.last_blocking_duration, self.max_duration
        )
        self.blocked_until = now + self.last_blocking_duration

        return self.last_blocking_duration

    def unblock(self) -> None:
        """Set the component as unblocked."""
        self.blocked_until = None

    def is_blocked(self) -> bool:
        """Check if the component is blocked.

        Returns:
            True if battery is blocked, False otherwise.
        """
        if self.blocked_until is None:
            return False
        return self.blocked_until > datetime.now(tz=timezone.utc)



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_status/_component_status.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Classes to track the status of components in the microgrid."""


import enum
from abc import ABC, abstractmethod
from collections import abc
from dataclasses import dataclass
from datetime import timedelta

from frequenz.channels import Receiver, Sender
from frequenz.client.common.microgrid.components import ComponentId

from ....actor._background_service import BackgroundService


@dataclass
class ComponentPoolStatus:
    """Status of all components of a certain category in the microgrid."""

    working: set[ComponentId]
    """Set of working component ids."""

    uncertain: set[ComponentId]
    """Set of components to be used only when there are none known to be working."""

    def get_working_components(
        self, components: abc.Set[ComponentId]
    ) -> set[ComponentId]:
        """From the given set of components return the working ones.

        Args:
            components: Set of components.

        Returns:
            Subset with working components.
        """
        working = self.working.intersection(components)
        if len(working) > 0:
            return working
        return self.uncertain.intersection(components)


class ComponentStatusEnum(enum.Enum):
    """Enum for component status."""

    NOT_WORKING = 0
    """Component is not working and should not be used."""

    UNCERTAIN = 1
    """Component should work, although the last request to it failed.

    It is blocked for few seconds and it is not recommended to use it unless it is
    necessary.
    """

    WORKING = 2
    """Component is working"""


@dataclass(frozen=True)
class ComponentStatus:
    """Status of a single component."""

    component_id: ComponentId
    """Component ID."""

    value: ComponentStatusEnum
    """Component status."""


@dataclass(frozen=True, kw_only=True)
class SetPowerResult:
    """Lists of components for which the last set power command succeeded or failed."""

    succeeded: abc.Set[ComponentId]
    """Component IDs for which the last set power command succeeded."""

    failed: abc.Set[ComponentId]
    """Component IDs for which the last set power command failed."""


class ComponentStatusTracker(BackgroundService, ABC):
    """Interface for specialized component status trackers to implement."""

    @abstractmethod
    def __init__(  # pylint: disable=too-many-arguments,super-init-not-called
        self,
        *,
        component_id: ComponentId,
        max_data_age: timedelta,
        max_blocking_duration: timedelta,
        status_sender: Sender[ComponentStatus],
        set_power_result_receiver: Receiver[SetPowerResult],
    ) -> None:
        """Create class instance.

        Args:
            component_id: Id of this component
            max_data_age: If component stopped sending data, then this is the maximum
                time when its last message should be considered as valid. After that
                time, component won't be used until it starts sending data.
            max_blocking_duration: This value tell what should be the maximum
                timeout used for blocking failing component.
            status_sender: Channel to send status updates.
            set_power_result_receiver: Channel to receive results of the requests to the
                components.
        """



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_status/_ev_charger_status_tracker.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Background service that tracks the status of an EV charger."""


import asyncio
import logging
from datetime import datetime, timedelta, timezone

from frequenz.channels import Receiver, Sender, select, selected_from
from frequenz.channels.timer import SkipMissedAndDrift, Timer
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    EVChargerCableState,
    EVChargerComponentState,
    EVChargerData,
)
from typing_extensions import override

from ...._internal._asyncio import run_forever
from ....actor._background_service import BackgroundService
from ... import connection_manager
from ._blocking_status import BlockingStatus
from ._component_status import (
    ComponentStatus,
    ComponentStatusEnum,
    ComponentStatusTracker,
    SetPowerResult,
)

_logger = logging.getLogger(__name__)


class EVChargerStatusTracker(ComponentStatusTracker, BackgroundService):
    """Status tracker for EV chargers.

    It reports an EV charger as `WORKING` when an EV is connected to it,
    and power can be allocated to it, and `NOT_WORKING` otherwise.

    If it receives a power assignment failure from the PowerDistributor,
    when the component is expected to be `WORKING`, it is marked as
    `UNCERTAIN` for a specific interval, before being marked `WORKING`
    again.
    """

    @override
    def __init__(  # pylint: disable=too-many-arguments
        self,
        *,
        component_id: ComponentId,
        max_data_age: timedelta,
        max_blocking_duration: timedelta,
        status_sender: Sender[ComponentStatus],
        set_power_result_receiver: Receiver[SetPowerResult],
    ) -> None:
        """Initialize this instance.

        Args:
            component_id: ID of the EV charger to monitor the status of.
            max_data_age: max duration to wait for, before marking a component as
                NOT_WORKING, unless new data arrives.
            max_blocking_duration: duration for which the component status should be
                UNCERTAIN if a request to the component failed unexpectedly.
            status_sender: Channel sender to send status updates to.
            set_power_result_receiver: Receiver to fetch PowerDistributor responses
                from, to get the status of the most recent request made for an EV
                Charger.
        """
        self._component_id = component_id
        self._max_data_age = max_data_age
        self._status_sender = status_sender
        self._set_power_result_receiver = set_power_result_receiver

        self._last_status = ComponentStatusEnum.NOT_WORKING
        self._blocking_status = BlockingStatus(
            min_duration=timedelta(seconds=1.0),
            max_duration=max_blocking_duration,
        )

        BackgroundService.__init__(self, name=f"EVChargerStatusTracker({component_id})")

    @override
    def start(self) -> None:
        """Start the status tracker."""
        self._tasks.add(asyncio.create_task(run_forever(self._run)))

    def _is_working(self, ev_data: EVChargerData) -> bool:
        """Return whether the given data indicates that the component is working."""
        return ev_data.cable_state in (
            EVChargerCableState.EV_PLUGGED,
            EVChargerCableState.EV_LOCKED,
        ) and ev_data.component_state in (
            EVChargerComponentState.READY,
            EVChargerComponentState.CHARGING,
            EVChargerComponentState.DISCHARGING,
        )

    def _is_stale(self, ev_data: EVChargerData) -> bool:
        """Return whether the given data is stale."""
        now = datetime.now(tz=timezone.utc)
        stale = now - ev_data.timestamp > self._max_data_age
        return stale

    def _handle_ev_data(self, ev_data: EVChargerData) -> ComponentStatusEnum:
        """Handle new EV charger data."""
        if self._is_stale(ev_data):
            if self._last_status == ComponentStatusEnum.WORKING:
                _logger.warning(
                    "EV charger %s data is stale. Last timestamp: %s",
                    self._component_id,
                    ev_data.timestamp,
                )
            return ComponentStatusEnum.NOT_WORKING

        if self._is_working(ev_data):
            if self._last_status == ComponentStatusEnum.NOT_WORKING:
                _logger.warning(
                    "EV charger %s is in WORKING state.",
                    self._component_id,
                )
            return ComponentStatusEnum.WORKING

        if self._last_status == ComponentStatusEnum.WORKING:
            _logger.warning(
                "EV charger %s is in NOT_WORKING state. "
                "Cable state: %s, component state: %s",
                self._component_id,
                ev_data.cable_state,
                ev_data.component_state,
            )
        return ComponentStatusEnum.NOT_WORKING

    def _handle_set_power_result(
        self, set_power_result: SetPowerResult
    ) -> ComponentStatusEnum:
        """Handle a new set power result."""
        if self._component_id in set_power_result.succeeded:
            return ComponentStatusEnum.WORKING

        self._blocking_status.block()
        if self._last_status == ComponentStatusEnum.WORKING:
            _logger.warning(
                "EV charger %s is in UNCERTAIN state. Set power result: %s",
                self._component_id,
                set_power_result,
            )
        return ComponentStatusEnum.UNCERTAIN

    async def _run(self) -> None:
        """Run the status tracker."""
        api_client = connection_manager.get().api_client
        ev_data_rx = await api_client.ev_charger_data(self._component_id)
        set_power_result_rx = self._set_power_result_receiver
        missing_data_timer = Timer(self._max_data_age, SkipMissedAndDrift())

        # Send initial status
        await self._status_sender.send(
            ComponentStatus(self._component_id, self._last_status)
        )

        async for selected in select(
            ev_data_rx, set_power_result_rx, missing_data_timer
        ):
            new_status = ComponentStatusEnum.NOT_WORKING
            if selected_from(selected, ev_data_rx):
                missing_data_timer.reset()
                new_status = self._handle_ev_data(selected.message)
            elif selected_from(selected, set_power_result_rx):
                new_status = self._handle_set_power_result(selected.message)
            elif selected_from(selected, missing_data_timer):
                _logger.warning(
                    "No EV charger %s data received for %s. Setting status to NOT_WORKING.",
                    self._component_id,
                    self._max_data_age,
                )

            # Send status update if status changed
            if (
                self._blocking_status.is_blocked()
                and new_status != ComponentStatusEnum.NOT_WORKING
            ):
                new_status = ComponentStatusEnum.UNCERTAIN

            if new_status != self._last_status:
                _logger.info(
                    "EV charger %s status changed from %s to %s",
                    self._component_id,
                    self._last_status,
                    new_status,
                )
                self._last_status = new_status
                await self._status_sender.send(
                    ComponentStatus(self._component_id, new_status)
                )



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_component_status/_pv_inverter_status_tracker.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Background service that tracks the status of a PV inverter."""

import asyncio
import logging
from datetime import datetime, timedelta, timezone

from frequenz.channels import Receiver, Sender, select, selected_from
from frequenz.channels.timer import SkipMissedAndDrift, Timer
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import InverterComponentState, InverterData
from typing_extensions import override

from ...._internal._asyncio import run_forever
from ....actor._background_service import BackgroundService
from ... import connection_manager
from ._blocking_status import BlockingStatus
from ._component_status import (
    ComponentStatus,
    ComponentStatusEnum,
    ComponentStatusTracker,
    SetPowerResult,
)

_logger = logging.getLogger(__name__)


class PVInverterStatusTracker(ComponentStatusTracker, BackgroundService):
    """Status tracker for PV inverters.

    It reports a PV inverter as `WORKING` or `NOT_WORKING` based on
    the status in the received component data from the microgrid API.
    When no data is received for a specific duration, the component is
    marked as `NOT_WORKING`.

    If it receives a power assignment failure from the PowerDistributor,
    when the component is expected to be `WORKING`, it is marked as
    `UNCERTAIN` for a specific interval, before being marked `WORKING`
    again.
    """

    @override
    def __init__(  # pylint: disable=too-many-arguments
        self,
        *,
        component_id: ComponentId,
        max_data_age: timedelta,
        max_blocking_duration: timedelta,
        status_sender: Sender[ComponentStatus],
        set_power_result_receiver: Receiver[SetPowerResult],
    ) -> None:
        """Initialize this instance.

        Args:
            component_id: ID of the PV inverter to monitor the status of.
            max_data_age: max duration to wait for, before marking a component as
                NOT_WORKING, unless new data arrives.
            max_blocking_duration: max duration to wait for, before marking a component
                as BLOCKING, unless new data arrives.
            status_sender: Sender to send the status of the PV inverter.
            set_power_result_receiver: Receiver for the power assignment result.
        """
        BackgroundService.__init__(
            self, name=f"PVInverterStatusTracker({component_id})"
        )
        self._component_id = component_id
        self._max_data_age = max_data_age
        self._status_sender = status_sender
        self._set_power_result_receiver = set_power_result_receiver

        self._last_status = ComponentStatusEnum.NOT_WORKING
        self._blocking_status = BlockingStatus(
            min_duration=timedelta(seconds=1.0),
            max_duration=max_blocking_duration,
        )

    @override
    def start(self) -> None:
        """Start the status tracker."""
        self._tasks.add(asyncio.create_task(run_forever(self._run)))

    def _is_working(self, pv_data: InverterData) -> bool:
        """Return whether the given data indicates that the PV inverter is working."""
        return pv_data.component_state in (
            InverterComponentState.DISCHARGING,
            InverterComponentState.CHARGING,
            InverterComponentState.IDLE,
            InverterComponentState.STANDBY,
        )

    def _is_stale(self, pv_data: InverterData) -> bool:
        """Return whether the given data is stale."""
        now = datetime.now(tz=timezone.utc)
        stale = now - pv_data.timestamp > self._max_data_age
        return stale

    def _handle_set_power_result(
        self, set_power_result: SetPowerResult
    ) -> ComponentStatusEnum:
        """Handle a new set power result."""
        if self._component_id in set_power_result.succeeded:
            return ComponentStatusEnum.WORKING

        self._blocking_status.block()
        if self._last_status == ComponentStatusEnum.WORKING:
            _logger.warning(
                "PV inverter %s is in UNCERTAIN state. Set power result: %s",
                self._component_id,
                set_power_result,
            )
        return ComponentStatusEnum.UNCERTAIN

    def _handle_pv_inverter_data(self, pv_data: InverterData) -> ComponentStatusEnum:
        """Handle new PV inverter data."""
        if self._is_stale(pv_data):
            if self._last_status == ComponentStatusEnum.WORKING:
                _logger.warning(
                    "PV inverter %s data is stale. Last timestamp: %s",
                    self._component_id,
                    pv_data.timestamp,
                )
            return ComponentStatusEnum.NOT_WORKING

        if self._is_working(pv_data):
            if self._last_status == ComponentStatusEnum.NOT_WORKING:
                _logger.warning(
                    "PV inverter %s is in WORKING state.",
                    self._component_id,
                )
            return ComponentStatusEnum.WORKING

        if self._last_status == ComponentStatusEnum.WORKING:
            _logger.warning(
                "PV inverter %s is in NOT_WORKING state.  Component state: %s",
                self._component_id,
                pv_data.component_state,
            )
        return ComponentStatusEnum.NOT_WORKING

    async def _run(self) -> None:
        """Run the status tracker."""
        api_client = connection_manager.get().api_client
        pv_data_rx = await api_client.inverter_data(self._component_id)
        set_power_result_rx = self._set_power_result_receiver
        missing_data_timer = Timer(self._max_data_age, SkipMissedAndDrift())

        # Send initial status
        await self._status_sender.send(
            ComponentStatus(self._component_id, self._last_status)
        )

        async for selected in select(
            pv_data_rx, set_power_result_rx, missing_data_timer
        ):
            new_status = ComponentStatusEnum.NOT_WORKING
            if selected_from(selected, pv_data_rx):
                missing_data_timer.reset()
                new_status = self._handle_pv_inverter_data(selected.message)
            elif selected_from(selected, set_power_result_rx):
                new_status = self._handle_set_power_result(selected.message)
            elif selected_from(selected, missing_data_timer):
                _logger.warning(
                    "No PV inverter %s data received for %s. "
                    "Setting status to NOT_WORKING.",
                    self._component_id,
                    self._max_data_age,
                )

            # Send status update if status changed
            if (
                self._blocking_status.is_blocked()
                and new_status != ComponentStatusEnum.NOT_WORKING
            ):
                new_status = ComponentStatusEnum.UNCERTAIN

            if new_status != self._last_status:
                _logger.info(
                    "PV inverter %s status changed from %s to %s",
                    self._component_id,
                    self._last_status,
                    new_status,
                )
                self._last_status = new_status
                await self._status_sender.send(
                    ComponentStatus(self._component_id, new_status)
                )



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_distribution_algorithm/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Utilities to manage power in a microgrid."""

from ._battery_distribution_algorithm import (
    AggregatedBatteryData,
    BatteryDistributionAlgorithm,
    DistributionResult,
    InvBatPair,
)

__all__ = [
    "BatteryDistributionAlgorithm",
    "DistributionResult",
    "InvBatPair",
    "AggregatedBatteryData",
]



================================================
FILE: src/frequenz/sdk/microgrid/_power_distributing/_distribution_algorithm/_battery_distribution_algorithm.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Power distribution algorithm to distribute power between batteries."""

import logging
import math
from dataclasses import dataclass
from typing import NamedTuple, Sequence

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import BatteryData, InverterData
from frequenz.quantities import Power

from ...._internal._math import is_close_to_zero
from ..result import PowerBounds

_logger = logging.getLogger(__name__)


@dataclass()
class AggregatedBatteryData:
    """Aggregated battery data."""

    component_id: ComponentId
    """The component ID of the first battery.

    This is only used to identify the pair of battery and inverter.
    """

    soc: float
    """The aggregated SoC of the batteries."""

    capacity: float
    """The aggregated capacity of the batteries."""

    soc_upper_bound: float
    """The aggregated upper SoC bound of the batteries."""

    soc_lower_bound: float
    """The aggregated lower SoC bound of the batteries."""

    power_bounds: PowerBounds
    """The aggregated power bounds of the batteries."""

    def __init__(self, batteries: list[BatteryData]) -> None:
        """Create DistBatteryData from BatteryData.

        Aggregates the data of the batteries:

        * Capacity: Sum of capacities of all batteries.
        * SoC: Weighted average of SoCs of all batteries.
        * SoC bounds: Weighted average of SoC bounds of all batteries.
        * Power inclusion bounds: Sum of power inclusion bounds of all batteries.
        * Power exclusion bounds: Largest power exclusion bound multiplied by
            the number of batteries.

        Args:
            batteries: The batteries to aggregate.
        """
        assert len(batteries) > 0, "AggregatedBatteryData: No batteries given."

        # We need only one component ID for DistBatteryData to be able to
        # identify the pair
        self.component_id = batteries[0].component_id

        self.capacity = sum(b.capacity for b in batteries)

        if self.capacity != 0.0:
            self.soc = sum(b.soc * b.capacity for b in batteries) / self.capacity
            self.soc_upper_bound = (
                sum(b.soc_upper_bound * b.capacity for b in batteries) / self.capacity
            )
            self.soc_lower_bound = (
                sum(b.soc_lower_bound * b.capacity for b in batteries) / self.capacity
            )
        else:
            self.soc = math.nan
            self.soc_upper_bound = math.nan
            self.soc_lower_bound = math.nan

        self.power_bounds = _aggregate_battery_power_bounds(
            list(
                map(
                    lambda metrics: PowerBounds(
                        inclusion_upper=Power.from_watts(
                            metrics.power_inclusion_upper_bound
                        ),
                        inclusion_lower=Power.from_watts(
                            metrics.power_inclusion_lower_bound
                        ),
                        exclusion_upper=Power.from_watts(
                            metrics.power_exclusion_upper_bound
                        ),
                        exclusion_lower=Power.from_watts(
                            metrics.power_exclusion_lower_bound
                        ),
                    ),
                    batteries,
                )
            )
        )


def _aggregate_battery_power_bounds(
    battery_metrics: Sequence[PowerBounds],
) -> PowerBounds:
    """Calculate bounds for a set of batteries located behind one set of inverters.

    Args:
        battery_metrics: List of PowerBounds for each battery.

    Returns:
        A PowerBounds object containing the aggregated bounds for all given batteries
    """
    assert len(battery_metrics) > 0, "No batteries given."

    # Calculate the aggregated bounds for the set of batteries
    power_inclusion_upper_bound = sum(
        (bounds.inclusion_upper for bounds in battery_metrics), start=Power.zero()
    )
    power_inclusion_lower_bound = sum(
        (bounds.inclusion_lower for bounds in battery_metrics), start=Power.zero()
    )

    # To satisfy the largest exclusion bounds in the set we need to
    # provide the power defined by the largest bounds multiplied by the
    # number of batteries in the set.
    power_exclusion_upper_bound = Power.from_watts(
        max(bounds.exclusion_upper for bounds in battery_metrics).as_watts()
        * len(battery_metrics)
    )
    power_exclusion_lower_bound = Power.from_watts(
        min(bounds.exclusion_lower for bounds in battery_metrics).as_watts()
        * len(battery_metrics)
    )

    return PowerBounds(
        inclusion_lower=power_inclusion_lower_bound,
        exclusion_lower=power_exclusion_lower_bound,
        exclusion_upper=power_exclusion_upper_bound,
        inclusion_upper=power_inclusion_upper_bound,
    )


class InvBatPair(NamedTuple):
    """InvBatPair with inverter and adjacent battery data."""

    battery: AggregatedBatteryData
    """The battery data."""

    inverter: list[InverterData]
    """The inverter data."""


@dataclass
class AvailabilityRatio:
    """Availability ratio for a battery-inverter pair."""

    battery_id: ComponentId
    """The battery ID."""

    inverter_ids: list[ComponentId]
    """The inverter IDs."""

    ratio: float
    """The availability ratio."""

    min_power: Power
    """The minimum power that can be set for the battery-inverters pair."""


@dataclass
class _Power:
    """Helper class for distribution algorithm."""

    upper_bound: Power
    """The upper bound of the power that can be set for the battery."""

    power: Power
    """The power to be set for the inverter."""


_InverterSet = frozenset[ComponentId]
"""A set of inverter IDs."""


@dataclass
class _Allocation:
    """Helper class for distribution algorithm."""

    inverter_ids: _InverterSet
    """The IDs of the inverters."""

    power: Power
    """The power to be set for the inverters."""


@dataclass
class DistributionResult:
    """Distribution result."""

    distribution: dict[ComponentId, Power]
    """The power to be set for each inverter.

    The key is inverter ID, and the value is the power that should be set for
    that inverter.
    """

    remaining_power: Power
    """The power which could not be distributed because of bounds."""


class BatteryDistributionAlgorithm:
    r"""Distribute power between many components.

    The purpose of this tool is to keep equal SoC level in the batteries.
    It takes total power that should be to be set for some subset of battery-inverter
    pairs. The total power is distributed between given battery-inverter pairs.
    Distribution is calculated based on data below:

    * Battery current SoC.
    * Battery upper and lower SoC bound.
    * Battery capacity.
    * Battery lower and upper power bound.
    * Inverter lower and upper active power bound.

    # Distribution algorithm

    Lets assume that:

    * `N` - number of batteries
    * `power_w` - power to distribute
    * `capacity[i]` - capacity of i'th battery
    * `available_soc[i]` - how much SoC remained to reach:
        * SoC upper bound - if need to distribute power that charges inverters.
        * SoC lower bound - if need to distribute power that discharges inverters.
        * `0` - if SoC is outside SoC bounds.

    * `total_capacity` - `sum(c for c in capacity.values())`
    * `capacity_ratio[i]` - `capacity[i]/total_capacity`


    We would like our distribution to meet the equation:

    ```
    distribution[i] = power_w * capacity_ratio[i] * x[i]
    ```

    where:

    ```
    sum(capacity_ratio[i] * x[i] for i in range(N)) == 1
    ```

    Let `y` be our unknown, the proportion to discharge each battery would be
    (1):

    ```
    x[i] = available_soc[i]*y
    ```

    We can compute `y` from equation above (2):

    ```
    sum(capacity_ratio[i] * x[i] for i in range(N)) == 1
    # =>
    sum(capacity_ratio[i] * available_soc[i] * y for i in range(N)) == 1
    # =>
    y = 1 / sum(capacity_ratio[i] * available_soc[i])
    ```

    Now we know everything and we can compute distribution:

    ```
    distribution[i] = power_w * capacity_ratio[i] * x[i]  # from (1)
    distribution[i] = \
            power_w * capacity_ratio[i] * available_soc[i] * y  # from (2)
    distribution[i] = power_w * capacity_ratio[i] * available_soc[i] * \
            1/sum(capacity_ratio[i] * available_soc[i])
    ```

    Let:

    ```
    battery_availability_ratio[i] = capacity_ratio[i] * available_soc[i]
    total_battery_availability_ratio = sum(battery_availability_ratio)
    ```

    Then:
    ```
    distribution[i] = power_w * battery_availability_ratio[i] \
            / total_battery_availability_ratio
    ```
    """

    _MINIMUM_DEFICIT_TO_CONSIDER = Power.from_watts(-0.1)

    def __init__(self, distributor_exponent: float = 1) -> None:
        """Create distribution algorithm instance.

        Args:
            distributor_exponent: How fast the batteries should strive to the
                equal SoC level. Should be float >= 0. Defaults=1.
                For example for distributor_exponent equal:
                    * 1 - means that proportion will be linear from SoC.
                    * 2 - means proportion would be like squared from SoC
                    * 3 - means proportion would be like x^3 from SoC.

        Example:
            Lets say we have two batteries `Bat1` and `Bat2`. All parameters
            except SoC are equal. SoC bounds for each battery is `lower = 20`,
            `upper = 80`.

            # Example 1

            Let:

            * `Bat1.soc = 70` and `Bat2.soc = 50`.
            * `Bat1.available_soc = 10`, `Bat2.available_soc = 30`
            * `Bat1.available_soc / Bat2.available_soc = 3`

            A request power of 8000W will be distributed as follows, for different
            values of `distribution_exponent`:

            | distribution_exponent | Bat1 | Bat2 |
            |-----------------------|------|------|
            | 0                     | 4000 | 4000 |
            | 1                     | 2000 | 6000 |
            | 2                     | 800  | 7200 |
            | 3                     | 285  | 7715 |


            # Example 2

            Let:

            * `Bat1.soc = 50` and `Bat2.soc = 20`.
            * `Bat1.available_soc = 30`, `Bat2.available_soc = 60`
            * `Bat1.available_soc / Bat2.available_soc = 2`

            A request power of 900W will be distributed as follows, for different
            values of `distribution_exponent`.

            | distribution_exponent | Bat1 | Bat2 |
            |-----------------------|------|------|
            | 0                     | 450  | 450  |
            | 1                     | 300  | 600  |
            | 2                     | 180  | 720  |
            | 3                     | 100  | 800  |

            # Example 3

            Let:

            * `Bat1.soc = 44` and `Bat2.soc = 64`.
            * `Bat1.available_soc = 36 (80 - 44)`, `Bat2.available_soc = 16 (80 - 64)`

            A request power of 900W will be distributed as follows, for these values of
            `distribution_exponent`:

            If `distribution_exponent` is:

            | distribution_exponent | Bat1 | Bat2 |
            |-----------------------|------|------|
            | 0                     | 450  | 450  |
            | 0.5                   | 600  | 400  |

        Raises:
            ValueError: If distributor_exponent < 0

        """
        super().__init__()

        if distributor_exponent < 0:
            raise ValueError("Distribution factor should be float >= 0.")
        self._distributor_exponent: float = distributor_exponent

    def _total_capacity(self, components: list[InvBatPair]) -> float:
        """Sum capacity between all batteries in the components list.

        Args:
            components: list of the components

        Raises:
            ValueError: If total capacity is 0.

        Returns:
            Sum of all batteries capacity in the components list.
        """
        total_capacity: float = sum(bat.capacity for bat, _ in components)
        if is_close_to_zero(total_capacity):
            msg = "All batteries have capacity 0."
            _logger.error(msg)
            raise ValueError(msg)

        return total_capacity

    def _compute_battery_availability_ratio(
        self,
        components: list[InvBatPair],
        available_soc: dict[ComponentId, float],
        excl_bounds: dict[ComponentId, Power],
    ) -> tuple[list[AvailabilityRatio], float]:
        r"""Compute battery ratio and the total sum of all of them.

        battery_availability_ratio = capacity_ratio[i] * available_soc[i]
        Where:
        capacity_ratio[i] = components[i].battery.capacity \
            / sum(battery.capacity for battery, _ in components)

        Args:
            components: list of the components
            available_soc: How much SoC remained to reach
                * SoC upper bound - if need to distribute consumption power
                * SoC lower bound - if need to distribute supply power
            excl_bounds: Exclusion bounds for each inverter

        Returns:
            Tuple where first argument is battery availability ratio for each
                battery-inverter pair. The list is sorted by ratio in
                descending order.  The second element of the tuple is total sum
                of all battery ratios in the list.
        """
        total_capacity = self._total_capacity(components)
        battery_availability_ratio: list[AvailabilityRatio] = []
        total_battery_availability_ratio: float = 0.0

        for pair in components:
            battery, inverters = pair
            capacity_ratio = battery.capacity / total_capacity
            soc_factor: float = pow(
                available_soc[battery.component_id], self._distributor_exponent
            )

            ratio = capacity_ratio * soc_factor

            inverter_ids = [inv.component_id for inv in inverters]
            inverter_ids.sort(key=lambda item: (excl_bounds[item], item), reverse=True)

            battery_availability_ratio.append(
                AvailabilityRatio(
                    battery.component_id,
                    inverter_ids,
                    ratio,
                    # Min power we need to request from the pair.
                    # Note that indvidual inverters may have lower min power
                    # and need to be checked individually.
                    min_power=max(
                        excl_bounds[battery.component_id],
                        min(excl_bounds[inverter_id] for inverter_id in inverter_ids),
                    ),
                )
            )

            total_battery_availability_ratio += ratio

        battery_availability_ratio.sort(
            key=lambda item: (item.min_power, item.ratio), reverse=True
        )

        return battery_availability_ratio, total_battery_availability_ratio

    # pylint: disable-next=too-many-arguments,too-many-locals,too-many-branches,too-many-statements
    def _distribute_power(
        self,
        *,
        components: list[InvBatPair],
        power: Power,
        available_soc: dict[ComponentId, float],
        incl_bounds: dict[ComponentId, Power],
        excl_bounds: dict[ComponentId, Power],
    ) -> DistributionResult:
        """Distribute power between given components.

        After this method power should be distributed between batteries
        in a way that equalize SoC between batteries.

        Args:
            components: list of components.
            power: power to distribute
            available_soc: how much SoC remained to reach:
                * SoC upper bound - if need to distribute consumption power
                * SoC lower bound - if need to distribute supply power
            incl_bounds: Inclusion bounds for each inverter
            excl_bounds: Exclusion bounds for each inverter

        Returns:
            Distribution result.
        """
        (
            battery_availability_ratio,
            sum_ratio,
        ) = self._compute_battery_availability_ratio(
            components, available_soc, excl_bounds
        )

        # sum_ratio == 0 means that all batteries are fully charged / discharged
        if is_close_to_zero(sum_ratio):
            final_distribution = {
                inverter.component_id: Power.zero()
                for _, inverters in components
                for inverter in inverters
            }
            return DistributionResult(final_distribution, power)

        # key: inverter_ids, value: _Power(upper_bound, power)
        distribution: dict[_InverterSet, _Power] = {}
        distributed_power: Power = Power.zero()
        reserved_power: Power = Power.zero()
        power_to_distribute: Power = power
        used_ratio: float = 0.0
        ratio = sum_ratio
        excess_reserved: dict[_InverterSet, Power] = {}
        deficits: dict[_InverterSet, Power] = {}

        for ratio_data in battery_availability_ratio:
            inverter_set = _InverterSet(ratio_data.inverter_ids)
            # ratio = 0, means all remaining batteries reach max SoC lvl or have no
            # capacity
            if is_close_to_zero(ratio):
                distribution[inverter_set] = _Power(
                    upper_bound=Power.zero(),
                    power=Power.zero(),
                )
                continue

            power_to_distribute = power - reserved_power
            calculated_power = power_to_distribute * ratio_data.ratio / ratio
            reserved_power += max(calculated_power, ratio_data.min_power)
            used_ratio += ratio_data.ratio
            ratio = sum_ratio - used_ratio

            # If the power allocated for that inverter set is out of bound,
            # then we need to distribute more power over all remaining batteries.
            incl_bound = min(
                sum(
                    (
                        incl_bounds[inverter_id]
                        for inverter_id in ratio_data.inverter_ids
                    ),
                    start=Power.zero(),
                ),
                incl_bounds[ratio_data.battery_id],
            )
            if calculated_power > incl_bound:
                excess_reserved[inverter_set] = incl_bound - ratio_data.min_power
            # # Distribute between remaining batteries
            elif calculated_power < ratio_data.min_power:
                deficits[inverter_set] = calculated_power - ratio_data.min_power
            else:
                excess_reserved[inverter_set] = calculated_power - ratio_data.min_power

            distributed_power += ratio_data.min_power
            distribution[inverter_set] = _Power(
                upper_bound=incl_bound,
                power=ratio_data.min_power,
            )

        for inverter_ids, deficit in deficits.items():
            while not deficit.isclose(Power.zero()) and deficit < Power.zero():
                if not excess_reserved:
                    break
                largest = _Allocation(
                    *max(excess_reserved.items(), key=lambda item: item[1])
                )

                if largest.power.isclose(Power.zero()) or largest.power < Power.zero():
                    break
                if largest.power >= -deficit or largest.power.isclose(-deficit):
                    excess_reserved[largest.inverter_ids] += deficit
                    deficits[inverter_ids] = Power.zero()
                    deficit = Power.zero()
                else:
                    deficit += excess_reserved[largest.inverter_ids]
                    deficits[inverter_ids] = deficit
                    excess_reserved[largest.inverter_ids] = Power.zero()
            if deficit < self._MINIMUM_DEFICIT_TO_CONSIDER:
                left_over = power - distributed_power
                if left_over > -deficit:
                    distributed_power += deficit
                elif left_over > Power.zero():
                    distributed_power += left_over

        for inverter_ids, excess in excess_reserved.items():
            distributed_power += excess
            battery_power = distribution[inverter_ids]
            battery_power.power += excess
            # Add excess power to the inverter set
            distribution[inverter_ids] = battery_power

        left_over = power - distributed_power

        distribution, left_over = self._greedy_distribute_remaining_power(
            distribution, left_over
        )
        inverter_distribution = self._distribute_multi_inverter_pairs(
            distribution, excl_bounds, incl_bounds
        )

        return DistributionResult(
            distribution=inverter_distribution, remaining_power=left_over
        )

    def _distribute_multi_inverter_pairs(
        self,
        distribution: dict[_InverterSet, _Power],
        excl_bounds: dict[ComponentId, Power],
        incl_bounds: dict[ComponentId, Power],
    ) -> dict[ComponentId, Power]:
        """Distribute power between inverters in a set for a single pair.

        Args:
            distribution: distribution with key: inverter_ids, value: (battery_id, power)
            excl_bounds: exclusion bounds for inverters and batteries
            incl_bounds: inclusion bounds for inverters and batteries

        Returns:
            Return the power for each inverter in given distribution.
        """
        new_distribution: dict[ComponentId, Power] = {}

        for inverter_ids, power in distribution.items():
            if len(inverter_ids) == 1:
                inverter_id = next(iter(inverter_ids))
                new_distribution[inverter_id] = power.power
            else:
                remaining_power = power.power

                # Sort inverters to have the largest exclusion bounds first
                sorted_inverter_ids = sorted(
                    inverter_ids, key=lambda inv_id: excl_bounds[inv_id], reverse=True
                )

                for inverter_id in sorted_inverter_ids:
                    if (
                        not remaining_power.isclose(Power.zero())
                        and excl_bounds[inverter_id] <= remaining_power
                    ):
                        new_power = min(incl_bounds[inverter_id], remaining_power)

                        new_distribution[inverter_id] = new_power
                        remaining_power -= new_power
                    else:
                        new_distribution[inverter_id] = Power.zero()

        return new_distribution

    def _greedy_distribute_remaining_power(
        self, distribution: dict[_InverterSet, _Power], remaining_power: Power
    ) -> tuple[dict[_InverterSet, _Power], Power]:
        """Add remaining power greedily to the given distribution.

        Distribution for each inverter will not exceed its upper bound.

        Args:
            distribution: distribution
            remaining_power: power to distribute

        Returns:
            Return the new distribution and remaining power.
        """
        if remaining_power.isclose(Power.zero()):
            return distribution, remaining_power

        for inverter_ids, power in distribution.items():
            # The power.power == 0 means the inverter shall not be used due to
            # SoC bounds or no capacity
            if remaining_power.isclose(Power.zero()) or power.power.isclose(
                Power.zero()
            ):
                distribution[inverter_ids] = power
            else:
                additional_power = min(power.upper_bound - power.power, remaining_power)
                power.power += additional_power
                remaining_power -= additional_power

        return distribution, remaining_power

    def distribute_power_equally(
        self, power: Power, inverters: set[ComponentId]
    ) -> DistributionResult:
        """Distribute the power equally between the inverters in the set.

        This function is mainly useful to set the power for components that are
        broken or have no metrics available.

        Args:
            power: the power to distribute.
            inverters: the inverters to set the power to.

        Returns:
            the power distribution result.
        """
        power_per_inverter = power / len(inverters)
        return DistributionResult(
            distribution={id: power_per_inverter for id in inverters},
            remaining_power=Power.zero(),
        )

    def distribute_power(
        self, power: Power, components: list[InvBatPair]
    ) -> DistributionResult:
        """Distribute given power between given components.

        Args:
            power: Power to distribute
            components: InvBatPaired components data. Each pair should have data
                for battery and adjacent inverter.

        Returns:
            Distribution result
        """
        if power.isclose(Power.zero()):
            return DistributionResult(
                distribution={
                    inverter.component_id: Power.zero()
                    for _, inverters in components
                    for inverter in inverters
                },
                remaining_power=Power.zero(),
            )
        if power > Power.zero():
            return self._distribute_consume_power(power, components)
        return self._distribute_supply_power(power, components)

    def _distribute_consume_power(
        self, power: Power, components: list[InvBatPair]
    ) -> DistributionResult:
        """Distribute power between the given components.

        Distribute power in a way that the SoC level between given components will:
            * stay on the same level, equal in all given components
            * will try to align himself to the same level.

        Args:
            power: power to distribute
            components: list of components between which the power should be
                distributed.

        Returns:
            Distribution result, batteries with no SoC and capacity won't be used.
        """
        # If SoC exceeded bound then remaining SoC should be 0.
        # Otherwise algorithm would try to supply power from that battery
        # in order to keep equal SoC level.
        available_soc: dict[ComponentId, float] = {}
        for battery, _ in components:
            available_soc[battery.component_id] = max(
                0.0, battery.soc_upper_bound - battery.soc
            )

        incl_bounds, excl_bounds = self._inclusion_exclusion_bounds(
            components, supply=False
        )

        return self._distribute_power(
            components=components,
            power=power,
            available_soc=available_soc,
            incl_bounds=incl_bounds,
            excl_bounds=excl_bounds,
        )

    def _distribute_supply_power(
        self, power: Power, components: list[InvBatPair]
    ) -> DistributionResult:
        """Distribute power between the given components.

        Distribute power in a way that the SoC level between given components will:
            * stay on the same level, equal in all given components
            * will try to align himself to the same level.

        Args:
            power: power to distribute
            components: list of components between which the power should be
                distributed.

        Returns:
            Distribution result.
        """
        available_soc: dict[ComponentId, float] = {}
        for battery, _ in components:
            available_soc[battery.component_id] = max(
                0.0, battery.soc - battery.soc_lower_bound
            )

        incl_bounds, excl_bounds = self._inclusion_exclusion_bounds(
            components, supply=True
        )

        result: DistributionResult = self._distribute_power(
            components=components,
            power=-power,
            available_soc=available_soc,
            incl_bounds=incl_bounds,
            excl_bounds=excl_bounds,
        )

        for inverter_id in result.distribution.keys():
            result.distribution[inverter_id] = -result.distribution[inverter_id]
        result.remaining_power = -result.remaining_power

        return result

    def _inclusion_exclusion_bounds(
        self, components: list[InvBatPair], supply: bool = False
    ) -> tuple[dict[ComponentId, Power], dict[ComponentId, Power]]:
        """Calculate inclusion and exclusion bounds for given components.

        Inverter exclusion bounds are _not_ adjusted to battery inclusion
        bounds, as the battery exclusion bounds can be satisfied by multiple
        inverters with lower exclusion bounds.

        Args:
            components: list of components.
            supply: if True then supply bounds will be calculated, otherwise
                consume bounds.

        Returns:
            inclusion and exclusion bounds.
        """
        incl_bounds: dict[ComponentId, Power] = {}
        excl_bounds: dict[ComponentId, Power] = {}
        for battery, inverters in components:
            if supply:
                excl_bounds[battery.component_id] = (
                    -battery.power_bounds.exclusion_lower
                )
                incl_bounds[battery.component_id] = (
                    -battery.power_bounds.inclusion_lower
                )
            else:
                excl_bounds[battery.component_id] = battery.power_bounds.exclusion_upper
                incl_bounds[battery.component_id] = battery.power_bounds.inclusion_upper

            for inverter in inverters:
                if supply:
                    incl_bounds[inverter.component_id] = -max(
                        Power.from_watts(inverter.active_power_inclusion_lower_bound),
                        battery.power_bounds.inclusion_lower,
                    )
                    excl_bounds[inverter.component_id] = Power.from_watts(
                        -inverter.active_power_exclusion_lower_bound
                    )

                else:
                    incl_bounds[inverter.component_id] = min(
                        Power.from_watts(inverter.active_power_inclusion_upper_bound),
                        battery.power_bounds.inclusion_upper,
                    )
                    excl_bounds[inverter.component_id] = Power.from_watts(
                        inverter.active_power_exclusion_upper_bound
                    )
        return incl_bounds, excl_bounds



================================================
FILE: src/frequenz/sdk/microgrid/_power_managing/__init__.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""A power manager implementation."""

from ._base_classes import Algorithm, Proposal, ReportRequest, _Report
from ._power_managing_actor import PowerManagingActor

__all__ = [
    "Algorithm",
    "PowerManagingActor",
    "Proposal",
    "_Report",
    "ReportRequest",
]



================================================
FILE: src/frequenz/sdk/microgrid/_power_managing/_base_classes.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Base classes for the power manager."""

from __future__ import annotations

import abc
import dataclasses
import enum
import typing

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.quantities import Power

from ... import timeseries
from . import _bounds

if typing.TYPE_CHECKING:
    from ...timeseries._base_types import SystemBounds


@dataclasses.dataclass(frozen=True, kw_only=True)
class ReportRequest:
    """A request to start a reporting stream."""

    source_id: str
    """The source ID of the actor sending the request."""

    component_ids: frozenset[ComponentId]
    """The component IDs to report on."""

    priority: int
    """The priority of the actor ."""

    def get_channel_name(self) -> str:
        """Get the channel name for the report request.

        Returns:
            The channel name to use to identify the corresponding report channel
                from the channel registry.
        """
        return f"power_manager.report.{self.component_ids=}.{self.priority=}"


@dataclasses.dataclass(frozen=True, kw_only=True)
class _Report:
    """Current PowerManager report for a set of components."""

    target_power: Power | None
    """The currently set power for the components."""

    _inclusion_bounds: timeseries.Bounds[Power] | None
    """The available inclusion bounds for the components, for the actor's priority.

    These bounds are adjusted to any restrictions placed by actors with higher
    priorities.
    """

    _exclusion_bounds: timeseries.Bounds[Power] | None
    """The exclusion bounds for the components.

    The power manager doesn't manage exclusion bounds, so these are aggregations of
    values reported by the microgrid API.

    These bounds are adjusted to any restrictions placed by actors with higher
    priorities.
    """

    @property
    def bounds(self) -> timeseries.Bounds[Power] | None:
        """The bounds for the components.

        These bounds are adjusted to any restrictions placed by actors with higher
        priorities.

        There might be exclusion zones within these bounds. If necessary, the
        `adjust_to_bounds` method may be used to check if a desired power value fits the
        bounds, or to get the closest possible power values that do fit the bounds.
        """
        return self._inclusion_bounds

    def adjust_to_bounds(self, power: Power) -> tuple[Power | None, Power | None]:
        """Adjust a power value to the bounds.

        This method can be used to adjust a desired power value to the power bounds
        available to the actor.

        If the given power value falls within the usable bounds, it will be returned
        unchanged.

        If it falls outside the usable bounds, the closest possible value on the
        corresponding side will be returned.  For example, if the given power is lower
        than the lowest usable power, only the lowest usable power will be returned, and
        similarly for the highest usable power.

        If the given power falls within an exclusion zone that's contained within the
        usable bounds, the closest possible power values on both sides will be returned.

        !!! note
            It is completely optional to use this method to adjust power values before
            proposing them, because the PowerManager will do this automatically.  This
            method is provided for convenience, and for granular control when there are
            two possible power values, both of which fall within the available bounds.

        Args:
            power: The power value to adjust.

        Returns:
            A tuple of the closest power values to the desired power that fall within
                the available bounds for the actor.
        """
        if self._inclusion_bounds is None:
            return None, None

        return _bounds.clamp_to_bounds(
            power,
            self._inclusion_bounds.lower,
            self._inclusion_bounds.upper,
            self._exclusion_bounds,
        )


@dataclasses.dataclass(frozen=True, kw_only=True)
class Proposal:
    """A proposal for a set of components to be charged or discharged."""

    source_id: str
    """The source ID of the actor sending the request."""

    preferred_power: Power | None
    """The preferred power to be distributed to the components.

    If `None`, the preferred power of higher priority actors will get precedence.
    """

    bounds: timeseries.Bounds[Power | None]
    """The power bounds for the proposal.

    These bounds will apply to actors with a lower priority, and can be overridden by
    bounds from actors with a higher priority.  If None, the power bounds will be set to
    the maximum power of the components in the pool.  This is currently an experimental
    feature.
    """

    component_ids: frozenset[ComponentId]
    """The component IDs to distribute the power to."""

    priority: int
    """The priority of the actor sending the proposal."""

    creation_time: float
    """The loop time when the proposal is created.

    This is used by the power manager to determine the age of the proposal.
    """

    def __lt__(self, other: Proposal) -> bool:
        """Compare two proposals by their priority.

        When they have the same priority, compare them by their source ID.

        Args:
            other: The other proposal to compare to.

        Returns:
            Whether this proposal has a higher priority than the other proposal.
        """
        return (self.priority < other.priority) or (
            self.priority == other.priority and self.source_id < other.source_id
        )

    def __eq__(self, other: object) -> bool:
        """Check if two proposals are equal.

        Equality is determined by the priority and source ID of the proposals, so
        two proposals are equal if they have the same priority and source ID, even
        if they have different power values or creation times.

        This is so that there is only one active proposal for each actor in the bucket,
        and older proposals are replaced by newer ones.

        Args:
            other: The other proposal to compare to.

        Returns:
            Whether the two proposals are equal.
        """
        if not isinstance(other, Proposal):
            return NotImplemented

        return self.priority == other.priority and self.source_id == other.source_id

    def __hash__(self) -> int:
        """Get the hash of the proposal.

        Returns:
            The hash of the proposal.
        """
        return hash((self.priority, self.source_id))


class DefaultPower(enum.Enum):
    """The default power for a component category."""

    ZERO = "zero"
    """The default power is 0 W."""

    MIN = "min"
    """The default power is the minimum power of the component."""

    MAX = "max"
    """The default power is the maximum power of the component."""


class Algorithm(enum.Enum):
    """The available algorithms for the power manager."""

    MATRYOSHKA = "matryoshka"
    SHIFTING_MATRYOSHKA = "shifting_matryoshka"


class BaseAlgorithm(abc.ABC):
    """The base class for algorithms."""

    @abc.abstractmethod
    def calculate_target_power(
        self,
        component_ids: frozenset[ComponentId],
        proposal: Proposal | None,
        system_bounds: SystemBounds,
    ) -> Power | None:
        """Calculate and return the target power for the given components.

        Args:
            component_ids: The component IDs to calculate the target power for.
            proposal: If given, the proposal to added to the bucket, before the target
                power is calculated.
            system_bounds: The system bounds for the components in the proposal.

        Returns:
            The new target power for the components, or `None` if the target power
                couldn't be calculated.
        """

    # The arguments for this method are tightly coupled to the `Matryoshka` algorithm.
    # It can be loosened up when more algorithms are added.
    @abc.abstractmethod
    def get_status(
        self,
        component_ids: frozenset[ComponentId],
        priority: int,
        system_bounds: SystemBounds,
    ) -> _Report:
        """Get the bounds for a set of components, for the given priority.

        Args:
            component_ids: The IDs of the components to get the bounds for.
            priority: The priority of the actor for which the bounds are requested.
            system_bounds: The system bounds for the components.

        Returns:
            The bounds for the components.
        """

    @abc.abstractmethod
    def drop_old_proposals(self, loop_time: float) -> None:
        """Drop old proposals.

        This method is called periodically by the power manager.

        Args:
            loop_time: The current loop time.
        """



================================================
FILE: src/frequenz/sdk/microgrid/_power_managing/_bounds.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Utilities for checking and clamping bounds and power values to exclusion bounds."""

from frequenz.quantities import Power

from ...timeseries import Bounds


def check_exclusion_bounds_overlap(
    lower_bound: Power,
    upper_bound: Power,
    exclusion_bounds: Bounds[Power] | None,
) -> tuple[bool, bool]:
    """Check if the given bounds overlap with the given exclusion bounds.

    Example:

        ```
                       lower                        upper
                          .----- exclusion zone -----.
        -----|✓✓✓✓✓✓✓✓✓✓✓✓|xxxxxxxxxxxxxxx|----------|----
             `-- usable --'-- exclusion --´
             |                 overlap    |
             |                            |
           lower                        upper
           bound                        bound
                              (inside the exclusion zone)
        ```

        Resulting in `(False, True)` because only the upper bound is inside the
        exclusion zone.

    Args:
        lower_bound: The lower bound to check.
        upper_bound: The upper bound to check.
        exclusion_bounds: The exclusion bounds to check against.

    Returns:
        A tuple containing a boolean indicating if the lower bound is bounded by the
            exclusion bounds, and a boolean indicating if the upper bound is bounded by
            the exclusion bounds.
    """
    if exclusion_bounds is None:
        return False, False

    bounded_lower = False
    bounded_upper = False

    if exclusion_bounds.lower < lower_bound < exclusion_bounds.upper:
        bounded_lower = True
    if exclusion_bounds.lower < upper_bound < exclusion_bounds.upper:
        bounded_upper = True

    return bounded_lower, bounded_upper


def adjust_exclusion_bounds(
    lower_bound: Power,
    upper_bound: Power,
    exclusion_bounds: Bounds[Power] | None,
) -> tuple[Power, Power]:
    """Adjust the given bounds to exclude the given exclusion bounds.

    Args:
        lower_bound: The lower bound to adjust.
        upper_bound: The upper bound to adjust.
        exclusion_bounds: The exclusion bounds to adjust to.

    Returns:
        The adjusted lower and upper bounds.
    """
    if exclusion_bounds is None:
        return lower_bound, upper_bound

    # If the given bounds are within the exclusion bounds, there's no room to adjust,
    # so return zero.
    #
    # And if the given bounds overlap with the exclusion bounds on one side, then clamp
    # the given bounds on that side.
    match check_exclusion_bounds_overlap(lower_bound, upper_bound, exclusion_bounds):
        case (True, True):
            return Power.zero(), Power.zero()
        case (False, True):
            return lower_bound, exclusion_bounds.lower
        case (True, False):
            return exclusion_bounds.upper, upper_bound
    return lower_bound, upper_bound


# Just 20 lines of code in this function, but unfortunately 8 of those are return
# statements, and that's too many for pylint.
def clamp_to_bounds(  # pylint: disable=too-many-return-statements
    value: Power,
    lower_bound: Power,
    upper_bound: Power,
    exclusion_bounds: Bounds[Power] | None,
) -> tuple[Power | None, Power | None]:
    """Clamp the given value to the given bounds.

    When the given value can falls within the exclusion zone, and can be clamped to
    both sides, both options will be returned.

    When the given value falls outside the usable bounds and can be clamped only to
    one side, only that option will be returned.

    Args:
        value: The value to clamp.
        lower_bound: The lower bound to clamp to.
        upper_bound: The upper bound to clamp to.
        exclusion_bounds: The exclusion bounds to clamp outside of.

    Returns:
        The clamped value.
    """
    # If the given bounds are within the exclusion bounds, return zero.
    #
    # And if the given bounds overlap with the exclusion bounds on one side, and the
    # given power is in that overlap region, clamp it to the exclusion bounds on that
    # side.
    if exclusion_bounds is not None:
        match check_exclusion_bounds_overlap(
            lower_bound, upper_bound, exclusion_bounds
        ):
            case (True, True):
                return None, None
            case (True, False):
                if value < exclusion_bounds.upper:
                    return None, exclusion_bounds.upper
            case (False, True):
                if value > exclusion_bounds.lower:
                    return exclusion_bounds.lower, None

    # If the given value is outside the given bounds, clamp it to the closest bound.
    if value < lower_bound:
        return lower_bound, None
    if value > upper_bound:
        return None, upper_bound

    # If the given value is within the exclusion bounds and the exclusion bounds are
    # within the given bounds, clamp the given value to the closest exclusion bound.
    if exclusion_bounds is not None and not value.isclose(Power.zero()):
        if exclusion_bounds.lower < value < exclusion_bounds.upper:
            return exclusion_bounds.lower, exclusion_bounds.upper

    return value, value



================================================
FILE: src/frequenz/sdk/microgrid/_power_managing/_matryoshka.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""A power manager implementation that uses the matryoshka algorithm.

When there are multiple proposals from different actors for the same set of components,
the matryoshka algorithm will consider the priority of the actors, the bounds they set
and their preferred power to determine the target power for the components.

The preferred power of lower priority actors will take precedence as long as they
respect the bounds set by higher priority actors.  If lower priority actors request
power values outside the bounds set by higher priority actors, the target power will
be the closest value to the preferred power that is within the bounds.

When there is only a single proposal for a set of components, its preferred power would
be the target power, as long as it falls within the system power bounds for the
components.
"""

from __future__ import annotations

import logging
import typing
from datetime import timedelta

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.quantities import Power
from typing_extensions import override

from ... import timeseries
from . import _bounds
from ._base_classes import BaseAlgorithm, DefaultPower, Proposal, _Report

if typing.TYPE_CHECKING:
    from ...timeseries._base_types import SystemBounds

_logger = logging.getLogger(__name__)


class Matryoshka(BaseAlgorithm):
    """The matryoshka algorithm."""

    def __init__(
        self, max_proposal_age: timedelta, default_power: DefaultPower
    ) -> None:
        """Create a new instance of the matryoshka algorithm."""
        self._max_proposal_age_sec = max_proposal_age.total_seconds()
        self._default_power = default_power
        self._component_buckets: dict[frozenset[ComponentId], set[Proposal]] = {}
        self._target_power: dict[frozenset[ComponentId], Power] = {}

    def _calc_target_power(
        self,
        proposals: set[Proposal],
        system_bounds: SystemBounds,
    ) -> Power | None:
        """Calculate the target power for the given components.

        Args:
            proposals: The proposals for the given components.
            system_bounds: The system bounds for the components in the proposal.

        Returns:
            The new target power for the components.
        """
        lower_bound = (
            system_bounds.inclusion_bounds.lower
            if system_bounds.inclusion_bounds
            # if a target power exists from a previous proposal, and the system bounds
            # have become unavailable, force the target power to be zero, by narrowing
            # the bounds to zero.
            else Power.zero()
        )
        upper_bound = (
            system_bounds.inclusion_bounds.upper
            if system_bounds.inclusion_bounds
            else Power.zero()
        )

        exclusion_bounds = None
        if system_bounds.exclusion_bounds is not None and (
            system_bounds.exclusion_bounds.lower != Power.zero()
            or system_bounds.exclusion_bounds.upper != Power.zero()
        ):
            exclusion_bounds = system_bounds.exclusion_bounds

        target_power = None
        for next_proposal in sorted(proposals, reverse=True):
            if upper_bound < lower_bound:
                break
            if next_proposal.preferred_power:
                match _bounds.clamp_to_bounds(
                    next_proposal.preferred_power,
                    lower_bound,
                    upper_bound,
                    exclusion_bounds,
                ):
                    case (None, power) | (power, None) if power:
                        target_power = power
                    case (power_low, power_high) if power_low and power_high:
                        if (
                            power_high - next_proposal.preferred_power
                            < next_proposal.preferred_power - power_low
                        ):
                            target_power = power_high
                        else:
                            target_power = power_low

            proposal_lower = next_proposal.bounds.lower or lower_bound
            proposal_upper = next_proposal.bounds.upper or upper_bound
            # If the bounds from the current proposal are fully within the exclusion
            # bounds, then don't use them to narrow the bounds further. This allows
            # subsequent proposals to not be blocked by the current proposal.
            match _bounds.check_exclusion_bounds_overlap(
                proposal_lower, proposal_upper, exclusion_bounds
            ):
                case (True, True):
                    continue
            lower_bound = max(lower_bound, proposal_lower)
            upper_bound = min(upper_bound, proposal_upper)
            lower_bound, upper_bound = _bounds.adjust_exclusion_bounds(
                lower_bound, upper_bound, exclusion_bounds
            )

        return target_power

    def _validate_component_ids(
        self,
        component_ids: frozenset[ComponentId],
        proposal: Proposal | None,
        system_bounds: SystemBounds,
    ) -> bool:
        if component_ids not in self._component_buckets:
            # if there are no previous proposals and there are no system bounds, then
            # don't calculate a target power and fail the validation.
            if (
                system_bounds.inclusion_bounds is None
                and system_bounds.exclusion_bounds is None
            ):
                if proposal is not None:
                    _logger.warning(
                        "PowerManagingActor: No system bounds available for component "
                        "IDs %s, but a proposal was given.  The proposal will be "
                        "ignored.",
                        component_ids,
                    )
                return False

            for bucket in self._component_buckets:
                if any(component_id in bucket for component_id in component_ids):
                    comp_ids = ", ".join(map(str, sorted(component_ids)))
                    raise NotImplementedError(
                        f"PowerManagingActor: {comp_ids} are already part of another "
                        "bucket. Overlapping buckets are not yet supported."
                    )
        return True

    @override
    def calculate_target_power(
        self,
        component_ids: frozenset[ComponentId],
        proposal: Proposal | None,
        system_bounds: SystemBounds,
    ) -> Power | None:
        """Calculate and return the target power for the given components.

        Args:
            component_ids: The component IDs to calculate the target power for.
            proposal: If given, the proposal to added to the bucket, before the target
                power is calculated.
            system_bounds: The system bounds for the components in the proposal.

        Returns:
            The new target power for the components, or `None` if the target power
                couldn't be calculated.

        Raises:  # noqa: DOC502
            NotImplementedError: When the proposal contains component IDs that are
                already part of another bucket.
        """
        if not self._validate_component_ids(component_ids, proposal, system_bounds):
            return None

        if proposal is not None:
            bucket = self._component_buckets.setdefault(component_ids, set())
            if proposal in bucket:
                bucket.remove(proposal)
            if (
                proposal.preferred_power is not None
                or proposal.bounds.lower is not None
                or proposal.bounds.upper is not None
            ):
                bucket.add(proposal)
            elif not bucket:
                del self._component_buckets[component_ids]

        # If there has not been any proposal for the given components, don't calculate a
        # target power and just return `None`.
        proposals = self._component_buckets.get(component_ids)

        target_power = None
        if proposals is not None:
            target_power = self._calc_target_power(proposals, system_bounds)

        if target_power is not None:
            self._target_power[component_ids] = target_power
        elif self._target_power.get(component_ids) is not None:
            # If the target power was previously set, but is now `None`, then we send
            # the default power of the component category, to reset it immediately.
            del self._target_power[component_ids]
            bounds = system_bounds.inclusion_bounds
            if bounds is None:
                return None
            match self._default_power:
                case DefaultPower.MIN:
                    return bounds.lower
                case DefaultPower.MAX:
                    return bounds.upper
                case DefaultPower.ZERO:
                    return Power.zero()
                case other:
                    typing.assert_never(other)

        return target_power

    @override
    def get_status(
        self,
        component_ids: frozenset[ComponentId],
        priority: int,
        system_bounds: SystemBounds,
    ) -> _Report:
        """Get the bounds for the algorithm.

        Args:
            component_ids: The IDs of the components to get the bounds for.
            priority: The priority of the actor for which the bounds are requested.
            system_bounds: The system bounds for the components.

        Returns:
            The target power and the available bounds for the given components, for
                the given priority.
        """
        target_power = self._target_power.get(component_ids)
        if system_bounds.inclusion_bounds is None:
            return _Report(
                target_power=target_power,
                _inclusion_bounds=None,
                _exclusion_bounds=system_bounds.exclusion_bounds,
            )

        lower_bound = system_bounds.inclusion_bounds.lower
        upper_bound = system_bounds.inclusion_bounds.upper

        exclusion_bounds = None
        if system_bounds.exclusion_bounds is not None and (
            system_bounds.exclusion_bounds.lower != Power.zero()
            or system_bounds.exclusion_bounds.upper != Power.zero()
        ):
            exclusion_bounds = system_bounds.exclusion_bounds

        for next_proposal in sorted(
            self._component_buckets.get(component_ids, []), reverse=True
        ):
            if next_proposal.priority <= priority:
                break
            proposal_lower = next_proposal.bounds.lower or lower_bound
            proposal_upper = next_proposal.bounds.upper or upper_bound
            match _bounds.check_exclusion_bounds_overlap(
                proposal_lower, proposal_upper, exclusion_bounds
            ):
                case (True, True):
                    continue
            calc_lower_bound = max(lower_bound, proposal_lower)
            calc_upper_bound = min(upper_bound, proposal_upper)
            if calc_lower_bound <= calc_upper_bound:
                lower_bound, upper_bound = _bounds.adjust_exclusion_bounds(
                    calc_lower_bound, calc_upper_bound, exclusion_bounds
                )
            else:
                break
        return _Report(
            target_power=target_power,
            _inclusion_bounds=timeseries.Bounds[Power](
                lower=lower_bound, upper=upper_bound
            ),
            _exclusion_bounds=system_bounds.exclusion_bounds,
        )

    @override
    def drop_old_proposals(self, loop_time: float) -> None:
        """Drop old proposals.

        This will remove all proposals that have not been updated for longer than
        `max_proposal_age`.

        Args:
            loop_time: The current loop time.
        """
        buckets_to_delete: list[frozenset[ComponentId]] = []
        for component_ids, proposals in self._component_buckets.items():
            to_delete: list[Proposal] = []
            for proposal in proposals:
                if (loop_time - proposal.creation_time) > self._max_proposal_age_sec:
                    to_delete.append(proposal)
            for proposal in to_delete:
                proposals.remove(proposal)
            if not proposals:
                buckets_to_delete.append(component_ids)

        for component_ids in buckets_to_delete:
            del self._component_buckets[component_ids]
            _ = self._target_power.pop(component_ids, None)



================================================
FILE: src/frequenz/sdk/microgrid/_power_managing/_power_managing_actor.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""The power manager."""

from __future__ import annotations

import asyncio
import logging
import sys
from datetime import datetime, timedelta, timezone
from typing import assert_never

from frequenz.channels import Receiver, Sender, select, selected_from
from frequenz.channels.timer import SkipMissedAndDrift, Timer
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentCategory, ComponentType, InverterType
from typing_extensions import override

from ..._internal._asyncio import run_forever
from ..._internal._channels import ChannelRegistry
from ...actor import Actor
from ...timeseries._base_types import SystemBounds
from .. import _data_pipeline, _power_distributing
from ._base_classes import (
    Algorithm,
    BaseAlgorithm,
    DefaultPower,
    Proposal,
    ReportRequest,
    _Report,
)
from ._matryoshka import Matryoshka
from ._shifting_matryoshka import ShiftingMatryoshka

_logger = logging.getLogger(__name__)


class PowerManagingActor(Actor):
    """The power manager."""

    def __init__(  # pylint: disable=too-many-arguments
        self,
        *,
        proposals_receiver: Receiver[Proposal],
        bounds_subscription_receiver: Receiver[ReportRequest],
        power_distributing_requests_sender: Sender[_power_distributing.Request],
        power_distributing_results_receiver: Receiver[_power_distributing.Result],
        channel_registry: ChannelRegistry,
        algorithm: Algorithm,
        default_power: DefaultPower,
        component_category: ComponentCategory,
        component_type: ComponentType | None = None,
    ):
        """Create a new instance of the power manager.

        Args:
            proposals_receiver: The receiver for proposals.
            bounds_subscription_receiver: The receiver for bounds subscriptions.
            power_distributing_requests_sender: The sender for power distribution
                requests.
            power_distributing_results_receiver: The receiver for power distribution
                results.
            channel_registry: The channel registry.
            algorithm: The power management algorithm to use.
            default_power: The default power to use for the components.
            component_category: The category of the component this power manager
                instance is going to support.
            component_type: The type of the component of the given category that this
                actor is responsible for.  This is used only when the component category
                is not enough to uniquely identify the component.  For example, when the
                category is `ComponentCategory.INVERTER`, the type is needed to identify
                the inverter as a solar inverter or a battery inverter.  This can be
                `None` when the component category is enough to uniquely identify the
                component.
        """
        self._component_category = component_category
        self._component_type = component_type
        self._default_power = default_power
        self._bounds_subscription_receiver = bounds_subscription_receiver
        self._power_distributing_requests_sender = power_distributing_requests_sender
        self._power_distributing_results_receiver = power_distributing_results_receiver
        self._channel_registry = channel_registry
        self._proposals_receiver = proposals_receiver

        self._system_bounds: dict[frozenset[ComponentId], SystemBounds] = {}
        self._bound_tracker_tasks: dict[frozenset[ComponentId], asyncio.Task[None]] = {}
        self._subscriptions: dict[
            frozenset[ComponentId], dict[int, Sender[_Report]]
        ] = {}

        match algorithm:
            case Algorithm.MATRYOSHKA:
                self._algorithm: BaseAlgorithm = Matryoshka(
                    max_proposal_age=timedelta(seconds=60.0),
                    default_power=default_power,
                )
            case Algorithm.SHIFTING_MATRYOSHKA:
                self._algorithm = ShiftingMatryoshka(
                    max_proposal_age=timedelta(seconds=60.0),
                    default_power=default_power,
                )
            case _:
                assert_never(algorithm)

        super().__init__()

    async def _send_reports(self, component_ids: frozenset[ComponentId]) -> None:
        """Send reports for a set of components, to all subscribers.

        Args:
            component_ids: The component IDs for which a collective report should be
                sent.
        """
        bounds = self._system_bounds.get(component_ids)
        if bounds is None:
            _logger.warning("PowerManagingActor: No bounds for %s", component_ids)
            return
        for priority, sender in self._subscriptions.get(component_ids, {}).items():
            status = self._algorithm.get_status(
                component_ids,
                priority,
                bounds,
            )
            await sender.send(status)

    async def _bounds_tracker(
        self,
        component_ids: frozenset[ComponentId],
        bounds_receiver: Receiver[SystemBounds],
    ) -> None:
        """Track the power bounds of a set of components and update the cache.

        Args:
            component_ids: The component IDs for which this task should track the
                collective bounds of.
            bounds_receiver: The receiver for power bounds.
        """
        last_bounds: SystemBounds | None = None
        async for bounds in bounds_receiver:
            if (
                last_bounds is not None
                and bounds.inclusion_bounds == last_bounds.inclusion_bounds
            ):
                continue
            last_bounds = bounds
            self._system_bounds[component_ids] = bounds
            await self._send_updated_target_power(component_ids, None)
            await self._send_reports(component_ids)

    def _add_system_bounds_tracker(self, component_ids: frozenset[ComponentId]) -> None:
        """Add a system bounds tracker for the given components.

        Args:
            component_ids: The component IDs for which to add a bounds tracker.

        Raises:
            NotImplementedError: When the pool type is not supported.
        """
        bounds_receiver: Receiver[SystemBounds]
        if self._component_category is ComponentCategory.BATTERY:
            battery_pool = _data_pipeline.new_battery_pool(
                priority=-sys.maxsize - 1, component_ids=component_ids
            )
            # pylint: disable-next=protected-access
            bounds_receiver = battery_pool._system_power_bounds.new_receiver()
        elif self._component_category is ComponentCategory.EV_CHARGER:
            ev_charger_pool = _data_pipeline.new_ev_charger_pool(
                priority=-sys.maxsize - 1, component_ids=component_ids
            )
            # pylint: disable-next=protected-access
            bounds_receiver = ev_charger_pool._system_power_bounds.new_receiver()
        elif (
            self._component_category is ComponentCategory.INVERTER
            and self._component_type is InverterType.SOLAR
        ):
            pv_pool = _data_pipeline.new_pv_pool(
                priority=-sys.maxsize - 1, component_ids=component_ids
            )
            # pylint: disable-next=protected-access
            bounds_receiver = pv_pool._system_power_bounds.new_receiver()
        else:
            err = (
                "PowerManagingActor: Unsupported component category: "
                f"{self._component_category}"
            )
            _logger.error(err)
            raise NotImplementedError(err)

        self._system_bounds[component_ids] = SystemBounds(
            timestamp=datetime.now(tz=timezone.utc),
            inclusion_bounds=None,
            exclusion_bounds=None,
        )

        # Start the bounds tracker, for ongoing updates.
        self._bound_tracker_tasks[component_ids] = asyncio.create_task(
            run_forever(lambda: self._bounds_tracker(component_ids, bounds_receiver))
        )

    async def _send_updated_target_power(
        self,
        component_ids: frozenset[ComponentId],
        proposal: Proposal | None,
    ) -> None:
        target_power = self._algorithm.calculate_target_power(
            component_ids,
            proposal,
            self._system_bounds[component_ids],
        )
        if target_power is not None:
            await self._power_distributing_requests_sender.send(
                _power_distributing.Request(
                    power=target_power,
                    component_ids=component_ids,
                    adjust_power=True,
                )
            )

    @override
    async def _run(self) -> None:
        """Run the power managing actor."""
        last_result_partial_failure = False
        drop_old_proposals_timer = Timer(timedelta(seconds=1.0), SkipMissedAndDrift())
        async for selected in select(
            self._proposals_receiver,
            self._bounds_subscription_receiver,
            self._power_distributing_results_receiver,
            drop_old_proposals_timer,
        ):
            if selected_from(selected, self._proposals_receiver):
                proposal = selected.message
                if proposal.component_ids not in self._bound_tracker_tasks:
                    self._add_system_bounds_tracker(proposal.component_ids)

                # TODO: must_send=True forces a new request to # pylint: disable=fixme
                # be sent to the PowerDistributor, even if there's no change in power.
                #
                # This is needed because requests would expire in the microgrid service
                # otherwise.
                #
                # This can be removed as soon as
                # https://github.com/frequenz-floss/frequenz-sdk-python/issues/293 is
                # implemented.
                await self._send_updated_target_power(proposal.component_ids, proposal)
                await self._send_reports(proposal.component_ids)

            elif selected_from(selected, self._bounds_subscription_receiver):
                sub = selected.message
                component_ids = sub.component_ids
                priority = sub.priority

                if component_ids not in self._subscriptions:
                    self._subscriptions[component_ids] = {
                        priority: self._channel_registry.get_or_create(
                            _Report, sub.get_channel_name()
                        ).new_sender()
                    }
                elif priority not in self._subscriptions[component_ids]:
                    self._subscriptions[component_ids][priority] = (
                        self._channel_registry.get_or_create(
                            _Report, sub.get_channel_name()
                        ).new_sender()
                    )

                if sub.component_ids not in self._bound_tracker_tasks:
                    self._add_system_bounds_tracker(sub.component_ids)

            elif selected_from(selected, self._power_distributing_results_receiver):
                result = selected.message
                if not isinstance(result, _power_distributing.Success):
                    _logger.warning(
                        "PowerManagingActor: PowerDistributing failed: %s", result
                    )
                match result:
                    case _power_distributing.PartialFailure(request):
                        if not last_result_partial_failure:
                            last_result_partial_failure = True
                            await self._send_updated_target_power(
                                frozenset(request.component_ids), None
                            )
                    case _power_distributing.Success():
                        last_result_partial_failure = False
                await self._send_reports(frozenset(result.request.component_ids))

            elif selected_from(selected, drop_old_proposals_timer):
                self._algorithm.drop_old_proposals(asyncio.get_event_loop().time())



================================================
FILE: src/frequenz/sdk/microgrid/_power_managing/_shifting_matryoshka.py
================================================
# License: MIT
# Copyright © 2025 Frequenz Energy-as-a-Service GmbH

"""A power manager implementation that uses the shifting matryoshka algorithm."""

from __future__ import annotations

import logging
import typing
from datetime import timedelta

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.quantities import Power
from typing_extensions import override

from frequenz.sdk.timeseries._base_types import Bounds

from ... import timeseries
from . import _bounds
from ._base_classes import BaseAlgorithm, DefaultPower, Proposal, _Report

if typing.TYPE_CHECKING:
    from ...timeseries._base_types import SystemBounds

_logger = logging.getLogger(__name__)


def _get_nearest_possible_power(
    power: Power,
    lower_bound: Power,
    upper_bound: Power,
    exclusion_bounds: Bounds[Power] | None,
) -> Power:
    match _bounds.clamp_to_bounds(
        power,
        lower_bound,
        upper_bound,
        exclusion_bounds,
    ):
        case (None, p) | (p, None) if p:
            return p
        case (low, high) if low and high:
            if high - power < power - low:
                return high
            return low
        case _:
            return Power.zero()


class ShiftingMatryoshka(BaseAlgorithm):
    """The ShiftingMatryoshka algorithm.

    When there are multiple actors trying to control the same set of components, this
    algorithm will reconcile the different proposals and calculate the target power.

    Details about the algorithm can be found in the [microgrid module documentation](https://frequenz-floss.github.io/frequenz-sdk-python/v1.0-dev/user-guide/microgrid-concepts/#frequenz.sdk.microgrid--setting-power).
    """  # noqa: E501 (line too long)

    def __init__(
        self,
        max_proposal_age: timedelta,
        default_power: DefaultPower,
    ) -> None:
        """Create a new instance of the matryoshka algorithm."""
        self._default_power = default_power
        self._max_proposal_age_sec = max_proposal_age.total_seconds()
        self._component_buckets: dict[frozenset[ComponentId], set[Proposal]] = {}
        self._target_power: dict[frozenset[ComponentId], Power] = {}

    def _calc_targets(
        self,
        component_ids: frozenset[ComponentId],
        system_bounds: SystemBounds,
        priority: int | None = None,
    ) -> tuple[Power | None, Bounds[Power]]:
        """Calculate the target power and bounds for the given components.

        Args:
            component_ids: The component IDs to calculate the target power for.
            system_bounds: The system bounds for the components in the proposal.
            priority: The priority of the actor for which the target power is calculated.

        Returns:
            The new target power and bounds for the components.
        """
        proposals = self._component_buckets.get(component_ids, set())
        lower_bound = (
            system_bounds.inclusion_bounds.lower
            if system_bounds.inclusion_bounds
            # if a target power exists from a previous proposal, and the system bounds
            # have become unavailable, force the target power to be zero, by narrowing
            # the bounds to zero.
            else Power.zero()
        )
        upper_bound = (
            system_bounds.inclusion_bounds.upper
            if system_bounds.inclusion_bounds
            else Power.zero()
        )

        if not proposals:
            return None, Bounds[Power](lower=lower_bound, upper=upper_bound)

        available_bounds = Bounds[Power](lower=lower_bound, upper=upper_bound)
        top_pri_bounds: Bounds[Power] | None = None

        target_power = Power.zero()

        allocations: dict[str, str] = {}

        for next_proposal in sorted(proposals, reverse=True):
            # if a priority is given, the bounds calculated until that priority is
            # reached will be the bounds available to an actor with the given priority.
            #
            # This could mean that the calculated target power is incorrect and should
            # not be used.
            if priority is not None and next_proposal.priority <= priority:
                break

            # When the upper bound is less than the lower bound, if means that there's
            # no more room to process further proposals, so we break out of the loop.
            if upper_bound < lower_bound:
                break

            proposal_lower = next_proposal.bounds.lower or lower_bound
            proposal_upper = next_proposal.bounds.upper or upper_bound
            proposal_power = next_proposal.preferred_power

            # Make sure that if the proposal specified bounds, they make sense.
            if proposal_upper < proposal_lower:
                continue

            # If the proposal bounds are outside the available bounds, we need to
            # adjust the proposal bounds to fit within the available bounds.
            if proposal_lower >= upper_bound:
                proposal_lower = upper_bound
                proposal_upper = upper_bound
            elif proposal_upper <= lower_bound:
                proposal_lower = lower_bound
                proposal_upper = lower_bound

            # Clamp the available bounds by the proposal bounds.
            lower_bound = max(lower_bound, proposal_lower)
            upper_bound = min(upper_bound, proposal_upper)

            if proposal_power is not None:
                # If this is the first power setting proposal, then hold on to the
                # bounds that were available at that time, for use when applying the
                # exclusion bounds to the target power at the end.
                if top_pri_bounds is None and proposal_power != Power.zero():
                    top_pri_bounds = Bounds[Power](lower=lower_bound, upper=upper_bound)
                # Clamp the proposal power to its available bounds.
                proposal_power = _get_nearest_possible_power(
                    proposal_power,
                    lower_bound,
                    upper_bound,
                    None,
                )
                # Shift the available bounds by the proposal power.
                lower_bound = lower_bound - proposal_power
                upper_bound = upper_bound - proposal_power
                # Add the proposal power to the target power (aka shift in the opposite direction).
                target_power += proposal_power

                allocations[next_proposal.source_id] = str(proposal_power)

        # The `top_pri_bounds` is to ensure that when applying the exclusion bounds to
        # the target power at the end, we respect the bounds that were set by the first
        # power-proposing actor.
        if top_pri_bounds is not None:
            available_bounds = top_pri_bounds

        # Apply the exclusion bounds to the target power.
        target_power = _get_nearest_possible_power(
            target_power,
            available_bounds.lower,
            available_bounds.upper,
            system_bounds.exclusion_bounds,
        )

        if allocations:
            _logger.info(
                "PowerManager allocations for component IDs: %s: %s",
                sorted(component_ids),
                allocations,
            )

        return target_power, Bounds[Power](lower=lower_bound, upper=upper_bound)

    def _validate_component_ids(
        self,
        component_ids: frozenset[ComponentId],
        proposal: Proposal | None,
        system_bounds: SystemBounds,
    ) -> bool:
        if component_ids not in self._component_buckets:
            # if there are no previous proposals and there are no system bounds, then
            # don't calculate a target power and fail the validation.
            if (
                system_bounds.inclusion_bounds is None
                and system_bounds.exclusion_bounds is None
            ):
                if proposal is not None:
                    _logger.warning(
                        "PowerManagingActor: No system bounds available for component "
                        + "IDs %s, but a proposal was given.  The proposal will be "
                        + "ignored.",
                        component_ids,
                    )
                return False

            for bucket in self._component_buckets:
                if any(component_id in bucket for component_id in component_ids):
                    comp_ids = ", ".join(map(str, sorted(component_ids)))
                    raise NotImplementedError(
                        f"PowerManagingActor: {comp_ids} are already part of another "
                        + "bucket.  Overlapping buckets are not yet supported."
                    )
        return True

    @override
    def calculate_target_power(
        self,
        component_ids: frozenset[ComponentId],
        proposal: Proposal | None,
        system_bounds: SystemBounds,
    ) -> Power | None:
        """Calculate and return the target power for the given components.

        Args:
            component_ids: The component IDs to calculate the target power for.
            proposal: If given, the proposal to added to the bucket, before the target
                power is calculated.
            system_bounds: The system bounds for the components in the proposal.

        Returns:
            The new target power for the components, or `None` if the target power
                couldn't be calculated.

        Raises:  # noqa: DOC502
            NotImplementedError: When the proposal contains component IDs that are
                already part of another bucket.
        """
        if not self._validate_component_ids(component_ids, proposal, system_bounds):
            return None

        if proposal is not None:
            bucket = self._component_buckets.setdefault(component_ids, set())
            if proposal in bucket:
                bucket.remove(proposal)
            if (
                proposal.preferred_power is not None
                or proposal.bounds.lower is not None
                or proposal.bounds.upper is not None
            ):
                bucket.add(proposal)
            elif not bucket:
                del self._component_buckets[component_ids]

        target_power, _ = self._calc_targets(component_ids, system_bounds)

        if target_power is not None:
            self._target_power[component_ids] = target_power
        elif self._target_power.get(component_ids) is not None:
            # If the target power was previously set, but is now `None`, then we send
            # the default power of the component category, to reset it immediately.
            del self._target_power[component_ids]
            bounds = system_bounds.inclusion_bounds
            if bounds is None:
                return None
            match self._default_power:
                case DefaultPower.MIN:
                    return bounds.lower
                case DefaultPower.MAX:
                    return bounds.upper
                case DefaultPower.ZERO:
                    return Power.zero()
                case other:
                    typing.assert_never(other)

        return target_power

    @override
    def get_status(  # pylint: disable=too-many-locals
        self,
        component_ids: frozenset[ComponentId],
        priority: int,
        system_bounds: SystemBounds,
    ) -> _Report:
        """Get the bounds for the algorithm.

        Args:
            component_ids: The IDs of the components to get the bounds for.
            priority: The priority of the actor for which the bounds are requested.
            system_bounds: The system bounds for the components.

        Returns:
            The target power and the available bounds for the given components, for
                the given priority.
        """
        target_power = self._target_power.get(component_ids)
        _, bounds = self._calc_targets(component_ids, system_bounds, priority)
        return _Report(
            target_power=target_power,
            _inclusion_bounds=timeseries.Bounds[Power](
                lower=bounds.lower, upper=bounds.upper
            ),
            _exclusion_bounds=system_bounds.exclusion_bounds,
        )

    @override
    def drop_old_proposals(self, loop_time: float) -> None:
        """Drop old proposals.

        This will remove all proposals that have not been updated for longer than
        `max_proposal_age`.

        Args:
            loop_time: The current loop time.
        """
        buckets_to_delete: list[frozenset[ComponentId]] = []
        for component_ids, proposals in self._component_buckets.items():
            to_delete: list[Proposal] = []
            for proposal in proposals:
                if (loop_time - proposal.creation_time) > self._max_proposal_age_sec:
                    to_delete.append(proposal)
            for proposal in to_delete:
                proposals.remove(proposal)
            if not proposals:
                buckets_to_delete.append(component_ids)

        for component_ids in buckets_to_delete:
            del self._component_buckets[component_ids]
            _ = self._target_power.pop(component_ids, None)



================================================
FILE: src/frequenz/sdk/microgrid/_power_managing/_sorted_set.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""A SortedSet implemented with a Left-Leaning Red-Black tree."""

from __future__ import annotations

import abc
import typing

_RED = True
_BLACK = False


_C = typing.TypeVar("_C", bound="_Comparable")


class _Comparable(typing.Protocol):
    @abc.abstractmethod
    def __lt__(self: _C, other: _C, /) -> bool: ...


_ComparableT = typing.TypeVar("_ComparableT", bound=_Comparable)


class _Node(typing.Generic[_ComparableT]):
    def __init__(self, value: _ComparableT, color: bool = _RED):
        """Create a new node with the given value and color.

        Args:
            value: The value to store in the node.
            color: The color of the node.
        """
        self.value = value
        self.color = color
        self.left: _Node[_ComparableT] | None = None
        self.right: _Node[_ComparableT] | None = None

    def rotate_left(self) -> _Node[_ComparableT]:
        """Rotate the node to the left.

        Returns:
            The new root of the subtree.
        """
        right = self.right
        assert right is not None
        self.right = right.left
        right.left = self
        right.color = self.color
        self.color = _RED
        return right

    def rotate_right(self) -> _Node[_ComparableT]:
        """Rotate the node to the right.

        Returns:
            The new root of the subtree.
        """
        left = self.left
        assert left is not None
        self.left = left.right
        left.right = self
        left.color = self.color
        self.color = _RED
        return left

    def flip_colors(self) -> None:
        """Flip the colors of the node and its children."""
        assert self.left is not None
        assert self.right is not None

        self.color = not self.color
        self.left.color = not self.left.color
        self.right.color = not self.right.color


class SortedSet(typing.Generic[_ComparableT]):
    """A sorted set implemented using a Left-Leaning Red-Black tree.

    It requires that the values stored in the tree are comparable.
    """

    def __init__(self) -> None:
        """Create a new LLRB tree that stores just values."""
        self._root: _Node[_ComparableT] | None = None

    def __iter__(self) -> typing.Iterator[_ComparableT]:
        """Iterate over the values in the tree in order.

        Returns:
            An iterator over the values in the tree.
        """
        return self._iter(self._root)

    def __reversed__(self) -> typing.Iterator[_ComparableT]:
        """Iterate over the values in the tree in reverse order.

        Returns:
            An iterator over the values in the tree.
        """
        return self._reverse_iter(self._root)

    def insert(self, value: _ComparableT) -> None:
        """Insert a value into the tree.

        Args:
            value: The value to insert.
        """
        self._root = self._insert(self._root, value)
        self._root.color = _BLACK

    def search(self, value: _ComparableT) -> _ComparableT | None:
        """Search for a value in the tree.

        Args:
            value: The value to search for.

        Returns:
            The value if it is found, otherwise None.
        """
        root = self._root
        while root is not None:
            if value < root.value:
                root = root.left
            elif value > root.value:
                root = root.right
            else:
                return root.value
        return None

    def min(self) -> _ComparableT | None:
        """Get the minimum value in the tree.

        Returns:
            The minimum value, or None if the tree is empty.
        """
        ret = self._min(self._root)
        return ret.value if ret else None

    def delete_min(self) -> None:
        """Delete the minimum value in the tree."""
        if self._root is None:
            return
        self._root = self._delete_min(self._root)
        if self._root is not None:
            self._root.color = _BLACK

    def delete(self, value: _ComparableT) -> None:
        """Delete a value from the tree.

        Args:
            value: The value to delete.
        """
        self._root = self._delete(self._root, value)
        if self._root is not None:
            self._root.color = _BLACK

    def _iter(self, node: _Node[_ComparableT] | None) -> typing.Iterator[_ComparableT]:
        if node is None:
            return
        yield from self._iter(node.left)
        yield node.value
        yield from self._iter(node.right)

    def _reverse_iter(
        self, node: _Node[_ComparableT] | None
    ) -> typing.Iterator[_ComparableT]:
        if node is None:
            return
        yield from self._reverse_iter(node.right)
        yield node.value
        yield from self._reverse_iter(node.left)

    def __len__(self) -> int:
        """Get the number of values in the tree.

        Returns:
            The number of values in the tree.
        """
        return self._len(self._root)

    def _is_red(
        self, node: _Node[_ComparableT] | None
    ) -> typing.TypeGuard[_Node[_ComparableT]]:
        if node is None:
            return False
        return node.color == _RED

    def _insert(
        self, node: _Node[_ComparableT] | None, value: _ComparableT
    ) -> _Node[_ComparableT]:
        if node is None:
            return _Node(value)

        if self._is_red(node.left) and self._is_red(node.right):
            node.flip_colors()

        if value < node.value:
            node.left = self._insert(node.left, value)
        elif value > node.value:
            node.right = self._insert(node.right, value)
        else:
            node.value = value

        if self._is_red(node.right) and not self._is_red(node.left):
            node = node.rotate_left()
        if self._is_red(node.left) and self._is_red(node.left.left):
            node = node.rotate_right()

        return node

    def _len(self, node: _Node[_ComparableT] | None) -> int:
        if node is None:
            return 0
        return 1 + self._len(node.left) + self._len(node.right)

    def _move_red_left(self, node: _Node[_ComparableT]) -> _Node[_ComparableT]:
        node.flip_colors()
        assert node.right is not None
        if self._is_red(node.right.left):
            node.right = node.right.rotate_right()
            node = node.rotate_left()
            node.flip_colors()
        return node

    def _move_red_right(self, node: _Node[_ComparableT]) -> _Node[_ComparableT]:
        node.flip_colors()
        assert node.left is not None
        if self._is_red(node.left.left):
            node = node.rotate_right()
            node.flip_colors()
        return node

    def _min(self, node: _Node[_ComparableT] | None) -> _Node[_ComparableT] | None:
        if node is None or node.left is None:
            return node
        return self._min(node.left)

    def _fix_up(self, node: _Node[_ComparableT]) -> _Node[_ComparableT]:
        if self._is_red(node.right):
            node = node.rotate_left()
        if self._is_red(node.left) and self._is_red(node.left.left):
            node = node.rotate_right()
        if self._is_red(node.left) and self._is_red(node.right):
            node.flip_colors()
        return node

    def _delete_min(
        self, node: _Node[_ComparableT] | None
    ) -> _Node[_ComparableT] | None:
        if node is None or node.left is None:
            return None

        if not self._is_red(node.left) and not self._is_red(node.left.left):
            node = self._move_red_left(node)

        if node.left is None:
            return None

        node.left = self._delete_min(node.left)
        return self._fix_up(node)

    def _delete(
        self, node: _Node[_ComparableT] | None, value: _ComparableT
    ) -> _Node[_ComparableT] | None:
        if node is None:
            return None

        if value < node.value:
            assert node.left is not None
            if not self._is_red(node.left) and not self._is_red(node.left.left):
                node = self._move_red_left(node)
            node.left = self._delete(node.left, value)
        else:
            if self._is_red(node.left):
                node = node.rotate_right()
            if value == node.value and node.right is None:
                return None
            assert node.right is not None
            if not self._is_red(node.right) and not self._is_red(node.right.left):
                node = self._move_red_right(node)
            if value == node.value:
                min_ = self._min(node.right)
                assert min_ is not None
                node.value = min_.value
                node.right = self._delete_min(node.right)
            else:
                node.right = self._delete(node.right, value)

        return self._fix_up(node)



================================================
FILE: src/frequenz/sdk/timeseries/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""
Handling of timeseries streams.

A timeseries is a stream (normally an async iterator) of
[`Sample`][frequenz.sdk.timeseries.Sample]s.

# Periodicity and alignment

All the data produced by this package is always periodic, in UTC, and aligned to the
[Epoch](https://en.wikipedia.org/wiki/Epoch_(computing)) (by default).

Classes normally take a (re)sampling period as and argument and, optionally, an
`align_to` argument.

This means timestamps are always separated exactly by a period, and that this
timestamp falls always at multiples of the period, starting at the `align_to`.

This ensures that the data is predictable and consistent among restarts.

Example:
    If we have a period of 10 seconds, and are aligning to the UNIX
    epoch. Assuming the following timeline starts in 1970-01-01 00:00:00
    UTC and our current `now` is 1970-01-01 00:00:32 UTC, then the next
    timestamp will be at 1970-01-01 00:00:40 UTC:

    ```
    align_to = 1970-01-01 00:00:00         next event = 1970-01-01 00:00:40
    |                                       |
    |---------|---------|---------|-|-------|---------|---------|---------|
    0        10        20        30 |      40        50        60        70
                                   now = 1970-01-01 00:00:32
    ```
"""

from .._internal._channels import ReceiverFetcher
from ._base_types import Bounds, Sample, Sample3Phase
from ._fuse import Fuse
from ._moving_window import MovingWindow
from ._periodic_feature_extractor import PeriodicFeatureExtractor
from ._resampling._base_types import Sink, Source, SourceProperties
from ._resampling._config import (
    DEFAULT_BUFFER_LEN_INIT,
    DEFAULT_BUFFER_LEN_MAX,
    DEFAULT_BUFFER_LEN_WARN,
    ResamplerConfig,
    ResamplerConfig2,
    ResamplingFunction,
    ResamplingFunction2,
)
from ._resampling._exceptions import ResamplingError, SourceStoppedError
from ._resampling._wall_clock_timer import (
    ClocksInfo,
    TickInfo,
    WallClockTimer,
    WallClockTimerConfig,
)

__all__ = [
    "Bounds",
    "ClocksInfo",
    "DEFAULT_BUFFER_LEN_INIT",
    "DEFAULT_BUFFER_LEN_MAX",
    "DEFAULT_BUFFER_LEN_WARN",
    "Fuse",
    "MovingWindow",
    "PeriodicFeatureExtractor",
    "ReceiverFetcher",
    "ResamplerConfig",
    "ResamplerConfig2",
    "ResamplingError",
    "ResamplingFunction",
    "ResamplingFunction2",
    "Sample",
    "Sample3Phase",
    "Sink",
    "Source",
    "SourceProperties",
    "SourceStoppedError",
    "TickInfo",
    "WallClockTimer",
    "WallClockTimerConfig",
]



================================================
FILE: src/frequenz/sdk/timeseries/_base_types.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Timeseries basic types."""

import dataclasses
import functools
from collections.abc import Callable, Iterator
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Generic, Protocol, Self, TypeVar, cast, overload

from frequenz.quantities import Power, Quantity

QuantityT = TypeVar("QuantityT", bound=Quantity)
"""Type variable for representing various quantity types."""


@dataclass(frozen=True, order=True)
class Sample(Generic[QuantityT]):
    """A measurement taken at a particular point in time.

    The `value` could be `None` if a component is malfunctioning or data is
    lacking for another reason, but a sample still needs to be sent to have a
    coherent view on a group of component metrics for a particular timestamp.
    """

    timestamp: datetime
    """The time when this sample was generated."""

    value: QuantityT | None = None
    """The value of this sample."""

    def __str__(self) -> str:
        """Return a string representation of the sample."""
        return f"{type(self).__name__}({self.timestamp}, {self.value})"

    def __repr__(self) -> str:
        """Return a string representation of the sample."""
        return f"{type(self).__name__}({self.timestamp=}, {self.value=})"


@dataclass(frozen=True)
class Sample3Phase(Generic[QuantityT]):
    """A 3-phase measurement made at a particular point in time.

    Each of the `value` fields could be `None` if a component is malfunctioning
    or data is lacking for another reason, but a sample still needs to be sent
    to have a coherent view on a group of component metrics for a particular
    timestamp.
    """

    timestamp: datetime
    """The time when this sample was generated."""
    value_p1: QuantityT | None
    """The value of the 1st phase in this sample."""

    value_p2: QuantityT | None
    """The value of the 2nd phase in this sample."""

    value_p3: QuantityT | None
    """The value of the 3rd phase in this sample."""

    def __iter__(self) -> Iterator[QuantityT | None]:
        """Return an iterator that yields values from each of the phases.

        Yields:
            Per-phase measurements one-by-one.
        """
        yield self.value_p1
        yield self.value_p2
        yield self.value_p3

    @overload
    def max(self, default: QuantityT) -> QuantityT: ...

    @overload
    def max(self, default: None = None) -> QuantityT | None: ...

    def max(self, default: QuantityT | None = None) -> QuantityT | None:
        """Return the max value among all phases, or default if they are all `None`.

        Args:
            default: value to return if all phases are `None`.

        Returns:
            Max value among all phases, if available, default value otherwise.
        """
        if not any(self):
            return default
        value: QuantityT = functools.reduce(
            lambda x, y: x if x > y else y,
            filter(None, self),
        )
        return value

    @overload
    def min(self, default: QuantityT) -> QuantityT: ...

    @overload
    def min(self, default: None = None) -> QuantityT | None: ...

    def min(self, default: QuantityT | None = None) -> QuantityT | None:
        """Return the min value among all phases, or default if they are all `None`.

        Args:
            default: value to return if all phases are `None`.

        Returns:
            Min value among all phases, if available, default value otherwise.
        """
        if not any(self):
            return default
        value: QuantityT = functools.reduce(
            lambda x, y: x if x < y else y,
            filter(None, self),
        )
        return value

    def map(
        self,
        function: Callable[[QuantityT], QuantityT],
        default: QuantityT | None = None,
    ) -> Self:
        """Apply the given function on each of the phase values and return the result.

        If a phase value is `None`, replace it with `default` instead.

        Args:
            function: The function to apply on each of the phase values.
            default: The value to apply if a phase value is `None`.

        Returns:
            A new instance, with the given function applied on values for each of the
                phases.
        """
        return self.__class__(
            timestamp=self.timestamp,
            value_p1=default if self.value_p1 is None else function(self.value_p1),
            value_p2=default if self.value_p2 is None else function(self.value_p2),
            value_p3=default if self.value_p3 is None else function(self.value_p3),
        )


class Comparable(Protocol):
    """A protocol that requires the implementation of comparison methods.

    This protocol is used to ensure that types can be compared using
    the less than or equal to (`<=`) and greater than or equal to (`>=`)
    operators.
    """

    def __le__(self, other: Any, /) -> bool:
        """Return whether this instance is less than or equal to `other`."""

    def __ge__(self, other: Any, /) -> bool:
        """Return whether this instance is greater than or equal to `other`."""


_T = TypeVar("_T", bound=Comparable | None)


@dataclass(frozen=True)
class Bounds(Generic[_T]):
    """Lower and upper bound values.

    Depending on the genertic type `_T`, the lower and upper bounds can be `None`, in
    which case it means that there is no lower or upper bound, respectively.

    When checking if an item is within the bounds, the item must always be not `None`.
    """

    lower: _T
    """Lower bound."""

    upper: _T
    """Upper bound."""

    def __contains__(self, item: _T) -> bool:
        """
        Check if the value is within the range of the container.

        Args:
            item: The value to check. Can't be `None` even if `_T` can be `None`.

        Returns:
            bool: True if value is within the range, otherwise False.
        """
        assert item is not None, "Can't check if `None` is within the bounds."
        if self.lower is None and self.upper is None:
            return True
        if self.lower is None:
            return item <= self.upper
        if self.upper is None:
            return self.lower <= item

        return cast(Comparable, self.lower) <= item <= cast(Comparable, self.upper)


@dataclass(frozen=True, kw_only=True)
class SystemBounds:
    """Internal representation of system bounds for groups of components."""

    # compare = False tells the dataclass to not use name for comparison methods
    timestamp: datetime = dataclasses.field(compare=False)
    """Timestamp of the metrics."""

    inclusion_bounds: Bounds[Power] | None
    """Total inclusion power bounds for all components of a pool.

    This is the range within which power requests would be allowed by the pool.

    When exclusion bounds are present, they will exclude a subset of the inclusion
    bounds.
    """

    exclusion_bounds: Bounds[Power] | None
    """Total exclusion power bounds for all components of a pool.

    This is the range within which power requests are NOT allowed by the pool.
    If present, they will be a subset of the inclusion bounds.
    """

    def __contains__(self, item: Power) -> bool:
        """
        Check if the value is within the range of the container.

        Args:
            item: The value to check.

        Returns:
            bool: True if value is within the range, otherwise False.
        """
        if not self.inclusion_bounds or item not in self.inclusion_bounds:
            return False
        if self.exclusion_bounds and item in self.exclusion_bounds:
            return False
        return True



================================================
FILE: src/frequenz/sdk/timeseries/_fuse.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Fuse data class."""
from __future__ import annotations

from dataclasses import dataclass

from frequenz.quantities import Current


@dataclass(frozen=True)
class Fuse:
    """Fuse data class."""

    max_current: Current
    """Rated current of the fuse."""



================================================
FILE: src/frequenz/sdk/timeseries/_grid_frequency.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Fetches the Grid Frequency."""

from __future__ import annotations

import asyncio
import logging

from frequenz.channels import Receiver, Sender
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import Component, ComponentCategory, ComponentMetricId
from frequenz.quantities import Frequency, Quantity

from .._internal._channels import ChannelRegistry
from ..microgrid import connection_manager
from ..microgrid._data_sourcing import ComponentMetricRequest
from ..timeseries._base_types import Sample

_logger = logging.getLogger(__name__)


def create_request(component_id: ComponentId) -> ComponentMetricRequest:
    """Create a request for grid frequency.

    Args:
        component_id: The component id to use for the request.

    Returns:
        A component metric request for grid frequency.
    """
    return ComponentMetricRequest(
        "grid-frequency", component_id, ComponentMetricId.FREQUENCY, None
    )


class GridFrequency:
    """Grid Frequency."""

    def __init__(
        self,
        data_sourcing_request_sender: Sender[ComponentMetricRequest],
        channel_registry: ChannelRegistry,
        source: Component | None = None,
    ):
        """Initialize the grid frequency formula generator.

        Args:
            data_sourcing_request_sender: The sender to use for requests.
            channel_registry: The channel registry to use for the grid frequency.
            source: The source component to use to receive the grid frequency.
        """
        if not source:
            component_graph = connection_manager.get().component_graph
            source = component_graph.find_first_descendant_component(
                descendant_categories=(
                    ComponentCategory.METER,
                    ComponentCategory.INVERTER,
                    ComponentCategory.EV_CHARGER,
                ),
            )

        self._request_sender: Sender[ComponentMetricRequest] = (
            data_sourcing_request_sender
        )
        self._channel_registry: ChannelRegistry = channel_registry
        self._source_component: Component = source
        self._component_metric_request: ComponentMetricRequest = create_request(
            self._source_component.component_id
        )

        self._task: None | asyncio.Task[None] = None

    @property
    def source(self) -> Component:
        """The component that is used to fetch the grid frequency.

        Returns:
            The component that is used for grid frequency.
        """
        return self._source_component

    def new_receiver(self) -> Receiver[Sample[Frequency]]:
        """Create a receiver for grid frequency.

        Returns:
            A receiver that will receive grid frequency samples.
        """
        receiver = self._channel_registry.get_or_create(
            Sample[Quantity], self._component_metric_request.get_channel_name()
        ).new_receiver()

        if not self._task:
            self._task = asyncio.create_task(self._send_request())
        else:
            _logger.info(
                "Grid frequency request already sent: %s", self._source_component
            )

        return receiver.map(
            lambda sample: (
                Sample[Frequency](sample.timestamp, None)
                if sample.value is None or sample.value.isnan()
                else Sample(
                    sample.timestamp, Frequency.from_hertz(sample.value.base_value)
                )
            )
        )

    async def _send_request(self) -> None:
        """Send the request for grid frequency."""
        await self._request_sender.send(self._component_metric_request)
        _logger.debug("Sent request for grid frequency: %s", self._source_component)



================================================
FILE: src/frequenz/sdk/timeseries/_moving_window.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""A data window that moves with the latest datapoints of a data stream."""


import asyncio
import logging
import math
from collections.abc import Sequence
from datetime import datetime, timedelta
from typing import SupportsIndex, assert_never, overload

import numpy as np
from frequenz.channels import Broadcast, Receiver, Sender
from frequenz.core.datetime import UNIX_EPOCH
from frequenz.quantities import Quantity
from numpy.typing import ArrayLike

from ..actor._background_service import BackgroundService
from ._base_types import Sample
from ._resampling._config import ResamplerConfig
from ._resampling._resampler import Resampler
from ._ringbuffer import OrderedRingBuffer

_logger = logging.getLogger(__name__)


class MovingWindow(BackgroundService):
    """
    A data window that moves with the latest datapoints of a data stream.

    After initialization the `MovingWindow` can be accessed by an integer
    index or a timestamp. A sub window can be accessed by using a slice of
    integers or timestamps.

    Note that a numpy ndarray is returned and thus users can use
    numpys operations directly on a window.

    The window uses a ring buffer for storage and the first element is aligned to
    a fixed defined point in time. Since the moving nature of the window, the
    date of the first and the last element are constantly changing and therefore
    the point in time that defines the alignment can be outside of the time window.
    Modulo arithmetic is used to move the `align_to` timestamp into the latest
    window.

    If for example the `align_to` parameter is set to
    `datetime(1, 1, 1, tzinfo=timezone.utc)` and the window size is bigger than
    one day then the first element will always be aligned to midnight.

    Resampling might be required to reduce the number of samples to store, and
    it can be set by specifying the resampler config parameter so that the user
    can control the granularity of the samples to be stored in the underlying
    buffer.

    If resampling is not required, the resampler config parameter can be
    set to None in which case the MovingWindow will not perform any resampling.

    Example: Calculate the mean of a time interval

        ```python
        from datetime import datetime, timedelta, timezone

        async def send_mock_data(sender: Sender[Sample]) -> None:
            while True:
                await sender.send(Sample(datetime.now(tz=timezone.utc), 10.0))
                await asyncio.sleep(1.0)

        async def run() -> None:
            resampled_data_channel = Broadcast[Sample](name="sample-data")
            resampled_data_receiver = resampled_data_channel.new_receiver()
            resampled_data_sender = resampled_data_channel.new_sender()

            send_task = asyncio.create_task(send_mock_data(resampled_data_sender))

            async with MovingWindow(
                size=timedelta(seconds=5),
                resampled_data_recv=resampled_data_receiver,
                input_sampling_period=timedelta(seconds=1),
            ) as window:
                time_start = datetime.now(tz=timezone.utc)
                time_end = time_start + timedelta(seconds=5)

                # ... wait for 5 seconds until the buffer is filled
                await asyncio.sleep(5)

                # return an numpy array from the window
                array = window[time_start:time_end]
                # and use it to for example calculate the mean
                mean = array.mean()

        asyncio.run(run())
        ```

    Example: Create a polars data frame from a `MovingWindow`

        ```python
        from datetime import datetime, timedelta, timezone

        async def send_mock_data(sender: Sender[Sample]) -> None:
            while True:
                await sender.send(Sample(datetime.now(tz=timezone.utc), 10.0))
                await asyncio.sleep(1.0)

        async def run() -> None:
            resampled_data_channel = Broadcast[Sample](name="sample-data")
            resampled_data_receiver = resampled_data_channel.new_receiver()
            resampled_data_sender = resampled_data_channel.new_sender()

            send_task = asyncio.create_task(send_mock_data(resampled_data_sender))

            # create a window that stores two days of data
            # starting at 1.1.23 with samplerate=1
            async with MovingWindow(
                size=timedelta(days=2),
                resampled_data_recv=resampled_data_receiver,
                input_sampling_period=timedelta(seconds=1),
            ) as window:
                # wait for one full day until the buffer is filled
                await asyncio.sleep(60*60*24)

                time_start = datetime(2023, 1, 1, tzinfo=timezone.utc)
                time_end = datetime(2023, 1, 2, tzinfo=timezone.utc)

                # You can now create a polars series with one full day of data by
                # passing the window slice, like:
                # series = pl.Series("Jan_1", window[time_start:time_end])

        asyncio.run(run())
        ```
    """

    def __init__(  # pylint: disable=too-many-arguments
        self,
        *,
        size: timedelta,
        resampled_data_recv: Receiver[Sample[Quantity]],
        input_sampling_period: timedelta,
        resampler_config: ResamplerConfig | None = None,
        align_to: datetime = UNIX_EPOCH,
        name: str | None = None,
    ) -> None:
        """
        Initialize the MovingWindow.

        This method creates the underlying ring buffer and starts a
        new task that updates the ring buffer with new incoming samples.
        The task stops running only if the channel receiver is closed.

        Args:
            size: The time span of the moving window over which samples will be stored.
            resampled_data_recv: A receiver that delivers samples with a
                given sampling period.
            input_sampling_period: The time interval between consecutive input samples.
            resampler_config: The resampler configuration in case resampling is required.
            align_to: A timestamp that defines a point in time to which
                the window is aligned to modulo window size. For further
                information, consult the class level documentation.
            name: The name of this moving window. If `None`, `str(id(self))` will be
                used. This is used mostly for debugging purposes.
        """
        assert (
            input_sampling_period.total_seconds() > 0
        ), "The input sampling period should be greater than zero."
        assert (
            input_sampling_period <= size
        ), "The input sampling period should be equal to or lower than the window size."
        super().__init__(name=name)

        self._sampling_period = input_sampling_period

        self._resampler: Resampler | None = None
        self._resampler_sender: Sender[Sample[Quantity]] | None = None

        if resampler_config:
            assert (
                resampler_config.resampling_period <= size
            ), "The resampling period should be equal to or lower than the window size."

            self._resampler = Resampler(resampler_config)
            self._sampling_period = resampler_config.resampling_period

        # Sampling period might not fit perfectly into the window size.
        num_samples = math.ceil(
            size.total_seconds() / self._sampling_period.total_seconds()
        )

        self._resampled_data_recv = resampled_data_recv
        self._buffer = OrderedRingBuffer(
            np.empty(shape=num_samples, dtype=float),
            sampling_period=self._sampling_period,
            align_to=align_to,
        )

        self._condition_new_sample = asyncio.Condition()

    def start(self) -> None:
        """Start the MovingWindow.

        This method starts the MovingWindow tasks.
        """
        if self._resampler:
            self._configure_resampler()
        self._tasks.add(asyncio.create_task(self._run_impl(), name="update-window"))

    @property
    def sampling_period(self) -> timedelta:
        """
        Return the sampling period of the MovingWindow.

        Returns:
            The sampling period of the MovingWindow.
        """
        return self._sampling_period

    @property
    def oldest_timestamp(self) -> datetime | None:
        """
        Return the oldest timestamp of the MovingWindow.

        Returns:
            The oldest timestamp of the MovingWindow or None if the buffer is empty.
        """
        return self._buffer.oldest_timestamp

    @property
    def newest_timestamp(self) -> datetime | None:
        """
        Return the newest timestamp of the MovingWindow.

        Returns:
            The newest timestamp of the MovingWindow or None if the buffer is empty.
        """
        return self._buffer.newest_timestamp

    @property
    def capacity(self) -> int:
        """
        Return the capacity of the MovingWindow.

        Capacity is the maximum number of samples that can be stored in the
        MovingWindow.

        Returns:
            The capacity of the MovingWindow.
        """
        return self._buffer.maxlen

    # pylint before 3.0 only accepts names with 3 or more chars
    def at(self, key: int | datetime) -> float:  # pylint: disable=invalid-name
        """
        Return the sample at the given index or timestamp.

        In contrast to the [`window`][frequenz.sdk.timeseries.MovingWindow.window] method,
        which expects a slice as argument, this method expects a single index as argument
        and returns a single value.

        Args:
            key: The index or timestamp of the sample to return.

        Returns:
            The sample at the given index or timestamp.

        Raises:
            IndexError: If the buffer is empty or the index is out of bounds.
        """
        if self._buffer.count_valid() == 0:
            raise IndexError("The buffer is empty.")

        if isinstance(key, datetime):
            assert self._buffer.oldest_timestamp is not None
            assert self._buffer.newest_timestamp is not None
            if (
                key < self._buffer.oldest_timestamp
                or key > self._buffer.newest_timestamp
            ):
                raise IndexError(
                    f"Timestamp {key} is out of range [{self._buffer.oldest_timestamp}, "
                    f"{self._buffer.newest_timestamp}]"
                )
            return self._buffer[self._buffer.to_internal_index(key)]

        if isinstance(key, int):
            _logger.debug("Returning value at index %s ", key)
            timestamp = self._buffer.get_timestamp(key)
            assert timestamp is not None
            return self._buffer[self._buffer.to_internal_index(timestamp)]

        assert_never(key)

    def window(
        self,
        start: datetime | int | None,
        end: datetime | int | None,
        *,
        force_copy: bool = True,
        fill_value: float | None = np.nan,
    ) -> ArrayLike:
        """
        Return an array containing the samples in the given time interval.

        In contrast to the [`at`][frequenz.sdk.timeseries.MovingWindow.at] method,
        which expects a single index as argument, this method expects a slice as argument
        and returns an array.

        Args:
            start: The start timestamp of the time interval. If `None`, the
                start of the window is used.
            end: The end timestamp of the time interval. If `None`, the end of
                the window is used.
            force_copy: If `True`, the returned array is a copy of the underlying
                data. Otherwise, if possible, a view of the underlying data is
                returned.
            fill_value: If not None, will use this value to fill missing values.
                If missing values should be set, force_copy must be True.
                Defaults to NaN to avoid returning outdated data unexpectedly.

        Returns:
            An array containing the samples in the given time interval.
        """
        return self._buffer.window(
            start, end, force_copy=force_copy, fill_value=fill_value
        )

    async def wait_for_samples(self, n: int) -> None:
        """Wait until the next `n` samples are available in the MovingWindow.

        This function returns after `n` new samples are available in the MovingWindow,
        without considering whether the new samples are valid.  The validity of the
        samples can be verified by calling the
        [`count_valid`][frequenz.sdk.timeseries.MovingWindow.count_valid] method.

        Args:
            n: The number of samples to wait for.

        Raises:
            ValueError: If `n` is less than or equal to 0 or greater than the capacity
                of the MovingWindow.
        """
        if n == 0:
            return
        if n < 0:
            raise ValueError("The number of samples to wait for must be 0 or greater.")
        if n > self.capacity:
            raise ValueError(
                "The number of samples to wait for must be less than or equal to the "
                + f"capacity of the MovingWindow ({self.capacity})."
            )
        start_timestamp = (
            # Start from the next expected timestamp.
            self.newest_timestamp + self.sampling_period
            if self.newest_timestamp is not None
            else None
        )
        while True:
            async with self._condition_new_sample:
                # Every time a new sample is received, this condition gets notified and
                # will wake up.
                _ = await self._condition_new_sample.wait()
            if self.count_covered(since=start_timestamp) >= n:
                return

    async def _run_impl(self) -> None:
        """Awaits samples from the receiver and updates the underlying ring buffer.

        Raises:
            asyncio.CancelledError: if the MovingWindow task is cancelled.
        """
        try:
            async for sample in self._resampled_data_recv:
                _logger.debug("Received new sample: %s", sample)
                if self._resampler and self._resampler_sender:
                    await self._resampler_sender.send(sample)
                else:
                    self._buffer.update(sample)
                    async with self._condition_new_sample:
                        # Wake up all coroutines waiting for new samples.
                        self._condition_new_sample.notify_all()

        except asyncio.CancelledError:
            _logger.info("MovingWindow task has been cancelled.")
            raise

        _logger.error("Channel has been closed")

    def _configure_resampler(self) -> None:
        """Configure the components needed to run the resampler."""
        assert self._resampler is not None

        async def sink_buffer(sample: Sample[Quantity]) -> None:
            self._buffer.update(sample)
            async with self._condition_new_sample:
                # Wake up all coroutines waiting for new samples.
                self._condition_new_sample.notify_all()

        resampler_channel = Broadcast[Sample[Quantity]](name="average")
        self._resampler_sender = resampler_channel.new_sender()
        self._resampler.add_timeseries(
            "avg", resampler_channel.new_receiver(), sink_buffer
        )
        self._tasks.add(
            asyncio.create_task(self._resampler.resample(), name="resample")
        )

    def count_valid(
        self, *, since: datetime | None = None, until: datetime | None = None
    ) -> int:
        """Count the number of valid samples in this `MovingWindow`.

        If `since` and `until` are provided, the count is limited to the samples between
        (and including) the given timestamps.

        Args:
            since: The timestamp from which to start counting.  If `None`, the oldest
                timestamp of the buffer is used.
            until: The timestamp until (and including) which to count.  If `None`, the
                newest timestamp of the buffer is used.

        Returns:
            The number of valid samples in this `MovingWindow`.
        """
        return self._buffer.count_valid(since=since, until=until)

    def count_covered(
        self, *, since: datetime | None = None, until: datetime | None = None
    ) -> int:
        """Count the number of samples that are covered by the oldest and newest valid samples.

        If `since` and `until` are provided, the count is limited to the samples between
        (and including) the given timestamps.

        Args:
            since: The timestamp from which to start counting.  If `None`, the oldest
                timestamp of the buffer is used.
            until: The timestamp until (and including) which to count.  If `None`, the
                newest timestamp of the buffer is used.

        Returns:
            The count of samples between the oldest and newest (inclusive) valid samples
                or 0 if there are is no time range covered.
        """
        return self._buffer.count_covered(since=since, until=until)

    @overload
    def __getitem__(self, key: SupportsIndex) -> float:
        """See the main __getitem__ method."""

    @overload
    def __getitem__(self, key: datetime) -> float:
        """See the main __getitem__ method."""

    @overload
    def __getitem__(self, key: slice) -> ArrayLike:
        """See the main __getitem__ method."""

    # We need the noqa because `IndexError` is raised indirectly by `at()` and `window()`
    def __getitem__(  # noqa: DOC503
        self, key: SupportsIndex | datetime | slice
    ) -> float | ArrayLike:
        """
        Return a sub window of the `MovingWindow`.

        The `MovingWindow` is accessed either by timestamp or by index
        or by a slice of timestamps or integers.

        * If the key is an integer, the float value of that key
          at the given position is returned.
        * If the key is a timestamp, the float value of that key
          that corresponds to the timestamp is returned.
        * If the key is a slice of timestamps or integers, an ndarray is returned,
          where the bounds correspond to the slice bounds.
          Note that a half open interval, which is open at the end, is returned.

        Note:
            Slicing with a step other than 1 is not supported.

        Args:
            key: Either an integer or a timestamp or a slice of timestamps or integers.

        Raises:
            IndexError: when requesting an out of range timestamp or index
            ValueError: when requesting a slice with a step other than 1

        Returns:
            A float if the key is a number or a timestamp.
            an numpy array if the key is a slice.
        """
        if isinstance(key, slice):
            if not (key.step is None or key.step == 1):
                raise ValueError("Slicing with a step other than 1 is not supported.")
            return self.window(key.start, key.stop)

        if isinstance(key, datetime):
            return self.at(key)

        if isinstance(key, SupportsIndex):
            return self.at(key.__index__())

        assert_never(key)


# We need to register the class as a subclass of Sequence like this because
# otherwise type-checking fails complaining that MovingWindow has more
# overloads of __getitem__() than Sequence (which doesn't have an overload with
# a datetime key)
Sequence.register(MovingWindow)



================================================
FILE: src/frequenz/sdk/timeseries/_periodic_feature_extractor.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""
A feature extractor for historical timeseries data.

It creates a profile from periodically occurring windows in a buffer of
historical data.

The profile is created out of all windows that are fully contained in the
underlying buffer, e.g. the moving window, with the same start and end time
modulo a fixed period.
"""


import logging
from dataclasses import dataclass
from datetime import datetime, timedelta

import numpy as np
from numpy.typing import NDArray

from .._internal._math import is_close_to_zero
from ._moving_window import MovingWindow
from ._ringbuffer import OrderedRingBuffer

_logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class RelativePositions:
    """
    Holds the relative positions of a window in a buffer.

    When calculating a profile the user has to pass two datetime objects which
    are determining a window. It's allowed that this window is not contained in
    the historical data buffer.
    In order to calculate the profile the window is shifted to the first
    position in the buffer where it occurs first and represented by indexes
    relative to the oldest sample in the buffer.
    """

    start: int
    """The relative start position of the window."""
    end: int
    """The relative end position of the window."""
    next: int
    """The relative position of the next incoming sample."""


class PeriodicFeatureExtractor:
    """
    A feature extractor for historical timeseries data.

    This class is creating a profile from periodically occurring windows in a
    buffer of historical data.

    The profile is created out of all windows that are fully contained in the
    underlying buffer with the same start and end time modulo a fixed period.

    Consider for example a timeseries $T$ of historical data and sub-series
    $S_i$ of $T$ all having the same size $l$ and the same fixed distance $p$
    called period, where period of two sub-windows is defined as the distance
    of two points at the same position within the sub-windows.

    This class calculates a statistical profile $S$ over all $S_i$, i.e. the
    value of $S$ at position $i$ is calculated by performing a certain
    calculation, e.g. an average, over all values of $S_i$ at position $i$.

    Note:
        The oldest window or the window that is currently overwritten in the
        `MovingWindow` is not considered in the profile.

    Note:
        When constructing a `PeriodicFeatureExtractor` object the
        `MovingWindow` size has to be a integer multiple of the period.

    Example:
        ```python
        from frequenz.sdk import microgrid
        from datetime import datetime, timedelta, timezone

        async with MovingWindow(
            size=timedelta(days=35),
            resampled_data_recv=microgrid.grid().power.new_receiver(),
            input_sampling_period=timedelta(seconds=1),
        ) as moving_window:
            feature_extractor = PeriodicFeatureExtractor(
                moving_window=moving_window,
                period=timedelta(days=7),
            )

            now = datetime.now(timezone.utc)

            # create a daily weighted average for the next 24h
            avg_24h = feature_extractor.avg(
                now,
                now + timedelta(hours=24),
                weights=[0.1, 0.2, 0.3, 0.4]
            )

            # create a daily average for Thursday March 23 2023
            th_avg_24h = feature_extractor.avg(datetime(2023, 3, 23), datetime(2023, 3, 24))
        ```
    """

    def __init__(
        self,
        moving_window: MovingWindow,
        period: timedelta,
    ) -> None:
        """
        Initialize a PeriodicFeatureExtractor object.

        Args:
            moving_window: The MovingWindow that is used for the average calculation.
            period: The distance between two succeeding intervals.

        Raises:
            ValueError: If the MovingWindow size is not a integer multiple of the period.
        """
        self._moving_window = moving_window

        self._sampling_period = self._moving_window.sampling_period
        """The sampling_period as float to use it for indexing of samples."""

        self._period = int(period / self._sampling_period)
        """Distance between two succeeding intervals in samples."""

        _logger.debug("Initializing PeriodicFeatureExtractor!")
        _logger.debug("MovingWindow size: %i", self._moving_window.count_valid())
        _logger.debug(
            "Period between two succeeding intervals (in samples): %i",
            self._period,
        )

        if not self._moving_window.count_valid() % self._period == 0:
            raise ValueError(
                "The MovingWindow size is not a integer multiple of the period."
            )

        if not is_close_to_zero(self._period - period / self._sampling_period):
            raise ValueError(
                "The period is not a multiple of the sampling period. "
                "This might result in unexpected behaviour."
            )

    @property
    def _buffer(self) -> OrderedRingBuffer[NDArray[np.float64]]:
        return self._moving_window._buffer  # pylint: disable=protected-access

    def _timestamp_to_rel_index(self, timestamp: datetime) -> int:
        """
        Get the index of a timestamp relative to the oldest sample in the MovingWindow.

        In other word consider an integer axis where the zero is defined as the
        oldest element in the MovingWindow. This function returns the index of
        the given timestamp an this axis.

        This method can return negative values.

        Args:
            timestamp: A timestamp that we want to shift into the window.

        Returns:
            The index of the timestamp shifted into the MovingWindow.
        """
        # align timestamp to the sampling period
        timestamp = self._buffer.normalize_timestamp(timestamp)

        # distance between the input ts and the ts of oldest known samples (in samples)
        dist_to_oldest = int(
            (timestamp - self._buffer.time_bound_oldest) / self._sampling_period
        )

        _logger.debug("Shifting ts: %s", timestamp)
        _logger.debug("Oldest timestamp in buffer: %s", self._buffer.time_bound_oldest)
        _logger.debug("Distance to the oldest sample: %i", dist_to_oldest)

        return dist_to_oldest

    def _reshape_np_array(
        self, array: NDArray[np.float64], window_size: int
    ) -> NDArray[np.float64]:
        """
        Reshape a numpy array to a 2D array where each row represents a window.

        There are three cases to consider

        1. The array size is a multiple of window_size + period,
           i.e. num_windows is integer and we can simply reshape.
        2. The array size is NOT a multiple of window_size + period and
           it has length n * period + m, where m < window_size.
        3. The array size is NOT a multiple of window_size + period and
           it has length n * period + m, where m >= window_size.

        Note that in the current implementation of this class we have the restriction
        that period is a multiple integer of the size of the MovingWindow and hence
        only case 1 can occur.

        Args:
            array: The numpy array to reshape.
            window_size: The size of the window in samples.

        Returns:
            The reshaped 2D array.

        Raises:
            ValueError: If the array is smaller or equal to the given period.
        """
        # Not using the num_windows function here because we want to
        # differentiate between the three cases.
        if len(array) < self._period:
            raise ValueError(
                f"The array (length:{len(array)}) is too small to be reshaped."
            )

        num_windows = len(array) // self._period

        # Case 1:
        if len(array) - num_windows * self._period == 0:
            resized_array = array
        # Case 2
        elif len(array) - num_windows * self._period < window_size:
            resized_array = array[: num_windows * self._period]
        # Case 3
        else:
            num_windows += 1
            resized_array = np.resize(array, num_windows * self._period)

        return resized_array.reshape(num_windows, self._period)

    def _get_relative_positions(
        self, start: datetime, window_size: int
    ) -> RelativePositions:
        """
        Return relative positions of the MovingWindow.

        This method calculates the shifted relative positions of the start
        timestamp, the end timestamps as well as the next position that is
        overwritten in the ringbuffer.
        Shifted in that context means that the positions are moved as close
        assume possible to the oldest sample in the MovingWindow.

        Args:
            start: The start timestamp of the window.
            window_size: The size of the window in samples.

        Returns:
            The relative positions of the start, end and next samples.
        """
        # The number of usable windows can change, when the current position of
        # the ringbuffer is inside one of the windows inside the MovingWindow.
        # Since this is possible, we assume that one window is always not used
        # for the average calculation.
        #
        # We are ignoring either the window that is currently overwritten if
        # the current position is inside that window or the window that would
        # be overwritten next.
        #
        # Move the window to its first appearance in the MovingWindow relative
        # to the oldest sample stored in the MovingWindow.
        #
        # In other words the oldest stored sample is considered to have index 0.
        #
        # Note that the returned value is a index not a timestamp
        rel_start_sample = self._timestamp_to_rel_index(start) % self._period
        rel_end_sample = rel_start_sample + window_size

        # check if the newest time bound, i.e. the sample that is currently written,
        # is inside the interval
        rb_current_position = self._buffer.time_bound_newest
        rel_next_position = (
            self._timestamp_to_rel_index(rb_current_position) + 1
        ) % self._period
        # fix the rel_next_position if modulo period the next position
        # is smaller then the start sample position
        if rel_next_position < rel_start_sample:
            rel_next_position += self._period

        rel_next_position += self._period * (window_size // self._period)

        _logger.debug("current position of the ringbuffer: %s", rb_current_position)
        _logger.debug("relative start_sample: %s", rel_start_sample)
        _logger.debug("relative end_sample: %s", rel_end_sample)
        _logger.debug("relative next_position: %s", rel_next_position)

        return RelativePositions(rel_start_sample, rel_end_sample, rel_next_position)

    def _get_buffer_bounds(
        self, start: datetime, end: datetime
    ) -> tuple[int, int, int]:
        """
        Get the bounds of the ringbuffer used for further operations.

        This method uses the given start and end timestamps to calculate the
        part of the ringbuffer that can be used for further operations, like
        average or min/max.

        Here we cut out the oldest window or the window that is currently
        overwritten in the MovingWindow such that it is not considered in any
        further operation.

        Args:
            start: The start timestamp of the window.
            end: The end timestamp of the window.

        Returns:
            The bounds of the to be used buffer and the window size.

        Raises:
            ValueError: If the start timestamp is after the end timestamp.
        """
        window_size = self._timestamp_to_rel_index(end) - self._timestamp_to_rel_index(
            start
        )
        if window_size <= 0:
            raise ValueError("Start timestamp must be before end timestamp")
        if window_size > self._period:
            raise ValueError(
                "The window size must be smaller or equal than the period."
            )

        rel_pos = self._get_relative_positions(start, window_size)

        if window_size > self._moving_window.count_valid():
            raise ValueError(
                "The window size must be smaller than the size of the `MovingWindow`"
            )

        # shifted distance between the next incoming sample and the start of the window
        dist_to_start = rel_pos.next - rel_pos.start

        # get the start and end position inside the ringbuffer
        end_pos = (
            self._timestamp_to_rel_index(self._buffer.time_bound_newest) + 1
        ) - dist_to_start

        # Note that these check is working since we are using the positions
        # relative to the oldest sample stored in the MovingWindow.
        if rel_pos.start <= rel_pos.next < rel_pos.end:
            # end position is start_position of the window that is currently written
            # that's how end_pos is currently set
            _logger.debug("Next sample will be inside the window time interval!")
        else:
            _logger.debug("Next sample will be outside the window time interval!")
            # end position is start_position of the window that
            # is overwritten next, hence we adding period.
            end_pos += self._period

        # add the offset to the oldest sample in the ringbuffer and wrap around
        # to get the start and end positions in the ringbuffer
        rb_offset = self._buffer.to_internal_index(self._buffer.time_bound_oldest)
        start_pos = self._buffer.wrap(end_pos + self._period + rb_offset)
        end_pos = self._buffer.wrap(end_pos + rb_offset)

        _logger.debug("start_pos in ringbuffer: %s", start_pos)
        _logger.debug("end_pos in ringbuffer: %s", end_pos)

        return (start_pos, end_pos, window_size)

    def _get_reshaped_np_array(
        self, start: datetime, end: datetime
    ) -> tuple[NDArray[np.float64], int]:
        """
        Create a reshaped numpy array from the MovingWindow.

        The reshaped array is a two dimensional array, where one dimension is
        the window_size and the other the number of windows returned by the
        `_get_buffer_bounds` method.

        Args:
            start: The start timestamp of the window.
            end: The end timestamp of the window.

        Returns:
            A tuple containing the reshaped numpy array and the window size.
        """
        (start_pos, end_pos, window_size) = self._get_buffer_bounds(start, end)

        if start_pos >= end_pos:
            window_start = self._buffer[start_pos : self._moving_window.count_valid()]
            window_end = self._buffer[0:end_pos]
            # make the linter happy
            assert isinstance(window_start, np.ndarray)
            assert isinstance(window_end, np.ndarray)
            window_array = np.concatenate((window_start, window_end))
        else:
            window_array = self._buffer[start_pos:end_pos]

        return (self._reshape_np_array(window_array, window_size), window_size)

    def avg(
        self, start: datetime, end: datetime, weights: list[float] | None = None
    ) -> NDArray[np.float64]:
        """
        Create the average window out of the window defined by `start` and `end`.

        This method calculates the average of a window by averaging over all
        windows fully inside the MovingWindow having the period
        `self.period`.

        Args:
            start: The start of the window to average over.
            end: The end of the window to average over.
            weights: The weights to use for the average calculation (oldest first).

        Returns:
            The averaged timeseries window.
        """
        (reshaped, window_size) = self._get_reshaped_np_array(start, end)
        return np.average(  # type: ignore[no-any-return]
            reshaped[:, :window_size], axis=0, weights=weights
        )



================================================
FILE: src/frequenz/sdk/timeseries/_voltage_streamer.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Fetch and stream the phase-to-neutral voltage from a source component.

Each phase of the phase-to-neutral voltage is fetched from the source
component individually and then streamed as a 3-phase sample.
"""

from __future__ import annotations

import asyncio
import logging
from typing import TYPE_CHECKING

from frequenz.channels import Receiver, Sender
from frequenz.client.microgrid import Component, ComponentCategory, ComponentMetricId
from frequenz.quantities import Quantity, Voltage

from .._internal._channels import ChannelRegistry
from ..timeseries._base_types import Sample, Sample3Phase

if TYPE_CHECKING:
    # Imported here to avoid a circular import.
    from ..microgrid._data_sourcing import ComponentMetricRequest

_logger = logging.getLogger(__name__)


class VoltageStreamer:
    """Fetch and stream the phase-to-neutral voltage from a source component.

    Example:
        ```python
        from datetime import timedelta

        from frequenz.sdk import microgrid
        from frequenz.sdk.timeseries import ResamplerConfig2

        await microgrid.initialize(
            "grpc://127.0.0.1:50051",
            ResamplerConfig2(resampling_period=timedelta(seconds=1))
        )

        # Get a receiver for the phase-to-neutral voltage.
        voltage_recv = microgrid.voltage_per_phase().new_receiver()

        async for voltage_sample in voltage_recv:
            print(voltage_sample)
        ```
    """

    def __init__(
        self,
        resampler_subscription_sender: Sender[ComponentMetricRequest],
        channel_registry: ChannelRegistry,
        source_component: Component | None = None,
    ):
        """Initialize the phase-to-neutral voltage streaming.

        Args:
            resampler_subscription_sender: The sender for sending metric
                requests to the resampling actor.
            channel_registry: The channel registry for the phase-to-neutral
                voltage streaming.
            source_component: The source component to receive the
                phase-to-neutral voltage. If None, it fetches the source
                component from the connection manager.

        """
        self._resampler_subscription_sender = resampler_subscription_sender
        """The sender for sending metric requests to the resampling actor."""

        self._channel_registry = channel_registry
        """The channel registry for the phase-to-neutral voltage streaming."""

        from ..microgrid import (  # pylint: disable=import-outside-toplevel
            connection_manager,
        )

        if not source_component:
            component_graph = connection_manager.get().component_graph
            source_component = component_graph.find_first_descendant_component(
                descendant_categories=[
                    ComponentCategory.METER,
                    ComponentCategory.INVERTER,
                    ComponentCategory.EV_CHARGER,
                ],
            )

        self._source_component = source_component
        """The source component to receive the phase-to-neutral voltage."""

        self._task: None | asyncio.Task[None] = None
        """The task for fetching and stream the phase-to-neutral voltage."""

        self._namespace = "microgrid-voltage"
        """The namespace for the phase-to-neutral voltage streaming."""

        self._channel_key = f"{self._namespace}-all-phases"
        """The channel key for the phase-to-neutral voltage streaming."""

    @property
    def source(self) -> Component:
        """Get the component to fetch the phase-to-neutral voltage from.

        Returns:
            The component to fetch the phase-to-neutral voltage from.
        """
        return self._source_component

    def new_receiver(self) -> Receiver[Sample3Phase[Voltage]]:
        """Create a receiver for the phase-to-neutral voltage.

        Each phase of the phase-to-neutral voltage is fetched from the source
        component individually and then streamed as a 3-phase sample.

        Returns:
            A receiver that will receive the phase-to-neutral voltage as a
            3-phase sample.
        """
        receiver = self._channel_registry.get_or_create(
            Sample3Phase[Voltage], self._channel_key
        ).new_receiver()

        if not self._task:
            self._task = asyncio.create_task(self._send_request())
        else:
            _logger.info("Voltage request already sent: %s", self._source_component)

        return receiver

    async def _send_request(self) -> None:
        """Send the request to fetch each voltage phase and stream 3-phase voltage."""
        # Imported here to avoid a circular import.
        from ..microgrid._data_sourcing import (  # pylint: disable=import-outside-toplevel
            ComponentMetricRequest,
        )

        metric_ids = (
            ComponentMetricId.VOLTAGE_PHASE_1,
            ComponentMetricId.VOLTAGE_PHASE_2,
            ComponentMetricId.VOLTAGE_PHASE_3,
        )
        phases_rx: list[Receiver[Sample[Quantity]]] = []
        for metric_id in metric_ids:
            req = ComponentMetricRequest(
                self._namespace, self._source_component.component_id, metric_id, None
            )

            await self._resampler_subscription_sender.send(req)

            phases_rx.append(
                self._channel_registry.get_or_create(
                    Sample[Quantity], req.get_channel_name()
                ).new_receiver()
            )

        sender = self._channel_registry.get_or_create(
            Sample3Phase[Voltage], self._channel_key
        ).new_sender()

        _logger.debug(
            "Sent request for fetching voltage from: %s", self._source_component
        )

        while True:
            try:
                phases = [await r.receive() for r in phases_rx]

                if not all(map(lambda p: p is not None, phases)):
                    _logger.warning(
                        "Received None from voltage request: %s %s",
                        self._source_component,
                        phases,
                    )
                    continue

                msg = Sample3Phase(
                    phases[0].timestamp,
                    *map(
                        lambda p: (
                            Voltage.from_volts(p.value.base_value) if p.value else None
                        ),
                        phases,
                    ),
                )
            except asyncio.CancelledError:
                _logger.exception(
                    "Phase-to-neutral 3-phase voltage streaming task cancelled: %s",
                    self._source_component,
                )
                raise
            await sender.send(msg)



================================================
FILE: src/frequenz/sdk/timeseries/consumer.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""The logical component for calculating high level consumer metrics for a microgrid."""

import uuid

from frequenz.channels import Sender
from frequenz.quantities import Power

from .._internal._channels import ChannelRegistry
from ..microgrid._data_sourcing import ComponentMetricRequest
from .formula_engine import FormulaEngine
from .formula_engine._formula_engine_pool import FormulaEnginePool
from .formula_engine._formula_generators import ConsumerPowerFormula


class Consumer:
    """Calculate high level consumer metrics in a microgrid.

    Under normal circumstances this is expected to correspond to the gross
    consumption of the site excluding active parts and battery.

    Consumer provides methods for fetching power values from different points
    in the microgrid. These methods return `FormulaReceiver` objects, which can
    be used like normal `Receiver`s, but can also be composed to form
    higher-order formula streams.

    !!! note
        `Consumer` instances are not meant to be created directly by users.
        Use the [`microgrid.consumer`][frequenz.sdk.microgrid.consumer] method
        for creating `Consumer` instances.

    Example:
        ```python
        from datetime import timedelta

        from frequenz.sdk import microgrid
        from frequenz.sdk.timeseries import ResamplerConfig2

        await microgrid.initialize(
            "grpc://127.0.0.1:50051",
            ResamplerConfig2(resampling_period=timedelta(seconds=1.0))
        )

        consumer = microgrid.consumer()

        # Get a receiver for a builtin formula
        consumer_power_recv = consumer.power.new_receiver()
        async for consumer_power_sample in consumer_power_recv:
            print(consumer_power_sample)
        ```
    """

    _formula_pool: FormulaEnginePool
    """The formula engine pool to generate consumer metrics."""

    def __init__(
        self,
        channel_registry: ChannelRegistry,
        resampler_subscription_sender: Sender[ComponentMetricRequest],
    ) -> None:
        """Initialize the consumer formula generator.

        Args:
            channel_registry: The channel registry to use for the consumer.
            resampler_subscription_sender: The sender to use for resampler subscriptions.
        """
        namespace = f"consumer-{uuid.uuid4()}"
        self._formula_pool = FormulaEnginePool(
            namespace,
            channel_registry,
            resampler_subscription_sender,
        )

    @property
    def power(self) -> FormulaEngine[Power]:
        """Fetch the consumer power for the microgrid.

        This formula produces values that are in the Passive Sign Convention (PSC).

        It will start the formula engine to calculate consumer power if it is
        not already running.

        A receiver from the formula engine can be created using the
        `new_receiver` method.

        Returns:
            A FormulaEngine that will calculate and stream consumer power.
        """
        engine = self._formula_pool.from_power_formula_generator(
            "consumer_power",
            ConsumerPowerFormula,
        )
        assert isinstance(engine, FormulaEngine)
        return engine

    async def stop(self) -> None:
        """Stop all formula engines."""
        await self._formula_pool.stop()



================================================
FILE: src/frequenz/sdk/timeseries/grid.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Grid connection point.

This module provides the `Grid` type, which represents a grid connection point
in a microgrid.
"""
from __future__ import annotations

import logging
import uuid
from dataclasses import dataclass

from frequenz.channels import Sender
from frequenz.client.microgrid._component import ComponentCategory, ComponentMetricId
from frequenz.quantities import Current, Power, ReactivePower

from .._internal._channels import ChannelRegistry
from ..microgrid import connection_manager
from ..microgrid._data_sourcing import ComponentMetricRequest
from ._fuse import Fuse
from .formula_engine import FormulaEngine, FormulaEngine3Phase
from .formula_engine._formula_engine_pool import FormulaEnginePool
from .formula_engine._formula_generators import (
    GridCurrentFormula,
    GridPower3PhaseFormula,
    GridPowerFormula,
    GridReactivePowerFormula,
)

_logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class Grid:
    """A grid connection point.

    !!! note
        The `Grid` instance is not meant to be created directly by users.
        Use the [`microgrid.grid`][frequenz.sdk.microgrid.grid] method for
        creating or getting the `Grid` instance.

    Example:
        ```python
        from datetime import timedelta

        from frequenz.sdk import microgrid
        from frequenz.sdk.timeseries import ResamplerConfig2

        await microgrid.initialize(
            "grpc://127.0.0.1:50051",
            ResamplerConfig2(resampling_period=timedelta(seconds=1))
        )

        grid = microgrid.grid()

        # Get a receiver for a builtin formula
        grid_power_recv = grid.power.new_receiver()
        async for grid_power_sample in grid_power_recv:
            print(grid_power_sample)
        ```
    """

    fuse: Fuse | None
    """The fuse protecting the grid connection point.

    The rated current of the fuse is set to zero in case of an islanded
    microgrid.
    And the fuse is set to `None` when the grid connection component metadata
    lacks information about the fuse.
    """

    _formula_pool: FormulaEnginePool
    """The formula engine pool to generate grid metrics."""

    @property
    def power(self) -> FormulaEngine[Power]:
        """Fetch the grid power for the microgrid.

        This formula produces values that are in the Passive Sign Convention (PSC).

        If a formula engine to calculate grid power is not already running, it will be
        started.

        A receiver from the formula engine can be created using the `new_receiver`
        method.

        Returns:
            A FormulaEngine that will calculate and stream grid power.
        """
        engine = self._formula_pool.from_power_formula_generator(
            "grid_power",
            GridPowerFormula,
        )
        assert isinstance(engine, FormulaEngine)
        return engine

    @property
    def reactive_power(self) -> FormulaEngine[ReactivePower]:
        """Fetch the grid reactive power for the microgrid.

        This formula produces values that are in the Passive Sign Convention (PSC).

        If a formula engine to calculate grid power is not already running, it will be
        started.

        A receiver from the formula engine can be created using the `new_receiver`
        method.

        Returns:
            A FormulaEngine that will calculate and stream grid reactive power.
        """
        engine = self._formula_pool.from_reactive_power_formula_generator(
            f"grid-{ComponentMetricId.REACTIVE_POWER.value}",
            GridReactivePowerFormula,
        )
        assert isinstance(engine, FormulaEngine)
        return engine

    @property
    def _power_per_phase(self) -> FormulaEngine3Phase[Power]:
        """Fetch the per-phase grid power for the microgrid.

        This formula produces values that are in the Passive Sign Convention (PSC).

        A receiver from the formula engine can be created using the
        `new_receiver`method.

        Returns:
            A FormulaEngine that will calculate and stream grid 3-phase power.
        """
        engine = self._formula_pool.from_power_3_phase_formula_generator(
            "grid_power_3_phase", GridPower3PhaseFormula
        )
        assert isinstance(engine, FormulaEngine3Phase)
        return engine

    @property
    def current_per_phase(self) -> FormulaEngine3Phase[Current]:
        """Fetch the per-phase grid current for the microgrid.

        This formula produces values that are in the Passive Sign Convention (PSC).

        If a formula engine to calculate grid current is not already running, it will be
        started.

        A receiver from the formula engine can be created using the `new_receiver`
        method.

        Returns:
            A FormulaEngine that will calculate and stream grid current.
        """
        engine = self._formula_pool.from_3_phase_current_formula_generator(
            "grid_current",
            GridCurrentFormula,
        )
        assert isinstance(engine, FormulaEngine3Phase)
        return engine

    async def stop(self) -> None:
        """Stop all formula engines."""
        await self._formula_pool.stop()


_GRID: Grid | None = None


def initialize(
    channel_registry: ChannelRegistry,
    resampler_subscription_sender: Sender[ComponentMetricRequest],
) -> None:
    """Initialize the grid connection.

    Args:
        channel_registry: The channel registry instance shared with the
            resampling actor.
        resampler_subscription_sender: The sender for sending metric requests
            to the resampling actor.

    Raises:
        RuntimeError: If there is more than 1 grid connection point in the
            microgrid, or if the grid connection point is not initialized.
    """
    global _GRID  # pylint: disable=global-statement

    grid_connections = list(
        connection_manager.get().component_graph.components(
            component_categories={ComponentCategory.GRID},
        )
    )

    grid_connections_count = len(grid_connections)

    fuse: Fuse | None = None

    match grid_connections_count:
        case 0:
            fuse = Fuse(max_current=Current.zero())
            _logger.info(
                "No grid connection found for this microgrid. "
                "This is normal for an islanded microgrid."
            )
        case 1:
            metadata = grid_connections[0].metadata
            if metadata is None:
                _logger.warning(
                    "Unable to get grid metadata, the grid connection point is "
                    "considered to have no fuse"
                )
            elif metadata.fuse is None:
                _logger.warning("The grid connection point does not have a fuse")
            else:
                fuse = Fuse(max_current=Current.from_amperes(metadata.fuse.max_current))
        case _:
            raise RuntimeError(
                f"Expected at most one grid connection, got {grid_connections_count}"
            )

    namespace = f"grid-{uuid.uuid4()}"
    formula_pool = FormulaEnginePool(
        namespace,
        channel_registry,
        resampler_subscription_sender,
    )

    _GRID = Grid(fuse, formula_pool)


def get() -> Grid:
    """Get the grid connection.

    Note that the rated current of the fuse is set to zero in case of an
    islanded microgrid.
    And the fuse is set to `None` when the grid connection component metadata
    lacks information about the fuse.

    Returns:
        The grid connection.
    """
    assert _GRID, "Grid is not initialized"
    return _GRID



================================================
FILE: src/frequenz/sdk/timeseries/producer.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""The logical component for calculating high level producer metrics for a microgrid."""

import uuid

from frequenz.channels import Sender
from frequenz.quantities import Power

from .._internal._channels import ChannelRegistry
from ..microgrid._data_sourcing import ComponentMetricRequest
from .formula_engine import FormulaEngine
from .formula_engine._formula_engine_pool import FormulaEnginePool
from .formula_engine._formula_generators import ProducerPowerFormula


class Producer:
    """Calculate high level producer metrics in a microgrid.

    Under normal circumstances this is expected to correspond to the gross
    production of the sites active parts excluding EV chargers and batteries.

    Producer provides methods for fetching power values from different points
    in the microgrid. These methods return `FormulaReceiver` objects, which can
    be used like normal `Receiver`s, but can also be composed to form
    higher-order formula streams.

    !!! note
        `Producer` instances are not meant to be created directly by users.
        Use the [`microgrid.producer`][frequenz.sdk.microgrid.producer] method
        for creating `Producer` instances.

    Example:
        ```python
        from datetime import timedelta

        from frequenz.sdk import microgrid
        from frequenz.sdk.timeseries import ResamplerConfig2

        await microgrid.initialize(
            "grpc://127.0.0.1:50051",
            ResamplerConfig2(resampling_period=timedelta(seconds=1.0))
        )

        producer = microgrid.producer()

        # Get a receiver for a builtin formula
        producer_power_recv = producer.power.new_receiver()
        async for producer_power_sample in producer_power_recv:
            print(producer_power_sample)
        ```
    """

    _formula_pool: FormulaEnginePool
    """The formula engine pool to generate producer metrics."""

    def __init__(
        self,
        channel_registry: ChannelRegistry,
        resampler_subscription_sender: Sender[ComponentMetricRequest],
    ) -> None:
        """Initialize the producer formula generator.

        Args:
            channel_registry: The channel registry to use for the producer.
            resampler_subscription_sender: The sender to use for resampler subscriptions.
        """
        namespace = f"producer-{uuid.uuid4()}"
        self._formula_pool = FormulaEnginePool(
            namespace,
            channel_registry,
            resampler_subscription_sender,
        )

    @property
    def power(self) -> FormulaEngine[Power]:
        """Fetch the producer power for the microgrid.

        This formula produces values that are in the Passive Sign Convention (PSC).

        It will start the formula engine to calculate producer power if it is
        not already running.

        A receiver from the formula engine can be created using the
        `new_receiver` method.

        Returns:
            A FormulaEngine that will calculate and stream producer power.
        """
        engine = self._formula_pool.from_power_formula_generator(
            "producer_power",
            ProducerPowerFormula,
        )
        assert isinstance(engine, FormulaEngine)
        return engine

    async def stop(self) -> None:
        """Stop all formula engines."""
        await self._formula_pool.stop()



================================================
FILE: src/frequenz/sdk/timeseries/_resampling/__init__.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Timeseries resampling package."""



================================================
FILE: src/frequenz/sdk/timeseries/_resampling/_base_types.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Resampler base types."""

from collections.abc import AsyncIterator, Callable, Coroutine
from dataclasses import dataclass
from datetime import datetime, timedelta

from frequenz.quantities import Quantity

from .._base_types import Sample

Source = AsyncIterator[Sample[Quantity]]
"""A source for a timeseries.

A timeseries can be received sample by sample in a streaming way
using a source.
"""

Sink = Callable[[Sample[Quantity]], Coroutine[None, None, None]]
"""A sink for a timeseries.

A new timeseries can be generated by sending samples to a sink.

This should be an `async` callable, for example:

```python
async some_sink(Sample) -> None:
    ...
```

Args:
    sample (Sample): A sample to be sent out.
"""


@dataclass
class SourceProperties:
    """Properties of a resampling source."""

    sampling_start: datetime | None = None
    """The time when resampling started for this source.

    `None` means it didn't started yet.
    """

    received_samples: int = 0
    """Total samples received by this source so far."""

    sampling_period: timedelta | None = None
    """The sampling period of this source.

    This means we receive (on average) one sample for this source every
    `sampling_period` time.

    `None` means it is unknown.
    """



================================================
FILE: src/frequenz/sdk/timeseries/_resampling/_config.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Resampler configuration."""

from __future__ import annotations

import logging
import statistics
from collections.abc import Sequence
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Protocol

from frequenz.core.datetime import UNIX_EPOCH

from ._base_types import SourceProperties
from ._wall_clock_timer import WallClockTimerConfig

_logger = logging.getLogger(__name__)


DEFAULT_BUFFER_LEN_INIT = 16
"""Default initial buffer length.

Buffers will be created initially with this length, but they could grow or
shrink depending on the source properties, like sampling rate, to make
sure all the requested past sampling periods can be stored.
"""


DEFAULT_BUFFER_LEN_MAX = 1024
"""Default maximum allowed buffer length.

If a buffer length would get bigger than this, it will be truncated to this
length.
"""


DEFAULT_BUFFER_LEN_WARN = 128
"""Default minimum buffer length that will produce a warning.

If a buffer length would get bigger than this, a warning will be logged.
"""


class ResamplingFunction(Protocol):
    """Combine multiple samples into a new one.

    A resampling function produces a new sample based on a list of pre-existing
    samples. It can do "upsampling" when the data rate of the `input_samples`
    period is smaller than the `resampling_period`, or "downsampling" if it is
    bigger.

    In general, a resampling window is the same as the `resampling_period`, and
    this function might receive input samples from multiple windows in the past to
    enable extrapolation, but no samples from the future (so the timestamp of the
    new sample that is going to be produced will always be bigger than the biggest
    timestamp in the input data).
    """

    def __call__(
        self,
        input_samples: Sequence[tuple[datetime, float]],
        resampler_config: ResamplerConfig,
        source_properties: SourceProperties,
        /,
    ) -> float:
        """Call the resampling function.

        Args:
            input_samples: The sequence of pre-existing samples, where the first item is
                the timestamp of the sample, and the second is the value of the sample.
                The sequence must be non-empty.
            resampler_config: The configuration of the resampler calling this
                function.
            source_properties: The properties of the source being resampled.

        Returns:
            The value of new sample produced after the resampling.
        """
        ...  # pylint: disable=unnecessary-ellipsis


@dataclass(frozen=True)
class ResamplerConfig:
    """Resampler configuration."""

    resampling_period: timedelta
    """The resampling period.

    This is the time it passes between resampled data should be calculated.

    It must be a positive time span.
    """

    max_data_age_in_periods: float = 3.0
    """The maximum age a sample can have to be considered *relevant* for resampling.

    Expressed in number of periods, where period is the `resampling_period`
    if we are downsampling (resampling period bigger than the input period) or
    the *input sampling period* if we are upsampling (input period bigger than
    the resampling period).

    It must be bigger than 1.0.

    Example:
        If `resampling_period` is 3 seconds, the input sampling period is
        1 and `max_data_age_in_periods` is 2, then data older than 3*2
        = 6 seconds will be discarded when creating a new sample and never
        passed to the resampling function.

        If `resampling_period` is 3 seconds, the input sampling period is
        5 and `max_data_age_in_periods` is 2, then data older than 5*2
        = 10 seconds will be discarded when creating a new sample and never
        passed to the resampling function.
    """

    resampling_function: ResamplingFunction = lambda samples, _, __: statistics.fmean(
        s[1] for s in samples
    )
    """The resampling function.

    This function will be applied to the sequence of relevant samples at
    a given time. The result of the function is what is sent as the resampled
    value.
    """

    initial_buffer_len: int = DEFAULT_BUFFER_LEN_INIT
    """The initial length of the resampling buffer.

    The buffer could grow or shrink depending on the source properties,
    like sampling rate, to make sure all the requested past sampling periods
    can be stored.

    It must be at least 1 and at most `max_buffer_len`.
    """

    warn_buffer_len: int = DEFAULT_BUFFER_LEN_WARN
    """The minimum length of the resampling buffer that will emit a warning.

    If a buffer grows bigger than this value, it will emit a warning in the
    logs, so buffers don't grow too big inadvertently.

    It must be at least 1 and at most `max_buffer_len`.
    """

    max_buffer_len: int = DEFAULT_BUFFER_LEN_MAX
    """The maximum length of the resampling buffer.

    Buffers won't be allowed to grow beyond this point even if it would be
    needed to keep all the requested past sampling periods. An error will be
    emitted in the logs if the buffer length needs to be truncated to this
    value.

    It must be at bigger than `warn_buffer_len`.
    """

    align_to: datetime | None = UNIX_EPOCH
    """The time to align the resampling period to.

    The resampling period will be aligned to this time, so the first resampled
    sample will be at the first multiple of `resampling_period` starting from
    `align_to`. It must be an aware datetime and can be in the future too.

    If `align_to` is `None`, the resampling period will be aligned to the
    time the resampler is created.
    """

    def __post_init__(self) -> None:
        """Check that config values are valid.

        Raises:
            ValueError: If any value is out of range.
        """
        if self.resampling_period.total_seconds() < 0.0:
            raise ValueError(
                f"resampling_period ({self.resampling_period}) must be positive"
            )
        if self.max_data_age_in_periods < 1.0:
            raise ValueError(
                f"max_data_age_in_periods ({self.max_data_age_in_periods}) should be at least 1.0"
            )
        if self.warn_buffer_len < 1:
            raise ValueError(
                f"warn_buffer_len ({self.warn_buffer_len}) should be at least 1"
            )
        if self.max_buffer_len <= self.warn_buffer_len:
            raise ValueError(
                f"max_buffer_len ({self.max_buffer_len}) should "
                f"be bigger than warn_buffer_len ({self.warn_buffer_len})"
            )

        if self.initial_buffer_len < 1:
            raise ValueError(
                f"initial_buffer_len ({self.initial_buffer_len}) should at least 1"
            )
        if self.initial_buffer_len > self.max_buffer_len:
            raise ValueError(
                f"initial_buffer_len ({self.initial_buffer_len}) is bigger "
                f"than max_buffer_len ({self.max_buffer_len}), use a smaller "
                "initial_buffer_len or a bigger max_buffer_len"
            )
        if self.initial_buffer_len > self.warn_buffer_len:
            _logger.warning(
                "initial_buffer_len (%s) is bigger than warn_buffer_len (%s)",
                self.initial_buffer_len,
                self.warn_buffer_len,
            )
        if self.align_to is not None and self.align_to.tzinfo is None:
            raise ValueError(
                f"align_to ({self.align_to}) should be a timezone aware datetime"
            )


class ResamplingFunction2(Protocol):
    """Combine multiple samples into a new one.

    A resampling function produces a new sample based on a list of pre-existing
    samples. It can do "upsampling" when the data rate of the `input_samples`
    period is smaller than the `resampling_period`, or "downsampling" if it is
    bigger.

    In general, a resampling window is the same as the `resampling_period`, and
    this function might receive input samples from multiple windows in the past to
    enable extrapolation, but no samples from the future (so the timestamp of the
    new sample that is going to be produced will always be bigger than the biggest
    timestamp in the input data).
    """

    def __call__(
        self,
        input_samples: Sequence[tuple[datetime, float]],
        resampler_config: ResamplerConfig | ResamplerConfig2,
        source_properties: SourceProperties,
        /,
    ) -> float:
        """Call the resampling function.

        Args:
            input_samples: The sequence of pre-existing samples, where the first item is
                the timestamp of the sample, and the second is the value of the sample.
                The sequence must be non-empty.
            resampler_config: The configuration of the resampler calling this
                function.
            source_properties: The properties of the source being resampled.

        Returns:
            The value of new sample produced after the resampling.
        """
        ...  # pylint: disable=unnecessary-ellipsis


@dataclass(frozen=True)
class ResamplerConfig2(ResamplerConfig):
    """Resampler configuration."""

    resampling_period: timedelta
    """The resampling period.

    This is the time it passes between resampled data should be calculated.

    It must be a positive time span.
    """

    max_data_age_in_periods: float = 3.0
    """The maximum age a sample can have to be considered *relevant* for resampling.

    Expressed in number of periods, where period is the `resampling_period`
    if we are downsampling (resampling period bigger than the input period) or
    the *input sampling period* if we are upsampling (input period bigger than
    the resampling period).

    It must be bigger than 1.0.

    Example:
        If `resampling_period` is 3 seconds, the input sampling period is
        1 and `max_data_age_in_periods` is 2, then data older than 3*2
        = 6 seconds will be discarded when creating a new sample and never
        passed to the resampling function.

        If `resampling_period` is 3 seconds, the input sampling period is
        5 and `max_data_age_in_periods` is 2, then data older than 5*2
        = 10 seconds will be discarded when creating a new sample and never
        passed to the resampling function.
    """

    resampling_function: ResamplingFunction2 = lambda samples, _, __: statistics.fmean(
        s[1] for s in samples
    )
    """The resampling function.

    This function will be applied to the sequence of relevant samples at
    a given time. The result of the function is what is sent as the resampled
    value.
    """

    initial_buffer_len: int = DEFAULT_BUFFER_LEN_INIT
    """The initial length of the resampling buffer.

    The buffer could grow or shrink depending on the source properties,
    like sampling rate, to make sure all the requested past sampling periods
    can be stored.

    It must be at least 1 and at most `max_buffer_len`.
    """

    warn_buffer_len: int = DEFAULT_BUFFER_LEN_WARN
    """The minimum length of the resampling buffer that will emit a warning.

    If a buffer grows bigger than this value, it will emit a warning in the
    logs, so buffers don't grow too big inadvertently.

    It must be at least 1 and at most `max_buffer_len`.
    """

    max_buffer_len: int = DEFAULT_BUFFER_LEN_MAX
    """The maximum length of the resampling buffer.

    Buffers won't be allowed to grow beyond this point even if it would be
    needed to keep all the requested past sampling periods. An error will be
    emitted in the logs if the buffer length needs to be truncated to this
    value.

    It must be at bigger than `warn_buffer_len`.
    """

    align_to: datetime | None = field(default=None, init=False)
    """Deprecated: Use timer_config.align_to instead."""

    timer_config: WallClockTimerConfig | None = None
    """The custom configuration of the wall clock timer used to keep track of time.

    If not provided or `None`, a configuration will be created by passing the
    [`resampling_period`][frequenz.sdk.timeseries.ResamplerConfig2.resampling_period] to
    the [`from_interval()`][frequenz.sdk.timeseries.WallClockTimerConfig.from_interval]
    method.
    """

    def __post_init__(self) -> None:
        """Check that config values are valid.

        Raises:
            ValueError: If any value is out of range.
        """
        if self.resampling_period.total_seconds() < 0.0:
            raise ValueError(
                f"resampling_period ({self.resampling_period}) must be positive"
            )
        if self.max_data_age_in_periods < 1.0:
            raise ValueError(
                f"max_data_age_in_periods ({self.max_data_age_in_periods}) should be at least 1.0"
            )
        if self.warn_buffer_len < 1:
            raise ValueError(
                f"warn_buffer_len ({self.warn_buffer_len}) should be at least 1"
            )
        if self.max_buffer_len <= self.warn_buffer_len:
            raise ValueError(
                f"max_buffer_len ({self.max_buffer_len}) should "
                f"be bigger than warn_buffer_len ({self.warn_buffer_len})"
            )

        if self.initial_buffer_len < 1:
            raise ValueError(
                f"initial_buffer_len ({self.initial_buffer_len}) should at least 1"
            )
        if self.initial_buffer_len > self.max_buffer_len:
            raise ValueError(
                f"initial_buffer_len ({self.initial_buffer_len}) is bigger "
                f"than max_buffer_len ({self.max_buffer_len}), use a smaller "
                "initial_buffer_len or a bigger max_buffer_len"
            )
        if self.initial_buffer_len > self.warn_buffer_len:
            _logger.warning(
                "initial_buffer_len (%s) is bigger than warn_buffer_len (%s)",
                self.initial_buffer_len,
                self.warn_buffer_len,
            )
        if self.align_to is not None:
            raise ValueError(
                f"align_to ({self.align_to}) must be specified via timer_config"
            )



================================================
FILE: src/frequenz/sdk/timeseries/_resampling/_exceptions.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Resampler exceptions."""

import asyncio

from ._base_types import Source


class SourceStoppedError(RuntimeError):
    """A timeseries stopped producing samples."""

    def __init__(self, source: Source) -> None:
        """Create an instance.

        Args:
            source: The source of the timeseries that stopped producing samples.
        """
        super().__init__(f"Timeseries stopped producing samples, source: {source}")
        self.source = source
        """The source of the timeseries that stopped producing samples."""

    def __repr__(self) -> str:
        """Return the representation of the instance.

        Returns:
            The representation of the instance.
        """
        return f"{self.__class__.__name__}({self.source!r})"


class ResamplingError(RuntimeError):
    """An Error occurred while resampling.

    This error is a container for errors raised by the underlying sources and
    or sinks.
    """

    def __init__(
        self,
        exceptions: dict[Source, Exception | asyncio.CancelledError],
    ) -> None:
        """Create an instance.

        Args:
            exceptions: A mapping of timeseries source and the exception
                encountered while resampling that timeseries. Note that the
                error could be raised by the sink, while trying to send
                a resampled data for this timeseries, the source key is only
                used to identify the timeseries with the issue, it doesn't
                necessarily mean that the error was raised by the source. The
                underlying exception should provide information about what was
                the actual source of the exception.
        """
        super().__init__(f"Some error were found while resampling: {exceptions}")
        self.exceptions = exceptions
        """A mapping of timeseries source and the exception encountered.

        Note that the error could be raised by the sink, while trying to send
        a resampled data for this timeseries, the source key is only used to
        identify the timeseries with the issue, it doesn't necessarily mean
        that the error was raised by the source. The underlying exception
        should provide information about what was the actual source of the
        exception.
        """

    def __repr__(self) -> str:
        """Return the representation of the instance.

        Returns:
            The representation of the instance.
        """
        return f"{self.__class__.__name__}({self.exceptions=})"



================================================
FILE: src/frequenz/sdk/timeseries/_resampling/_resampler.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Timeseries resampler."""

from __future__ import annotations

import asyncio
import itertools
import logging
import math
from bisect import bisect
from collections import deque
from datetime import datetime, timedelta, timezone
from typing import assert_never

from frequenz.channels.timer import Timer, TriggerAllMissed, _to_microseconds
from frequenz.quantities import Quantity

from ..._internal._asyncio import cancel_and_await
from .._base_types import Sample
from ._base_types import Sink, Source, SourceProperties
from ._config import ResamplerConfig, ResamplerConfig2
from ._exceptions import ResamplingError, SourceStoppedError
from ._wall_clock_timer import TickInfo, WallClockTimer

_logger = logging.getLogger(__name__)


class Resampler:
    """A timeseries resampler.

    In general timeseries [`Source`][frequenz.sdk.timeseries.Source]s don't
    necessarily come at periodic intervals. You can use this class to normalize
    timeseries to produce `Sample`s at regular periodic intervals.

    This class uses
    a [`ResamplingFunction`][frequenz.sdk.timeseries._resampling.ResamplingFunction]
    to produce a new sample from samples received in the past. If there are no
    samples coming to a resampled timeseries for a while, eventually the
    `Resampler` will produce `Sample`s with `None` as value, meaning there is
    no way to produce meaningful samples with the available data.
    """

    def __init__(self, config: ResamplerConfig) -> None:
        """Initialize an instance.

        Args:
            config: The configuration for the resampler. If a `ResamplerConfig2` is
                provided, the resampler will use a
                [`WallClockTimer`][frequenz.sdk.timeseries.WallClockTimer] instead of a
                [`Timer`][frequenz.channels.timer.Timer].
        """
        self._config = config
        """The configuration for this resampler."""

        self._resamplers: dict[Source, _StreamingHelper] = {}
        """A mapping between sources and the streaming helper handling that source."""

        self._timer: Timer | WallClockTimer
        """The timer used to trigger the resampling windows."""

        if isinstance(config, ResamplerConfig2):
            self._timer = WallClockTimer(config.resampling_period, config.timer_config)
            return

        window_end, start_delay_time = self._calculate_window_end()
        self._window_end: datetime = window_end
        """The time in which the current window ends.

        This is used to make sure every resampling window is generated at
        precise times. We can't rely on the timer timestamp because timers will
        never fire at the exact requested time, so if we don't use a precise
        time for the end of the window, the resampling windows we produce will
        have different sizes.

        The window end will also be aligned to the `config.align_to` time, so
        the window end is deterministic.
        """

        self._timer = Timer(config.resampling_period, TriggerAllMissed())
        """The timer used to trigger the resampling windows."""

        # Hack to align the timer, this should be implemented in the Timer class
        self._timer._next_tick_time = _to_microseconds(
            timedelta(seconds=asyncio.get_running_loop().time())
            + config.resampling_period
            + start_delay_time
        )  # pylint: disable=protected-access

    @property
    def config(self) -> ResamplerConfig:
        """Get the resampler configuration.

        Returns:
            The resampler configuration.
        """
        return self._config

    def get_source_properties(self, source: Source) -> SourceProperties:
        """Get the properties of a timeseries source.

        Args:
            source: The source from which to get the properties.

        Returns:
            The timeseries source properties.
        """
        return self._resamplers[source].source_properties

    async def stop(self) -> None:
        """Cancel all receiving tasks."""
        await asyncio.gather(*[helper.stop() for helper in self._resamplers.values()])

    def add_timeseries(self, name: str, source: Source, sink: Sink) -> bool:
        """Start resampling a new timeseries.

        Args:
            name: The name of the timeseries (for logging purposes).
            source: The source of the timeseries to resample.
            sink: The sink to use to send the resampled data.

        Returns:
            `True` if the timeseries was added, `False` if the timeseries was
            not added because there already a timeseries using the provided
            receiver.
        """
        if source in self._resamplers:
            return False

        resampler = _StreamingHelper(
            _ResamplingHelper(name, self._config), source, sink
        )
        self._resamplers[source] = resampler
        return True

    def remove_timeseries(self, source: Source) -> bool:
        """Stop resampling the timeseries produced by `source`.

        Args:
            source: The source of the timeseries to stop resampling.

        Returns:
            `True` if the timeseries was removed, `False` if nothing was
                removed (because the a timeseries with that `source` wasn't
                being resampled).
        """
        try:
            del self._resamplers[source]
        except KeyError:
            return False
        return True

    async def resample(self, *, one_shot: bool = False) -> None:
        """Start resampling all known timeseries.

        This method will run forever unless there is an error while receiving
        from a source or sending to a sink (or `one_shot` is used).

        Args:
            one_shot: Wether the resampling should run only for one resampling
                period.

        Raises:
            ResamplingError: If some timeseries source or sink encounters any
                errors while receiving or sending samples. In this case the
                timer still runs and the timeseries will keep receiving data.
                The user should remove (and re-add if desired) the faulty
                timeseries from the resampler before calling this method
                again).
        """
        # We use a tolerance of 10% of the resampling period
        tolerance = timedelta(
            seconds=self._config.resampling_period.total_seconds() / 10.0
        )

        async for tick_info in self._timer:
            next_tick_time: datetime
            match tick_info:
                case TickInfo():  # WallClockTimer
                    next_tick_time = tick_info.expected_tick_time

                case timedelta() as drift:  # Timer (monotonic)
                    next_tick_time = self._window_end

                    if drift > tolerance:
                        _logger.warning(
                            "The resampling task woke up too late. Resampling should have "
                            "started at %s, but it started at %s (tolerance: %s, "
                            "difference: %s; resampling period: %s)",
                            self._window_end,
                            datetime.now(tz=timezone.utc),
                            tolerance,
                            tick_info,
                            self._config.resampling_period,
                        )

                    self._window_end += self._config.resampling_period

                case unexpected:
                    assert_never(unexpected)

            # We need to make a copy here because we need to match the results to the
            # current resamplers, and since we await here, new resamplers could be added
            # or removed from the dict while we awaiting the resampling, which would
            # cause the results to be out of sync.
            resampler_sources = list(self._resamplers)
            results = await asyncio.gather(
                *[r.resample(next_tick_time) for r in self._resamplers.values()],
                return_exceptions=True,
            )

            exceptions = {
                source: result
                for source, result in zip(resampler_sources, results)
                # CancelledError inherits from BaseException, but we don't want
                # to catch *all* BaseExceptions here.
                if isinstance(result, (Exception, asyncio.CancelledError))
            }
            if exceptions:
                raise ResamplingError(exceptions)
            if one_shot:
                break

    def _calculate_window_end(self) -> tuple[datetime, timedelta]:
        """Calculate the end of the current resampling window.

        The calculated resampling window end is a multiple of
        `self._config.resampling_period` starting at `self._config.align_to`.

        if `self._config.align_to` is `None`, the current time is used.

        If the current time is not aligned to `self._config.resampling_period`, then
        the end of the current resampling window will be more than one period away, to
        make sure to have some time to collect samples if the misalignment is too big.

        Returns:
            A tuple with the end of the current resampling window aligned to
                `self._config.align_to` as the first item and the time we need to
                delay the timer start to make sure it is also aligned.
        """
        now = datetime.now(timezone.utc)
        period = self._config.resampling_period
        align_to = self._config.align_to

        if align_to is None:
            return (now + period, timedelta(0))

        elapsed = (now - align_to) % period

        # If we are already in sync, we don't need to add an extra period
        if not elapsed:
            return (now + period, timedelta(0))

        return (
            # We add an extra period when it is not aligned to make sure we collected
            # enough samples before the first resampling, otherwise the initial window
            # to collect samples could be too small.
            now + period * 2 - elapsed,
            period - elapsed if elapsed else timedelta(0),
        )


class _ResamplingHelper:
    """Keeps track of *relevant* samples to pass them to the resampling function.

    Samples are stored in an internal ring buffer. All collected samples that
    are newer than `max(resampling_period, input_period)
    * max_data_age_in_periods` are considered *relevant* and are passed
    to the provided `resampling_function` when calling the `resample()` method.
    All older samples are discarded.
    """

    def __init__(self, name: str, config: ResamplerConfig) -> None:
        """Initialize an instance.

        Args:
            name: The name of this resampler helper (for logging purposes).
            config: The configuration for this resampler helper.
        """
        self._name = name
        self._config = config
        self._buffer: deque[tuple[datetime, float]] = deque(
            maxlen=config.initial_buffer_len
        )
        self._source_properties: SourceProperties = SourceProperties()

    @property
    def source_properties(self) -> SourceProperties:
        """Return the properties of the source.

        Returns:
            The properties of the source.
        """
        return self._source_properties

    def add_sample(self, sample: tuple[datetime, float]) -> None:
        """Add a new sample to the internal buffer.

        Args:
            sample: The sample to be added to the buffer.
        """
        self._buffer.append(sample)
        if self._source_properties.sampling_start is None:
            self._source_properties.sampling_start = sample[0]
        self._source_properties.received_samples += 1

    def _update_source_sample_period(self, now: datetime) -> bool:
        """Update the source sample period.

        Args:
            now: The datetime in which this update happens.

        Returns:
            Whether the source sample period was changed (was really updated).
        """
        assert (
            self._buffer.maxlen is not None and self._buffer.maxlen > 0
        ), "We need a maxlen of at least 1 to update the sample period"

        config = self._config
        props = self._source_properties

        # We only update it if we didn't before and we have enough data
        if (
            props.sampling_period is not None
            or props.sampling_start is None
            or props.received_samples
            < config.resampling_period.total_seconds() * config.max_data_age_in_periods
            or len(self._buffer) < self._buffer.maxlen
            # There might be a race between the first sample being received and
            # this function being called
            or now <= props.sampling_start
        ):
            return False

        samples_time_delta = now - props.sampling_start
        props.sampling_period = timedelta(
            seconds=samples_time_delta.total_seconds() / props.received_samples
        )

        _logger.debug(
            "New input sampling period calculated for %r: %ss",
            self._name,
            props.sampling_period,
        )
        return True

    def _update_buffer_len(self) -> bool:
        """Update the length of the buffer based on the source properties.

        Returns:
            Whether the buffer length was changed (was really updated).
        """
        # To make type checking happy
        assert self._buffer.maxlen is not None
        assert self._source_properties.sampling_period is not None

        input_sampling_period = self._source_properties.sampling_period

        config = self._config

        new_buffer_len = math.ceil(
            # If we are upsampling, one sample could be enough for
            # back-filling, but we store max_data_age_in_periods for input
            # periods, so resampling functions can do more complex
            # inter/extrapolation if they need to.
            (input_sampling_period.total_seconds() * config.max_data_age_in_periods)
            if input_sampling_period > config.resampling_period
            # If we are downsampling, we want a buffer that can hold
            # max_data_age_in_periods * resampling_period of data, and we one
            # sample every input_sampling_period.
            else (
                config.resampling_period.total_seconds()
                / input_sampling_period.total_seconds()
                * config.max_data_age_in_periods
            )
        )

        new_buffer_len = max(1, new_buffer_len)
        if new_buffer_len > config.max_buffer_len:
            _logger.error(
                "The new buffer length (%s) for timeseries %s is too big, using %s instead",
                new_buffer_len,
                self._name,
                config.max_buffer_len,
            )
            new_buffer_len = config.max_buffer_len
        elif new_buffer_len > config.warn_buffer_len:
            _logger.warning(
                "The new buffer length (%s) for timeseries %s bigger than %s",
                new_buffer_len,
                self._name,
                config.warn_buffer_len,
            )

        if new_buffer_len == self._buffer.maxlen:
            return False

        _logger.debug(
            "New buffer length calculated for %r: %s",
            self._name,
            new_buffer_len,
        )

        self._buffer = deque(self._buffer, maxlen=new_buffer_len)

        return True

    def resample(self, timestamp: datetime) -> Sample[Quantity]:
        """Generate a new sample based on all the current *relevant* samples.

        Args:
            timestamp: The timestamp to be used to calculate the new sample.

        Returns:
            A new sample generated by calling the resampling function with all
                the current *relevant* samples in the internal buffer, if any.
                If there are no *relevant* samples, then the new sample will
                have `None` as `value`.
        """
        if self._update_source_sample_period(timestamp):
            self._update_buffer_len()

        conf = self._config
        props = self._source_properties

        # To see which samples are relevant we need to consider if we are down
        # or upsampling.
        period = (
            max(
                conf.resampling_period,
                props.sampling_period,
            )
            if props.sampling_period is not None
            else conf.resampling_period
        )
        minimum_relevant_timestamp = timestamp - period * conf.max_data_age_in_periods

        min_index = bisect(
            self._buffer,
            minimum_relevant_timestamp,
            key=lambda s: s[0],
        )
        max_index = bisect(self._buffer, timestamp, key=lambda s: s[0])
        # Using itertools for slicing doesn't look very efficient, but
        # experiments with a custom (ring) buffer that can slice showed that
        # it is not that bad. See:
        # https://github.com/frequenz-floss/frequenz-sdk-python/pull/130
        # So if we need more performance beyond this point, we probably need to
        # resort to some C (or similar) implementation.
        relevant_samples = list(itertools.islice(self._buffer, min_index, max_index))
        if not relevant_samples:
            self._log_no_relevant_samples(minimum_relevant_timestamp, timestamp)

        value = (
            conf.resampling_function(relevant_samples, conf, props)
            if relevant_samples
            else None
        )
        return Sample(timestamp, None if value is None else Quantity(value))

    def _log_no_relevant_samples(
        self, minimum_relevant_timestamp: datetime, timestamp: datetime
    ) -> None:
        """Log that no relevant samples were found.

        Args:
            minimum_relevant_timestamp: Minimum timestamp that was requested
            timestamp: Timestamp that was requested
        """
        if not _logger.isEnabledFor(logging.WARNING):
            return

        if self._buffer:
            buffer_info = (
                f"{self._buffer[0][0]} - "
                f"{self._buffer[-1][0]} ({len(self._buffer)} samples)"
            )
        else:
            buffer_info = "Empty"

        _logger.warning(
            "No relevant samples found for: %s\n  Requested: %s - %s\n     Buffer: %s",
            self._name,
            minimum_relevant_timestamp,
            timestamp,
            buffer_info,
        )


class _StreamingHelper:
    """Resample data coming from a source, sending the results to a sink."""

    def __init__(
        self,
        helper: _ResamplingHelper,
        source: Source,
        sink: Sink,
    ) -> None:
        """Initialize an instance.

        Args:
            helper: The helper instance to use to resample incoming data.
            source: The source to use to get the samples to be resampled.
            sink: The sink to use to send the resampled data.
        """
        self._helper: _ResamplingHelper = helper
        self._source: Source = source
        self._sink: Sink = sink
        self._receiving_task: asyncio.Task[None] = asyncio.create_task(
            self._receive_samples()
        )

    @property
    def source_properties(self) -> SourceProperties:
        """Get the source properties.

        Returns:
            The source properties.
        """
        return self._helper.source_properties

    async def stop(self) -> None:
        """Cancel the receiving task."""
        await cancel_and_await(self._receiving_task)

    async def _receive_samples(self) -> None:
        """Pass received samples to the helper.

        This method keeps running until the source stops (or fails with an
        error).
        """
        async for sample in self._source:
            if sample.value is not None and not sample.value.isnan():
                self._helper.add_sample((sample.timestamp, sample.value.base_value))

    # We need the noqa because pydoclint can't figure out that `recv_exception` is an
    # `Exception` instance.
    async def resample(self, timestamp: datetime) -> None:  # noqa: DOC503
        """Calculate a new sample for the passed `timestamp` and send it.

        The helper is used to calculate the new sample and the sender is used
        to send it.

        Args:
            timestamp: The timestamp to be used to calculate the new sample.

        Raises:
            SourceStoppedError: If the source stopped sending samples.
            Exception: if there was any error while receiving from the source
                or sending to the sink.

                If the error was in the source, then this helper will stop
                working, as the internal task to receive samples will stop due
                to the exception. Any subsequent call to `resample()` will keep
                raising the same exception.

                If the error is in the sink, the receiving part will continue
                working while this helper is alive.
        """
        if self._receiving_task.done():
            if recv_exception := self._receiving_task.exception():
                raise recv_exception
            raise SourceStoppedError(self._source)

        await self._sink(self._helper.resample(timestamp))



================================================
FILE: src/frequenz/sdk/timeseries/_resampling/_wall_clock_timer.py
================================================
# License: MIT
# Copyright © 2025 Frequenz Energy-as-a-Service GmbH

"""A timer attached to the wall clock for the resampler."""

from __future__ import annotations

import asyncio
import logging
import math
from collections.abc import Sequence
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Literal, Self, assert_never

from frequenz.channels import Receiver, ReceiverStoppedError
from frequenz.core.datetime import UNIX_EPOCH
from typing_extensions import override

_logger = logging.getLogger(__name__)

_TD_ZERO = timedelta()


@dataclass(frozen=True, kw_only=True)
class WallClockTimerConfig:
    """Configuration for a [wall clock timer][frequenz.sdk.timeseries.WallClockTimer]."""

    align_to: datetime | None = UNIX_EPOCH
    """The time to align the timer to.

    The first timer tick will occur at the first multiple of the timer's interval after
    this value.

    It must be a timezone aware `datetime` or `None`. If `None`, the timer aligns to the
    time it is started.
    """

    async_drift_tolerance: timedelta | None = None
    """The maximum allowed difference between the requested and the real sleep time.

    The timer will emit a warning if the difference is bigger than this value.

    It must be bigger than 0 or `None`. If `None`, no warnings will ever be emitted.
    """

    wall_clock_drift_tolerance_factor: float | None = None
    """The maximum allowed relative difference between the wall clock and monotonic time.

    The timer will emit a warning if the relative difference is bigger than this value.
    If the difference remains constant, the warning will be emitted only once, as the
    previous drift is taken into account. If there is information on the previous drift,
    the previous and current factor will be used to determine if a warning should be
    emitted.

    It must be bigger than 0 or `None`. If `None`, no warnings will be ever emitted.

    Info:
        The calculation is as follows:

        ```py
        tolerance = wall_clock_drift_tolerance_factor
        factor = monotonic_elapsed / wall_clock_elapsed
        previous_factor = previous_monotonic_elapsed / previous_wall_clock_elapsed
        if abs(factor - previous_factor) > tolerance:
            emit warning
        ```

        If there is no previous information, a `previous_factor` of 1.0 will be used.
    """

    wall_clock_jump_threshold: timedelta | None = None
    """The amount of time that's considered a wall clock jump.

    When the drift between the wall clock and monotonic time is too big, it is
    considered a time jump and the timer will be resynced to the wall clock.

    This value determines how big the difference needs to be to be considered a
    jump.

    Smaller values are considered wall clock *expansions* or *compressions* and are
    always gradually adjusted, instead of triggering a resync.

    Must be bigger than 0 or `None`. If `None`, a resync will never be triggered due to
    time jumps.
    """

    def __post_init__(self) -> None:
        """Check that config values are valid.

        Raises:
            ValueError: If any value is out of range.
        """
        if self.align_to is not None and self.align_to.tzinfo is None:
            raise ValueError(
                f"align_to ({self.align_to}) should be a timezone aware datetime"
            )

        def _is_strictly_positive_or_none(value: float | timedelta | None) -> bool:
            match value:
                case None:
                    return True
                case timedelta() as delta:
                    return delta > _TD_ZERO
                case float() as num:
                    return math.isfinite(num) and num > 0.0
                case int() as num:
                    return num > 0
                case _ as unknown:
                    assert_never(unknown)

        if not _is_strictly_positive_or_none(self.async_drift_tolerance):
            raise ValueError(
                "async_drift_tolerance should be positive or None, not "
                f"{self.async_drift_tolerance!r}"
            )
        if not _is_strictly_positive_or_none(self.wall_clock_drift_tolerance_factor):
            raise ValueError(
                "wall_clock_drift_tolerance_factor should be positive or None, not "
                f"{self.wall_clock_drift_tolerance_factor!r}"
            )
        if not _is_strictly_positive_or_none(self.wall_clock_jump_threshold):
            raise ValueError(
                "wall_clock_jump_threshold should be positive or None, not "
                f"{self.wall_clock_jump_threshold!r}"
            )

    @classmethod
    def from_interval(  # pylint: disable=too-many-arguments
        cls,
        interval: timedelta,
        *,
        align_to: datetime | None = UNIX_EPOCH,
        async_drift_tolerance_factor: float = 0.1,
        wall_clock_drift_tolerance_factor: float = 0.1,
        wall_clock_jump_threshold_factor: float = 1.0,
    ) -> Self:
        """Create a timer configuration based on an interval.

        This will set the tolerance and threshold values proportionally to the interval.

        Args:
            interval: The interval between timer ticks. Must be bigger than 0.
            align_to: The time to align the timer to. See the
                [`WallClockTimer`][frequenz.sdk.timeseries.WallClockTimer] documentation
                for details.
            async_drift_tolerance_factor: The maximum allowed difference between the
                requested and the real sleep time, relative to the interval.
                `async_drift_tolerance` will be set to `interval * this_factor`.  See
                the [`WallClockTimer`][frequenz.sdk.timeseries.WallClockTimer]
                documentation for details.
            wall_clock_drift_tolerance_factor: The maximum allowed relative difference
                between the wall clock and monotonic time. See the
                [`WallClockTimer`][frequenz.sdk.timeseries.WallClockTimer] documentation
                for details.
            wall_clock_jump_threshold_factor: The amount of time that's considered a
                wall clock jump, relative to the interval. This will be set to
                `interval * this_factor`. See the
                [`WallClockTimer`][frequenz.sdk.timeseries.WallClockTimer] documentation
                for details.

        Returns:
            The created timer configuration.

        Raises:
            ValueError: If any value is out of range.
        """
        if interval <= _TD_ZERO:
            raise ValueError(f"interval must be bigger than 0, not {interval!r}")

        return cls(
            align_to=align_to,
            wall_clock_drift_tolerance_factor=wall_clock_drift_tolerance_factor,
            async_drift_tolerance=interval * async_drift_tolerance_factor,
            wall_clock_jump_threshold=interval * wall_clock_jump_threshold_factor,
        )


@dataclass(frozen=True, kw_only=True)
class ClocksInfo:
    """Information about the wall clock and monotonic clock and their drift.

    The `monotonic_requested_sleep` and `monotonic_elapsed` values must be strictly
    positive, while the `wall_clock_elapsed` can be negative if the wall clock jumped
    back in time.
    """

    monotonic_requested_sleep: timedelta
    """The requested monotonic sleep time used to gather the information (must be positive)."""

    monotonic_time: float
    """The monotonic time right after the sleep was done."""

    wall_clock_time: datetime
    """The wall clock time right after the sleep was done."""

    monotonic_elapsed: timedelta
    """The elapsed time in monotonic time (must be non-negative)."""

    wall_clock_elapsed: timedelta
    """The elapsed time in wall clock time."""

    wall_clock_factor: float = float("nan")
    """The factor to convert wall clock time to monotonic time.

    Typically, if the wall clock time expanded compared to the monotonic time (i.e.
    is more in the future), the returned value will be smaller than 1. If the wall
    clock time compressed compared to the monotonic time (i.e. is more in the past),
    the returned value will be bigger than 1.

    In cases where there are big time jumps this might be overridden by the previous
    wall clock factor to avoid adjusting by excessive amounts, when the time will
    resync anyway to catch up.
    """

    def __post_init__(self) -> None:
        """Check that the values are valid.

        Raises:
            ValueError: If any value is out of range.
        """
        if self.monotonic_requested_sleep <= _TD_ZERO:
            raise ValueError(
                f"monotonic_requested_sleep must be strictly positive, not "
                f"{self.monotonic_requested_sleep!r}"
            )
        if not math.isfinite(self.monotonic_time):
            raise ValueError(
                f"monotonic_time must be a number, not {self.monotonic_time!r}"
            )
        if self.monotonic_elapsed <= _TD_ZERO:
            raise ValueError(
                f"monotonic_elapsed must be strictly positive, not {self.monotonic_elapsed!r}"
            )

        # This is a hack to cache the calculated value, once set it will be "immutable"
        # too, so it shouldn't change the logical "frozenness" of the class.
        if math.isnan(self.wall_clock_factor):
            wall_clock_elapsed = self.wall_clock_elapsed
            if wall_clock_elapsed <= _TD_ZERO:
                _logger.warning(
                    "The monotonic clock advanced %s, but the wall clock stayed still or "
                    "jumped back (elapsed: %s)! Hopefully this was just a singular jump in "
                    "time and not a permanent issue with the wall clock not moving at all. "
                    "For purposes of calculating the wall clock factor, a fake elapsed time "
                    "of one tenth of the elapsed monotonic time will be used.",
                    self.monotonic_elapsed,
                    wall_clock_elapsed,
                )
                wall_clock_elapsed = self.monotonic_elapsed * 0.1
            # We need to use __setattr__ here to bypass the frozen nature of the
            # dataclass. Since we are constructing the class, this is fine and the only
            # way to set calculated defaults in frozen dataclasses at the moment.
            super().__setattr__(
                "wall_clock_factor", self.monotonic_elapsed / wall_clock_elapsed
            )

    @property
    def monotonic_drift(self) -> timedelta:
        """The difference between the monotonic elapsed and requested sleep time.

        This number should be always positive, as the monotonic time should never
        jump back in time.
        """
        return self.monotonic_elapsed - self.monotonic_requested_sleep

    @property
    def wall_clock_jump(self) -> timedelta:
        """The amount of time the wall clock jumped compared to the monotonic time.

        If the wall clock is faster then the monotonic time (or jumped forward in time),
        the returned value will be positive. If the wall clock is slower than the
        monotonic time (or jumped backwards in time), the returned value will be
        negative.

        Note:
            Strictly speaking, both could be in sync and the result would be 0.0, but
            this is extremely unlikely due to floating point precision and the fact
            that both clocks are obtained as slightly different times.
        """
        return self.wall_clock_elapsed - self.monotonic_elapsed

    def wall_clock_to_monotonic(self, wall_clock_timedelta: timedelta, /) -> timedelta:
        """Convert a wall clock timedelta to a monotonic timedelta.

        This is useful to calculate how much one should sleep on the monotonic clock
        to reach a particular wall clock time, adjusting to the difference in speed
        or jumps between both.

        Args:
            wall_clock_timedelta: The wall clock timedelta to convert.

        Returns:
            The monotonic timedelta corresponding to `wall_clock_time` using the
                `wall_clock_factor`.
        """
        return wall_clock_timedelta * self.wall_clock_factor


@dataclass(frozen=True, kw_only=True)
class TickInfo:
    """Information about a `WallClockTimer` tick."""

    expected_tick_time: datetime
    """The expected time when the timer should have triggered."""

    sleep_infos: Sequence[ClocksInfo]
    """The information about every sleep performed to trigger this tick.

    If the timer didn't have do to a [`sleep()`][asyncio.sleep] to trigger the tick
    (i.e. the timer is catching up because there were big drifts in previous ticks),
    this will be empty.
    """

    @property
    def latest_sleep_info(self) -> ClocksInfo | None:
        """The clocks information from the last sleep done to trigger this tick.

        If no sleeps were done, this will be `None`.
        """
        return self.sleep_infos[-1] if self.sleep_infos else None


class WallClockTimer(Receiver[TickInfo]):
    """A timer synchronized with the wall clock.

    This timer uses the wall clock to trigger ticks and handles discrepancies between
    the wall clock and monotonic time. Since sleeping is performed using monotonic time,
    differences between the two clocks can occur.

    When the wall clock progresses slower than monotonic time, it is referred to as
    *compression* (wall clock time appears in the past relative to monotonic time).
    Conversely, when the wall clock progresses faster, it is called *expansion*
    (wall clock time appears in the future relative to monotonic time). If these
    differences exceed a configured threshold, a warning is emitted. The threshold
    is defined by the
    [`wall_clock_drift_tolerance_factor`][frequenz.sdk.timeseries.WallClockTimerConfig.wall_clock_drift_tolerance_factor].

    If the difference becomes excessively large, it is treated as a *time jump*.
    Time jumps can occur, for example, when the wall clock is adjusted by NTP after
    being out of sync for an extended period. In such cases, the timer resynchronizes
    with the wall clock and triggers an immediate tick. The threshold for detecting
    time jumps is controlled by the
    [`wall_clock_jump_threshold`][frequenz.sdk.timeseries.WallClockTimerConfig.wall_clock_jump_threshold].

    The timer ensures ticks are aligned to the
    [`align_to`][frequenz.sdk.timeseries.WallClockTimerConfig.align_to] configuration,
    even after time jumps.

    Additionally, the timer emits warnings if the actual sleep duration deviates
    significantly from the requested duration. This can happen due to event loop
    blocking or system overload. The tolerance for such deviations is defined by the
    [`async_drift_tolerance`][frequenz.sdk.timeseries.WallClockTimerConfig.async_drift_tolerance].

    To account for these complexities, each tick provides a
    [`TickInfo`][frequenz.sdk.timeseries.TickInfo] object, which includes detailed
    information about the clocks and their drift.
    """

    def __init__(
        self,
        interval: timedelta,
        config: WallClockTimerConfig | None = None,
        *,
        auto_start: bool = True,
    ) -> None:
        """Initialize this timer.

        See the class documentation for details.

        Args:
            interval: The time between timer ticks. Must be positive.
            config: The configuration for the timer. If `None`, a default configuration
                will be created using `from_interval()`.
            auto_start: Whether the timer should start automatically. If `False`,
                `reset()` must be called before the timer can be used.

        Raises:
            ValueError: If any value is out of range.
        """
        if interval <= _TD_ZERO:
            raise ValueError(f"interval must be positive, not {interval}")

        self._interval: timedelta = interval
        """The time to between timer ticks.

        The wall clock is used, so this will be added to the current time to calculate
        the next tick time.
        """

        self._config = config or WallClockTimerConfig.from_interval(interval)
        """The configuration for this timer."""

        self._closed: bool = True
        """Whether the timer was requested to close.

        If this is `False`, then the timer is running.

        If this is `True`, then it is closed or there is a request to close it
        or it was not started yet:

        * If `_next_tick_time` is `None`, it means it wasn't started yet (it was
          created with `auto_start=False`).  Any receiving method will start
          it by calling `reset()` in this case.

        * If `_next_tick_time` is not `None`, it means there was a request to
          close it.  In this case receiving methods will raise
          a `ReceiverStoppedError`.
        """

        self._next_tick_time: datetime | None = None
        """The wall clock time when the next tick should happen.

        If this is `None`, it means the timer didn't start yet, but it should
        be started as soon as it is used.
        """

        self._current_tick_info: TickInfo | None = None
        """The current tick information.

        This is calculated by `ready()` but is returned by `consume()`. If
        `None` it means `ready()` wasn't called and `consume()` will assert.
        `consume()` will set it back to `None` to tell `ready()` that it needs
        to wait again.
        """

        self._clocks_info: ClocksInfo | None = None
        """The latest information about the clocks and their drift.

        Or `None` if no sleeps were done yet.
        """

        if auto_start:
            self.reset()

    @property
    def interval(self) -> timedelta:
        """The interval between timer ticks.

        Since the wall clock is used, this will be added to the current time to
        calculate the next tick time.

        Danger:
            In real (monotonic) time, the actual time it passes between ticks could be
            smaller, bigger, or even **negative** if the wall clock jumped back in time!
        """
        return self._interval

    @property
    def config(self) -> WallClockTimerConfig:
        """The configuration for this timer."""
        return self._config

    @property
    def is_running(self) -> bool:
        """Whether the timer is running."""
        return not self._closed

    @property
    def next_tick_time(self) -> datetime | None:
        """The wall clock time when the next tick should happen, or `None` if it is not running."""
        return None if self._closed else self._next_tick_time

    def reset(self) -> None:
        """Reset the timer to start timing from now (plus an optional alignment).

        If the timer was closed, or not started yet, it will be started.
        """
        self._closed = False
        self._update_next_tick_time()
        self._current_tick_info = None
        # We assume the clocks will behave similarly after the timer was reset, so we
        # purposefully don't reset the clocks info.
        _logger.debug("reset(): _next_tick_time=%s", self._next_tick_time)

    @override
    def close(self) -> None:
        """Close and stop the timer.

        Once `close` has been called, all subsequent calls to `ready()` will immediately
        return False and calls to `consume()` / `receive()` or any use of the async
        iterator interface will raise a
        [`ReceiverStoppedError`][frequenz.channels.ReceiverStoppedError].

        You can restart the timer with `reset()`.
        """
        self._closed = True
        # We need to make sure it's not None, otherwise `ready()` will start it
        self._next_tick_time = datetime.now(timezone.utc)

    def _should_resync(self, info: ClocksInfo | timedelta | None) -> bool:
        """Check if the timer needs to resynchronize with the wall clock.

        This checks if the wall clock jumped beyond the configured threshold, which
        is defined in the timer configuration.

        Args:
            info: The information about the clocks and their drift. If `None`, it will
                not check for a resync, and will return `False`. If it is a
                `ClocksInfo`, it will check the `wall_clock_jump` property. If it is a
                `timedelta`, it will check if the absolute value is greater than the
                configured threshold.

        Returns:
            Whether the timer should resync to the wall clock.
        """
        threshold = self._config.wall_clock_jump_threshold
        if threshold is None or info is None:
            return False
        if isinstance(info, ClocksInfo):
            info = info.wall_clock_jump
        return abs(info) > threshold

    # We need to disable too many branches here, because the method is too complex but
    # it is not trivial to split into smaller parts.
    @override
    async def ready(self) -> bool:  # pylint: disable=too-many-branches
        """Wait until the timer `interval` passed.

        Once a call to `ready()` has finished, the resulting tick information
        must be read with a call to `consume()` (`receive()` or iterated over)
        to tell the timer it should wait for the next interval.

        The timer will remain ready (this method will return immediately)
        until it is consumed.

        Returns:
            Whether the timer was started and it is still running.
        """
        # If there are messages waiting to be consumed, return immediately.
        if self._current_tick_info is not None:
            return True

        # If `_next_tick_time` is `None`, it means it was created with
        # `auto_start=True` and should be started.
        if self._next_tick_time is None:
            self.reset()
            assert (
                self._next_tick_time is not None
            ), "This should be assigned by reset()"

        # If a close was explicitly requested, we bail out.
        if self._closed:
            return False

        wall_clock_now = datetime.now(timezone.utc)
        wall_clock_time_to_next_tick = self._next_tick_time - wall_clock_now

        # If we didn't reach the tick yet, sleep until we do.
        # We need to do this in a loop to react to resets, time jumps and wall clock
        # time compression, in which cases we need to recalculate the time to the next
        # tick and try again.
        sleeps: list[ClocksInfo] = []
        should_resync: bool = self._should_resync(self._clocks_info)
        while wall_clock_time_to_next_tick > _TD_ZERO:
            prev_clocks_info = self._clocks_info
            # We don't assign directly to self._clocks_info because its type is
            # ClocksInfo | None, and sleep() returns ClocksInfo, so we can avoid some
            # None checks further in the code with `clocks_info` (and we make the code
            # more succinct).
            clocks_info = await self._sleep(
                wall_clock_time_to_next_tick, prev_clocks_info=prev_clocks_info
            )
            should_resync = self._should_resync(clocks_info)
            wall_clock_now = datetime.now(timezone.utc)
            self._clocks_info = clocks_info

            sleeps.append(clocks_info)

            if previous_factor := self._has_drifted_beyond_tolerance(
                new_clocks_info=clocks_info, prev_clocks_info=prev_clocks_info
            ):
                # If we are resyncing we have a different issue, and we are not going to
                # use the factor to adjust the clock, but will just resync
                if not should_resync:
                    _logger.warning(
                        "The wall clock time drifted too much from the monotonic time. The "
                        "monotonic time will be adjusted to compensate for this difference. "
                        "We expected the wall clock time to have advanced (%s), but the "
                        "monotonic time advanced (%s) [previous_factor=%s "
                        "current_factor=%s, factor_change_absolute_tolerance=%s].",
                        clocks_info.wall_clock_elapsed,
                        clocks_info.monotonic_elapsed,
                        previous_factor,
                        clocks_info.wall_clock_factor,
                        self._config.wall_clock_drift_tolerance_factor,
                    )

            wall_clock_time_to_next_tick = self._next_tick_time - wall_clock_now

            # Technically the monotonic drift should always be positive, but we handle
            # negative values just in case, we've seen a lot of weird things happen.
            monotonic_drift = abs(clocks_info.monotonic_drift)
            drift_tolerance = self._config.async_drift_tolerance
            if drift_tolerance is not None and monotonic_drift > drift_tolerance:
                _logger.warning(
                    "The timer was supposed to sleep for %s, but it slept for %s "
                    "instead [difference=%s, tolerance=%s]. This is likely due to a "
                    "task taking too much time to complete and blocking the event "
                    "loop for too long. You probably should profile your code to "
                    "find out what's taking too long.",
                    clocks_info.monotonic_requested_sleep,
                    clocks_info.monotonic_elapsed,
                    monotonic_drift,
                    drift_tolerance,
                )

            # If we detect a time jump, we exit the loop and handle it outside of it, to
            # also account for time jumps in the past that could happen without even
            # having entered into the sleep loop.
            if should_resync:
                _logger.debug(
                    "ready(): Exiting the wait loop because we detected a time jump "
                    "and need to re-sync."
                )
                break

            if _logger.isEnabledFor(logging.DEBUG):
                _logger.debug(
                    "ready(): In sleep loop:\n"
                    "    next_tick_time=%s (%s)\n"
                    "    now=%s (%s)\n"
                    "    mono_now=%s\n"
                    "    wall_clock_time_to_next_tick=%s (%s)",
                    self._next_tick_time,
                    self._next_tick_time.timestamp(),
                    wall_clock_now,
                    wall_clock_now.timestamp(),
                    asyncio.get_running_loop().time(),
                    wall_clock_time_to_next_tick,
                    wall_clock_time_to_next_tick.total_seconds(),
                )

        # If there was a time jump, we need to resync the timer to the wall clock,
        # otherwise we can be sleeping for a long time until the timer catches up,
        # which is not suitable for many use cases.
        #
        # Resyncing the timer ensures that we keep ticking more or less at `interval`
        # even in the event of time jumps, with the downside that the timer will
        # trigger more than once for the same timestamp if it jumps back in time,
        # and will skip ticks if it jumps forward in time.
        #
        # When there is no threshold, so there is no resync, the ticks will be
        # contigous in time from the wall clock perspective, waiting until we reach
        # the expected next tick time when jumping back in time, and bursting all
        # missed ticks when jumping forward in time.
        if should_resync:
            assert self._clocks_info is not None
            _logger.warning(
                "The wall clock jumped %s (%s seconds) in time (threshold=%s). "
                "A tick will be triggered immediately with the `expected_tick_time` "
                "as it was before the time jump and the timer will be resynced to "
                "the wall clock.",
                self._clocks_info.wall_clock_jump,
                self._clocks_info.wall_clock_jump.total_seconds(),
                self._config.wall_clock_jump_threshold,
            )

        # If a close was explicitly requested during the sleep, we bail out.
        if self._closed:
            return False

        self._current_tick_info = TickInfo(
            expected_tick_time=self._next_tick_time, sleep_infos=sleeps
        )

        if should_resync:
            _logger.debug(
                "ready(): Before resync:\n"
                "    next_tick_time=%s\n"
                "    now=%s\n"
                "    wall_clock_time_to_next_tick=%s",
                self._next_tick_time,
                wall_clock_now,
                wall_clock_time_to_next_tick,
            )
            self._update_next_tick_time(now=wall_clock_now)
            _logger.debug(
                "ready(): After resync: next_tick_time=%s", self._next_tick_time
            )
        else:
            self._next_tick_time += self._interval
            _logger.debug(
                "ready(): No resync needed: next_tick_time=%s",
                self._next_tick_time,
            )

        return True

    @override
    def consume(self) -> TickInfo:
        """Return the latest tick information once `ready()` is complete.

        Once the timer has triggered
        ([`ready()`][frequenz.sdk.timeseries.WallClockTimer.ready] is done), this method
        returns the information about the tick that just happened.

        Returns:
            The information about the tick that just happened.

        Raises:
            ReceiverStoppedError: If the timer was closed via `close()`.
        """
        # If it was closed and there it no pending result, we raise
        # (if there is a pending result, then we still want to return it first)
        if self._closed and self._current_tick_info is None:
            raise ReceiverStoppedError(self)

        assert (
            self._current_tick_info is not None
        ), "calls to `consume()` must be follow a call to `ready()`"
        info = self._current_tick_info
        self._current_tick_info = None
        return info

    def _update_next_tick_time(self, *, now: datetime | None = None) -> None:
        """Update the next tick time, aligning it to `self._align_to` or now."""
        if now is None:
            now = datetime.now(timezone.utc)

        elapsed = _TD_ZERO

        if self._config.align_to is not None:
            elapsed = (now - self._config.align_to) % self._interval

        self._next_tick_time = now + self._interval - elapsed

    def _has_drifted_beyond_tolerance(
        self, *, new_clocks_info: ClocksInfo, prev_clocks_info: ClocksInfo | None
    ) -> float | Literal[False]:
        """Check if the wall clock drifted beyond the configured tolerance.

        This checks the relative difference between the wall clock and monotonic time
        based on the `wall_clock_drift_tolerance_factor` configuration.

        Args:
            new_clocks_info: The information about the clocks and their drift from the
                current sleep.
            prev_clocks_info: The information about the clocks and their drift from the
                previous sleep. If `None`, the previous factor will be considered 1.0.

        Returns:
            The previous wall clock factor if the drift is beyond the tolerance, or
                `False` if it is not.
        """
        tolerance = self._config.wall_clock_drift_tolerance_factor
        if tolerance is None:
            return False

        previous_factor = (
            prev_clocks_info.wall_clock_factor if prev_clocks_info else 1.0
        )
        current_factor = new_clocks_info.wall_clock_factor
        if abs(current_factor - previous_factor) > tolerance:
            return previous_factor
        return False

    async def _sleep(
        self, delay: timedelta, /, *, prev_clocks_info: ClocksInfo | None
    ) -> ClocksInfo:
        """Sleep for a given time and return information about the clocks and their drift.

        The time to sleep is adjusted based on the previously observed drift between the
        wall clock and monotonic time, if any.

        Also saves the information about the clocks and their drift for the next sleep.

        Args:
            delay: The time to sleep. Will be adjusted based on `prev_clocks_info` if
                available.
            prev_clocks_info: The information about the clocks and their drift from the
                previous sleep. If `None`, the sleep will be done as requested, without
                adjusting the time to sleep.

        Returns:
            The information about the clocks and their drift for this sleep.
        """
        if prev_clocks_info is not None:
            _logger.debug(
                "_sleep(): Adjusted original requested delay (%s) with factor %s",
                delay.total_seconds(),
                prev_clocks_info.wall_clock_factor,
            )
            delay = prev_clocks_info.wall_clock_to_monotonic(delay)

        delay_s = delay.total_seconds()

        _logger.debug("_sleep(): Will sleep for %s seconds", delay_s)
        start_monotonic_time = asyncio.get_running_loop().time()
        start_wall_clock_time = datetime.now(timezone.utc)
        await asyncio.sleep(delay_s)

        end_monotonic_time = asyncio.get_running_loop().time()
        end_wall_clock_time = datetime.now(timezone.utc)

        elapsed_monotonic = timedelta(seconds=end_monotonic_time - start_monotonic_time)
        elapsed_wall_clock = end_wall_clock_time - start_wall_clock_time

        wall_clock_jump = elapsed_wall_clock - elapsed_monotonic
        should_resync = self._should_resync(wall_clock_jump)
        _logger.debug("_sleep(): SHOULD RESYNC? %s", should_resync)
        clocks_info = ClocksInfo(
            monotonic_requested_sleep=delay,
            monotonic_time=end_monotonic_time,
            wall_clock_time=end_wall_clock_time,
            monotonic_elapsed=elapsed_monotonic,
            wall_clock_elapsed=elapsed_wall_clock,
            # If we should resync it means there was a big time jump, which should be
            # exceptional (NTP adjusting the clock or something like that), in this case
            # we want to use the previous factor as the current one will be way off.
            wall_clock_factor=(
                prev_clocks_info.wall_clock_factor
                if prev_clocks_info and should_resync
                else float("nan")  # nan means let ClocksInfo calculate it
            ),
        )
        _logger.debug(
            "_sleep(): After sleeping:\n"
            "    monotonic_requested_sleep=%s\n"
            "    monotonic_time=%s\n"
            "    wall_clock_time=%s\n"
            "    monotonic_elapsed=%s\n"
            "    wall_clock_elapsed=%s\n"
            "    factor=%s\n",
            clocks_info.monotonic_requested_sleep,
            clocks_info.monotonic_time,
            clocks_info.wall_clock_time,
            clocks_info.monotonic_elapsed,
            clocks_info.wall_clock_elapsed,
            clocks_info.wall_clock_factor,
        )

        return clocks_info

    def __str__(self) -> str:
        """Return a string representation of this timer."""
        return f"{type(self).__name__}({self.interval})"

    def __repr__(self) -> str:
        """Return a string representation of this timer."""
        next_tick = (
            ""
            if self._next_tick_time is None
            else f", next_tick_time={self._next_tick_time!r}"
        )
        return (
            f"{type(self).__name__}<interval={self.interval!r}, "
            f"is_running={self.is_running!r}{next_tick}>"
        )



================================================
FILE: src/frequenz/sdk/timeseries/_ringbuffer/__init__.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Ringbuffer implementation & utilities."""

from .buffer import Gap, OrderedRingBuffer
from .serialization import dump, load

__all__ = ["OrderedRingBuffer", "Gap", "load", "dump"]



================================================
FILE: src/frequenz/sdk/timeseries/_ringbuffer/buffer.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Ringbuffer implementation with focus on time & memory efficiency."""


from copy import deepcopy
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Generic, SupportsIndex, TypeVar, overload

import numpy as np
import numpy.typing as npt
from frequenz.core.datetime import UNIX_EPOCH

from .._base_types import QuantityT, Sample

FloatArray = TypeVar("FloatArray", list[float], npt.NDArray[np.float64])
"""Type variable of the buffer container."""


@dataclass
class Gap:
    """A gap defines the range for which we haven't received values yet."""

    start: datetime
    """Start timestamp of the range, inclusive."""
    end: datetime
    """End timestamp of the range, exclusive."""

    def contains(self, timestamp: datetime) -> bool:
        """Check if a given timestamp is inside this gap.

        Args:
            timestamp: Timestamp to check.

        Returns:
            True if the timestamp is in the gap.
        """
        if self.start <= timestamp < self.end:
            return True

        return False


class OrderedRingBuffer(Generic[FloatArray]):
    """Time aware ringbuffer that keeps its entries sorted by time."""

    _TIMESTAMP_MIN = datetime.min.replace(tzinfo=timezone.utc)
    """The minimum representable timestamp."""

    _TIMESTAMP_MAX = datetime.max.replace(tzinfo=timezone.utc)
    """The maximum representable timestamp."""

    def __init__(
        self,
        buffer: FloatArray,
        sampling_period: timedelta,
        align_to: datetime = UNIX_EPOCH,
    ) -> None:
        """Initialize the time aware ringbuffer.

        Args:
            buffer: Instance of a buffer container to use internally.
            sampling_period: Timedelta of the desired sampling period.
            align_to: Arbitrary point in time used to align
                timestamped data with the index position in the buffer.
                Used to make the data stored in the buffer align with the
                beginning and end of the buffer borders.

                For example, if the `align_to` is set to
                "0001-01-01 12:00:00", and the `sampling_period` is set to
                1 hour and the length of the buffer is 24, then the data
                stored in the buffer could correspond to the time range from
                "2022-01-01 12:00:00" to "2022-01-02 12:00:00" (date chosen
                arbitrarily here).
        """
        assert len(buffer) > 0, "The buffer capacity must be higher than zero"

        self._buffer: FloatArray = buffer
        self._sampling_period: timedelta = sampling_period
        self._time_index_alignment: datetime = align_to

        self._gaps: list[Gap] = []
        self._timestamp_newest: datetime = self._TIMESTAMP_MIN
        self._timestamp_oldest: datetime = self._TIMESTAMP_MAX
        self._full_time_range: timedelta = len(self._buffer) * self._sampling_period

    @property
    def sampling_period(self) -> timedelta:
        """Return the sampling period of the ring buffer.

        Returns:
            Sampling period of the ring buffer.
        """
        return self._sampling_period

    @property
    def gaps(self) -> list[Gap]:
        """Get the list of ranges for which no values were provided.

        See definition of dataclass @Gaps for more info.

        Returns:
            List of gaps.
        """
        return self._gaps

    def has_value(self, sample: Sample[QuantityT]) -> bool:
        """Check if a sample has a value and it's not NaN.

        Args:
            sample: sample to check.

        Returns:
            True if the sample has a value and it's not NaN.
        """
        return not (sample.value is None or sample.value.isnan())

    @property
    def maxlen(self) -> int:
        """Get the max length.

        Returns:
            The max amount of items this container can hold.
        """
        return len(self._buffer)

    def update(self, sample: Sample[QuantityT]) -> None:
        """Update the buffer with a new value for the given timestamp.

        Missing values are written as NaN. Be advised that when
        `update()` is called with samples newer than the current time
        + `sampling_period` (as is the case for loading historical data
        after an app restart), a gap of missing data exists. This gap
        does not contain NaN values but simply the old invalid values.
        The list of gaps returned by `gaps()` will reflect this and
        should be used as the only source of truth for unwritten data.

        Args:
            sample: Sample to add to the ringbuffer

        Raises:
            IndexError: When the timestamp to be added is too old.
        """
        # adjust timestamp to be exactly on the sample period time point
        timestamp = self.normalize_timestamp(sample.timestamp)

        # Don't add outdated entries
        if (
            timestamp < self._timestamp_oldest
            and self._timestamp_oldest != self._TIMESTAMP_MAX
        ):
            raise IndexError(
                f"Timestamp {timestamp} too old (cut-off is at {self._timestamp_oldest})."
            )

        # Update timestamps
        prev_newest = self._timestamp_newest
        self._timestamp_newest = max(self._timestamp_newest, timestamp)
        self._timestamp_oldest = self._timestamp_newest - (
            self._full_time_range - self._sampling_period
        )

        # Update data
        if self.has_value(sample):
            assert sample.value is not None
            value = sample.value.base_value
        else:
            value = np.nan
        self._buffer[self.to_internal_index(timestamp)] = value

        self._update_gaps(timestamp, prev_newest, not self.has_value(sample))

    @property
    def time_bound_oldest(self) -> datetime:
        """
        Return the time bounds of the ring buffer.

        Returns:
            The timestamp of the oldest sample of the ring buffer.
        """
        return self._timestamp_oldest

    @property
    def time_bound_newest(self) -> datetime:
        """
        Return the time bounds of the ring buffer.

        Returns:
            The timestamp of the newest sample of the ring buffer
            or None if the buffer is empty.
        """
        return self._timestamp_newest

    @property
    def oldest_timestamp(self) -> datetime | None:
        """Return the oldest timestamp in the buffer.

        Returns:
            The oldest timestamp in the buffer
            or None if the buffer is empty.
        """
        if self.count_valid() == 0:
            return None

        if self.is_missing(self.time_bound_oldest):
            return min(g.end for g in self.gaps)

        return self.time_bound_oldest

    @property
    def newest_timestamp(self) -> datetime | None:
        """Return the newest timestamp in the buffer.

        Returns:
            The newest timestamp in the buffer.
        """
        if self.count_valid() == 0:
            return None

        return self.time_bound_newest

    def to_internal_index(
        self, timestamp: datetime, allow_outside_range: bool = False
    ) -> int:
        """Convert the given timestamp to the position in the buffer.

        !!! Note: This method is meant for advanced use cases and should not generally be used.

        Args:
            timestamp: Timestamp to convert.
            allow_outside_range: If True, don't throw an exception when the
                timestamp is outside our bounds

        Raises:
            IndexError: When requesting a timestamp outside the range this container holds.

        Returns:
            Index where the value for the given timestamp can be found.
        """
        timestamp = self.normalize_timestamp(timestamp)

        if not allow_outside_range and (
            self._timestamp_newest + self._sampling_period < timestamp
            or timestamp < self._timestamp_oldest
        ):
            raise IndexError(
                f"Requested timestamp {timestamp} is "
                f"outside the range [{self._timestamp_oldest} - {self._timestamp_newest}]"
            )

        return self.wrap(
            round(
                (timestamp - self._time_index_alignment).total_seconds()
                / self._sampling_period.total_seconds()
            )
        )

    def get_timestamp(self, index: int) -> datetime | None:
        """Convert the given index to the underlying timestamp.

        Index 0 corresponds to the oldest timestamp in the buffer.
        If negative indices are used, the newest timestamp is used as reference.

        !!!warning

            The resulting timestamp can be outside the range of the buffer.

        Args:
            index: Index to convert.

        Returns:
            Timestamp where the value for the given index can be found.
                Or None if the buffer is empty.
        """
        if self.oldest_timestamp is None:
            return None
        assert self.newest_timestamp is not None
        ref_ts = (
            self.oldest_timestamp
            if index >= 0
            else self.newest_timestamp + self._sampling_period
        )
        return ref_ts + index * self._sampling_period

    def _to_covered_indices(
        self, start: int | None, end: int | None = None
    ) -> tuple[int, int]:
        """Project the given indices via slice onto the covered range.

        Args:
            start: Start index.
            end: End index. Optional, defaults to None.

        Returns:
            tuple of start and end indices on the range currently covered by the buffer.
        """
        return slice(start, end).indices(self.count_covered())[:2]

    def window(
        self,
        start: datetime | int | None,
        end: datetime | int | None,
        *,
        force_copy: bool = True,
        fill_value: float | None = np.nan,
    ) -> FloatArray:
        """Request a copy or view on the data between start timestamp and end timestamp.

        Always request a copy if you keep the data around for longer.
        Otherwise, if the data is not used immediately it could be overwritten.

        Will return a copy in the following cases:
        * The requested time period is crossing the start/end of the buffer.
        * The force_copy parameter was set to True (default True).

        The first case can be avoided by using the appropriate
        `align_to` value in the constructor so that the data lines up
        with the start/end of the buffer.

        This means, if the caller needs to modify the data to account for
        missing entries, they can safely do so.

        Args:
            start: start timestamp of the window.
            end: end timestamp of the window.
            force_copy: optional, default True. If True, will always create a
                copy of the data.
            fill_value: If not None, will use this value to fill missing values.
                If missing values should be filled, force_copy must be True.
                Defaults to NaN to avoid returning outdated data unexpectedly.

        Raises:
            IndexError: When start and end are not both datetime or index.
            ValueError: When fill_value is not None and force_copy is False.

        Returns:
            The requested window
        """
        # We don't want to modify the original buffer
        if fill_value is not None and not force_copy:
            raise ValueError("fill_value only supported for force_copy=True")

        if self.count_covered() == 0:
            return np.array([]) if isinstance(self._buffer, np.ndarray) else []

        # If both are indices or None convert to datetime
        if not isinstance(start, datetime) and not isinstance(end, datetime):
            start, end = self._to_covered_indices(start, end)
            start = self.get_timestamp(start)
            end = self.get_timestamp(end)

        # Here we should have both as datetime
        if not isinstance(start, datetime) or not isinstance(end, datetime):
            raise IndexError(
                f"start ({start}) and end ({end}) must both be either datetime or index."
            )

        # Ensure that the window is within the bounds of the buffer
        assert self.oldest_timestamp is not None and self.newest_timestamp is not None
        start = max(start, self.oldest_timestamp)
        end = min(end, self.newest_timestamp + self._sampling_period)

        if start >= end:
            return np.array([]) if isinstance(self._buffer, np.ndarray) else []

        start_pos = self.to_internal_index(start)
        end_pos = self.to_internal_index(end)

        window = self._wrapped_buffer_window(
            self._buffer, start_pos, end_pos, force_copy
        )

        if fill_value is not None:
            window = self._fill_gaps(window, fill_value, start, self.gaps)
        return window

    def _fill_gaps(
        self,
        data: FloatArray,
        fill_value: float,
        oldest_timestamp: datetime,
        gaps: list[Gap],
    ) -> FloatArray:
        """Fill the gaps in the data with the given fill_value.

        Args:
            data: The data to fill.
            fill_value: The value to fill the gaps with.
            oldest_timestamp: The oldest timestamp in the data.
            gaps: List of gaps to fill.

        Returns:
            The filled data.
        """
        assert isinstance(
            data, (np.ndarray, list)
        ), f"Unsupported data type {type(data)}"
        for gap in gaps:
            start_index = (gap.start - oldest_timestamp) // self._sampling_period
            end_index = (gap.end - oldest_timestamp) // self._sampling_period
            start_index = max(start_index, 0)
            end_index = min(end_index, len(data))
            if start_index < end_index:
                if isinstance(data, np.ndarray):
                    data[start_index:end_index] = fill_value
                elif isinstance(data, list):
                    data[start_index:end_index] = [fill_value] * (
                        end_index - start_index
                    )
        return data

    @staticmethod
    def _wrapped_buffer_window(
        buffer: FloatArray,
        start_pos: int,
        end_pos: int,
        force_copy: bool = True,
    ) -> FloatArray:
        """Get a wrapped window from the given buffer.

        If start_pos == end_pos, the full wrapped buffer is returned starting at start_pos.

        Copies can only be avoided for numpy arrays and when the window is not wrapped.
        Lists of floats are always copies.

        Args:
            buffer: The buffer to get the window from.
            start_pos: The start position of the window in the buffer.
            end_pos: The end position of the window in the buffer (exclusive).
            force_copy: If True, will always create a copy of the data.

        Returns:
            The requested window.
        """
        # Requested window wraps around the ends
        if start_pos >= end_pos:
            if isinstance(buffer, list):
                return buffer[start_pos:] + buffer[0:end_pos]
            assert isinstance(
                buffer, np.ndarray
            ), f"Unsupported buffer type: {type(buffer)}"
            if end_pos > 0:
                return np.concatenate((buffer[start_pos:], buffer[0:end_pos]))
            arr = buffer[start_pos:]
        else:
            arr = buffer[start_pos:end_pos]

        if force_copy:
            return deepcopy(arr)
        return arr

    def is_missing(self, timestamp: datetime) -> bool:
        """Check if the given timestamp falls within a gap.

        Args:
            timestamp: The timestamp to check for missing data.

        Returns:
            True if the given timestamp falls within a gap, False otherwise.
        """
        return any(map(lambda gap: gap.contains(timestamp), self._gaps))

    def _update_gaps(
        self, timestamp: datetime, newest: datetime, record_as_missing: bool
    ) -> None:
        """Update gap list with new timestamp.

        Args:
            timestamp: Timestamp of the new value.
            newest: Timestamp of the newest value before the current update.
            record_as_missing: if `True`, the given timestamp will be recorded as missing.
        """
        found_in_gaps = self.is_missing(timestamp)

        if not record_as_missing:
            # Replace all gaps with one if we went far into then future
            if self._timestamp_newest - newest >= self._full_time_range:
                self._gaps = [
                    Gap(start=self._timestamp_oldest, end=self._timestamp_newest)
                ]
                return

            # Check if we created a gap with the addition of the new value
            if not found_in_gaps and timestamp > newest + self._sampling_period:
                self._gaps.append(
                    Gap(start=newest + self._sampling_period, end=timestamp)
                )

        # New missing entry that is not already in a gap?
        if record_as_missing:
            if not found_in_gaps:
                # If there are no gaps and the new value is not subsequent to the
                # newest value, we need to start the new gap after the newest value
                start_gap = min(newest + self._sampling_period, timestamp)
                self._gaps.append(
                    Gap(start=start_gap, end=timestamp + self._sampling_period)
                )
        elif len(self._gaps) > 0:
            if found_in_gaps:
                self._remove_gap(timestamp)

        self._cleanup_gaps()

    def _cleanup_gaps(self) -> None:
        """Clean up the list of gaps.

        * Merge existing gaps
        * remove overlaps
        * delete outdated gaps
        """
        self._gaps = sorted(self._gaps, key=lambda x: x.start.timestamp())

        i = 0
        while i < len(self._gaps):
            w_1 = self._gaps[i]
            if i < len(self._gaps) - 1:
                w_2 = self._gaps[i + 1]
            else:
                w_2 = None

            # Delete out-of-date gaps
            if w_1.end <= self._timestamp_oldest:
                del self._gaps[i]
            # Update start of gap if it's rolled out of the buffer
            elif w_1.start < self._timestamp_oldest:
                self._gaps[i].start = self._timestamp_oldest
            # If w2 is a subset of w1 we can delete it
            elif w_2 and w_1.start <= w_2.start and w_1.end >= w_2.end:
                del self._gaps[i + 1]
            # If the gaps are direct neighbors, merge them
            elif w_2 and w_1.end >= w_2.start:
                w_1.end = w_2.end
                del self._gaps[i + 1]
            else:
                i += 1

    def _remove_gap(self, timestamp: datetime) -> None:
        """Update the list of gaps to not contain the given timestamp.

        Args:
            timestamp: Timestamp that is no longer missing.
        """
        gap_index, gap = next(
            filter(
                lambda gap: gap[1].contains(timestamp),
                enumerate(self._gaps),
            ),
            (0, None),
        )

        if gap is None:
            return

        if gap.start == timestamp:
            # Is the whole gap consisting only of the timestamp?
            if gap.end == timestamp + self._sampling_period:
                del self._gaps[gap_index]
            # Otherwise, make the gap smaller
            else:
                gap.start = timestamp + self._sampling_period
        # Is the timestamp at the end? Shrink by one then
        elif gap.end - self._sampling_period == timestamp:
            gap.end = timestamp
        # Otherwise it must be in the middle and we need to create a new
        # gap
        else:
            new_gap = deepcopy(gap)
            gap.end = timestamp
            new_gap.start = timestamp + self._sampling_period
            self._gaps.append(new_gap)

    def normalize_timestamp(self, timestamp: datetime) -> datetime:
        """Normalize the given timestamp to fall exactly on the resampling period.

        Args:
            timestamp: The timestamp to normalize.

        Returns:
            The normalized timestamp.
        """
        num_samples, remainder = divmod(
            (timestamp - self._time_index_alignment), self._sampling_period
        )

        # Round towards the closer number and towards the even one in case of
        # equal distance
        if remainder != timedelta(0) and (
            self._sampling_period / 2 == remainder
            and num_samples % 2 != 0
            or self._sampling_period / 2 < remainder
        ):
            num_samples += 1

        normalized_timestamp = (
            self._time_index_alignment + num_samples * self._sampling_period
        )

        return normalized_timestamp

    def wrap(self, index: int) -> int:
        """Normalize the given index to fit in the buffer by wrapping it around.

        Args:
            index: index to normalize.

        Returns:
            An index that will be within maxlen.
        """
        return index % self.maxlen

    @overload
    def __getitem__(self, index_or_slice: SupportsIndex) -> float:
        """Get item at requested position.

        No wrapping of the index will be done.
        Create a feature request if you require this function.

        Args:
            index_or_slice: Index of the requested data.

        Returns:
            The requested value.
        """

    @overload
    def __getitem__(self, index_or_slice: slice) -> FloatArray:
        """Get the data described by the given slice.

        No wrapping of the index will be done.
        Create a feature request if you require this function.

        Args:
            index_or_slice: Slice specification of where the requested data is.

        Returns:
            The requested slice.
        """

    def __getitem__(self, index_or_slice: SupportsIndex | slice) -> float | FloatArray:
        """Get item or slice at requested position.

        No wrapping of the index will be done.
        Create a feature request if you require this function.

        Args:
            index_or_slice: Index or slice specification of the requested data.

        Returns:
            The requested value or slice.
        """
        return self._buffer.__getitem__(index_or_slice)

    def _covered_time_range(
        self, since: datetime | None = None, until: datetime | None = None
    ) -> timedelta:
        """Return the time range that is covered by the oldest and newest valid samples.

        If `since` and `until` are provided, the time range is limited to the items
        between (and including) the given timestamps.

        Args:
            since: The timestamp from which to start counting.  If `None`, the oldest
                timestamp in the buffer is used.
            until: The timestamp until (and including) which to count.  If `None`, the
                newest timestamp in the buffer is used.

        Returns:
            The time range between the oldest and newest valid samples or 0 if
                there are is no time range covered.
        """
        if not self.oldest_timestamp:
            return timedelta(0)

        assert (
            self.newest_timestamp is not None
        ), "Newest timestamp cannot be None here."

        if since is None or since < self.oldest_timestamp:
            since = self.oldest_timestamp
        if until is None or until > self.newest_timestamp:
            until = self.newest_timestamp

        if until < since:
            return timedelta(0)

        return until - since + self._sampling_period

    def count_covered(
        self, *, since: datetime | None = None, until: datetime | None = None
    ) -> int:
        """Count the number of samples that are covered by the oldest and newest valid samples.

        If `since` and `until` are provided, the count is limited to the items between
        (and including) the given timestamps.

        Args:
            since: The timestamp from which to start counting.  If `None`, the oldest
                timestamp in the buffer is used.
            until: The timestamp until (and including) which to count.  If `None`, the
                newest timestamp in the buffer is used.

        Returns:
            The count of samples between the oldest and newest (inclusive) valid samples
                or 0 if there are is no time range covered.
        """
        return int(
            self._covered_time_range(since, until).total_seconds()
            // self._sampling_period.total_seconds()
        )

    def count_valid(
        self, *, since: datetime | None = None, until: datetime | None = None
    ) -> int:
        """Count the number of valid items in this buffer.

        If `since` and `until` are provided, the count is limited to the items between
        (and including) the given timestamps.

        Args:
            since: The timestamp from which to start counting.  If `None`, the oldest
                timestamp in the buffer is used.
            until: The timestamp until (and including) which to count.  If `None`, the
                newest timestamp in the buffer is used.

        Returns:
            The number of valid items in this buffer.
        """
        if since is None or since < self._timestamp_oldest:
            since = self._timestamp_oldest
        if until is None or until > self._timestamp_newest:
            until = self._timestamp_newest

        if until == self._TIMESTAMP_MIN or until < since:
            return 0

        # Sum of all elements in the gap ranges
        sum_missing_entries = max(
            0,
            sum(
                (
                    min(gap.end, until + self._sampling_period)
                    # Don't look further back than oldest timestamp
                    - max(gap.start, since)
                )
                // self._sampling_period
                for gap in self._gaps
                if gap.start <= until and gap.end >= since
            ),
        )

        start_pos = self.to_internal_index(since)
        end_pos = self.to_internal_index(until)

        if end_pos < start_pos:
            return len(self._buffer) - start_pos + end_pos + 1 - sum_missing_entries

        return end_pos + 1 - start_pos - sum_missing_entries



================================================
FILE: src/frequenz/sdk/timeseries/_ringbuffer/serialization.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Ringbuffer dumping & loading functions."""

# For use of the class type hint inside the class itself.


import pickle
from os.path import exists

from .buffer import FloatArray, OrderedRingBuffer

FILE_FORMAT_VERSION: int = 1
"""Version of the latest file dumping/loading format."""


def load(path: str) -> OrderedRingBuffer[FloatArray] | None:
    """Load a ringbuffer from disk.

    Args:
        path: Path to the file where the data is stored.

    Raises:
        RuntimeError: when the file format version is unknown.

    Returns:
        `None` when the file doesn't exist, otherwise an instance of the
        `OrderedRingBuffer` class, loaded from disk.
    """
    if not exists(path):
        return None

    with open(path, mode="rb") as fileobj:
        instance: OrderedRingBuffer[FloatArray]
        file_format_version: int

        file_format_version, instance = pickle.load(fileobj)

    if file_format_version != FILE_FORMAT_VERSION:
        raise RuntimeError(
            f"Unknown file format version: {file_format_version}. Can load: {FILE_FORMAT_VERSION}"
        )

    return instance


def dump(  # noqa: DOC502 (OSError is raised indirectly by open and pickle.dump)
    ringbuffer: OrderedRingBuffer[FloatArray],
    path: str,
    file_format_version: int = FILE_FORMAT_VERSION,
) -> None:
    """Dump a ringbuffer to disk.

    Args:
        ringbuffer: Instance of the ringbuffer to dump.
        path: Path to where the data should be saved to.
        file_format_version: Version of the file format, optional.

    Raises:
        OSError: When the file cannot be opened or written.
    """
    with open(path, mode="wb+") as fileobj:
        pickle.dump((file_format_version, ringbuffer), fileobj)



================================================
FILE: src/frequenz/sdk/timeseries/battery_pool/__init__.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Manage a pool of batteries."""

from ._battery_pool import BatteryPool
from .messages import BatteryPoolReport

__all__ = [
    "BatteryPool",
    "BatteryPoolReport",
]



================================================
FILE: src/frequenz/sdk/timeseries/battery_pool/_battery_pool.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""An external interface for the BatteryPool.

Allows for actors interested in operating on the same set of batteries to share
underlying formula engine and metric calculator instances, but without having to specify
their individual priorities with each request.
"""

import asyncio
import uuid
from collections import abc

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.quantities import Energy, Percentage, Power, Temperature

from ... import timeseries
from ..._internal._channels import MappingReceiverFetcher, ReceiverFetcher
from ...microgrid import _power_distributing, _power_managing
from ...timeseries import Sample
from .._base_types import SystemBounds
from ..formula_engine import FormulaEngine
from ..formula_engine._formula_generators import (
    BatteryPowerFormula,
    FormulaGeneratorConfig,
)
from ._battery_pool_reference_store import BatteryPoolReferenceStore
from ._methods import SendOnUpdate
from ._metric_calculator import (
    CapacityCalculator,
    PowerBoundsCalculator,
    SoCCalculator,
    TemperatureCalculator,
)
from .messages import BatteryPoolReport

# pylint: disable=protected-access


class BatteryPool:
    """An interface for interaction with pools of batteries.

    Provides:
      - properties for fetching reporting streams of instantaneous
        [power][frequenz.sdk.timeseries.battery_pool.BatteryPool.power],
        [soc][frequenz.sdk.timeseries.battery_pool.BatteryPool.soc],
        [capacity][frequenz.sdk.timeseries.battery_pool.BatteryPool.capacity] values and
        available power bounds and other status through
        [power_status][frequenz.sdk.timeseries.battery_pool.BatteryPool.power_status].
      - control methods for proposing power values, namely:
        [propose_power][frequenz.sdk.timeseries.battery_pool.BatteryPool.propose_power],
        [propose_charge][frequenz.sdk.timeseries.battery_pool.BatteryPool.propose_charge] and
        [propose_discharge][frequenz.sdk.timeseries.battery_pool.BatteryPool.propose_discharge].
    """

    def __init__(
        self,
        *,
        pool_ref_store: BatteryPoolReferenceStore,
        name: str | None,
        priority: int,
    ):
        """Create a BatteryPool instance.

        !!! note
            `BatteryPool` instances are not meant to be created directly by users.  Use
            the [`microgrid.new_battery_pool`][frequenz.sdk.microgrid.new_battery_pool]
            method for creating `BatteryPool` instances.

        Args:
            pool_ref_store: The battery pool reference store instance.
            name: An optional name used to identify this instance of the pool or a
                corresponding actor in the logs.
            priority: The priority of the actor using this wrapper.
        """
        self._pool_ref_store = pool_ref_store
        unique_id = str(uuid.uuid4())
        self._source_id = unique_id if name is None else f"{name}-{unique_id}"
        self._priority = priority

    async def propose_power(
        self,
        power: Power | None,
        *,
        bounds: timeseries.Bounds[Power | None] = timeseries.Bounds(None, None),
    ) -> None:
        """Send a proposal to the power manager for the pool's set of batteries.

        Power values need to follow the Passive Sign Convention (PSC). That is, positive
        values indicate charge power and negative values indicate discharge power.

        Details on how the power manager handles proposals can be found in the
        [Microgrid][frequenz.sdk.microgrid--setting-power] documentation.

        Args:
            power: The power to propose for the batteries in the pool.  If `None`, this
                proposal will not have any effect on the target power, unless bounds are
                specified.  When specified without bounds, bounds for lower priority
                actors will be shifted by this power.  If both are `None`, it is
                equivalent to not having a proposal or withdrawing a previous one.
            bounds: The power bounds for the proposal.  When specified, this will limit
                the bounds for lower priority actors.
        """
        await self._pool_ref_store._power_manager_requests_sender.send(
            _power_managing.Proposal(
                source_id=self._source_id,
                preferred_power=power,
                bounds=bounds,
                component_ids=self._pool_ref_store._batteries,
                priority=self._priority,
                creation_time=asyncio.get_running_loop().time(),
            )
        )

    async def propose_charge(self, power: Power | None) -> None:
        """Set the given charge power for the batteries in the pool.

        Power values need to be positive values, indicating charge power.

        When using the Passive Sign Convention, the
        [`propose_power`][frequenz.sdk.timeseries.battery_pool.BatteryPool.propose_power]
        method might be more convenient.

        If the same batteries are shared by multiple actors, the behaviour is the same
        as that of the `propose_power` method, when calling it with `None` bounds.  The
        bounds for lower priority actors can't be specified with this method.  If that's
        required, use the `propose_power` method instead.

        Args:
            power: The unsigned charge power to propose for the batteries in the pool.
                If None, the proposed power of higher priority actors will take
                precedence as the target power.

        Raises:
            ValueError: If the given power is negative.
        """
        if power and power < Power.zero():
            raise ValueError("Charge power must be positive.")
        await self._pool_ref_store._power_manager_requests_sender.send(
            _power_managing.Proposal(
                source_id=self._source_id,
                preferred_power=power,
                bounds=timeseries.Bounds(None, None),
                component_ids=self._pool_ref_store._batteries,
                priority=self._priority,
                creation_time=asyncio.get_running_loop().time(),
            )
        )

    async def propose_discharge(self, power: Power | None) -> None:
        """Set the given discharge power for the batteries in the pool.

        Power values need to be positive values, indicating discharge power.

        When using the Passive Sign Convention, the
        [`propose_power`][frequenz.sdk.timeseries.battery_pool.BatteryPool.propose_power]
        method might be more convenient.

        If the same batteries are shared by multiple actors, the behaviour is the same
        as that of the `propose_power` method, when calling it with `None` bounds.  The
        bounds for lower priority actors can't be specified with this method.  If that's
        required, use the `propose_power` method instead.

        Args:
            power: The unsigned discharge power to propose for the batteries in the
                pool.  If None, the proposed power of higher priority actors will take
                precedence as the target power.

        Raises:
            ValueError: If the given power is negative.
        """
        if power:
            if power < Power.zero():
                raise ValueError("Discharge power must be positive.")
            power = -power
        await self._pool_ref_store._power_manager_requests_sender.send(
            _power_managing.Proposal(
                source_id=self._source_id,
                preferred_power=power,
                bounds=timeseries.Bounds(None, None),
                component_ids=self._pool_ref_store._batteries,
                priority=self._priority,
                creation_time=asyncio.get_running_loop().time(),
            )
        )

    @property
    def component_ids(self) -> abc.Set[ComponentId]:
        """Return ids of the batteries in the pool.

        Returns:
            Ids of the batteries in the pool
        """
        return self._pool_ref_store._batteries

    @property
    def power(self) -> FormulaEngine[Power]:
        """Fetch the total power of the batteries in the pool.

        This formula produces values that are in the Passive Sign Convention (PSC).

        If a formula engine to calculate this metric is not already running, it will be
        started.

        A receiver from the formula engine can be obtained by calling the `new_receiver`
        method.

        Returns:
            A FormulaEngine that will calculate and stream the total power of all
                batteries in the pool.
        """
        engine = self._pool_ref_store._formula_pool.from_power_formula_generator(
            "battery_pool_power",
            BatteryPowerFormula,
            FormulaGeneratorConfig(
                component_ids=self._pool_ref_store._batteries,
                allow_fallback=True,
            ),
        )
        assert isinstance(engine, FormulaEngine)
        return engine

    @property
    def soc(self) -> ReceiverFetcher[Sample[Percentage]]:
        """Fetch the normalized average weighted-by-capacity SoC values for the pool.

        The SoC values are normalized to the 0-100% range and clamped if they are out
        of bounds. Only values from working batteries with operational inverters are
        considered in the calculation.

        Average SoC is calculated using the formula:
        ```
        working_batteries: Set[BatteryData] # working batteries from the battery pool

        soc_scaled = min(max(
            0,
            (soc - soc_lower_bound) / (soc_upper_bound - soc_lower_bound) * 100,
        ), 100)
        used_capacity = sum(
            battery.usable_capacity * battery.soc_scaled
            for battery in working_batteries
        )
        total_capacity = sum(battery.usable_capacity for battery in working_batteries)
        average_soc = used_capacity/total_capacity
        ```

        `None` values will be sent if there are no working batteries with operational
        inverters to calculate the metric with.

        A receiver from the MetricAggregator can be obtained by calling the
        `new_receiver` method.

        Returns:
            A MetricAggregator that will calculate and stream the aggregate SoC of all
                batteries in the pool, considering only working batteries with
                operational inverters.
        """
        method_name = SendOnUpdate.name() + "_" + SoCCalculator.name()

        if method_name not in self._pool_ref_store._active_methods:
            calculator = SoCCalculator(self._pool_ref_store._batteries)
            self._pool_ref_store._active_methods[method_name] = SendOnUpdate(
                metric_calculator=calculator,
                working_batteries=self._pool_ref_store._working_batteries,
                min_update_interval=self._pool_ref_store._min_update_interval,
            )

        return self._pool_ref_store._active_methods[method_name]

    @property
    def temperature(self) -> ReceiverFetcher[Sample[Temperature]]:
        """Fetch the average temperature of the batteries in the pool.

        Returns:
            A MetricAggregator that will calculate and stream the average temperature
                of all batteries in the pool.
        """
        method_name = SendOnUpdate.name() + "_" + TemperatureCalculator.name()
        if method_name not in self._pool_ref_store._active_methods:
            calculator = TemperatureCalculator(self._pool_ref_store._batteries)
            self._pool_ref_store._active_methods[method_name] = SendOnUpdate(
                metric_calculator=calculator,
                working_batteries=self._pool_ref_store._working_batteries,
                min_update_interval=self._pool_ref_store._min_update_interval,
            )
        return self._pool_ref_store._active_methods[method_name]

    @property
    def capacity(self) -> ReceiverFetcher[Sample[Energy]]:
        """Get a receiver to receive new capacity metrics when they change.

        The reported capacity values consider only working batteries with operational
        inverters.

        Calculated with the formula:
        ```
        working_batteries: Set[BatteryData] # working batteries from the battery pool
        total_capacity = sum(
            battery.capacity * (soc_upper_bound - soc_lower_bound) / 100
            for battery in working_batteries
        )
        ```

        `None` will be sent if there are no working batteries with operational
        inverters to calculate metrics.

        A receiver from the MetricAggregator can be obtained by calling the
        `new_receiver` method.

        Returns:
            A MetricAggregator that will calculate and stream the capacity of all
                batteries in the pool, considering only working batteries with
                operational inverters.
        """
        method_name = SendOnUpdate.name() + "_" + CapacityCalculator.name()

        if method_name not in self._pool_ref_store._active_methods:
            calculator = CapacityCalculator(self._pool_ref_store._batteries)
            self._pool_ref_store._active_methods[method_name] = SendOnUpdate(
                metric_calculator=calculator,
                working_batteries=self._pool_ref_store._working_batteries,
                min_update_interval=self._pool_ref_store._min_update_interval,
            )

        return self._pool_ref_store._active_methods[method_name]

    @property
    def power_status(self) -> ReceiverFetcher[BatteryPoolReport]:
        """Get a receiver to receive new power status reports when they change.

        These include
          - the current inclusion/exclusion bounds available for the pool's priority,
          - the current target power for the pool's set of batteries,
          - the result of the last distribution request for the pool's set of batteries.

        Returns:
            A receiver that will stream power status reports for the pool's priority.
        """
        sub = _power_managing.ReportRequest(
            source_id=self._source_id,
            priority=self._priority,
            component_ids=self._pool_ref_store._batteries,
        )
        self._pool_ref_store._power_bounds_subs[sub.get_channel_name()] = (
            asyncio.create_task(
                self._pool_ref_store._power_manager_bounds_subscription_sender.send(sub)
            )
        )
        channel = self._pool_ref_store._channel_registry.get_or_create(
            _power_managing._Report, sub.get_channel_name()
        )
        channel.resend_latest = True

        return channel

    @property
    def power_distribution_results(self) -> ReceiverFetcher[_power_distributing.Result]:
        """Get a receiver to receive power distribution results.

        Returns:
            A receiver that will stream power distribution results for the pool's set of
            batteries.
        """
        return MappingReceiverFetcher(
            self._pool_ref_store._power_dist_results_fetcher,
            lambda recv: recv.filter(
                lambda x: x.request.component_ids == self._pool_ref_store._batteries
            ),
        )

    @property
    def _system_power_bounds(self) -> ReceiverFetcher[SystemBounds]:
        """Get receiver to receive new power bounds when they change.

        Power bounds refer to the min and max power that a battery can
        discharge or charge at and is also denoted as SoP.

        Power bounds formulas are described in the receiver return type.
        None will be send if there is no component to calculate metrics.

        A receiver from the MetricAggregator can be obtained by calling the
        `new_receiver` method.

        Returns:
            A MetricAggregator that will calculate and stream the power bounds
                of all batteries in the pool.
        """
        method_name = SendOnUpdate.name() + "_" + PowerBoundsCalculator.name()

        if method_name not in self._pool_ref_store._active_methods:
            calculator = PowerBoundsCalculator(self._pool_ref_store._batteries)
            self._pool_ref_store._active_methods[method_name] = SendOnUpdate(
                metric_calculator=calculator,
                working_batteries=self._pool_ref_store._working_batteries,
                min_update_interval=self._pool_ref_store._min_update_interval,
            )

        return self._pool_ref_store._active_methods[method_name]

    async def stop(self) -> None:
        """Stop all tasks and channels owned by the BatteryPool."""
        # This was closing the pool_ref_store, which is not correct, because those are
        # shared.
        #
        # This method will do until we have a mechanism to track the resources created
        # through it.  It can also eventually cleanup the pool_ref_store, when it is
        # holding the last reference to it.



================================================
FILE: src/frequenz/sdk/timeseries/battery_pool/_battery_pool_reference_store.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""User interface for requesting aggregated battery-inverter data."""


import asyncio
import uuid
from collections.abc import Awaitable, Set
from datetime import timedelta
from typing import Any

from frequenz.channels import Receiver, Sender
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentCategory

from ..._internal._asyncio import cancel_and_await
from ..._internal._channels import ChannelRegistry, ReceiverFetcher
from ...microgrid import connection_manager
from ...microgrid._data_sourcing import ComponentMetricRequest
from ...microgrid._power_distributing import Result
from ...microgrid._power_distributing._component_status import ComponentPoolStatus
from ...microgrid._power_managing._base_classes import Proposal, ReportRequest
from ..formula_engine._formula_engine_pool import FormulaEnginePool
from ._methods import MetricAggregator


class BatteryPoolReferenceStore:  # pylint: disable=too-many-instance-attributes
    """A class for maintaining the shared state/tasks for a set of pool of batteries.

    This includes ownership of
    - the formula engine pool and metric calculators.
    - the tasks for updating the battery status for the metric calculators.

    These are independent of the priority of the actors and can be shared between
    multiple users of the same set of batteries.

    They are exposed through the BatteryPool class.
    """

    def __init__(  # pylint: disable=too-many-arguments
        self,
        *,
        channel_registry: ChannelRegistry,
        resampler_subscription_sender: Sender[ComponentMetricRequest],
        batteries_status_receiver: Receiver[ComponentPoolStatus],
        power_manager_requests_sender: Sender[Proposal],
        power_manager_bounds_subscription_sender: Sender[ReportRequest],
        power_distribution_results_fetcher: ReceiverFetcher[Result],
        min_update_interval: timedelta,
        batteries_id: Set[ComponentId] | None = None,
    ) -> None:
        """Create the class instance.

        Args:
            channel_registry: A channel registry instance shared with the resampling
                actor.
            resampler_subscription_sender: A sender for sending metric requests to the
                resampling actor.
            batteries_status_receiver: Receiver to receive status of the batteries.
                Receivers should has maxsize = 1 to fetch only the latest status.
                Battery status channel should has resend_latest = True.
                It should send information when any battery changed status.
                Battery status should include status of the inverter adjacent to this
                battery.
            power_manager_requests_sender: A Channel sender for sending power
                requests to the power managing actor.
            power_manager_bounds_subscription_sender: A Channel sender for sending
                power bounds requests to the power managing actor.
            power_distribution_results_fetcher: A ReceiverFetcher for the results from
                the power distributing actor.
            min_update_interval: Some metrics in BatteryPool are send only when they
                change. For these metrics min_update_interval is the minimum time
                interval between the following messages.
                Note that this argument is similar to the resampling period
                argument in the ComponentMetricsResamplingActor. But as opposed to
                ResamplingActor, timestamp returned in the resulting message will be
                the timestamp of the last received component data.
                It is currently impossible to use resampling actor for these metrics,
                because we can't specify resampling function for them.
            batteries_id: Subset of the batteries that should be included in the
                battery pool. If None or empty, then all batteries from the microgrid
                will be used.
        """
        self._batteries: frozenset[ComponentId]
        if batteries_id:
            self._batteries = frozenset(batteries_id)
        else:
            self._batteries = self._get_all_batteries()

        self._working_batteries: set[ComponentId] = set()

        self._update_battery_status_task: asyncio.Task[None] | None = None
        self._batteries_status_receiver: Receiver[ComponentPoolStatus] = (
            batteries_status_receiver
        )
        if self._batteries:
            self._update_battery_status_task = asyncio.create_task(
                self._update_battery_status(self._batteries_status_receiver)
            )

        self._min_update_interval: timedelta = min_update_interval

        self._power_manager_requests_sender: Sender[Proposal] = (
            power_manager_requests_sender
        )

        self._power_manager_bounds_subscription_sender: Sender[ReportRequest] = (
            power_manager_bounds_subscription_sender
        )

        self._active_methods: dict[str, MetricAggregator[Any]] = {}
        self._power_bounds_subs: dict[str, asyncio.Task[None]] = {}
        self._namespace: str = f"battery-pool-{self._batteries}-{uuid.uuid4()}"
        self._power_distributing_namespace: str = f"power-distributor-{self._namespace}"
        self._channel_registry: ChannelRegistry = channel_registry
        self._power_dist_results_fetcher: ReceiverFetcher[Result] = (
            power_distribution_results_fetcher
        )
        self._formula_pool: FormulaEnginePool = FormulaEnginePool(
            self._namespace,
            self._channel_registry,
            resampler_subscription_sender,
        )

    async def stop(self) -> None:
        """Stop all pending async tasks."""
        tasks_to_stop: list[Awaitable[Any]] = [
            method.stop() for method in self._active_methods.values()
        ]
        tasks_to_stop.append(self._formula_pool.stop())
        if self._update_battery_status_task:
            tasks_to_stop.append(cancel_and_await(self._update_battery_status_task))
        await asyncio.gather(*tasks_to_stop)
        self._batteries_status_receiver.close()

    def _get_all_batteries(self) -> frozenset[ComponentId]:
        """Get all batteries from the microgrid.

        Returns:
            All batteries in the microgrid.
        """
        graph = connection_manager.get().component_graph
        return frozenset(
            {
                battery.component_id
                for battery in graph.components(
                    component_categories={ComponentCategory.BATTERY}
                )
            }
        )

    async def _update_battery_status(
        self, receiver: Receiver[ComponentPoolStatus]
    ) -> None:
        async for status in receiver:
            self._working_batteries = status.get_working_components(self._batteries)
            for item in self._active_methods.values():
                item.update_working_batteries(self._working_batteries)



================================================
FILE: src/frequenz/sdk/timeseries/battery_pool/_component_metric_fetcher.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Module to define how to subscribe and fetch component data."""

from __future__ import annotations

import asyncio
import logging
import math
from abc import ABC, abstractmethod
from collections.abc import Iterable
from datetime import datetime, timezone
from typing import Any, Generic, Self, TypeVar

from frequenz.channels import ChannelClosedError, Receiver
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    BatteryData,
    ComponentCategory,
    ComponentData,
    ComponentMetricId,
    InverterData,
)
from typing_extensions import override

from ..._internal._asyncio import AsyncConstructible
from ..._internal._constants import MAX_BATTERY_DATA_AGE_SEC
from ...microgrid import connection_manager
from ...microgrid._data_sourcing.microgrid_api_source import (
    _BatteryDataMethods,
    _InverterDataMethods,
)
from ._component_metrics import ComponentMetricsData

_logger = logging.getLogger(__name__)

T = TypeVar("T", bound=ComponentData)
"""Type variable for component data."""


class ComponentMetricFetcher(AsyncConstructible, ABC):
    """Define how to subscribe for and fetch the component metrics data."""

    _component_id: ComponentId
    _metrics: Iterable[ComponentMetricId]

    @classmethod
    async def async_new(
        cls, component_id: ComponentId, metrics: Iterable[ComponentMetricId]
    ) -> Self:
        """Create an instance of this class.

        Subscribe for the given component metrics and return them if method
        `fetch_next` is called.

        Args:
            component_id: component id
            metrics: metrics that should be fetched from this component.

        Returns:
            This class instance.
        """
        self: Self = cls.__new__(cls)
        self._component_id = component_id
        self._metrics = metrics
        return self

    @abstractmethod
    async def fetch_next(self) -> ComponentMetricsData | None:
        """Fetch metrics for this component."""

    @abstractmethod
    def stop(self) -> None:
        """Stop the metric fetcher."""


class LatestMetricsFetcher(ComponentMetricFetcher, Generic[T], ABC):
    """Subscribe for the latest component data and extract the needed metrics."""

    _receiver: Receiver[T]
    _max_waiting_time: float

    @classmethod
    async def async_new(
        cls,
        component_id: ComponentId,
        metrics: Iterable[ComponentMetricId],
    ) -> Self:
        """Create instance of this class.

        Subscribe for the requested component data and fetch only the latest component
        metrics.

        Args:
            component_id: component id
            metrics: metrics

        Raises:
            ValueError: If any requested metric id is not supported.

        Returns:
            This class instance
        """
        self: Self = await super().async_new(component_id, metrics)

        # pylint: disable=protected-access
        for metric in metrics:
            if metric not in self._supported_metrics():
                category = self._component_category()
                raise ValueError(f"Metric {metric} not supported for {category}")

        self._receiver = await self._subscribe()
        self._max_waiting_time = MAX_BATTERY_DATA_AGE_SEC
        # pylint: enable=protected-access
        return self

    async def fetch_next(self) -> ComponentMetricsData | None:
        """Fetch the latest component metrics.

        Returns:
            Component metrics data.
            None if the channel was closed and fetching next element is impossible.
        """
        try:
            data = await asyncio.wait_for(
                self._receiver.receive(), self._max_waiting_time
            )

        except ChannelClosedError:
            _logger.exception(
                "Channel for component %d was closed.", self._component_id
            )
            return None
        except asyncio.TimeoutError:
            # Next time wait infinitely until we receive any message.
            _logger.debug("Component %d stopped sending data.", self._component_id)
            return ComponentMetricsData(
                self._component_id, datetime.now(tz=timezone.utc), {}
            )

        self._max_waiting_time = MAX_BATTERY_DATA_AGE_SEC
        metrics = {}
        for mid in self._metrics:
            value = self._extract_metric(data, mid)
            # There is no guarantee that all fields in component message are populated
            if not math.isnan(value):
                metrics[mid] = value

        return ComponentMetricsData(self._component_id, data.timestamp, metrics)

    @override
    def stop(self) -> None:
        """Stop the metric fetcher."""
        self._receiver.close()

    @abstractmethod
    def _extract_metric(self, data: T, mid: ComponentMetricId) -> float: ...

    @abstractmethod
    def _supported_metrics(self) -> set[ComponentMetricId]: ...

    @abstractmethod
    def _component_category(self) -> ComponentCategory: ...

    @abstractmethod
    async def _subscribe(self) -> Receiver[Any]:
        """Subscribe for this component data.

        Size of the receiver buffer should should be 1 to make sure we receive only
        the latest component data.

        Returns:
            Receiver for this component metrics.
        """


class LatestBatteryMetricsFetcher(LatestMetricsFetcher[BatteryData]):
    """Subscribe for the latest battery data using MicrogridApiClient."""

    @classmethod
    async def async_new(  # noqa: DOC502 (ValueError is raised indirectly super.async_new)
        cls,
        component_id: ComponentId,
        metrics: Iterable[ComponentMetricId],
    ) -> LatestBatteryMetricsFetcher:
        """Create instance of this class.

        Subscribe for the requested component data and fetch only the latest component
        metrics.

        Args:
            component_id: component id
            metrics: metrics

        Raises:
            ValueError: If any requested metric id is not supported.

        Returns:
            This class instance
        """
        self: LatestBatteryMetricsFetcher = await super().async_new(
            component_id, metrics
        )
        return self

    def _supported_metrics(self) -> set[ComponentMetricId]:
        return set(_BatteryDataMethods.keys())

    def _extract_metric(self, data: BatteryData, mid: ComponentMetricId) -> float:
        return _BatteryDataMethods[mid](data)

    async def _subscribe(self) -> Receiver[BatteryData]:
        """Subscribe for this component data.

        Size of the receiver buffer should should be 1 to make sure we receive only
        the latest component data.

        Returns:
            Receiver for this component metrics.
        """
        api = connection_manager.get().api_client
        return await api.battery_data(self._component_id, maxsize=1)

    def _component_category(self) -> ComponentCategory:
        return ComponentCategory.BATTERY


class LatestInverterMetricsFetcher(LatestMetricsFetcher[InverterData]):
    """Subscribe for the latest inverter data using MicrogridApiClient."""

    @classmethod
    async def async_new(  # noqa: DOC502 (ValueError is raised indirectly by super.async_new)
        cls,
        component_id: ComponentId,
        metrics: Iterable[ComponentMetricId],
    ) -> LatestInverterMetricsFetcher:
        """Create instance of this class.

        Subscribe for the requested component data and fetch only the latest component
        metrics.

        Args:
            component_id: component id
            metrics: metrics

        Raises:
            ValueError: If any requested metric id is not supported.

        Returns:
            This class instance
        """
        self: LatestInverterMetricsFetcher = await super().async_new(
            component_id, metrics
        )
        return self

    def _supported_metrics(self) -> set[ComponentMetricId]:
        return set(_InverterDataMethods.keys())

    def _extract_metric(self, data: InverterData, mid: ComponentMetricId) -> float:
        return _InverterDataMethods[mid](data)

    async def _subscribe(self) -> Receiver[InverterData]:
        """Subscribe for this component data.

        Size of the receiver buffer should should be 1 to make sure we receive only
        the latest component data.

        Returns:
            Receiver for this component metrics.
        """
        api = connection_manager.get().api_client
        return await api.inverter_data(self._component_id, maxsize=1)

    def _component_category(self) -> ComponentCategory:
        return ComponentCategory.INVERTER



================================================
FILE: src/frequenz/sdk/timeseries/battery_pool/_component_metrics.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Class that stores values of the component metrics."""


from collections.abc import Mapping
from dataclasses import dataclass
from datetime import datetime

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentMetricId


@dataclass(frozen=True, eq=False)
class ComponentMetricsData:
    """Store values of the component metrics."""

    component_id: ComponentId
    """The component ID the data is for."""

    timestamp: datetime
    """The timestamp for all the metrics."""

    metrics: Mapping[ComponentMetricId, float]
    """The values for each metric."""

    def get(self, metric: ComponentMetricId) -> float | None:
        """Get metric value.

        Args:
            metric: metric id

        Returns:
            Value of the metric.
        """
        return self.metrics.get(metric, None)

    def __eq__(self, other: object) -> bool:
        """Compare two objects of this class.

        Object are considered as equal if all stored values except for timestamp
        are equal.

        Args:
            other: object to compare.

        Returns:
            True if two objects are equal, false otherwise.
        """
        if not isinstance(other, ComponentMetricsData):
            return False

        return self.component_id == other.component_id and self.metrics == other.metrics



================================================
FILE: src/frequenz/sdk/timeseries/battery_pool/_methods.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Methods for processing battery-inverter data."""


import asyncio
import logging
from abc import ABC, abstractmethod
from datetime import datetime, timedelta, timezone
from typing import Generic

from frequenz.channels import Broadcast, Receiver
from frequenz.client.common.microgrid.components import ComponentId

from ..._internal._asyncio import cancel_and_await, run_forever
from ..._internal._constants import RECEIVER_MAX_SIZE, WAIT_FOR_COMPONENT_DATA_SEC
from ...microgrid._power_distributing._component_managers._battery_manager import (
    _get_battery_inverter_mappings,
)
from ._component_metric_fetcher import (
    ComponentMetricFetcher,
    LatestBatteryMetricsFetcher,
    LatestInverterMetricsFetcher,
)
from ._component_metrics import ComponentMetricsData
from ._metric_calculator import MetricCalculator, T

_logger = logging.getLogger(__name__)


class MetricAggregator(Generic[T], ABC):
    """Interface to control how the component data should be aggregated and send."""

    @abstractmethod
    def update_working_batteries(self, new_working_batteries: set[ComponentId]) -> None:
        """Update set of the working batteries.

        Args:
            new_working_batteries: Set of the working batteries.
        """

    @abstractmethod
    def new_receiver(self, limit: int | None = RECEIVER_MAX_SIZE) -> Receiver[T]:
        """Return new receiver for the aggregated metric results.

        Args:
            limit: Buffer size of the receiver

        Returns:
            Receiver for the metric results.
        """

    @abstractmethod
    async def stop(self) -> None:
        """Stop the all pending async task."""

    @classmethod
    @abstractmethod
    def name(cls) -> str:
        """Return name of this method.

        Returns:
            Name of this method.
        """


class SendOnUpdate(MetricAggregator[T]):
    """Wait for the change of the components metrics and send updated result.

    This method will cache the component metrics. When any metric change it will
    recalculate high level metric. If the calculation result change, it will
    send the new value.
    """

    def __init__(
        self,
        working_batteries: set[ComponentId],
        metric_calculator: MetricCalculator[T],
        min_update_interval: timedelta,
    ) -> None:
        """Create class instance.

        Args:
            working_batteries: Set of the working batteries
            metric_calculator: Module that tells how to aggregate the component metrics.
            min_update_interval: Minimum frequency for sending update about the change.
        """
        self._metric_calculator: MetricCalculator[T] = metric_calculator
        self._bat_inv_map = _get_battery_inverter_mappings(
            self._metric_calculator.batteries,
            inv_bats=False,
            bat_bats=False,
            inv_invs=False,
        )["bat_invs"]

        self._working_batteries: set[ComponentId] = working_batteries.intersection(
            metric_calculator.batteries
        )
        self._result_channel: Broadcast[T] = Broadcast(
            name=SendOnUpdate.name() + "_" + metric_calculator.name(),
            resend_latest=True,
        )

        self._update_event = asyncio.Event()
        self._cached_metrics: dict[ComponentId, ComponentMetricsData] = {}

        self._update_task = asyncio.create_task(run_forever(self._update_and_notify))
        self._send_task = asyncio.create_task(
            run_forever(lambda: self._send_on_update(min_update_interval))
        )
        self._pending_data_fetchers: set[asyncio.Task[ComponentMetricsData | None]] = (
            set()
        )

        self._fetchers: dict[ComponentId, ComponentMetricFetcher] = {}

    @classmethod
    def name(cls) -> str:
        """Get name of the method.

        Returns:
            Name of the method.
        """
        return "SendOnUpdate"

    def new_receiver(self, limit: int | None = RECEIVER_MAX_SIZE) -> Receiver[T]:
        """Return new receiver for the aggregated metric results.

        Args:
            limit: Buffer size of the receiver

        Returns:
            Receiver for the metric results.
        """
        if limit is None:
            return self._result_channel.new_receiver()
        return self._result_channel.new_receiver(limit=limit)

    def update_working_batteries(self, new_working_batteries: set[ComponentId]) -> None:
        """Update set of the working batteries.

        Recalculate metric if set changed.

        Args:
            new_working_batteries: Set of the working batteries.
        """
        # MetricCalculator can discard some batteries if they doesn't meet requirements.
        # For example batteries without adjacent inverter won't be included
        # int he PowerBounds metrics.
        new_set = new_working_batteries.intersection(self._metric_calculator.batteries)

        stopped_working = self._working_batteries - new_set
        for battery_id in stopped_working:
            # Removed cached metrics for components that stopped working.
            self._cached_metrics.pop(battery_id, None)
            for inv_id in self._bat_inv_map[battery_id]:
                self._cached_metrics.pop(inv_id, None)

        if new_set != self._working_batteries:
            self._working_batteries = new_set
            self._update_event.set()

    async def stop(self) -> None:
        """Stop the all pending async task."""
        await asyncio.gather(
            *[cancel_and_await(task) for task in self._pending_data_fetchers]
        )
        await asyncio.gather(
            *[cancel_and_await(self._send_task), cancel_and_await(self._update_task)]
        )
        for fetcher in self._fetchers.values():
            fetcher.stop()

    async def _create_data_fetchers(self) -> None:
        fetchers: dict[ComponentId, ComponentMetricFetcher] = {
            cid: await LatestBatteryMetricsFetcher.async_new(cid, metrics)
            for cid, metrics in self._metric_calculator.battery_metrics.items()
        }
        self._fetchers.update(fetchers)
        inverter_fetchers = {
            cid: await LatestInverterMetricsFetcher.async_new(cid, metrics)
            for cid, metrics in self._metric_calculator.inverter_metrics.items()
        }
        self._fetchers.update(inverter_fetchers)

    def _remove_metric_fetcher(self, component_id: ComponentId) -> None:
        _logger.error(
            "Removing component %d from the %s formula.",
            component_id,
            self._result_channel._name,  # pylint: disable=protected-access
        )
        fetcher = self._fetchers.pop(component_id)
        fetcher.stop()

    def _metric_updated(self, new_metrics: ComponentMetricsData) -> bool:
        cid = new_metrics.component_id
        return (
            cid not in self._cached_metrics or new_metrics != self._cached_metrics[cid]
        )

    async def _update_and_notify(self) -> None:
        """Receive component metrics and send notification when they change."""
        await self._create_data_fetchers()

        self._pending_data_fetchers = {
            asyncio.create_task(fetcher.fetch_next(), name=str(cid))
            for cid, fetcher in self._fetchers.items()
        }
        while len(self._pending_data_fetchers) > 0:
            done, self._pending_data_fetchers = await asyncio.wait(
                self._pending_data_fetchers, return_when=asyncio.FIRST_COMPLETED
            )
            for item in done:
                metrics = item.result()
                if metrics is None:
                    self._remove_metric_fetcher(ComponentId(int(item.get_name())))
                    continue
                if self._metric_updated(metrics):
                    self._update_event.set()

                cid = metrics.component_id
                # Save metric even if not changed to update its timestamp.
                self._cached_metrics[cid] = metrics
                # Add fetcher back to the processing list.
                self._pending_data_fetchers.add(
                    asyncio.create_task(self._fetchers[cid].fetch_next(), name=str(cid))
                )

    async def _send_on_update(self, min_update_interval: timedelta) -> None:
        """Wait for an update notification, recalculate metric and send.

        If recalculated metric changed, then send new value.

        Args:
            min_update_interval: Minimum frequency for sending update about the change.
        """
        sender = self._result_channel.new_sender()
        latest_calculation_result: T | None = None
        # Wait for first data to come.
        # In that way the first outputs are reliable.
        await asyncio.sleep(WAIT_FOR_COMPONENT_DATA_SEC)

        while True:
            await self._update_event.wait()
            self._update_event.clear()

            result: T = self._metric_calculator.calculate(
                self._cached_metrics, self._working_batteries
            )
            if result != latest_calculation_result:
                latest_calculation_result = result
                await sender.send(result)

                if result is None:
                    sleep_for = min_update_interval.total_seconds()
                else:
                    # Sleep for the rest of the time.
                    # Then we won't send update more frequently then min_update_interval
                    time_diff = datetime.now(tz=timezone.utc) - result.timestamp
                    sleep_for = (min_update_interval - time_diff).total_seconds()
                await asyncio.sleep(sleep_for)



================================================
FILE: src/frequenz/sdk/timeseries/battery_pool/_metric_calculator.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Module that defines how to aggregate metrics from battery-inverter components."""


import logging
import math
from abc import ABC, abstractmethod
from collections.abc import Mapping, Set
from datetime import datetime, timezone
from typing import Generic, TypeVar

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentMetricId
from frequenz.quantities import Energy, Percentage, Power, Temperature

from ... import timeseries
from ..._internal import _math
from ...microgrid._power_distributing._component_managers._battery_manager import (
    _get_battery_inverter_mappings,
)
from ...microgrid._power_distributing._distribution_algorithm._battery_distribution_algorithm import (  # noqa: E501 # pylint: disable=line-too-long
    _aggregate_battery_power_bounds,
)
from ...microgrid._power_distributing.result import PowerBounds
from .._base_types import Sample, SystemBounds
from ._component_metrics import ComponentMetricsData

_logger = logging.getLogger(__name__)

_MIN_TIMESTAMP = datetime.min.replace(tzinfo=timezone.utc)
"""Minimal timestamp that can be used in the formula."""


# Formula output types class have no common interface
# Print all possible types here.
T = TypeVar("T", Sample[Percentage], Sample[Energy], SystemBounds, Sample[Temperature])
"""Type variable of the formula output."""


class MetricCalculator(ABC, Generic[T]):
    """Define how to calculate high level metrics from many components data.

    It specifies:
        * what components and metrics its needs to calculate the results,
        * how to calculate the result,
    """

    def __init__(self, batteries: Set[ComponentId]) -> None:
        """Create class instance.

        Args:
            batteries: From what batteries the data should be aggregated.
        """
        self._batteries = batteries

    @classmethod
    @abstractmethod
    def name(cls) -> str:
        """Return name of the formula.

        Returns:
            Name of the formula
        """

    @property
    def batteries(self) -> Set[ComponentId]:
        """Return set of batteries that should be used to calculate the metrics.

        Some batteries given in constructor can be discarded
        because of the failing preconditions. This method returns set of
        batteries that can be used in the calculator.

        Returns:
            Set of batteries that should be used.
        """
        return self._batteries

    @property
    @abstractmethod
    def battery_metrics(self) -> Mapping[ComponentId, list[ComponentMetricId]]:
        """Return what metrics are needed for each battery.

        Returns:
            Map between battery id and set of required metrics id.
        """

    @property
    @abstractmethod
    def inverter_metrics(self) -> Mapping[ComponentId, list[ComponentMetricId]]:
        """Return what metrics are needed for each inverter.

        Returns:
            Map between inverter id and set of required metrics id.
        """

    @abstractmethod
    def calculate(
        self,
        metrics_data: dict[ComponentId, ComponentMetricsData],
        working_batteries: set[ComponentId],
    ) -> T:
        """Aggregate the metrics_data and calculate high level metric.

        Missing components will be ignored. Formula will be calculated for all
        working batteries that are in metrics_data.

        Args:
            metrics_data: Components metrics data, that should be used to calculate the
                result.
            working_batteries: working batteries. These batteries will be used
                to calculate the result. It should be subset of the batteries given in a
                constructor.

        Returns:
            High level metric calculated from the given metrics.
            Return None if there are no component metrics.
        """


class CapacityCalculator(MetricCalculator[Sample[Energy]]):
    """Define how to calculate Capacity metrics."""

    def __init__(self, batteries: Set[ComponentId]) -> None:
        """Create class instance.

        Args:
            batteries: What batteries should be used for calculation.
        """
        super().__init__(batteries)

        self._metrics = [
            ComponentMetricId.CAPACITY,
            ComponentMetricId.SOC_LOWER_BOUND,
            ComponentMetricId.SOC_UPPER_BOUND,
        ]

    @classmethod
    def name(cls) -> str:
        """Return name of the calculator.

        Returns:
            Name of the calculator
        """
        return "Capacity"

    @property
    def battery_metrics(self) -> Mapping[ComponentId, list[ComponentMetricId]]:
        """Return what metrics are needed for each battery.

        Returns:
            Map between battery id and set of required metrics id.
        """
        return {bid: self._metrics for bid in self._batteries}

    @property
    def inverter_metrics(self) -> Mapping[ComponentId, list[ComponentMetricId]]:
        """Return what metrics are needed for each inverter.

        Returns:
            Map between inverter id and set of required metrics id.
        """
        return {}

    def calculate(
        self,
        metrics_data: dict[ComponentId, ComponentMetricsData],
        working_batteries: set[ComponentId],
    ) -> Sample[Energy]:
        """Aggregate the metrics_data and calculate high level metric.

        Missing components will be ignored. Formula will be calculated for all
        working batteries that are in metrics_data.

        Args:
            metrics_data: Components metrics data, that should be used to calculate the
                result.
            working_batteries: working batteries. These batteries will be used
                to calculate the result. It should be subset of the batteries given in a
                constructor.

        Returns:
            High level metric calculated from the given metrics.
            Return None if there are no component metrics.
        """
        timestamp = _MIN_TIMESTAMP
        total_capacity = 0.0

        for battery_id in working_batteries:
            if battery_id not in metrics_data:
                continue

            metrics = metrics_data[battery_id]

            capacity = metrics.get(ComponentMetricId.CAPACITY)
            soc_upper_bound = metrics.get(ComponentMetricId.SOC_UPPER_BOUND)
            soc_lower_bound = metrics.get(ComponentMetricId.SOC_LOWER_BOUND)

            # All metrics are related so if any is missing then we skip the component.
            if capacity is None or soc_lower_bound is None or soc_upper_bound is None:
                continue
            usable_capacity = capacity * (soc_upper_bound - soc_lower_bound) / 100
            timestamp = max(timestamp, metrics.timestamp)
            total_capacity += usable_capacity

        return (
            Sample(datetime.now(tz=timezone.utc), None)
            if timestamp == _MIN_TIMESTAMP
            else Sample[Energy](timestamp, Energy.from_watt_hours(total_capacity))
        )


class TemperatureCalculator(MetricCalculator[Sample[Temperature]]):
    """Define how to calculate temperature metrics."""

    def __init__(self, batteries: Set[ComponentId]) -> None:
        """Create class instance.

        Args:
            batteries: What batteries should be used for calculation.
        """
        super().__init__(batteries)

        self._metrics = [
            ComponentMetricId.TEMPERATURE,
        ]

    @classmethod
    def name(cls) -> str:
        """Return name of the calculator.

        Returns:
            Name of the calculator
        """
        return "temperature"

    @property
    def battery_metrics(self) -> Mapping[ComponentId, list[ComponentMetricId]]:
        """Return what metrics are needed for each battery.

        Returns:
            Map between battery id and set of required metrics id.
        """
        return {bid: self._metrics for bid in self._batteries}

    @property
    def inverter_metrics(self) -> Mapping[ComponentId, list[ComponentMetricId]]:
        """Return what metrics are needed for each inverter.

        Returns:
            Map between inverter id and set of required metrics id.
        """
        return {}

    def calculate(
        self,
        metrics_data: dict[ComponentId, ComponentMetricsData],
        working_batteries: set[ComponentId],
    ) -> Sample[Temperature]:
        """Aggregate the metrics_data and calculate high level metric for temperature.

        Missing components will be ignored. Formula will be calculated for all
        working batteries that are in metrics_data.

        Args:
            metrics_data: Components metrics data, that should be used to calculate the
                result.
            working_batteries: working batteries. These batteries will be used
                to calculate the result. It should be subset of the batteries given in a
                constructor.

        Returns:
            High level metric calculated from the given metrics.
            Return None if there are no component metrics.
        """
        timestamp = _MIN_TIMESTAMP
        temperature_sum: float = 0.0
        temperature_count: int = 0
        for battery_id in working_batteries:
            if battery_id not in metrics_data:
                continue
            metrics = metrics_data[battery_id]
            temperature = metrics.get(ComponentMetricId.TEMPERATURE)
            if temperature is None:
                continue
            timestamp = max(timestamp, metrics.timestamp)
            temperature_sum += temperature
            temperature_count += 1
        if timestamp == _MIN_TIMESTAMP:
            return Sample(datetime.now(tz=timezone.utc), None)

        temperature_avg = temperature_sum / temperature_count

        return Sample[Temperature](
            timestamp=timestamp,
            value=Temperature.from_celsius(value=temperature_avg),
        )


class SoCCalculator(MetricCalculator[Sample[Percentage]]):
    """Define how to calculate SoC metrics."""

    def __init__(self, batteries: Set[ComponentId]) -> None:
        """Create class instance.

        Args:
            batteries: What batteries should be used for calculation.
        """
        super().__init__(batteries)

        self._metrics = [
            ComponentMetricId.CAPACITY,
            ComponentMetricId.SOC_LOWER_BOUND,
            ComponentMetricId.SOC_UPPER_BOUND,
            ComponentMetricId.SOC,
        ]

    @classmethod
    def name(cls) -> str:
        """Return name of the calculator.

        Returns:
            Name of the calculator
        """
        return "SoC"

    @property
    def battery_metrics(self) -> Mapping[ComponentId, list[ComponentMetricId]]:
        """Return what metrics are needed for each battery.

        Returns:
            Map between battery id and set of required metrics id.
        """
        return {bid: self._metrics for bid in self._batteries}

    @property
    def inverter_metrics(self) -> Mapping[ComponentId, list[ComponentMetricId]]:
        """Return what metrics are needed for each inverter.

        Returns:
            Map between inverter id and set of required metrics id.
        """
        return {}

    def calculate(
        self,
        metrics_data: dict[ComponentId, ComponentMetricsData],
        working_batteries: set[ComponentId],
    ) -> Sample[Percentage]:
        """Aggregate the metrics_data and calculate high level metric.

        Missing components will be ignored. Formula will be calculated for all
        working batteries that are in metrics_data.

        Args:
            metrics_data: Components metrics data, that should be used to calculate the
                result.
            working_batteries: working batteries. These batteries will be used
                to calculate the result. It should be subset of the batteries given in a
                constructor.

        Returns:
            High level metric calculated from the given metrics.
            Return None if there are no component metrics.
        """
        timestamp = _MIN_TIMESTAMP
        usable_capacity_x100: float = 0.0
        used_capacity_x100: float = 0.0
        total_capacity_x100: float = 0.0

        for battery_id in working_batteries:
            if battery_id not in metrics_data:
                continue

            metrics = metrics_data[battery_id]

            capacity = metrics.get(ComponentMetricId.CAPACITY)
            soc_upper_bound = metrics.get(ComponentMetricId.SOC_UPPER_BOUND)
            soc_lower_bound = metrics.get(ComponentMetricId.SOC_LOWER_BOUND)
            soc = metrics.get(ComponentMetricId.SOC)

            # All metrics are related so if any is missing then we skip the component.
            if (
                capacity is None
                or soc_lower_bound is None
                or soc_upper_bound is None
                or soc is None
            ):
                continue

            # The SoC bounds are in the 0-100 range, so to get the actual usable
            # capacity, we need to divide by 100.
            #
            # We only want to calculate the SoC, and the usable capacity calculation is
            # just an intermediate step, so don't have to divide by 100 here, because it
            # gets cancelled out later.
            #
            # Therefore, the variables are named with a `_x100` suffix.
            usable_capacity_x100 = capacity * (soc_upper_bound - soc_lower_bound)
            if math.isclose(soc_upper_bound, soc_lower_bound):
                if soc < soc_lower_bound:
                    soc_scaled = 0.0
                else:
                    soc_scaled = 100.0
            else:
                soc_scaled = (
                    (soc - soc_lower_bound)
                    / (soc_upper_bound - soc_lower_bound)
                    * 100.0
                )
            # we are clamping here because the SoC might be out of bounds
            soc_scaled = min(max(soc_scaled, 0.0), 100.0)
            timestamp = max(timestamp, metrics.timestamp)
            used_capacity_x100 += usable_capacity_x100 * soc_scaled
            total_capacity_x100 += usable_capacity_x100

        if timestamp == _MIN_TIMESTAMP:
            return Sample(datetime.now(tz=timezone.utc), None)

        # When the calculated is close to 0.0 or 100.0, they are set to exactly 0.0 or
        # 100.0, to make full/empty checks using the == operator less error prone.
        pct = 0.0
        # To avoid zero division error
        if not _math.is_close_to_zero(total_capacity_x100):
            pct = used_capacity_x100 / total_capacity_x100
            if math.isclose(pct, 100.0):
                pct = 100.0

        return Sample(
            timestamp=timestamp,
            value=Percentage.from_percent(pct),
        )


class PowerBoundsCalculator(MetricCalculator[SystemBounds]):
    """Define how to calculate PowerBounds metrics."""

    def __init__(
        self,
        batteries: Set[ComponentId],
    ) -> None:
        """Create class instance.

        Args:
            batteries: What batteries should be used for calculation.
        """
        mappings: dict[str, dict[ComponentId, frozenset[ComponentId]]] = (
            _get_battery_inverter_mappings(
                batteries, inv_bats=False, bat_bats=True, inv_invs=False
            )
        )

        self._bat_inv_map = mappings["bat_invs"]
        self._bat_bats_map = mappings["bat_bats"]

        used_batteries = set(self._bat_inv_map.keys())

        if len(self._bat_inv_map) == 0:
            _logger.warning(
                "No battery in pool has adjacent inverter. Can't calculate %s.",
                PowerBoundsCalculator.name,
            )
        elif len(batteries) != len(self._bat_inv_map):
            _logger.warning(
                "Not all batteries in pool have adjacent inverter."
                "Use batteries %s for formula %s.",
                used_batteries,
                PowerBoundsCalculator.name,
            )

        super().__init__(used_batteries)
        self._battery_metrics = [
            ComponentMetricId.POWER_INCLUSION_LOWER_BOUND,
            ComponentMetricId.POWER_EXCLUSION_LOWER_BOUND,
            ComponentMetricId.POWER_EXCLUSION_UPPER_BOUND,
            ComponentMetricId.POWER_INCLUSION_UPPER_BOUND,
        ]

        self._inverter_metrics = [
            ComponentMetricId.ACTIVE_POWER_INCLUSION_LOWER_BOUND,
            ComponentMetricId.ACTIVE_POWER_EXCLUSION_LOWER_BOUND,
            ComponentMetricId.ACTIVE_POWER_EXCLUSION_UPPER_BOUND,
            ComponentMetricId.ACTIVE_POWER_INCLUSION_UPPER_BOUND,
        ]

    @classmethod
    def name(cls) -> str:
        """Return name of the calculator.

        Returns:
            Name of the calculator
        """
        return "PowerBounds"

    @property
    def battery_metrics(self) -> Mapping[ComponentId, list[ComponentMetricId]]:
        """Return what metrics are needed for each battery.

        Returns:
            Map between battery id and set of required metrics id.
        """
        return {bid: self._battery_metrics for bid in set(self._bat_inv_map.keys())}

    @property
    def inverter_metrics(self) -> Mapping[ComponentId, list[ComponentMetricId]]:
        """Return what metrics are needed for each inverter.

        Returns:
            Map between inverter id and set of required metrics id.
        """
        return {
            inverter_id: self._inverter_metrics
            for inverters in set(self._bat_inv_map.values())
            for inverter_id in inverters
        }

    # pylint: disable=too-many-locals
    def calculate(
        self,
        metrics_data: dict[ComponentId, ComponentMetricsData],
        working_batteries: set[ComponentId],
    ) -> SystemBounds:
        """Aggregate the metrics_data and calculate high level metric.

        Missing components will be ignored. Formula will be calculated for all
        working batteries that are in metrics_data.

        Args:
            metrics_data: Components metrics data.
            working_batteries: Set of working batteries. These batteries will be used
                to calculate the result.

        Returns:
            High level metric calculated from the given metrics.
            Return None if there are no component metrics.
        """
        timestamp = _MIN_TIMESTAMP
        loop_timestamp = _MIN_TIMESTAMP
        inclusion_bounds_lower = Power.zero()
        inclusion_bounds_upper = Power.zero()
        exclusion_bounds_lower = Power.zero()
        exclusion_bounds_upper = Power.zero()

        battery_sets = {
            self._bat_bats_map[battery_id] for battery_id in working_batteries
        }

        def get_validated_bounds(
            comp_id: ComponentId, comp_metric_ids: list[ComponentMetricId]
        ) -> PowerBounds | None:
            results: list[float] = []
            # Make timestamp accessible
            nonlocal loop_timestamp
            local_timestamp = loop_timestamp

            if data := metrics_data.get(comp_id):
                for comp_metric_id in comp_metric_ids:
                    val = data.get(comp_metric_id)
                    if val is not None:
                        local_timestamp = max(loop_timestamp, data.timestamp)
                        results.append(val)

            if len(results) != len(comp_metric_ids):
                return None

            loop_timestamp = local_timestamp
            return PowerBounds(
                inclusion_lower=Power.from_watts(results[0]),
                exclusion_lower=Power.from_watts(results[1]),
                exclusion_upper=Power.from_watts(results[2]),
                inclusion_upper=Power.from_watts(results[3]),
            )

        def get_bounds_list(
            comp_ids: frozenset[ComponentId], comp_metric_ids: list[ComponentMetricId]
        ) -> list[PowerBounds]:
            return list(
                x
                for x in map(
                    lambda comp_id: get_validated_bounds(comp_id, comp_metric_ids),
                    comp_ids,
                )
                if x is not None
            )

        for battery_ids in battery_sets:
            loop_timestamp = timestamp

            inverter_ids = self._bat_inv_map[next(iter(battery_ids))]

            battery_bounds = get_bounds_list(battery_ids, self._battery_metrics)

            if len(battery_bounds) == 0:
                continue

            aggregated_bat_bounds = _aggregate_battery_power_bounds(battery_bounds)

            inverter_bounds = get_bounds_list(inverter_ids, self._inverter_metrics)

            if len(inverter_bounds) == 0:
                continue

            timestamp = max(timestamp, loop_timestamp)

            inclusion_bounds_lower += max(
                aggregated_bat_bounds.inclusion_lower,
                sum(
                    (bound.inclusion_lower for bound in inverter_bounds),
                    start=Power.zero(),
                ),
            )
            inclusion_bounds_upper += min(
                aggregated_bat_bounds.inclusion_upper,
                sum(
                    (bound.inclusion_upper for bound in inverter_bounds),
                    start=Power.zero(),
                ),
            )
            exclusion_bounds_lower += min(
                aggregated_bat_bounds.exclusion_lower,
                sum(
                    (bound.exclusion_lower for bound in inverter_bounds),
                    start=Power.zero(),
                ),
            )
            exclusion_bounds_upper += max(
                aggregated_bat_bounds.exclusion_upper,
                sum(
                    (bound.exclusion_upper for bound in inverter_bounds),
                    start=Power.zero(),
                ),
            )

        if timestamp == _MIN_TIMESTAMP:
            return SystemBounds(
                timestamp=datetime.now(tz=timezone.utc),
                inclusion_bounds=None,
                exclusion_bounds=None,
            )

        return SystemBounds(
            timestamp=timestamp,
            inclusion_bounds=timeseries.Bounds(
                inclusion_bounds_lower,
                inclusion_bounds_upper,
            ),
            exclusion_bounds=timeseries.Bounds(
                exclusion_bounds_lower,
                exclusion_bounds_upper,
            ),
        )



================================================
FILE: src/frequenz/sdk/timeseries/battery_pool/messages.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Types for exposing battery pool reports."""

import abc
import typing

from frequenz.quantities import Power

from ...microgrid._power_distributing import (
    Error,
    OutOfBounds,
    PartialFailure,
    Result,
    Success,
)
from .._base_types import Bounds


# This class is used to expose the generic reports from the PowerManager with specific
# documentation for the battery pool.
class BatteryPoolReport(typing.Protocol):
    """A status report for a battery pool."""

    @property
    def target_power(self) -> Power | None:
        """The currently set power for the batteries."""

    @property
    def bounds(self) -> Bounds[Power] | None:
        """The usable bounds for the batteries.

        These bounds are adjusted to any restrictions placed by actors with higher
        priorities.

        There might be exclusion zones within these bounds. If necessary, the
        [`adjust_to_bounds`][frequenz.sdk.timeseries.battery_pool.messages.BatteryPoolReport.adjust_to_bounds]
        method may be used to check if a desired power value fits the bounds, or to get
        the closest possible power values that do fit the bounds.
        """

    @abc.abstractmethod
    def adjust_to_bounds(self, power: Power) -> tuple[Power | None, Power | None]:
        """Adjust a power value to the bounds.

        This method can be used to adjust a desired power value to the power bounds
        available to the actor.

        If the given power value falls within the usable bounds, it will be returned
        unchanged.

        If it falls outside the usable bounds, the closest possible value on the
        corresponding side will be returned.  For example, if the given power is lower
        than the lowest usable power, only the lowest usable power will be returned, and
        similarly for the highest usable power.

        If the given power falls within an exclusion zone that's contained within the
        usable bounds, the closest possible power values on both sides will be returned.

        !!! note
            It is completely optional to use this method to adjust power values before
            proposing them, because the battery pool will do this automatically.  This
            method is provided for convenience, and for granular control when there are
            two possible power values, both of which fall within the available bounds.

        Example:
            ```python
            from frequenz.sdk import microgrid

            power_status_rx = microgrid.new_battery_pool(
                priority=5,
            ).power_status.new_receiver()
            power_status = await power_status_rx.receive()
            desired_power = Power.from_watts(1000.0)

            match power_status.adjust_to_bounds(desired_power):
                case (power, _) if power == desired_power:
                    print("Desired power is available.")
                case (None, power) | (power, None) if power:
                    print(f"Closest available power is {power}.")
                case (lower, upper) if lower and upper:
                    print(f"Two options {lower}, {upper} to propose to battery pool.")
                case (None, None):
                    print("No available power")
            ```

        Args:
            power: The power value to adjust.

        Returns:
            A tuple of the closest power values to the desired power that fall within
                the available bounds for the actor.
        """


__all__ = [
    "BatteryPoolReport",
    "Error",
    "OutOfBounds",
    "PartialFailure",
    "Result",
    "Success",
]



================================================
FILE: src/frequenz/sdk/timeseries/ev_charger_pool/__init__.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Interactions with EV Chargers."""

from ._ev_charger_pool import EVChargerPool, EVChargerPoolError
from ._result_types import EVChargerPoolReport

__all__ = [
    "EVChargerPool",
    "EVChargerPoolError",
    "EVChargerPoolReport",
]



================================================
FILE: src/frequenz/sdk/timeseries/ev_charger_pool/_ev_charger_pool.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Interactions with pools of EV Chargers."""


import asyncio
import uuid
from collections import abc

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.quantities import Current, Power

from ..._internal._channels import MappingReceiverFetcher, ReceiverFetcher
from ...microgrid import _power_distributing, _power_managing
from ...timeseries import Bounds
from .._base_types import SystemBounds
from ..formula_engine import FormulaEngine, FormulaEngine3Phase
from ..formula_engine._formula_generators import (
    EVChargerCurrentFormula,
    EVChargerPowerFormula,
    FormulaGeneratorConfig,
)
from ._ev_charger_pool_reference_store import EVChargerPoolReferenceStore
from ._result_types import EVChargerPoolReport


class EVChargerPoolError(Exception):
    """An error that occurred in any of the EVChargerPool methods."""


class EVChargerPool:
    """An interface for interaction with pools of EV Chargers.

    Provides:
      - Aggregate [`power`][frequenz.sdk.timeseries.ev_charger_pool.EVChargerPool.power]
        and
        [`current_per_phase`][frequenz.sdk.timeseries.ev_charger_pool.EVChargerPool.current_per_phase]
        measurements of the EV Chargers in the pool.
    """

    def __init__(  # pylint: disable=too-many-arguments
        self,
        *,
        pool_ref_store: EVChargerPoolReferenceStore,
        name: str | None,
        priority: int,
    ) -> None:
        """Create an `EVChargerPool` instance.

        !!! note

            `EVChargerPool` instances are not meant to be created directly by users. Use
            the
            [`microgrid.new_ev_charger_pool`][frequenz.sdk.microgrid.new_ev_charger_pool]
            method for creating `EVChargerPool` instances.

        Args:
            pool_ref_store: The EV charger pool reference store instance.
            name: An optional name used to identify this instance of the pool or a
                corresponding actor in the logs.
            priority: The priority of the actor using this wrapper.
        """
        self._pool_ref_store = pool_ref_store
        unique_id = str(uuid.uuid4())
        self._source_id = unique_id if name is None else f"{name}-{unique_id}"
        self._priority = priority

    async def propose_power(
        self,
        power: Power | None,
        *,
        bounds: Bounds[Power | None] = Bounds(None, None),
    ) -> None:
        """Send a proposal to the power manager for the pool's set of EV chargers.

        This proposal is for the maximum power that can be set for the EV chargers in
        the pool.  The actual consumption might be lower based on the number of phases
        an EV is drawing power from, and its current state of charge.

        Details on how the power manager handles proposals can be found in the
        [Microgrid][frequenz.sdk.microgrid--setting-power] documentation.

        Args:
            power: The power to propose for the EV chargers in the pool.  If `None`,
                this proposal will not have any effect on the target power, unless
                bounds are specified.  When specified without bounds, bounds for lower
                priority actors will be shifted by this power.  If both are `None`, it
                is equivalent to not having a proposal or withdrawing a previous one.
            bounds: The power bounds for the proposal. When specified, these bounds will
                limit the bounds for lower priority actors.

        Raises:
            EVChargerPoolError: If a discharge power for EV chargers is requested.
        """
        if power is not None and power < Power.zero():
            raise EVChargerPoolError(
                "Discharging from EV chargers is currently not supported."
            )
        await self._pool_ref_store.power_manager_requests_sender.send(
            _power_managing.Proposal(
                source_id=self._source_id,
                preferred_power=power,
                bounds=bounds,
                component_ids=self._pool_ref_store.component_ids,
                priority=self._priority,
                creation_time=asyncio.get_running_loop().time(),
            )
        )

    @property
    def component_ids(self) -> abc.Set[ComponentId]:
        """Return component IDs of all EV Chargers managed by this EVChargerPool.

        Returns:
            Set of managed component IDs.
        """
        return self._pool_ref_store.component_ids

    @property
    def current_per_phase(self) -> FormulaEngine3Phase[Current]:
        """Fetch the total current for the EV Chargers in the pool.

        This formula produces values that are in the Passive Sign Convention (PSC).

        If a formula engine to calculate EV Charger current is not already running, it
        will be started.

        A receiver from the formula engine can be created using the `new_receiver`
        method.

        Returns:
            A FormulaEngine that will calculate and stream the total current of all EV
                Chargers.
        """
        engine = (
            self._pool_ref_store.formula_pool.from_3_phase_current_formula_generator(
                "ev_charger_total_current",
                EVChargerCurrentFormula,
                FormulaGeneratorConfig(
                    component_ids=self._pool_ref_store.component_ids
                ),
            )
        )
        assert isinstance(engine, FormulaEngine3Phase)
        return engine

    @property
    def power(self) -> FormulaEngine[Power]:
        """Fetch the total power for the EV Chargers in the pool.

        This formula produces values that are in the Passive Sign Convention (PSC).

        If a formula engine to calculate EV Charger power is not already running, it
        will be started.

        A receiver from the formula engine can be created using the `new_receiver`
        method.

        Returns:
            A FormulaEngine that will calculate and stream the total power of all EV
                Chargers.
        """
        engine = self._pool_ref_store.formula_pool.from_power_formula_generator(
            "ev_charger_power",
            EVChargerPowerFormula,
            FormulaGeneratorConfig(
                component_ids=self._pool_ref_store.component_ids,
            ),
        )
        assert isinstance(engine, FormulaEngine)
        return engine

    @property
    def power_status(self) -> ReceiverFetcher[EVChargerPoolReport]:
        """Get a receiver to receive new power status reports when they change.

        These include
          - the current inclusion/exclusion bounds available for the pool's priority,
          - the current target power for the pool's set of batteries,
          - the result of the last distribution request for the pool's set of batteries.

        Returns:
            A receiver that will stream power status reports for the pool's priority.
        """
        sub = _power_managing.ReportRequest(
            source_id=self._source_id,
            priority=self._priority,
            component_ids=self._pool_ref_store.component_ids,
        )
        self._pool_ref_store.power_bounds_subs[sub.get_channel_name()] = (
            asyncio.create_task(
                self._pool_ref_store.power_manager_bounds_subs_sender.send(sub)
            )
        )
        channel = self._pool_ref_store.channel_registry.get_or_create(
            _power_managing._Report,  # pylint: disable=protected-access
            sub.get_channel_name(),
        )
        channel.resend_latest = True

        return channel

    @property
    def power_distribution_results(self) -> ReceiverFetcher[_power_distributing.Result]:
        """Get a receiver to receive power distribution results.

        Returns:
            A receiver that will stream power distribution results for the pool's set of
            EV chargers.
        """
        return MappingReceiverFetcher(
            self._pool_ref_store.power_distribution_results_fetcher,
            lambda recv: recv.filter(
                lambda x: x.request.component_ids == self._pool_ref_store.component_ids
            ),
        )

    async def stop(self) -> None:
        """Stop all tasks and channels owned by the EVChargerPool."""
        # This was closing the pool_ref_store, which is not correct, because those are
        # shared.
        #
        # This method will do until we have a mechanism to track the resources created
        # through it.  It can also eventually cleanup the pool_ref_store, when it is
        # holding the last reference to it.

    @property
    def _system_power_bounds(self) -> ReceiverFetcher[SystemBounds]:
        """Return a receiver fetcher for the system power bounds."""
        return self._pool_ref_store.bounds_channel



================================================
FILE: src/frequenz/sdk/timeseries/ev_charger_pool/_ev_charger_pool_reference_store.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Manages shared state/tasks for a set of EV chargers."""

import asyncio
import uuid
from collections import abc

from frequenz.channels import Broadcast, Receiver, Sender
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentCategory

from ..._internal._channels import ChannelRegistry, ReceiverFetcher
from ...microgrid import connection_manager
from ...microgrid._data_sourcing import ComponentMetricRequest
from ...microgrid._power_distributing import ComponentPoolStatus, Result
from ...microgrid._power_managing._base_classes import Proposal, ReportRequest
from .._base_types import SystemBounds
from ..formula_engine._formula_engine_pool import FormulaEnginePool
from ._system_bounds_tracker import EVCSystemBoundsTracker


class EVChargerPoolReferenceStore:
    """A class for maintaining the shared state/tasks for a set of pool of EV chargers.

    This includes ownership of
    - the formula engine pool and metric calculators.
    - the tasks for calculating system bounds for the EV chargers.

    These are independent of the priority of the actors and can be shared between
    multiple users of the same set of EV chargers.

    They are exposed through the EVChargerPool class.
    """

    def __init__(  # pylint: disable=too-many-arguments
        self,
        *,
        channel_registry: ChannelRegistry,
        resampler_subscription_sender: Sender[ComponentMetricRequest],
        status_receiver: Receiver[ComponentPoolStatus],
        power_manager_requests_sender: Sender[Proposal],
        power_manager_bounds_subs_sender: Sender[ReportRequest],
        power_distribution_results_fetcher: ReceiverFetcher[Result],
        component_ids: abc.Set[ComponentId] | None = None,
    ):
        """Create an instance of the class.

        Args:
            channel_registry: A channel registry instance shared with the resampling
                actor.
            resampler_subscription_sender: A sender for sending metric requests to the
                resampling actor.
            status_receiver: A receiver that streams the status of the EV Chargers in
                the pool.
            power_manager_requests_sender: A Channel sender for sending power
                requests to the power managing actor.
            power_manager_bounds_subs_sender: A Channel sender for sending power bounds
                subscription requests to the power managing actor.
            power_distribution_results_fetcher: A ReceiverFetcher for the results from
                the power distributing actor.
            component_ids: An optional list of component_ids belonging to this pool.  If
                not specified, IDs of all EV Chargers in the microgrid will be fetched
                from the component graph.
        """
        self.channel_registry = channel_registry
        self.resampler_subscription_sender = resampler_subscription_sender
        self.status_receiver = status_receiver
        self.power_manager_requests_sender = power_manager_requests_sender
        self.power_manager_bounds_subs_sender = power_manager_bounds_subs_sender
        self.power_distribution_results_fetcher = power_distribution_results_fetcher

        if component_ids is not None:
            self.component_ids: frozenset[ComponentId] = frozenset(component_ids)
        else:
            graph = connection_manager.get().component_graph
            self.component_ids = frozenset(
                {
                    evc.component_id
                    for evc in graph.components(
                        component_categories={ComponentCategory.EV_CHARGER}
                    )
                }
            )

        self.power_bounds_subs: dict[str, asyncio.Task[None]] = {}

        self.namespace: str = f"ev-charger-pool-{uuid.uuid4()}"
        self.formula_pool = FormulaEnginePool(
            self.namespace,
            self.channel_registry,
            self.resampler_subscription_sender,
        )

        self.bounds_channel: Broadcast[SystemBounds] = Broadcast(
            name=f"System Bounds for EV Chargers: {component_ids}",
            resend_latest=True,
        )
        self.bounds_tracker: EVCSystemBoundsTracker = EVCSystemBoundsTracker(
            self.component_ids,
            self.status_receiver,
            self.bounds_channel.new_sender(),
        )
        self.bounds_tracker.start()

    async def stop(self) -> None:
        """Stop all tasks and channels owned by the EVChargerPool."""
        await self.formula_pool.stop()
        await self.bounds_tracker.stop()
        self.status_receiver.close()



================================================
FILE: src/frequenz/sdk/timeseries/ev_charger_pool/_result_types.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Types for exposing EV charger pool reports."""

import typing

from frequenz.quantities import Power

from .._base_types import Bounds


class EVChargerPoolReport(typing.Protocol):
    """A status report for an EV chargers pool."""

    @property
    def target_power(self) -> Power | None:
        """The currently set power for the EV chargers."""

    @property
    def bounds(self) -> Bounds[Power] | None:
        """The usable bounds for the EV chargers.

        These bounds are adjusted to any restrictions placed by actors with higher
        priorities.
        """



================================================
FILE: src/frequenz/sdk/timeseries/ev_charger_pool/_system_bounds_tracker.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""System bounds tracker for the EV chargers."""


import asyncio
from collections import abc

from frequenz.channels import Receiver, Sender, merge, select, selected_from
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import EVChargerData
from frequenz.quantities import Power

from ..._internal._asyncio import run_forever
from ...actor import BackgroundService
from ...microgrid import connection_manager
from ...microgrid._power_distributing._component_status import ComponentPoolStatus
from .._base_types import Bounds, SystemBounds


class EVCSystemBoundsTracker(BackgroundService):
    """Track the system bounds for the EV chargers.

    System bounds are the aggregate bounds for the EV chargers in the pool that are
    working and have an EV attached to them.  They are calculated from the individual
    bounds received from the microgrid API.

    The system bounds are sent to the `bounds_sender` whenever they change.
    """

    def __init__(
        self,
        component_ids: abc.Set[ComponentId],
        status_receiver: Receiver[ComponentPoolStatus],
        bounds_sender: Sender[SystemBounds],
    ):
        """Initialize this instance.

        Args:
            component_ids: The ids of the components to track.
            status_receiver: A receiver that streams the status of the EV Chargers in
                the pool.
            bounds_sender: A sender to send the system bounds to.
        """
        super().__init__()

        self._component_ids = component_ids
        self._status_receiver = status_receiver
        self._bounds_sender = bounds_sender
        self._latest_component_data: dict[ComponentId, EVChargerData] = {}
        self._last_sent_bounds: SystemBounds | None = None
        self._component_pool_status = ComponentPoolStatus(set(), set())

    def start(self) -> None:
        """Start the EV charger system bounds tracker."""
        self._tasks.add(asyncio.create_task(run_forever(self._run)))

    async def _send_bounds(self) -> None:
        """Calculate and send the aggregate system bounds if they have changed."""
        if not self._latest_component_data:
            return
        inclusion_bounds = Bounds(
            lower=Power.from_watts(
                sum(
                    data.active_power_inclusion_lower_bound
                    for data in self._latest_component_data.values()
                )
            ),
            upper=Power.from_watts(
                sum(
                    data.active_power_inclusion_upper_bound
                    for data in self._latest_component_data.values()
                )
            ),
        )
        exclusion_bounds = Bounds(
            lower=Power.from_watts(
                sum(
                    data.active_power_exclusion_lower_bound
                    for data in self._latest_component_data.values()
                )
            ),
            upper=Power.from_watts(
                sum(
                    data.active_power_exclusion_upper_bound
                    for data in self._latest_component_data.values()
                )
            ),
        )

        if (
            self._last_sent_bounds is None
            or inclusion_bounds != self._last_sent_bounds.inclusion_bounds
            or exclusion_bounds != self._last_sent_bounds.exclusion_bounds
        ):
            self._last_sent_bounds = SystemBounds(
                timestamp=max(
                    msg.timestamp for msg in self._latest_component_data.values()
                ),
                inclusion_bounds=inclusion_bounds,
                exclusion_bounds=exclusion_bounds,
            )
            await self._bounds_sender.send(self._last_sent_bounds)

    async def _run(self) -> None:
        """Run the system bounds tracker."""
        api_client = connection_manager.get().api_client
        status_rx = self._status_receiver
        ev_data_rx = merge(
            *(
                await asyncio.gather(
                    *[api_client.ev_charger_data(cid) for cid in self._component_ids]
                )
            )
        )

        async for selected in select(status_rx, ev_data_rx):
            if selected_from(selected, status_rx):
                self._component_pool_status = selected.message
                to_remove: list[ComponentId] = []
                for comp_id in self._latest_component_data:
                    if (
                        comp_id not in self._component_pool_status.working
                        and comp_id not in self._component_pool_status.uncertain
                    ):
                        to_remove.append(comp_id)
                for comp_id in to_remove:
                    del self._latest_component_data[comp_id]
            elif selected_from(selected, ev_data_rx):
                data = selected.message
                comp_id = data.component_id
                if (
                    comp_id not in self._component_pool_status.working
                    and comp_id not in self._component_pool_status.uncertain
                ):
                    continue
                self._latest_component_data[data.component_id] = data

            await self._send_bounds()



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/__init__.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Provides a way for the SDK to apply formulas on resampled data streams.

# Formula Engine

[`FormulaEngine`][frequenz.sdk.timeseries.formula_engine.FormulaEngine]s are used in the
SDK to calculate and stream metrics like
[`grid_power`][frequenz.sdk.timeseries.grid.Grid.power],
[`consumer_power`][frequenz.sdk.timeseries.consumer.Consumer.power], etc., which are
building blocks of the [Frequenz SDK Microgrid
Model][frequenz.sdk.microgrid--frequenz-sdk-microgrid-model].

The SDK creates the formulas by analysing the configuration of components in the
{{glossary("Component Graph")}}.

## Streaming Interface

The
[`FormulaEngine.new_receiver()`][frequenz.sdk.timeseries.formula_engine.FormulaEngine.new_receiver]
method can be used to create a [Receiver][frequenz.channels.Receiver] that streams the
[Sample][frequenz.sdk.timeseries.Sample]s calculated by the formula engine.

```python
from frequenz.sdk import microgrid

battery_pool = microgrid.new_battery_pool(priority=5)

async for power in battery_pool.power.new_receiver():
    print(f"{power=}")
```

## Composition

Composite `FormulaEngine`s can be built using arithmetic operations on
`FormulaEngine`s streaming the same type of data.

For example, if you're interested in a particular composite metric that can be
calculated by subtracting
[`new_battery_pool().power`][frequenz.sdk.timeseries.battery_pool.BatteryPool.power] and
[`new_ev_charger_pool().power`][frequenz.sdk.timeseries.ev_charger_pool.EVChargerPool]
from the
[`grid().power`][frequenz.sdk.timeseries.grid.Grid.power],
we can build a `FormulaEngine` that provides a stream of this calculated metric as
follows:

```python
from frequenz.sdk import microgrid

battery_pool = microgrid.new_battery_pool(priority=5)
ev_charger_pool = microgrid.new_ev_charger_pool(priority=5)
grid = microgrid.grid()

# apply operations on formula engines to create a formula engine that would
# apply these operations on the corresponding data streams.
net_power = (
    grid.power - (battery_pool.power + ev_charger_pool.power)
).build("net_power")

async for power in net_power.new_receiver():
    print(f"{power=}")
```

# Formula Engine 3-Phase

A [`FormulaEngine3Phase`][frequenz.sdk.timeseries.formula_engine.FormulaEngine3Phase]
is similar to a
[`FormulaEngine`][frequenz.sdk.timeseries.formula_engine.FormulaEngine], except that
they stream [3-phase samples][frequenz.sdk.timeseries.Sample3Phase].  All the
current formulas (like
[`Grid.current_per_phase`][frequenz.sdk.timeseries.grid.Grid.current_per_phase],
[`EVChargerPool.current_per_phase`][frequenz.sdk.timeseries.ev_charger_pool.EVChargerPool.current_per_phase],
etc.) are implemented as per-phase formulas.

## Streaming Interface

The
[`FormulaEngine3Phase.new_receiver()`][frequenz.sdk.timeseries.formula_engine.FormulaEngine3Phase.new_receiver]
method can be used to create a [Receiver][frequenz.channels.Receiver] that streams the
[Sample3Phase][frequenz.sdk.timeseries.Sample3Phase] values
calculated by the formula engine.

```python
from frequenz.sdk import microgrid

ev_charger_pool = microgrid.new_ev_charger_pool(priority=5)

async for sample in ev_charger_pool.current_per_phase.new_receiver():
    print(f"Current: {sample}")
```

## Composition

`FormulaEngine3Phase` instances can be composed together, just like `FormulaEngine`
instances.

```python
from frequenz.sdk import microgrid

ev_charger_pool = microgrid.new_ev_charger_pool(priority=5)
grid = microgrid.grid()

# Calculate grid consumption current that's not used by the EV chargers
other_current = (grid.current_per_phase - ev_charger_pool.current_per_phase).build(
    "other_current"
)

async for sample in other_current.new_receiver():
    print(f"Other current: {sample}")
```
"""

from ._formula_engine import FormulaEngine, FormulaEngine3Phase

__all__ = [
    "FormulaEngine",
    "FormulaEngine3Phase",
]



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_exceptions.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Formula Engine Exceptions."""


class FormulaEngineError(Exception):
    """An error occurred while fetching metrics or applying the formula on them."""



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_engine.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

# pylint: disable=too-many-lines

"""A formula engine that can apply formulas on streaming data."""

from __future__ import annotations

import asyncio
import logging
from abc import ABC
from collections import deque
from collections.abc import Callable
from typing import Any, Generic, Self, TypeVar

from frequenz.channels import Broadcast, Receiver
from frequenz.quantities import Quantity

from ..._internal._asyncio import cancel_and_await
from .._base_types import QuantityT, Sample, Sample3Phase
from ._formula_evaluator import FormulaEvaluator
from ._formula_formatter import format_formula
from ._formula_steps import (
    Adder,
    Clipper,
    ConstantValue,
    Consumption,
    Divider,
    FallbackMetricFetcher,
    FormulaStep,
    Maximizer,
    MetricFetcher,
    Minimizer,
    Multiplier,
    OpenParen,
    Production,
    Subtractor,
)
from ._tokenizer import TokenType

_logger = logging.Logger(__name__)

_operator_precedence = {
    "max": 0,
    "min": 1,
    "consumption": 2,
    "production": 3,
    "(": 4,
    "/": 5,
    "*": 6,
    "-": 7,
    "+": 8,
    ")": 9,
}
"""The dictionary of operator precedence for the shunting yard algorithm."""


class FormulaEngine(Generic[QuantityT]):
    """An engine to apply formulas on resampled data streams.

    Please refer to the [module documentation][frequenz.sdk.timeseries.formula_engine]
    for more information on how formula engines are used throughout the SDK.

    Example: Streaming the power of a battery pool.
        ```python
        from frequenz.sdk import microgrid

        battery_pool = microgrid.new_battery_pool(priority=5)

        async for power in battery_pool.power.new_receiver():
            print(f"{power=}")
        ```

    Example: Composition of formula engines.
        ```python
        from frequenz.sdk import microgrid

        battery_pool = microgrid.new_battery_pool(priority=5)
        ev_charger_pool = microgrid.new_ev_charger_pool(priority=5)
        grid = microgrid.grid()

        # apply operations on formula engines to create a formula engine that would
        # apply these operations on the corresponding data streams.
        net_power = (
            grid.power - (battery_pool.power + ev_charger_pool.power)
        ).build("net_power")

        async for power in net_power.new_receiver():
            print(f"{power=}")
        ```
    """

    def __init__(
        self,
        builder: FormulaBuilder[QuantityT],
        create_method: Callable[[float], QuantityT],
    ) -> None:
        """Create a `FormulaEngine` instance.

        Args:
            builder: A `FormulaBuilder` instance to get the formula steps and metric
                fetchers from.
            create_method: A method to generate the output `Sample` value with.  If the
                formula is for generating power values, this would be
                `Power.from_watts`, for example.
        """
        self._higher_order_builder = HigherOrderFormulaBuilder
        self._name: str = builder.name
        self._builder: FormulaBuilder[QuantityT] = builder
        self._create_method: Callable[[float], QuantityT] = create_method
        self._channel: Broadcast[Sample[QuantityT]] = Broadcast(name=self._name)
        self._task: asyncio.Task[None] | None = None

    async def stop(self) -> None:
        """Stop a running formula engine."""
        if self._task is None:
            return
        await cancel_and_await(self._task)

        _, fetchers = self._builder.finalize()
        for fetcher in fetchers.values():
            await fetcher.stop()

    @classmethod
    def from_receiver(
        cls,
        name: str,
        receiver: Receiver[Sample[QuantityT]],
        create_method: Callable[[float], QuantityT],
        *,
        nones_are_zeros: bool = False,
    ) -> FormulaEngine[QuantityT]:
        """
        Create a formula engine from a receiver.

        Can be used to compose a formula engine with a receiver. When composing
        the new engine with other engines, make sure that receiver gets data
        from the same resampler and that the `create_method`s match.

        Example:
            ```python
            from frequenz.sdk import microgrid
            from frequenz.quantities import Power

            async def run() -> None:
                producer_power_engine = microgrid.producer().power
                consumer_power_recv = microgrid.consumer().power.new_receiver()

                excess_power_recv = (
                    (
                        producer_power_engine
                        + FormulaEngine.from_receiver(
                            "consumer power",
                            consumer_power_recv,
                            Power.from_watts,
                        )
                    )
                    .build("excess power")
                    .new_receiver()
                )

            asyncio.run(run())
            ```

        Args:
            name: A name for the formula engine.
            receiver: A receiver that streams `Sample`s.
            create_method: A method to generate the output `Sample` value with,
                e.g. `Power.from_watts`.
            nones_are_zeros: If `True`, `None` values in the receiver are treated as 0.

        Returns:
            A formula engine that streams the `Sample`s from the receiver.
        """
        builder = FormulaBuilder(name, create_method)
        builder.push_metric(name, receiver, nones_are_zeros=nones_are_zeros)
        return cls(builder, create_method)

    def __add__(
        self,
        other: (
            FormulaEngine[QuantityT] | HigherOrderFormulaBuilder[QuantityT] | QuantityT
        ),
    ) -> HigherOrderFormulaBuilder[QuantityT]:
        """Return a formula builder that adds (data in) `other` to `self`.

        Args:
            other: A formula receiver, or a formula builder instance corresponding to a
                sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return HigherOrderFormulaBuilder(self, self._create_method) + other

    def __sub__(
        self,
        other: (
            FormulaEngine[QuantityT] | HigherOrderFormulaBuilder[QuantityT] | QuantityT
        ),
    ) -> HigherOrderFormulaBuilder[QuantityT]:
        """Return a formula builder that subtracts (data in) `other` from `self`.

        Args:
            other: A formula receiver, or a formula builder instance corresponding to a
                sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return HigherOrderFormulaBuilder(self, self._create_method) - other

    def __mul__(
        self,
        other: FormulaEngine[QuantityT] | HigherOrderFormulaBuilder[QuantityT] | float,
    ) -> HigherOrderFormulaBuilder[QuantityT]:
        """Return a formula builder that multiplies (data in) `self` with `other`.

        Args:
            other: A formula receiver, or a formula builder instance corresponding to a
                sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return HigherOrderFormulaBuilder(self, self._create_method) * other

    def __truediv__(
        self,
        other: FormulaEngine[QuantityT] | HigherOrderFormulaBuilder[QuantityT] | float,
    ) -> HigherOrderFormulaBuilder[QuantityT]:
        """Return a formula builder that divides (data in) `self` by `other`.

        Args:
            other: A formula receiver, or a formula builder instance corresponding to a
                sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return HigherOrderFormulaBuilder(self, self._create_method) / other

    def max(
        self,
        other: (
            FormulaEngine[QuantityT] | HigherOrderFormulaBuilder[QuantityT] | QuantityT
        ),
    ) -> HigherOrderFormulaBuilder[QuantityT]:
        """Return a formula engine that outputs the maximum of `self` and `other`.

        Args:
            other: A formula receiver, a formula builder or a QuantityT instance
                corresponding to a sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return HigherOrderFormulaBuilder(self, self._create_method).max(other)

    def min(
        self,
        other: (
            FormulaEngine[QuantityT] | HigherOrderFormulaBuilder[QuantityT] | QuantityT
        ),
    ) -> HigherOrderFormulaBuilder[QuantityT]:
        """Return a formula engine that outputs the minimum of `self` and `other`.

        Args:
            other: A formula receiver, a formula builder or a QuantityT instance
                corresponding to a sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return HigherOrderFormulaBuilder(self, self._create_method).min(other)

    def consumption(
        self,
    ) -> HigherOrderFormulaBuilder[QuantityT]:
        """
        Return a formula builder that applies the consumption operator on `self`.

        The consumption operator returns either the identity if the power value is
        positive or 0.
        """
        return HigherOrderFormulaBuilder(self, self._create_method).consumption()

    def production(
        self,
    ) -> HigherOrderFormulaBuilder[QuantityT]:
        """
        Return a formula builder that applies the production operator on `self`.

        The production operator returns either the absolute value if the power value is
        negative or 0.
        """
        return HigherOrderFormulaBuilder(self, self._create_method).production()

    async def _run(self) -> None:
        await self._builder.subscribe()
        steps, metric_fetchers = self._builder.finalize()
        evaluator = FormulaEvaluator[QuantityT](
            self._name, steps, metric_fetchers, self._create_method
        )
        sender = self._channel.new_sender()
        while True:
            try:
                msg = await evaluator.apply()
            except asyncio.CancelledError:
                _logger.debug("FormulaEngine task cancelled: %s", self._name)
                raise
            except Exception as err:  # pylint: disable=broad-except
                _logger.warning(
                    "Formula application failed: %s. Error: %s", self._name, err
                )
            else:
                await sender.send(msg)

    def __str__(self) -> str:
        """Return a string representation of the formula.

        Returns:
            A string representation of the formula.
        """
        steps = (
            self._builder._build_stack
            if len(self._builder._build_stack) > 0
            else self._builder._steps
        )
        return format_formula(steps)

    def new_receiver(
        self, name: str | None = None, max_size: int = 50
    ) -> Receiver[Sample[QuantityT]]:
        """Create a new receiver that streams the output of the formula engine.

        Args:
            name: An optional name for the receiver.
            max_size: The size of the receiver's buffer.

        Returns:
            A receiver that streams output `Sample`s from the formula engine.
        """
        if self._task is None:
            self._task = asyncio.create_task(self._run())

        recv = self._channel.new_receiver(name=name, limit=max_size)

        # This is a hack to ensure that the lifetime of the engine is tied to the
        # lifetime of the receiver.  This is necessary because the engine is a task that
        # runs forever, and in cases where higher order built for example with the below
        # idiom, the user would hold no references to the engine and it could get
        # garbage collected before the receiver.  This behaviour is explained in the
        # `asyncio.create_task` docs here:
        # https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task
        #
        #     formula = (grid_power_engine + bat_power_engine).build().new_receiver()
        recv._engine_reference = self  # type: ignore # pylint: disable=protected-access
        return recv


class FormulaEngine3Phase(Generic[QuantityT]):
    """An engine to apply formulas on 3-phase resampled data streams.

    Please refer to the [module documentation][frequenz.sdk.timeseries.formula_engine]
    for more information on how formula engines are used throughout the SDK.

    Example: Streaming the current of an EV charger pool.
        ```python
        from frequenz.sdk import microgrid

        ev_charger_pool = microgrid.new_ev_charger_pool(priority=5)

        async for sample in ev_charger_pool.current_per_phase.new_receiver():
            print(f"Current: {sample}")
        ```

    Example: Composition of formula engines.
        ```python
        from frequenz.sdk import microgrid

        ev_charger_pool = microgrid.new_ev_charger_pool(priority=5)
        grid = microgrid.grid()

        # Calculate grid consumption current that's not used by the EV chargers
        other_current = (grid.current_per_phase - ev_charger_pool.current_per_phase).build(
            "other_current"
        )

        async for sample in other_current.new_receiver():
            print(f"Other current: {sample}")
        ```
    """

    def __init__(
        self,
        name: str,
        create_method: Callable[[float], QuantityT],
        phase_streams: tuple[
            FormulaEngine[QuantityT],
            FormulaEngine[QuantityT],
            FormulaEngine[QuantityT],
        ],
    ) -> None:
        """Create a `FormulaEngine3Phase` instance.

        Args:
            name: A name for the formula.
            create_method: A method to generate the output `Sample` value with.  If the
                formula is for generating power values, this would be
                `Power.from_watts`, for example.
            phase_streams: output streams of formula engines running per-phase formulas.
        """
        self._higher_order_builder = HigherOrderFormulaBuilder3Phase
        self._name: str = name
        self._create_method: Callable[[float], QuantityT] = create_method
        self._channel: Broadcast[Sample3Phase[QuantityT]] = Broadcast(name=self._name)
        self._task: asyncio.Task[None] | None = None
        self._streams: tuple[
            FormulaEngine[QuantityT],
            FormulaEngine[QuantityT],
            FormulaEngine[QuantityT],
        ] = phase_streams

    async def stop(self) -> None:
        """Stop a running formula engine."""
        if self._task is None:
            return
        await cancel_and_await(self._task)

    def __add__(
        self,
        other: (
            FormulaEngine3Phase[QuantityT] | HigherOrderFormulaBuilder3Phase[QuantityT]
        ),
    ) -> HigherOrderFormulaBuilder3Phase[QuantityT]:
        """Return a formula builder that adds (data in) `other` to `self`.

        Args:
            other: A formula receiver, or a formula builder instance corresponding to a
                sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return HigherOrderFormulaBuilder3Phase(self, self._create_method) + other

    def __sub__(
        self: FormulaEngine3Phase[QuantityT],
        other: (
            FormulaEngine3Phase[QuantityT] | HigherOrderFormulaBuilder3Phase[QuantityT]
        ),
    ) -> HigherOrderFormulaBuilder3Phase[QuantityT]:
        """Return a formula builder that subtracts (data in) `other` from `self`.

        Args:
            other: A formula receiver, or a formula builder instance corresponding to a
                sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return HigherOrderFormulaBuilder3Phase(self, self._create_method) - other

    def __mul__(
        self: FormulaEngine3Phase[QuantityT],
        other: (
            FormulaEngine3Phase[QuantityT] | HigherOrderFormulaBuilder3Phase[QuantityT]
        ),
    ) -> HigherOrderFormulaBuilder3Phase[QuantityT]:
        """Return a formula builder that multiplies (data in) `self` with `other`.

        Args:
            other: A formula receiver, or a formula builder instance corresponding to a
                sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return HigherOrderFormulaBuilder3Phase(self, self._create_method) * other

    def __truediv__(
        self: FormulaEngine3Phase[QuantityT],
        other: (
            FormulaEngine3Phase[QuantityT] | HigherOrderFormulaBuilder3Phase[QuantityT]
        ),
    ) -> HigherOrderFormulaBuilder3Phase[QuantityT]:
        """Return a formula builder that divides (data in) `self` by `other`.

        Args:
            other: A formula receiver, or a formula builder instance corresponding to a
                sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return HigherOrderFormulaBuilder3Phase(self, self._create_method) / other

    def max(
        self: FormulaEngine3Phase[QuantityT],
        other: (
            FormulaEngine3Phase[QuantityT] | HigherOrderFormulaBuilder3Phase[QuantityT]
        ),
    ) -> HigherOrderFormulaBuilder3Phase[QuantityT]:
        """Return a formula engine that outputs the maximum of `self` and `other`.

        Args:
            other: A formula receiver, a formula builder or a QuantityT instance
                corresponding to a sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return HigherOrderFormulaBuilder3Phase(self, self._create_method).max(other)

    def min(
        self: FormulaEngine3Phase[QuantityT],
        other: (
            FormulaEngine3Phase[QuantityT] | HigherOrderFormulaBuilder3Phase[QuantityT]
        ),
    ) -> HigherOrderFormulaBuilder3Phase[QuantityT]:
        """Return a formula engine that outputs the minimum of `self` and `other`.

        Args:
            other: A formula receiver, a formula builder or a QuantityT instance
                corresponding to a sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return HigherOrderFormulaBuilder3Phase(self, self._create_method).min(other)

    def consumption(
        self: FormulaEngine3Phase[QuantityT],
    ) -> HigherOrderFormulaBuilder3Phase[QuantityT]:
        """
        Return a formula builder that applies the consumption operator on `self`.

        The consumption operator returns either the identity if the power value is
        positive or 0.
        """
        return HigherOrderFormulaBuilder3Phase(self, self._create_method).consumption()

    def production(
        self: FormulaEngine3Phase[QuantityT],
    ) -> HigherOrderFormulaBuilder3Phase[QuantityT]:
        """
        Return a formula builder that applies the production operator on `self`.

        The production operator returns either the absolute value if the power value is
        negative or 0.
        """
        return HigherOrderFormulaBuilder3Phase(self, self._create_method).production()

    async def _run(self) -> None:
        sender = self._channel.new_sender()
        phase_1_rx = self._streams[0].new_receiver()
        phase_2_rx = self._streams[1].new_receiver()
        phase_3_rx = self._streams[2].new_receiver()

        while True:
            try:
                phase_1 = await phase_1_rx.receive()
                phase_2 = await phase_2_rx.receive()
                phase_3 = await phase_3_rx.receive()
                msg = Sample3Phase(
                    phase_1.timestamp,
                    phase_1.value,
                    phase_2.value,
                    phase_3.value,
                )
            except asyncio.CancelledError:
                _logger.debug("FormulaEngine task cancelled: %s", self._name)
                raise
            await sender.send(msg)

    def new_receiver(
        self, name: str | None = None, max_size: int = 50
    ) -> Receiver[Sample3Phase[QuantityT]]:
        """Create a new receiver that streams the output of the formula engine.

        Args:
            name: An optional name for the receiver.
            max_size: The size of the receiver's buffer.

        Returns:
            A receiver that streams output `Sample`s from the formula engine.
        """
        if self._task is None:
            self._task = asyncio.create_task(self._run())

        recv = self._channel.new_receiver(name=name, limit=max_size)

        # This is a hack to ensure that the lifetime of the engine is tied to the
        # lifetime of the receiver.  This is necessary because the engine is a task that
        # runs forever, and in cases where higher order built for example with the below
        # idiom, the user would hold no references to the engine and it could get
        # garbage collected before the receiver.  This behaviour is explained in the
        # `asyncio.create_task` docs here:
        # https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task
        #
        #     formula = (grid_power_engine + bat_power_engine).build().new_receiver()
        recv._engine_reference = self  # type: ignore # pylint: disable=protected-access
        return recv


class FormulaBuilder(Generic[QuantityT]):
    """Builds a post-fix formula engine that operates on `Sample` receivers.

    Operators and metrics need to be pushed in in-fix order, and they get rearranged
    into post-fix order.  This is done using the [Shunting yard
    algorithm](https://en.wikipedia.org/wiki/Shunting_yard_algorithm).

    Example:
        To create an engine that adds the latest entries from two receivers, the
        following calls need to be made:

        ```python
        from frequenz.quantities import Power

        channel = Broadcast[Sample[Power]](name="channel")
        receiver_1 = channel.new_receiver(name="receiver_1")
        receiver_2 = channel.new_receiver(name="receiver_2")
        builder = FormulaBuilder("addition", Power)
        builder.push_metric("metric_1", receiver_1, nones_are_zeros=True)
        builder.push_oper("+")
        builder.push_metric("metric_2", receiver_2, nones_are_zeros=True)
        engine = builder.build()
        ```

        and then every call to `engine.apply()` would fetch a value from each receiver,
        add the values and return the result.
    """

    def __init__(self, name: str, create_method: Callable[[float], QuantityT]) -> None:
        """Create a `FormulaBuilder` instance.

        Args:
            name: A name for the formula being built.
            create_method: A method to generate the output `Sample` value with.  If the
                formula is for generating power values, this would be
                `Power.from_watts`, for example.
        """
        self._name = name
        self._create_method: Callable[[float], QuantityT] = create_method
        self._build_stack: list[FormulaStep] = []
        self._steps: list[FormulaStep] = []
        self._metric_fetchers: dict[str, MetricFetcher[QuantityT]] = {}

    def push_oper(self, oper: str) -> None:  # pylint: disable=too-many-branches
        """Push an operator into the engine.

        Args:
            oper: One of these strings - "+", "-", "*", "/", "(", ")"
        """
        if self._build_stack and oper != "(":
            op_prec = _operator_precedence[oper]
            while self._build_stack:
                prev_step = self._build_stack[-1]
                if op_prec < _operator_precedence[repr(prev_step)]:
                    break
                if oper == ")" and repr(prev_step) == "(":
                    self._build_stack.pop()
                    break
                if repr(prev_step) == "(":
                    break
                self._steps.append(prev_step)
                self._build_stack.pop()

        if oper == "+":
            self._build_stack.append(Adder())
        elif oper == "-":
            self._build_stack.append(Subtractor())
        elif oper == "*":
            self._build_stack.append(Multiplier())
        elif oper == "/":
            self._build_stack.append(Divider())
        elif oper == "(":
            self._build_stack.append(OpenParen())
        elif oper == "max":
            self._build_stack.append(Maximizer())
        elif oper == "min":
            self._build_stack.append(Minimizer())
        elif oper == "consumption":
            self._build_stack.append(Consumption())
        elif oper == "production":
            self._build_stack.append(Production())

    def push_metric(
        self,
        name: str,
        data_stream: Receiver[Sample[QuantityT]],
        *,
        nones_are_zeros: bool,
        fallback: FallbackMetricFetcher[QuantityT] | None = None,
    ) -> None:
        """Push a metric receiver into the engine.

        Args:
            name: A name for the metric.
            data_stream: A receiver to fetch this metric from.
            nones_are_zeros: Whether to treat None values from the stream as 0s.  If
                False, the returned value will be a None.
            fallback: Metric fetcher to use if primary one start sending
                invalid data (e.g. due to a component stop). If None, the data from
                primary metric fetcher will be used.
        """
        fetcher = self._metric_fetchers.get(name)
        if fetcher is None:
            fetcher = MetricFetcher(
                name,
                data_stream,
                nones_are_zeros=nones_are_zeros,
                fallback=fallback,
            )
            self._metric_fetchers[name] = fetcher
        self._steps.append(fetcher)

    def push_constant(self, value: float) -> None:
        """Push a constant value into the engine.

        Args:
            value: The constant value to push.
        """
        self._steps.append(ConstantValue(value))

    def push_clipper(self, min_value: float | None, max_value: float | None) -> None:
        """Push a clipper step into the engine.

        The clip will be applied on the last value available on the evaluation stack,
        before the clip step is called.

        So if an entire expression needs to be clipped, the expression should be
        enclosed in parentheses, before the clip step is added.

        For example, this clips the output of the entire expression:

        ```python
        from frequenz.quantities import Power

        builder = FormulaBuilder("example", Power)
        channel = Broadcast[Sample[Power]](name="channel")
        receiver_1 = channel.new_receiver(name="receiver_1")
        receiver_2 = channel.new_receiver(name="receiver_2")

        builder.push_oper("(")
        builder.push_metric("metric_1", receiver_1, nones_are_zeros=True)
        builder.push_oper("+")
        builder.push_metric("metric_2", receiver_2, nones_are_zeros=True)
        builder.push_oper(")")
        builder.push_clipper(min_value=0.0, max_value=None)
        ```

        And this clips the output of metric_2 only, and not the final result:

        ```python
        from frequenz.quantities import Power

        builder = FormulaBuilder("example", Power)
        channel = Broadcast[Sample[Power]](name="channel")
        receiver_1 = channel.new_receiver(name="receiver_1")
        receiver_2 = channel.new_receiver(name="receiver_2")

        builder.push_metric("metric_1", receiver_1, nones_are_zeros=True)
        builder.push_oper("+")
        builder.push_metric("metric_2", receiver_2, nones_are_zeros=True)
        builder.push_clipper(min_value=0.0, max_value=None)
        ```

        Args:
            min_value: The minimum value to clip to.
            max_value: The maximum value to clip to.
        """
        self._steps.append(Clipper(min_value, max_value))

    @property
    def name(self) -> str:
        """Return the name of the formula being built.

        Returns:
            The name of the formula being built.
        """
        return self._name

    async def subscribe(self) -> None:
        """Subscribe to metrics if needed.

        This is a no-op for the `FormulaBuilder` class, but is used by the
        `ResampledFormulaBuilder` class.
        """

    def finalize(
        self,
    ) -> tuple[list[FormulaStep], dict[str, MetricFetcher[QuantityT]]]:
        """Finalize and return the steps and fetchers for the formula.

        Returns:
            A tuple of the steps and fetchers for the formula.
        """
        while self._build_stack:
            self._steps.append(self._build_stack.pop())

        return self._steps, self._metric_fetchers

    def __str__(self) -> str:
        """Return a string representation of the formula.

        Returns:
            A string representation of the formula.
        """
        steps = self._steps if len(self._steps) > 0 else self._build_stack
        return format_formula(steps)

    def build(self) -> FormulaEngine[QuantityT]:
        """Create a formula engine with the steps and fetchers that have been pushed.

        Returns:
            A `FormulaEngine` instance.
        """
        self.finalize()
        return FormulaEngine(self, create_method=self._create_method)


FormulaEngineT = TypeVar(
    "FormulaEngineT", bound=(FormulaEngine[Any] | FormulaEngine3Phase[Any])
)


class _BaseHOFormulaBuilder(ABC, Generic[FormulaEngineT, QuantityT]):
    """Provides a way to build formulas from the outputs of other formulas."""

    def __init__(
        self,
        engine: FormulaEngineT,
        create_method: Callable[[float], QuantityT],
    ) -> None:
        """Create a `GenericHigherOrderFormulaBuilder` instance.

        Args:
            engine: A first input stream to create a builder with, so that python
                operators `+, -, *, /` can be used directly on newly created instances.
            create_method: A method to generate the output `Sample` value with.  If the
                formula is for generating power values, this would be
                `Power.from_watts`, for example.
        """
        self._steps: deque[
            tuple[
                TokenType,
                FormulaEngine[QuantityT]
                | FormulaEngine3Phase[QuantityT]
                | Quantity
                | float
                | str,
            ]
        ] = deque()
        self._steps.append((TokenType.COMPONENT_METRIC, engine))
        self._create_method: Callable[[float], QuantityT] = create_method

    def _push(
        self,
        oper: str,
        other: Self | FormulaEngineT | QuantityT | float,
    ) -> Self:
        self._steps.appendleft((TokenType.OPER, "("))
        self._steps.append((TokenType.OPER, ")"))
        self._steps.append((TokenType.OPER, oper))

        if isinstance(other, (FormulaEngine, FormulaEngine3Phase)):
            self._steps.append((TokenType.COMPONENT_METRIC, other))
        elif isinstance(other, (Quantity, float, int)):
            match oper:
                case "+" | "-" | "max" | "min":
                    if not isinstance(other, Quantity):
                        raise RuntimeError(
                            "A Quantity must be provided for addition,"
                            f" subtraction, min or max to {other}"
                        )
                case "*" | "/":
                    if not isinstance(other, (float, int)):
                        raise RuntimeError(
                            f"A float must be provided for scalar multiplication to {other}"
                        )
            self._steps.append((TokenType.CONSTANT, other))
        elif isinstance(other, _BaseHOFormulaBuilder):
            self._steps.append((TokenType.OPER, "("))
            self._steps.extend(other._steps)  # pylint: disable=protected-access
            self._steps.append((TokenType.OPER, ")"))
        else:
            raise RuntimeError(f"Can't build a formula from: {other}")
        return self

    def __add__(
        self,
        other: Self | FormulaEngineT | QuantityT,
    ) -> Self:
        """Return a formula builder that adds (data in) `other` to `self`.

        Args:
            other: A formula receiver, or a formula builder instance corresponding to a
                sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return self._push("+", other)

    def __sub__(
        self,
        other: Self | FormulaEngineT | QuantityT,
    ) -> Self:
        """Return a formula builder that subtracts (data in) `other` from `self`.

        Args:
            other: A formula receiver, or a formula builder instance corresponding to a
                sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return self._push("-", other)

    def __mul__(
        self,
        other: Self | FormulaEngineT | float,
    ) -> Self:
        """Return a formula builder that multiplies (data in) `self` with `other`.

        Args:
            other: A formula receiver, or a formula builder instance corresponding to a
                sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return self._push("*", other)

    def __truediv__(
        self,
        other: Self | FormulaEngineT | float,
    ) -> Self:
        """Return a formula builder that divides (data in) `self` by `other`.

        Args:
            other: A formula receiver, or a formula builder instance corresponding to a
                sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return self._push("/", other)

    def max(
        self,
        other: Self | FormulaEngineT | QuantityT,
    ) -> Self:
        """Return a formula builder that calculates the maximum of `self` and `other`.

        Args:
            other: A formula receiver, or a formula builder instance corresponding to a
                sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return self._push("max", other)

    def min(
        self,
        other: Self | FormulaEngineT | QuantityT,
    ) -> Self:
        """Return a formula builder that calculates the minimum of `self` and `other`.

        Args:
            other: A formula receiver, or a formula builder instance corresponding to a
                sub-expression.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        return self._push("min", other)

    def consumption(
        self,
    ) -> Self:
        """Apply the Consumption Operator.

        The consumption operator returns either the identity if the power value is
        positive or 0.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        self._steps.appendleft((TokenType.OPER, "("))
        self._steps.append((TokenType.OPER, ")"))
        self._steps.append((TokenType.OPER, "consumption"))
        return self

    def production(
        self,
    ) -> Self:
        """Apply the Production Operator.

        The production operator returns either the absolute value if the power value is
        negative or 0.

        Returns:
            A formula builder that can take further expressions, or can be built
                into a formula engine.
        """
        self._steps.appendleft((TokenType.OPER, "("))
        self._steps.append((TokenType.OPER, ")"))
        self._steps.append((TokenType.OPER, "production"))
        return self


class HigherOrderFormulaBuilder(
    Generic[QuantityT], _BaseHOFormulaBuilder[FormulaEngine[QuantityT], QuantityT]
):
    """A specialization of the _BaseHOFormulaBuilder for `FormulaReceiver`."""

    def build(
        self, name: str, *, nones_are_zeros: bool = False
    ) -> FormulaEngine[QuantityT]:
        """Build a `FormulaEngine` instance from the builder.

        Args:
            name: A name for the newly generated formula.
            nones_are_zeros: whether `None` values in any of the input streams should be
                treated as zeros.

        Returns:
            A `FormulaEngine` instance.
        """
        builder = FormulaBuilder(name, self._create_method)
        for typ, value in self._steps:
            if typ == TokenType.COMPONENT_METRIC:
                assert isinstance(value, FormulaEngine)
                builder.push_metric(
                    value._name,  # pylint: disable=protected-access
                    value.new_receiver(),
                    nones_are_zeros=nones_are_zeros,
                )
            elif typ == TokenType.OPER:
                assert isinstance(value, str)
                builder.push_oper(value)
            elif typ == TokenType.CONSTANT:
                assert isinstance(value, (Quantity, float))
                builder.push_constant(
                    value.base_value if isinstance(value, Quantity) else value
                )
        return builder.build()


class HigherOrderFormulaBuilder3Phase(
    Generic[QuantityT], _BaseHOFormulaBuilder[FormulaEngine3Phase[QuantityT], QuantityT]
):
    """A specialization of the _BaseHOFormulaBuilder for `FormulaReceiver3Phase`."""

    def build(
        self, name: str, *, nones_are_zeros: bool = False
    ) -> FormulaEngine3Phase[QuantityT]:
        """Build a `FormulaEngine3Phase` instance from the builder.

        Args:
            name: A name for the newly generated formula.
            nones_are_zeros: whether `None` values in any of the input streams should be
                treated as zeros.

        Returns:
            A `FormulaEngine3Phase` instance.
        """
        builders = [
            FormulaBuilder(name, self._create_method),
            FormulaBuilder(name, self._create_method),
            FormulaBuilder(name, self._create_method),
        ]
        for typ, value in self._steps:
            if typ == TokenType.COMPONENT_METRIC:
                assert isinstance(value, FormulaEngine3Phase)
                for phase in range(3):
                    builders[phase].push_metric(
                        f"{value._name}-{phase+1}",  # pylint: disable=protected-access
                        value._streams[  # pylint: disable=protected-access
                            phase
                        ].new_receiver(),
                        nones_are_zeros=nones_are_zeros,
                    )
            elif typ == TokenType.OPER:
                assert isinstance(value, str)
                for phase in range(3):
                    builders[phase].push_oper(value)
        return FormulaEngine3Phase(
            name,
            self._create_method,
            (
                builders[0].build(),
                builders[1].build(),
                builders[2].build(),
            ),
        )



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_engine_pool.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""A formula pool for helping with tracking running formula engines."""

from __future__ import annotations

from typing import TYPE_CHECKING

from frequenz.channels import Sender
from frequenz.client.microgrid import ComponentMetricId
from frequenz.quantities import Current, Power, Quantity, ReactivePower

from ..._internal._channels import ChannelRegistry
from ...microgrid._data_sourcing import ComponentMetricRequest
from ._formula_generators._formula_generator import (
    FormulaGenerator,
    FormulaGeneratorConfig,
)
from ._resampled_formula_builder import ResampledFormulaBuilder

if TYPE_CHECKING:
    # Break circular import
    from ..formula_engine import FormulaEngine, FormulaEngine3Phase


class FormulaEnginePool:
    """Creates and owns formula engines from string formulas, or formula generators.

    If an engine already exists with a given name, it is reused instead.
    """

    def __init__(
        self,
        namespace: str,
        channel_registry: ChannelRegistry,
        resampler_subscription_sender: Sender[ComponentMetricRequest],
    ) -> None:
        """Create a new instance.

        Args:
            namespace: namespace to use with the data pipeline.
            channel_registry: A channel registry instance shared with the resampling
                actor.
            resampler_subscription_sender: A sender for sending metric requests to the
                resampling actor.
        """
        self._namespace: str = namespace
        self._channel_registry: ChannelRegistry = channel_registry
        self._resampler_subscription_sender: Sender[ComponentMetricRequest] = (
            resampler_subscription_sender
        )
        self._string_engines: dict[str, FormulaEngine[Quantity]] = {}
        self._power_engines: dict[str, FormulaEngine[Power]] = {}
        self._power_3_phase_engines: dict[str, FormulaEngine3Phase[Power]] = {}
        self._current_engines: dict[str, FormulaEngine3Phase[Current]] = {}
        self._reactive_power_engines: dict[str, FormulaEngine[ReactivePower]] = {}

    def from_string(
        self,
        formula: str,
        component_metric_id: ComponentMetricId,
        *,
        nones_are_zeros: bool = False,
    ) -> FormulaEngine[Quantity]:
        """Get a receiver for a manual formula.

        Args:
            formula: formula to execute.
            component_metric_id: The metric ID to use when fetching receivers from the
                resampling actor.
            nones_are_zeros: Whether to treat None values from the stream as 0s.  If
                False, the returned value will be a None.

        Returns:
            A FormulaReceiver that streams values with the formulas applied.
        """
        channel_key = formula + component_metric_id.value
        if channel_key in self._string_engines:
            return self._string_engines[channel_key]

        builder = ResampledFormulaBuilder(
            namespace=self._namespace,
            formula_name=formula,
            channel_registry=self._channel_registry,
            resampler_subscription_sender=self._resampler_subscription_sender,
            metric_id=component_metric_id,
            create_method=Quantity,
        )
        formula_engine = builder.from_string(formula, nones_are_zeros=nones_are_zeros)
        self._string_engines[channel_key] = formula_engine

        return formula_engine

    def from_reactive_power_formula_generator(
        self,
        channel_key: str,
        generator: type[FormulaGenerator[ReactivePower]],
        config: FormulaGeneratorConfig = FormulaGeneratorConfig(),
    ) -> FormulaEngine[ReactivePower]:
        """Get a receiver for a formula from a generator.

        Args:
            channel_key: A string to uniquely identify the formula.
            generator: A formula generator.
            config: config to initialize the formula generator with.

        Returns:
            A FormulaReceiver or a FormulaReceiver3Phase instance based on what the
                FormulaGenerator returns.
        """
        from ._formula_engine import (  # pylint: disable=import-outside-toplevel
            FormulaEngine,
        )

        if channel_key in self._reactive_power_engines:
            return self._reactive_power_engines[channel_key]

        engine = generator(
            self._namespace,
            self._channel_registry,
            self._resampler_subscription_sender,
            config,
        ).generate()
        assert isinstance(engine, FormulaEngine)
        self._reactive_power_engines[channel_key] = engine
        return engine

    def from_power_formula_generator(
        self,
        channel_key: str,
        generator: type[FormulaGenerator[Power]],
        config: FormulaGeneratorConfig = FormulaGeneratorConfig(),
    ) -> FormulaEngine[Power]:
        """Get a receiver for a formula from a generator.

        Args:
            channel_key: A string to uniquely identify the formula.
            generator: A formula generator.
            config: config to initialize the formula generator with.

        Returns:
            A FormulaReceiver or a FormulaReceiver3Phase instance based on what the
                FormulaGenerator returns.
        """
        from ._formula_engine import (  # pylint: disable=import-outside-toplevel
            FormulaEngine,
        )

        if channel_key in self._power_engines:
            return self._power_engines[channel_key]

        engine = generator(
            self._namespace,
            self._channel_registry,
            self._resampler_subscription_sender,
            config,
        ).generate()
        assert isinstance(engine, FormulaEngine)
        self._power_engines[channel_key] = engine
        return engine

    def from_power_3_phase_formula_generator(
        self,
        channel_key: str,
        generator: type[FormulaGenerator[Power]],
        config: FormulaGeneratorConfig = FormulaGeneratorConfig(),
    ) -> FormulaEngine3Phase[Power]:
        """Get a formula engine that streams 3-phase power values.

        Args:
            channel_key: The string to uniquely identify the formula.
            generator: The formula generator.
            config: The config to initialize the formula generator with.

        Returns:
            A formula engine that streams [3-phase][frequenz.sdk.timeseries.Sample3Phase]
            power values.
        """
        from ._formula_engine import (  # pylint: disable=import-outside-toplevel
            FormulaEngine3Phase,
        )

        if channel_key in self._power_3_phase_engines:
            return self._power_3_phase_engines[channel_key]

        engine = generator(
            self._namespace,
            self._channel_registry,
            self._resampler_subscription_sender,
            config,
        ).generate()
        assert isinstance(engine, FormulaEngine3Phase)
        self._power_3_phase_engines[channel_key] = engine
        return engine

    def from_3_phase_current_formula_generator(
        self,
        channel_key: str,
        generator: type[FormulaGenerator[Current]],
        config: FormulaGeneratorConfig = FormulaGeneratorConfig(),
    ) -> FormulaEngine3Phase[Current]:
        """Get a receiver for a formula from a generator.

        Args:
            channel_key: A string to uniquely identify the formula.
            generator: A formula generator.
            config: config to initialize the formula generator with.

        Returns:
            A FormulaReceiver or a FormulaReceiver3Phase instance based on what the
                FormulaGenerator returns.
        """
        from ._formula_engine import (  # pylint: disable=import-outside-toplevel
            FormulaEngine3Phase,
        )

        if channel_key in self._current_engines:
            return self._current_engines[channel_key]

        engine = generator(
            self._namespace,
            self._channel_registry,
            self._resampler_subscription_sender,
            config,
        ).generate()
        assert isinstance(engine, FormulaEngine3Phase)
        self._current_engines[channel_key] = engine
        return engine

    async def stop(self) -> None:
        """Stop all formula engines in the pool."""
        for string_engine in self._string_engines.values():
            await string_engine.stop()
        for power_engine in self._power_engines.values():
            await power_engine.stop()
        for power_3_phase_engine in self._power_3_phase_engines.values():
            await power_3_phase_engine.stop()
        for current_engine in self._current_engines.values():
            await current_engine.stop()
        for reactive_power_engine in self._reactive_power_engines.values():
            await reactive_power_engine.stop()



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_evaluator.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""A post-fix formula evaluator that operates on `Sample` receivers."""

import asyncio
from collections.abc import Callable
from datetime import datetime
from math import isinf, isnan
from typing import Generic

from .._base_types import QuantityT, Sample
from ._formula_steps import FormulaStep, MetricFetcher


class FormulaEvaluator(Generic[QuantityT]):
    """A post-fix formula evaluator that operates on `Sample` receivers."""

    def __init__(
        self,
        name: str,
        steps: list[FormulaStep],
        metric_fetchers: dict[str, MetricFetcher[QuantityT]],
        create_method: Callable[[float], QuantityT],
    ) -> None:
        """Create a `FormulaEngine` instance.

        Args:
            name: A name for the formula.
            steps: Steps for the engine to execute, in post-fix order.
            metric_fetchers: Fetchers for each metric stream the formula depends on.
            create_method: A method to generate the output `Sample` value with.  If the
                formula is for generating power values, this would be
                `Power.from_watts`, for example.
        """
        self._name = name
        self._steps = steps
        self._metric_fetchers: dict[str, MetricFetcher[QuantityT]] = metric_fetchers
        self._first_run = True
        self._create_method: Callable[[float], QuantityT] = create_method

    async def _synchronize_metric_timestamps(
        self, metrics: set[asyncio.Task[Sample[QuantityT] | None]]
    ) -> datetime:
        """Synchronize the metric streams.

        For synchronised streams like data from the `ComponentMetricsResamplingActor`,
        this a call to this function is required only once, before the first set of
        inputs are fetched.

        Args:
            metrics: The finished tasks from the first `fetch_next` calls to all the
                `MetricFetcher`s.

        Returns:
            The timestamp of the latest metric value.

        Raises:
            RuntimeError: when some streams have no value, or when the synchronization
                of timestamps fails.
        """
        metrics_by_ts: dict[datetime, list[str]] = {}
        for metric in metrics:
            result = metric.result()
            name = metric.get_name()
            if result is None:
                raise RuntimeError(f"Stream closed for component: {name}")
            metrics_by_ts.setdefault(result.timestamp, []).append(name)
        latest_ts = max(metrics_by_ts)

        # fetch the metrics with non-latest timestamps again until we have the values
        # for the same ts for all metrics.
        for metric_ts, names in metrics_by_ts.items():
            if metric_ts == latest_ts:
                continue
            while metric_ts < latest_ts:
                for name in names:
                    fetcher = self._metric_fetchers[name]
                    next_val = await fetcher.fetch_next()
                    assert next_val is not None
                    metric_ts = next_val.timestamp
            if metric_ts > latest_ts:
                raise RuntimeError(
                    "Unable to synchronize resampled metric timestamps, "
                    f"for formula: {self._name}"
                )
        self._first_run = False
        return latest_ts

    async def apply(self) -> Sample[QuantityT]:
        """Fetch the latest metrics, apply the formula once and return the result.

        Returns:
            The result of the formula.

        Raises:
            RuntimeError: if some samples didn't arrive, or if formula application
                failed.
        """
        eval_stack: list[float] = []
        ready_metrics, pending = await asyncio.wait(
            [
                asyncio.create_task(fetcher.fetch_next(), name=name)
                for name, fetcher in self._metric_fetchers.items()
            ],
            return_when=asyncio.ALL_COMPLETED,
        )

        if pending or any(res.result() is None for res in iter(ready_metrics)):
            raise RuntimeError(
                f"Some resampled metrics didn't arrive, for formula: {self._name}"
            )

        if self._first_run:
            metric_ts = await self._synchronize_metric_timestamps(ready_metrics)
        else:
            sample = next(iter(ready_metrics)).result()
            assert sample is not None
            metric_ts = sample.timestamp

        for step in self._steps:
            step.apply(eval_stack)

        # if all steps were applied and the formula was correct, there should only be a
        # single value in the evaluation stack, and that would be the formula result.
        if len(eval_stack) != 1:
            raise RuntimeError(f"Formula application failed: {self._name}")

        res = eval_stack.pop()
        if isnan(res) or isinf(res):
            return Sample(metric_ts, None)

        return Sample(metric_ts, self._create_method(res))



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_formatter.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Formatter for the formula."""

from __future__ import annotations

import enum

from ._formula_steps import (
    Adder,
    Clipper,
    ConstantValue,
    Divider,
    FormulaStep,
    Maximizer,
    MetricFetcher,
    Minimizer,
    Multiplier,
    OpenParen,
    Subtractor,
)


class OperatorPrecedence(enum.Enum):
    """The precedence of an operator."""

    ADDITION = 1
    SUBTRACTION = 1
    MULTIPLICATION = 2
    DIVISION = 2
    PRIMARY = 9

    def __lt__(self, other: OperatorPrecedence) -> bool:
        """Test the precedence of this operator is less than the precedence of the other operator.

        Args:
            other: The other operator (on the right-hand side).

        Returns:
            Whether the precedence of this operator is less than the other operator.
        """
        return self.value < other.value

    def __le__(self, other: OperatorPrecedence) -> bool:
        """Test the precedence of this operator is less than or equal to the other operator.

        Args:
            other: The other operator (on the right-hand side).

        Returns:
            Whether the precedence of this operator is less than or equal to the other operator.
        """
        return self.value <= other.value


class Operator(enum.Enum):
    """The precedence of an operator."""

    ADDITION = "+"
    SUBTRACTION = "-"
    MULTIPLICATION = "*"
    DIVISION = "/"

    @property
    def precedence(self) -> OperatorPrecedence:
        """Return the precedence of this operator.

        Returns:
            The precedence of this operator.
        """
        match self:
            case Operator.SUBTRACTION:
                return OperatorPrecedence.SUBTRACTION
            case Operator.ADDITION:
                return OperatorPrecedence.ADDITION
            case Operator.DIVISION:
                return OperatorPrecedence.DIVISION
            case Operator.MULTIPLICATION:
                return OperatorPrecedence.MULTIPLICATION

    def __str__(self) -> str:
        """Return the string representation of the operator precedence.

        Returns:
            The string representation of the operator precedence.
        """
        return str(self.value)


class StackItem:
    """Stack item for the formula formatter."""

    def __init__(self, value: str, precedence: OperatorPrecedence, num_steps: int):
        """Initialize the StackItem.

        Args:
            value: The value of the stack item.
            precedence: The precedence of the stack item.
            num_steps: The number of steps of the stack item.
        """
        self.value = value
        self.precedence = precedence
        self.num_steps = num_steps

    def __str__(self) -> str:
        """Return the string representation of the stack item.

        This is used for debugging purposes.

        Returns:
            str: The string representation of the stack item.
        """
        return f'("{self.value}", {self.precedence}, {self.num_steps})'

    def as_left_value(self, outer_precedence: OperatorPrecedence) -> str:
        """Return the value of the stack item with parentheses if necessary.

        Args:
            outer_precedence: The precedence of the outer stack item.

        Returns:
            str: The value of the stack item with parentheses if necessary.
        """
        return f"({self.value})" if self.precedence < outer_precedence else self.value

    def as_right_value(self, outer_precedence: OperatorPrecedence) -> str:
        """Return the value of the stack item with parentheses if necessary.

        Args:
            outer_precedence: The precedence of the outer stack item.

        Returns:
            str: The value of the stack item with parentheses if necessary.
        """
        if self.num_steps > 1:
            return (
                f"({self.value})" if self.precedence <= outer_precedence else self.value
            )
        return f"({self.value})" if self.precedence < outer_precedence else self.value

    @staticmethod
    def create_binary(lhs: StackItem, operator: Operator, rhs: StackItem) -> StackItem:
        """Create a binary stack item.

        Args:
            lhs: The left-hand side of the binary operation.
            operator: The operator of the binary operation.
            rhs: The right-hand side of the binary operation.

        Returns:
            StackItem: The binary stack item.
        """
        pred = OperatorPrecedence(operator.precedence)
        return StackItem(
            f"{lhs.as_left_value(pred)} {operator} {rhs.as_right_value(pred)}",
            pred,
            lhs.num_steps + 1 + rhs.num_steps,
        )

    @staticmethod
    def create_primary(value: float) -> StackItem:
        """Create a stack item for literal values or function calls (primary expressions).

        Args:
            value: The value of the literal.

        Returns:
            StackItem: The literal stack item.
        """
        return StackItem(str(value), OperatorPrecedence.PRIMARY, 1)


class FormulaFormatter:
    """Formats a formula into a human readable string in infix-notation."""

    def __init__(self) -> None:
        """Initialize the FormulaFormatter."""
        self._stack = list[StackItem]()

    def format(self, postfix_expr: list[FormulaStep]) -> str:
        """Format the postfix expression to infix notation.

        Args:
            postfix_expr: The steps of the formula in postfix notation order.

        Returns:
            str: The formula in infix notation.
        """
        for step in postfix_expr:
            match step:
                case ConstantValue():
                    self._stack.append(StackItem.create_primary(step.value))
                case Adder():
                    self._format_binary(Operator.ADDITION)
                case Subtractor():
                    self._format_binary(Operator.SUBTRACTION)
                case Multiplier():
                    self._format_binary(Operator.MULTIPLICATION)
                case Divider():
                    self._format_binary(Operator.DIVISION)
                case Clipper():
                    the_value = self._stack.pop()
                    min_value = step.min_value if step.min_value is not None else "-inf"
                    max_value = step.max_value if step.max_value is not None else "inf"
                    value = f"clip({min_value}, {the_value.value}, {max_value})"
                    self._stack.append(StackItem(value, OperatorPrecedence.PRIMARY, 1))
                case Maximizer():
                    left, right = self._pop_two_from_stack()
                    value = f"max({left.value}, {right.value})"
                    self._stack.append(StackItem(value, OperatorPrecedence.PRIMARY, 1))
                case Minimizer():
                    left, right = self._pop_two_from_stack()
                    value = f"min({left.value}, {right.value})"
                    self._stack.append(StackItem(value, OperatorPrecedence.PRIMARY, 1))
                case MetricFetcher():
                    metric_fetcher = step
                    value = str(metric_fetcher)
                    if engine_reference := getattr(
                        metric_fetcher.stream, "_engine_reference", None
                    ):
                        value = f"[{value}]({str(engine_reference)})"
                    self._stack.append(StackItem(value, OperatorPrecedence.PRIMARY, 1))
                case OpenParen():
                    pass  # We gently ignore this one.

        assert (
            len(self._stack) == 1
        ), f"The formula {postfix_expr} is not valid. Evaluation stack left-over: {self._stack}"
        return self._stack[0].value

    def _format_binary(self, operator: Operator) -> None:
        """Format a binary operation.

        Pops the arguments of the binary expression from the stack
        and pushes the string representation of the binary operation to the stack.

        Args:
            operator: The operator of the binary operation.
        """
        left, right = self._pop_two_from_stack()
        self._stack.append(StackItem.create_binary(left, operator, right))

    def _pop_two_from_stack(self) -> tuple[StackItem, StackItem]:
        """Pop two items from the stack.

        Returns:
            The two items popped from the stack.
        """
        right = self._stack.pop()
        left = self._stack.pop()
        return left, right


def format_formula(postfix_expr: list[FormulaStep]) -> str:
    """Return the formula as a string in infix notation.

    Args:
        postfix_expr: The steps of the formula in postfix notation order.

    Returns:
        str: The formula in infix notation.
    """
    formatter = FormulaFormatter()
    return formatter.format(postfix_expr)



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_steps.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Steps for building formula engines with."""

from __future__ import annotations

import logging
import math
from abc import ABC, abstractmethod
from typing import Generic

from frequenz.channels import Receiver, ReceiverError, ReceiverStoppedError

from .._base_types import QuantityT, Sample

_logger = logging.getLogger(__name__)


class FormulaStep(ABC):
    """Represents an individual step/stage in a formula.

    Each step, when applied on to an evaluation stack, would pop its input parameters
    from the stack and push its result back in.
    """

    @abstractmethod
    def __repr__(self) -> str:
        """Return a string representation of the step.

        Returns:
            A string representation of the step.
        """

    @abstractmethod
    def apply(self, eval_stack: list[float]) -> None:
        """Apply a formula operation on the eval_stack.

        Args:
            eval_stack: An evaluation stack, to apply the formula step on.
        """


class Adder(FormulaStep):
    """A formula step for adding two values."""

    def __repr__(self) -> str:
        """Return a string representation of the step.

        Returns:
            A string representation of the step.
        """
        return "+"

    def apply(self, eval_stack: list[float]) -> None:
        """Extract two values from the stack, add them, push the result back in.

        Args:
            eval_stack: An evaluation stack, to apply the formula step on.
        """
        val2 = eval_stack.pop()
        val1 = eval_stack.pop()
        res = val1 + val2
        eval_stack.append(res)


class Subtractor(FormulaStep):
    """A formula step for subtracting one value from another."""

    def __repr__(self) -> str:
        """Return a string representation of the step.

        Returns:
            A string representation of the step.
        """
        return "-"

    def apply(self, eval_stack: list[float]) -> None:
        """Extract two values from the stack, subtract them, push the result back in.

        Args:
            eval_stack: An evaluation stack, to apply the formula step on.
        """
        val2 = eval_stack.pop()
        val1 = eval_stack.pop()
        res = val1 - val2
        eval_stack.append(res)


class Multiplier(FormulaStep):
    """A formula step for multiplying two values."""

    def __repr__(self) -> str:
        """Return a string representation of the step.

        Returns:
            A string representation of the step.
        """
        return "*"

    def apply(self, eval_stack: list[float]) -> None:
        """Extract two values from the stack, multiply them, push the result back in.

        Args:
            eval_stack: An evaluation stack, to apply the formula step on.
        """
        val2 = eval_stack.pop()
        val1 = eval_stack.pop()
        res = val1 * val2
        eval_stack.append(res)


class Divider(FormulaStep):
    """A formula step for dividing one value by another."""

    def __repr__(self) -> str:
        """Return a string representation of the step.

        Returns:
            A string representation of the step.
        """
        return "/"

    def apply(self, eval_stack: list[float]) -> None:
        """Extract two values from the stack, divide them, push the result back in.

        Args:
            eval_stack: An evaluation stack, to apply the formula step on.
        """
        val2 = eval_stack.pop()
        val1 = eval_stack.pop()
        res = val1 / val2
        eval_stack.append(res)


class Maximizer(FormulaStep):
    """A formula step that represents the max function."""

    def __repr__(self) -> str:
        """Return a string representation of the step.

        Returns:
            A string representation of the step.
        """
        return "max"

    def apply(self, eval_stack: list[float]) -> None:
        """Extract two values from the stack and pushes back the maximum.

        Args:
            eval_stack: An evaluation stack, to apply the formula step on.
        """
        val2 = eval_stack.pop()
        val1 = eval_stack.pop()
        res = max(val1, val2)
        eval_stack.append(res)


class Minimizer(FormulaStep):
    """A formula step that represents the min function."""

    def __repr__(self) -> str:
        """Return a string representation of the step.

        Returns:
            A string representation of the step.
        """
        return "min"

    def apply(self, eval_stack: list[float]) -> None:
        """Extract two values from the stack and pushes back the minimum.

        Args:
            eval_stack: An evaluation stack, to apply the formula step on.
        """
        val2 = eval_stack.pop()
        val1 = eval_stack.pop()
        res = min(val1, val2)
        eval_stack.append(res)


class Consumption(FormulaStep):
    """A formula step that represents the consumption operator.

    The consumption operator is the maximum of the value on top
    of the evaluation stack and 0.
    """

    def __repr__(self) -> str:
        """Return a string representation of the step.

        Returns:
            A string representation of the step.
        """
        return "consumption"

    def apply(self, eval_stack: list[float]) -> None:
        """
        Apply the consumption formula.

        Replace the top of the eval eval_stack with the same value if the value
        is positive or 0.

        Args:
            eval_stack: An evaluation stack, to apply the formula step on.
        """
        val = eval_stack.pop()
        eval_stack.append(max(val, 0))


class Production(FormulaStep):
    """A formula step that represents the production operator.

    The production operator is the maximum of the value times minus one on top
    of the evaluation stack and 0.
    """

    def __repr__(self) -> str:
        """Return a string representation of the step.

        Returns:
            A string representation of the step.
        """
        return "production"

    def apply(self, eval_stack: list[float]) -> None:
        """
        Apply the production formula.

        Replace the top of the eval eval_stack with its absolute value if the
        value is negative or 0.

        Args:
            eval_stack: An evaluation stack, to apply the formula step on.
        """
        val = eval_stack.pop()
        eval_stack.append(max(-val, 0))


class OpenParen(FormulaStep):
    """A no-op formula step used while building a prefix formula engine.

    Any OpenParen steps would get removed once a formula is built.
    """

    def __repr__(self) -> str:
        """Return a string representation of the step.

        Returns:
            A string representation of the step.
        """
        return "("

    def apply(self, _: list[float]) -> None:
        """No-op."""


class ConstantValue(FormulaStep):
    """A formula step for inserting a constant value."""

    def __init__(self, value: float) -> None:
        """Create a `ConstantValue` instance.

        Args:
            value: The constant value.
        """
        self._value = value

    @property
    def value(self) -> float:
        """Return the constant value.

        Returns:
            The constant value.
        """
        return self._value

    def __repr__(self) -> str:
        """Return a string representation of the step.

        Returns:
            A string representation of the step.
        """
        return str(self._value)

    def apply(self, eval_stack: list[float]) -> None:
        """Push the constant value to the eval_stack.

        Args:
            eval_stack: An evaluation stack, to append the constant value to.
        """
        eval_stack.append(self._value)


class Clipper(FormulaStep):
    """A formula step for clipping a value between a minimum and maximum."""

    def __init__(self, min_val: float | None, max_val: float | None) -> None:
        """Create a `Clipper` instance.

        Args:
            min_val: The minimum value.
            max_val: The maximum value.
        """
        self._min_val = min_val
        self._max_val = max_val

    @property
    def min_value(self) -> float | None:
        """Return the minimum value.

        Returns:
            The minimum value.
        """
        return self._min_val

    @property
    def max_value(self) -> float | None:
        """Return the maximum value.

        Returns:
            The maximum value.
        """
        return self._max_val

    def __repr__(self) -> str:
        """Return a string representation of the step.

        Returns:
            A string representation of the step.
        """
        return f"clip({self._min_val}, {self._max_val})"

    def apply(self, eval_stack: list[float]) -> None:
        """Clip the value at the top of the eval_stack.

        Args:
            eval_stack: An evaluation stack, to apply the formula step on.
        """
        val = eval_stack.pop()
        if self._min_val is not None:
            val = max(val, self._min_val)
        if self._max_val is not None:
            val = min(val, self._max_val)
        eval_stack.append(val)


class FallbackMetricFetcher(Receiver[Sample[QuantityT]], Generic[QuantityT]):
    """A fallback metric fetcher for formula engines.

    Generates a metric value from the fallback components if the primary metric
    is invalid.

    This class starts running when the primary MetricFetcher starts receiving invalid data.
    """

    @property
    @abstractmethod
    def name(self) -> str:
        """Get the name of the fetcher."""

    @property
    @abstractmethod
    def is_running(self) -> bool:
        """Check whether the metric fetcher is running."""

    @abstractmethod
    def start(self) -> None:
        """Initialize the metric fetcher and start fetching samples."""

    @abstractmethod
    async def stop(self) -> None:
        """Stope the fetcher if is running."""


class MetricFetcher(Generic[QuantityT], FormulaStep):
    """A formula step for fetching a value from a metric Receiver."""

    def __init__(
        self,
        name: str,
        stream: Receiver[Sample[QuantityT]],
        *,
        nones_are_zeros: bool,
        fallback: FallbackMetricFetcher[QuantityT] | None = None,
    ) -> None:
        """Create a `MetricFetcher` instance.

        Args:
            name: The name of the metric.
            stream: A channel receiver from which to fetch samples.
            nones_are_zeros: Whether to treat None values from the stream as 0s.
            fallback: Metric fetcher to use if primary one start sending
                invalid data (e.g. due to a component stop). If None, the data from
                primary metric fetcher will be used.
        """
        self._name = name
        self._stream: Receiver[Sample[QuantityT]] = stream
        self._next_value: Sample[QuantityT] | None = None
        self._nones_are_zeros = nones_are_zeros
        self._fallback: FallbackMetricFetcher[QuantityT] | None = fallback
        self._latest_fallback_sample: Sample[QuantityT] | None = None
        self._is_stopped = False

    @property
    def stream(self) -> Receiver[Sample[QuantityT]]:
        """Return the stream from which to fetch values.

        Returns:
            The stream from which to fetch values.
        """
        return self._stream

    async def stop(self) -> None:
        """Stop metric fetcher.

        If metric fetcher is stopped, it can't be started again.
        There is no use-case now to start it again.
        """
        self._is_stopped = True
        self.stream.close()
        if self._fallback:
            await self._fallback.stop()

    def stream_name(self) -> str:
        """Return the name of the stream.

        Returns:
            The name of the stream.
        """
        return str(self._stream.__doc__)

    def _is_value_valid(self, value: QuantityT | None) -> bool:
        return not (value is None or value.isnan() or value.isinf())

    async def _fetch_from_fallback(
        self, fallback_fetcher: FallbackMetricFetcher[QuantityT]
    ) -> Sample[QuantityT] | None:
        try:
            return await fallback_fetcher.receive()
        except ReceiverStoppedError:
            if self._is_stopped:
                _logger.debug(
                    "Stream for fallback metric fetcher %s closed.",
                    fallback_fetcher.name,
                )
            else:
                _logger.error(
                    "Failed to fetch next value from %s. Fallback stream closed.",
                    self._name,
                )
            return None
        except ReceiverError as err:
            _logger.error(
                "Failed to fetch next value from fallback stream %s: %s",
                self._name,
                err,
            )
            return None

    async def _synchronize_and_fetch_fallback(
        self,
        primary_fetcher_value: Sample[QuantityT] | None,
        fallback_fetcher: FallbackMetricFetcher[QuantityT],
    ) -> Sample[QuantityT] | None:
        """Synchronize the fallback fetcher and return the fallback value.

        Args:
            primary_fetcher_value: The sample fetched from the primary fetcher.
            fallback_fetcher: The fallback metric fetcher.

        Returns:
            The value from the synchronized stream. Returns None if the primary
            fetcher sample is older than the latest sample from the fallback
            fetcher or if the fallback fetcher fails to fetch the next value.
        """
        # We need to save value, because
        # primary_fetcher_value.timestamp < self._latest_fallback_sample.timestamp
        # In that case we should wait for our time window.
        if self._latest_fallback_sample is None:
            self._latest_fallback_sample = await self._fetch_from_fallback(
                fallback_fetcher
            )

        if primary_fetcher_value is None or self._latest_fallback_sample is None:
            return self._latest_fallback_sample

        if primary_fetcher_value.timestamp < self._latest_fallback_sample.timestamp:
            return None

        # Synchronize the fallback fetcher with primary one
        while primary_fetcher_value.timestamp > self._latest_fallback_sample.timestamp:
            self._latest_fallback_sample = await self._fetch_from_fallback(
                fallback_fetcher
            )
            if self._latest_fallback_sample is None:
                break
        return self._latest_fallback_sample

    async def fetch_next(self) -> Sample[QuantityT] | None:
        """Fetch the next value from the stream.

        To be called before each call to `apply`.

        Returns:
            The fetched Sample.
        """
        if self._is_stopped:
            _logger.error(
                "Metric fetcher %s stopped. Can't fetch new value.", self._name
            )
            return None

        self._next_value = await self._fetch_next()
        return self._next_value

    async def _fetch_next(self) -> Sample[QuantityT] | None:
        # First fetch from primary stream
        primary_value: Sample[QuantityT] | None = None
        try:
            primary_value = await self._stream.receive()
        except ReceiverStoppedError:
            if self._is_stopped:
                _logger.debug("Stream for metric fetcher %s closed.", self._name)
                return None
            _logger.error(
                "Failed to fetch next value from %s. Primary stream closed.",
                self._name,
            )
        except ReceiverError as err:
            _logger.error("Failed to fetch next value from %s: %s", self._name, err)

        # We have no fallback, so we just return primary value even if it is not correct.
        if self._fallback is None:
            return primary_value

        is_primary_value_valid = primary_value is not None and self._is_value_valid(
            primary_value.value
        )

        if is_primary_value_valid:
            # Primary stream is good again, so we can stop fallback and return primary_value.
            if self._fallback.is_running:
                _logger.info(
                    "Primary metric %s is good again, stopping fallback metric fetcher %s",
                    self._name,
                    self._fallback.name,
                )
                await self._fallback.stop()
            return primary_value

        if not self._fallback.is_running:
            _logger.warning(
                "Primary metric %s is invalid. Running fallback metric fetcher: %s",
                self._name,
                self._fallback.name,
            )
            # We started fallback, but it has to subscribe.
            # We will receive fallback values since the next time window.
            self._fallback.start()
            return primary_value

        return await self._synchronize_and_fetch_fallback(primary_value, self._fallback)

    @property
    def value(self) -> Sample[QuantityT] | None:
        """Get the next value in the stream.

        Returns:
            Next value in the stream.
        """
        return self._next_value

    def __repr__(self) -> str:
        """Return a string representation of the step.

        Returns:
            A string representation of the step.
        """
        return self._name

    def apply(self, eval_stack: list[float]) -> None:
        """Push the latest value from the stream into the evaluation stack.

        Args:
            eval_stack: An evaluation stack, to apply the formula step on.

        Raises:
            RuntimeError: No next value available to append.
        """
        if self._next_value is None:
            raise RuntimeError("No next value available to append.")

        next_value = self._next_value.value
        if next_value is None or next_value.isnan() or next_value.isinf():
            if self._nones_are_zeros:
                eval_stack.append(0.0)
            else:
                eval_stack.append(math.nan)
        else:
            eval_stack.append(next_value.base_value)



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_resampled_formula_builder.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""A builder for creating formula engines that operate on resampled component metrics."""

from __future__ import annotations

from collections.abc import Callable

from frequenz.channels import Receiver, Sender
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentMetricId
from frequenz.quantities import Quantity

from ..._internal._channels import ChannelRegistry
from ...microgrid._data_sourcing import ComponentMetricRequest
from .._base_types import QuantityT, Sample
from ._formula_engine import FormulaBuilder, FormulaEngine
from ._formula_steps import FallbackMetricFetcher
from ._tokenizer import Tokenizer, TokenType


class ResampledFormulaBuilder(FormulaBuilder[QuantityT]):
    """Provides a way to build a FormulaEngine from resampled data streams."""

    def __init__(  # pylint: disable=too-many-arguments
        self,
        *,
        namespace: str,
        formula_name: str,
        channel_registry: ChannelRegistry,
        resampler_subscription_sender: Sender[ComponentMetricRequest],
        metric_id: ComponentMetricId,
        create_method: Callable[[float], QuantityT],
    ) -> None:
        """Create a `ResampledFormulaBuilder` instance.

        Args:
            namespace: The unique namespace to allow reuse of streams in the data
                pipeline.
            formula_name: A name for the formula.
            channel_registry: The channel registry instance shared with the resampling
                and the data sourcing actors.
            resampler_subscription_sender: A sender to send metric requests to the
                resampling actor.
            metric_id: A metric ID to fetch for all components in this formula.
            create_method: A method to generate the output `Sample` value with.  If the
                formula is for generating power values, this would be
                `Power.from_watts`, for example.
        """
        self._channel_registry: ChannelRegistry = channel_registry
        self._resampler_subscription_sender: Sender[ComponentMetricRequest] = (
            resampler_subscription_sender
        )
        self._namespace: str = namespace
        self._metric_id: ComponentMetricId = metric_id
        self._resampler_requests: list[ComponentMetricRequest] = []
        super().__init__(formula_name, create_method)

    def _get_resampled_receiver(
        self, component_id: ComponentId, metric_id: ComponentMetricId
    ) -> Receiver[Sample[QuantityT]]:
        """Get a receiver with the resampled data for the given component id.

        Args:
            component_id: The component id for which to get a resampled data receiver.
            metric_id: A metric ID to fetch for all components in this formula.

        Returns:
            A receiver to stream resampled data for the given component id.
        """
        request = ComponentMetricRequest(self._namespace, component_id, metric_id, None)
        self._resampler_requests.append(request)
        resampled_channel = self._channel_registry.get_or_create(
            Sample[Quantity], request.get_channel_name()
        )
        resampled_receiver = resampled_channel.new_receiver().map(
            lambda sample: Sample(
                sample.timestamp,
                (
                    self._create_method(sample.value.base_value)
                    if sample.value is not None
                    else None
                ),
            )
        )
        return resampled_receiver

    async def subscribe(self) -> None:
        """Subscribe to all resampled component metric streams."""
        for request in self._resampler_requests:
            await self._resampler_subscription_sender.send(request)

    def push_component_metric(
        self,
        component_id: ComponentId,
        *,
        nones_are_zeros: bool,
        fallback: FallbackMetricFetcher[QuantityT] | None = None,
    ) -> None:
        """Push a resampled component metric stream to the formula engine.

        Args:
            component_id: The component id for which to push a metric fetcher.
            nones_are_zeros: Whether to treat None values from the stream as 0s.  If
                False, the returned value will be a None.
            fallback: Metric fetcher to use if primary one start sending
                invalid data (e.g. due to a component stop). If None the data from
                primary metric fetcher will be returned.
        """
        receiver = self._get_resampled_receiver(component_id, self._metric_id)
        self.push_metric(
            f"#{int(component_id)}",
            receiver,
            nones_are_zeros=nones_are_zeros,
            fallback=fallback,
        )

    def from_string(
        self,
        formula: str,
        *,
        nones_are_zeros: bool,
    ) -> FormulaEngine[QuantityT]:
        """Construct a `FormulaEngine` from the given formula string.

        Formulas can have Component IDs that are preceeded by a pound symbol("#"), and
        these operators: +, -, *, /, (, ).

        For example, the input string: "#20 + #5" is a formula for adding metrics from
        two components with ids 20 and 5.

        Args:
            formula: A string formula.
            nones_are_zeros: Whether to treat None values from the stream as 0s.  If
                False, the returned value will be a None.

        Returns:
            A FormulaEngine instance corresponding to the given formula.

        Raises:
            ValueError: when there is an unknown token type.
        """
        tokenizer = Tokenizer(formula)

        for token in tokenizer:
            if token.type == TokenType.COMPONENT_METRIC:
                self.push_component_metric(
                    ComponentId(int(token.value)), nones_are_zeros=nones_are_zeros
                )
            elif token.type == TokenType.OPER:
                self.push_oper(token.value)
            else:
                raise ValueError(f"Unknown token type: {token}")

        return self.build()



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_tokenizer.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""A tokenizer for data pipeline formulas."""

from __future__ import annotations

from dataclasses import dataclass
from enum import Enum


class StringIter:
    """An iterator for reading characters from a string."""

    def __init__(self, raw: str) -> None:
        """Create a `StringIter` instance.

        Args:
            raw: The raw string to create the iterator out of.
        """
        self._raw = raw
        self._max = len(raw)
        self._pos = 0

    @property
    def pos(self) -> int:
        """Return the position of the iterator in the raw string.

        Returns:
            The position of the iterator.
        """
        return self._pos

    @property
    def raw(self) -> str:
        """Return the raw string the iterator is created with.

        Returns:
            The base string of the iterator.
        """
        return self._raw

    def __iter__(self) -> StringIter:
        """Return an iterator to this class.

        Returns:
            self.
        """
        return self

    def __next__(self) -> str:
        """Return the next character in the raw string, and move forward.

        Returns:
            The next character.

        Raises:
            StopIteration: when there are no more characters in the string.
        """
        if self._pos < self._max:
            char = self._raw[self._pos]
            self._pos += 1
            return char
        raise StopIteration()

    def peek(self) -> str | None:
        """Return the next character in the raw string, without consuming it.

        Returns:
            The next character.
        """
        if self._pos < self._max:
            return self._raw[self._pos]
        return None


class TokenType(Enum):
    """Represents the types of tokens the Tokenizer can return."""

    COMPONENT_METRIC = 0
    """A component metric ID."""

    CONSTANT = 1
    """A constant value."""

    OPER = 2
    """An operator."""


@dataclass
class Token:
    """Represents a Token returned by the Tokenizer."""

    type: TokenType
    """The type of the token."""

    value: str
    """The value associated to the token."""


class Tokenizer:
    """A Tokenizer for breaking down a string formula into individual tokens.

    Every instance is an iterator that allows us to iterate over the individual tokens
    in the given formula.

    Formulas can have Component IDs that are preceeded by a pound symbol("#"), and these
    operators: +, -, *, /, (, ).

    For example, the input string: "#20 + #5" would produce three tokens:
     - COMPONENT_METRIC: 20
     - OPER: +
     - COMPONENT_METRIC: 5
    """

    def __init__(self, formula: str) -> None:
        """Create a `Tokenizer` instance.

        Args:
            formula: The string formula to tokenize.
        """
        self._formula = StringIter(formula)

    def _read_unsigned_int(self) -> str:
        """Read an unsigned int from the current position in the input string.

        Returns:
            A string containing the read unsigned int value.

        Raises:
            ValueError: when there is no unsigned int at the current position.
        """
        first_char = True
        result = ""

        while char := self._formula.peek():
            if not char.isdigit():
                if first_char:
                    raise ValueError(
                        f"Expected an integer. got '{char}', "
                        f"at pos {self._formula.pos} in formula {self._formula.raw}"
                    )
                break
            first_char = False
            result += char
            next(self._formula)
        return result

    def __iter__(self) -> Tokenizer:
        """Return an iterator to this class.

        Returns:
            self.
        """
        return self

    def __next__(self) -> Token:
        """Return the next token in the input string.

        Returns:
            The next token.

        Raises:
            ValueError: when there are unknown tokens in the input string.
            StopIteration: when there are no more tokens in the input string.
        """
        for char in self._formula:
            if char in (" ", "\n", "\r", "\t"):
                continue
            if char in ("+", "-", "*", "/", "(", ")"):
                return Token(TokenType.OPER, char)
            if char == "#":
                return Token(TokenType.COMPONENT_METRIC, self._read_unsigned_int())
            raise ValueError(
                f"Unable to parse character '{char}' at pos: {self._formula.pos}"
                f" in formula: {self._formula.raw}"
            )
        raise StopIteration()



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Generators for formulas from component graphs."""

from ._battery_power_formula import BatteryPowerFormula
from ._chp_power_formula import CHPPowerFormula
from ._consumer_power_formula import ConsumerPowerFormula
from ._ev_charger_current_formula import EVChargerCurrentFormula
from ._ev_charger_power_formula import EVChargerPowerFormula
from ._formula_generator import (
    ComponentNotFound,
    FormulaGenerationError,
    FormulaGenerator,
    FormulaGeneratorConfig,
)
from ._grid_current_formula import GridCurrentFormula
from ._grid_power_3_phase_formula import GridPower3PhaseFormula
from ._grid_power_formula import GridPowerFormula
from ._grid_reactive_power_formula import GridReactivePowerFormula
from ._producer_power_formula import ProducerPowerFormula
from ._pv_power_formula import PVPowerFormula

__all__ = [
    #
    # Base class
    #
    "FormulaGenerator",
    "FormulaGeneratorConfig",
    #
    # Power Formula generators
    #
    "CHPPowerFormula",
    "ConsumerPowerFormula",
    "GridPower3PhaseFormula",
    "GridPowerFormula",
    "GridReactivePowerFormula",
    "BatteryPowerFormula",
    "EVChargerPowerFormula",
    "PVPowerFormula",
    "ProducerPowerFormula",
    #
    # Current formula generators
    #
    "GridCurrentFormula",
    "EVChargerCurrentFormula",
    #
    # Exceptions
    #
    "ComponentNotFound",
    "FormulaGenerationError",
]



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_battery_power_formula.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Formula generator from component graph for Grid Power."""

import itertools
import logging

from frequenz.client.microgrid import Component, ComponentCategory, ComponentMetricId
from frequenz.quantities import Power

from ....microgrid import connection_manager
from ...formula_engine import FormulaEngine
from ._fallback_formula_metric_fetcher import FallbackFormulaMetricFetcher
from ._formula_generator import (
    NON_EXISTING_COMPONENT_ID,
    ComponentNotFound,
    FormulaGenerationError,
    FormulaGenerator,
    FormulaGeneratorConfig,
)

_logger = logging.getLogger(__name__)


class BatteryPowerFormula(FormulaGenerator[Power]):
    """Creates a formula engine from the component graph for calculating battery power."""

    def generate(
        self,
    ) -> FormulaEngine[Power]:
        """Make a formula for the cumulative AC battery power of a microgrid.

        The calculation is performed by adding the Active Powers of all the inverters
        that are attached to batteries.

        If there's no data coming from an inverter, that inverter's power will be
        treated as 0.

        Returns:
            A formula engine that will calculate cumulative battery power values.

        Raises:
            ComponentNotFound: if there are no batteries in the component graph, or if
                they don't have an inverter as a predecessor.
            FormulaGenerationError: If a battery has a non-inverter predecessor
                in the component graph, or if not all batteries behind a set of
                inverters have been requested.
        """
        builder = self._get_builder(
            "battery-power", ComponentMetricId.ACTIVE_POWER, Power.from_watts
        )

        if not self._config.component_ids:
            _logger.warning(
                "No Battery component IDs specified. "
                "Subscribing to the resampling actor with a non-existing "
                "component id, so that `0` values are sent from the formula."
            )
            # If there are no Batteries, we have to send 0 values as the same
            # frequency as the other streams. So we subscribe with a non-existing
            # component id, just to get a `None` message at the resampling interval.
            builder.push_component_metric(
                NON_EXISTING_COMPONENT_ID, nones_are_zeros=True
            )
            return builder.build()

        component_ids = set(self._config.component_ids)
        component_graph = connection_manager.get().component_graph
        inv_bat_mapping: dict[Component, set[Component]] = {}

        for bat_id in component_ids:
            inverters = set(
                filter(
                    component_graph.is_battery_inverter,
                    component_graph.predecessors(bat_id),
                )
            )
            if len(inverters) == 0:
                raise ComponentNotFound(
                    "All batteries must have at least one inverter as a predecessor."
                    f"Battery ID {bat_id} has no inverter as a predecessor.",
                )

            for inverter in inverters:
                all_connected_batteries = component_graph.successors(
                    inverter.component_id
                )
                battery_ids = set(
                    map(lambda battery: battery.component_id, all_connected_batteries)
                )
                if not battery_ids.issubset(component_ids):
                    raise FormulaGenerationError(
                        f"Not all batteries behind inverter {inverter.component_id} "
                        f"are requested. Missing: {battery_ids - component_ids}"
                    )

                inv_bat_mapping[inverter] = all_connected_batteries

        if self._config.allow_fallback:
            fallbacks = self._get_fallback_formulas(inv_bat_mapping)

            for idx, (primary_component, fallback_formula) in enumerate(
                fallbacks.items()
            ):
                if idx > 0:
                    builder.push_oper("+")

                builder.push_component_metric(
                    primary_component.component_id,
                    nones_are_zeros=(
                        primary_component.category != ComponentCategory.METER
                    ),
                    fallback=fallback_formula,
                )
        else:
            for idx, comp in enumerate(inv_bat_mapping.keys()):
                if idx > 0:
                    builder.push_oper("+")
                builder.push_component_metric(comp.component_id, nones_are_zeros=True)

        return builder.build()

    def _get_fallback_formulas(
        self, inv_bat_mapping: dict[Component, set[Component]]
    ) -> dict[Component, FallbackFormulaMetricFetcher[Power] | None]:
        """Find primary and fallback components and create fallback formulas.

        The primary component is the one that will be used to calculate the battery power.
        If it is not available, the fallback formula will be used instead.
        Fallback formulas calculate the battery power using the fallback components.
        Fallback formulas are wrapped in `FallbackFormulaMetricFetcher`.

        Args:
            inv_bat_mapping: A mapping from inverter to connected batteries.

        Returns:
            A dictionary mapping primary components to their FallbackFormulaMetricFetcher.
        """
        fallbacks = self._get_metric_fallback_components(set(inv_bat_mapping.keys()))

        fallback_formulas: dict[
            Component, FallbackFormulaMetricFetcher[Power] | None
        ] = {}
        for primary_component, fallback_components in fallbacks.items():
            if len(fallback_components) == 0:
                fallback_formulas[primary_component] = None
                continue

            battery_ids = set(
                map(
                    lambda battery: battery.component_id,
                    itertools.chain.from_iterable(
                        inv_bat_mapping[inv] for inv in fallback_components
                    ),
                )
            )

            generator = BatteryPowerFormula(
                f"{self._namespace}_fallback_{battery_ids}",
                self._channel_registry,
                self._resampler_subscription_sender,
                FormulaGeneratorConfig(
                    component_ids=battery_ids,
                    allow_fallback=False,
                ),
            )

            fallback_formulas[primary_component] = FallbackFormulaMetricFetcher(
                generator
            )

        return fallback_formulas



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_chp_power_formula.py
================================================
# License: MIT
# Copyright  © 2023 Frequenz Energy-as-a-Service GmbH

"""Formula generator from component graph for CHP Power."""


import logging
from collections import abc

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentCategory, ComponentMetricId
from frequenz.quantities import Power

from ....microgrid import connection_manager
from ...formula_engine import FormulaEngine
from ._formula_generator import (
    NON_EXISTING_COMPONENT_ID,
    FormulaGenerationError,
    FormulaGenerator,
)

_logger = logging.getLogger(__name__)


class CHPPowerFormula(FormulaGenerator[Power]):
    """Formula generator for CHP Power."""

    def generate(  # noqa: DOC502 (FormulaGenerationError is raised indirectly by _get_chp_meters)
        self,
    ) -> FormulaEngine[Power]:
        """Make a formula for the cumulative CHP power of a microgrid.

        The calculation is performed by adding the active power measurements from
        dedicated meters attached to CHPs.

        Returns:
            A formula engine that will calculate cumulative CHP power values.

        Raises:
            FormulaGenerationError: If there's no dedicated meter attached to every CHP.

        """
        builder = self._get_builder(
            "chp-power", ComponentMetricId.ACTIVE_POWER, Power.from_watts
        )

        chp_meter_ids = self._get_chp_meters()
        if not chp_meter_ids:
            _logger.warning("No CHPs found in the component graph.")
            builder.push_component_metric(
                NON_EXISTING_COMPONENT_ID, nones_are_zeros=True
            )
            return builder.build()

        for idx, chp_meter_id in enumerate(chp_meter_ids):
            if idx > 0:
                builder.push_oper("+")
            builder.push_component_metric(chp_meter_id, nones_are_zeros=False)

        return builder.build()

    def _get_chp_meters(self) -> abc.Set[ComponentId]:
        """Get the meter IDs of the CHPs from the component graph.

        Returns:
            A set of meter IDs of the CHPs in the component graph. If no CHPs are
                found, None is returned.

        Raises:
            FormulaGenerationError: If there's no dedicated meter attached to every CHP.
        """
        component_graph = connection_manager.get().component_graph
        chps = list(
            comp
            for comp in component_graph.components()
            if comp.category == ComponentCategory.CHP
        )

        chp_meters: set[ComponentId] = set()
        for chp in chps:
            predecessors = component_graph.predecessors(chp.component_id)
            if len(predecessors) != 1:
                raise FormulaGenerationError(
                    f"CHP {chp.component_id} has {len(predecessors)} predecessors. "
                    " Expected exactly one."
                )
            meter = next(iter(predecessors))
            if meter.category != ComponentCategory.METER:
                raise FormulaGenerationError(
                    f"CHP {chp.component_id} has a predecessor of category "
                    f"{meter.category}. Expected ComponentCategory.METER."
                )
            meter_successors = component_graph.successors(meter.component_id)
            if not all(successor in chps for successor in meter_successors):
                raise FormulaGenerationError(
                    f"Meter {meter.component_id} connected to CHP {chp.component_id}"
                    "has non-chp successors."
                )
            chp_meters.add(meter.component_id)
        return chp_meters



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_consumer_power_formula.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Formula generator from component graph for Consumer Power."""

import logging

from frequenz.client.microgrid import Component, ComponentCategory, ComponentMetricId
from frequenz.quantities import Power

from ....microgrid import connection_manager
from .._formula_engine import FormulaEngine
from .._resampled_formula_builder import ResampledFormulaBuilder
from ._fallback_formula_metric_fetcher import FallbackFormulaMetricFetcher
from ._formula_generator import (
    NON_EXISTING_COMPONENT_ID,
    ComponentNotFound,
    FormulaGenerator,
    FormulaGeneratorConfig,
)
from ._simple_formula import SimplePowerFormula

_logger = logging.getLogger(__name__)


class ConsumerPowerFormula(FormulaGenerator[Power]):
    """Formula generator from component graph for calculating the Consumer Power.

    The consumer power is calculated by summing up the power of all components that
    are not part of a battery, CHP, PV or EV charger chain.
    """

    def _are_grid_meters(self, grid_successors: set[Component]) -> bool:
        """Check if the grid successors are grid meters.

        Args:
            grid_successors: The successors of the grid component.

        Returns:
            True if the provided components are grid meters, False otherwise.
        """
        component_graph = connection_manager.get().component_graph
        return all(
            successor.category == ComponentCategory.METER
            and not component_graph.is_battery_chain(successor)
            and not component_graph.is_chp_chain(successor)
            and not component_graph.is_pv_chain(successor)
            and not component_graph.is_ev_charger_chain(successor)
            for successor in grid_successors
        )

    # We need the noqa here because `RuntimeError` is raised indirectly
    def generate(self) -> FormulaEngine[Power]:  # noqa: DOC503
        """Generate formula for calculating consumer power from the component graph.

        Returns:
            A formula engine that will calculate the consumer power.

        Raises:
            ComponentNotFound: If the component graph does not contain a consumer power
                component.
            RuntimeError: If the grid component has a single successor that is not a
                meter.
        """
        builder = self._get_builder(
            "consumer-power", ComponentMetricId.ACTIVE_POWER, Power.from_watts
        )

        grid_successors = self._get_grid_component_successors()

        if not grid_successors:
            raise ComponentNotFound("No components found in the component graph.")

        if self._are_grid_meters(grid_successors):
            return self._gen_with_grid_meter(builder, grid_successors)

        return self._gen_without_grid_meter(builder, self._get_grid_component())

    def _gen_with_grid_meter(
        self,
        builder: ResampledFormulaBuilder[Power],
        grid_meters: set[Component],
    ) -> FormulaEngine[Power]:
        """Generate formula for calculating consumer power with grid meter.

        Args:
            builder: The formula engine builder.
            grid_meters: The grid meter component.

        Returns:
            A formula engine that will calculate the consumer power.
        """
        assert grid_meters
        component_graph = connection_manager.get().component_graph

        def non_consumer_component(component: Component) -> bool:
            """
            Check if a component is not a consumer component.

            Args:
                component: The component to check.

            Returns:
                True if the component is not a consumer component, False otherwise.
            """
            # If the component graph supports additional types of grid successors in the
            # future, additional checks need to be added here.
            return (
                component_graph.is_battery_chain(component)
                or component_graph.is_chp_chain(component)
                or component_graph.is_pv_chain(component)
                or component_graph.is_ev_charger_chain(component)
            )

        # join all non consumer components reachable from the different grid meters
        non_consumer_components: set[Component] = set()
        for grid_meter in grid_meters:
            non_consumer_components = non_consumer_components.union(
                component_graph.dfs(grid_meter, set(), non_consumer_component)
            )

        # push all grid meters
        for idx, grid_meter in enumerate(grid_meters):
            if idx > 0:
                builder.push_oper("+")
            builder.push_component_metric(
                grid_meter.component_id, nones_are_zeros=False
            )

        if self._config.allow_fallback:
            fallbacks = self._get_fallback_formulas(non_consumer_components)

            for idx, (primary_component, fallback_formula) in enumerate(
                fallbacks.items()
            ):
                builder.push_oper("-")

                # should only be the case if the component is not a meter
                builder.push_component_metric(
                    primary_component.component_id,
                    nones_are_zeros=(
                        primary_component.category != ComponentCategory.METER
                    ),
                    fallback=fallback_formula,
                )
        else:
            # push all non consumer components and subtract them from the grid meters
            for component in non_consumer_components:
                builder.push_oper("-")
                builder.push_component_metric(
                    component.component_id,
                    nones_are_zeros=component.category != ComponentCategory.METER,
                )

        return builder.build()

    def _gen_without_grid_meter(
        self,
        builder: ResampledFormulaBuilder[Power],
        grid: Component,
    ) -> FormulaEngine[Power]:
        """Generate formula for calculating consumer power without a grid meter.

        Args:
            builder: The formula engine builder.
            grid: The grid component.

        Returns:
            A formula engine that will calculate the consumer power.
        """

        def consumer_component(component: Component) -> bool:
            """
            Check if a component is a consumer component.

            Args:
                component: The component to check.

            Returns:
                True if the component is a consumer component, False otherwise.
            """
            # If the component graph supports additional types of grid successors in the
            # future, additional checks need to be added here.
            return (
                component.category
                in {ComponentCategory.METER, ComponentCategory.INVERTER}
                and not component_graph.is_battery_chain(component)
                and not component_graph.is_chp_chain(component)
                and not component_graph.is_pv_chain(component)
                and not component_graph.is_ev_charger_chain(component)
            )

        component_graph = connection_manager.get().component_graph
        consumer_components = component_graph.dfs(grid, set(), consumer_component)

        if not consumer_components:
            _logger.warning(
                "Unable to find any consumers in the component graph. "
                "Subscribing to the resampling actor with a non-existing "
                "component id, so that `0` values are sent from the formula."
            )
            # If there are no consumer components, we have to send 0 values at the same
            # frequency as the other streams.  So we subscribe with a non-existing
            # component id, just to get a `None` message at the resampling interval.
            builder.push_component_metric(
                NON_EXISTING_COMPONENT_ID, nones_are_zeros=True
            )
            return builder.build()

        if self._config.allow_fallback:
            fallbacks = self._get_fallback_formulas(consumer_components)

            for idx, (primary_component, fallback_formula) in enumerate(
                fallbacks.items()
            ):
                if idx > 0:
                    builder.push_oper("+")

                # should only be the case if the component is not a meter
                builder.push_component_metric(
                    primary_component.component_id,
                    nones_are_zeros=(
                        primary_component.category != ComponentCategory.METER
                    ),
                    fallback=fallback_formula,
                )
        else:
            for idx, component in enumerate(consumer_components):
                if idx > 0:
                    builder.push_oper("+")

                builder.push_component_metric(
                    component.component_id,
                    nones_are_zeros=component.category != ComponentCategory.METER,
                )

        return builder.build()

    def _get_fallback_formulas(
        self, components: set[Component]
    ) -> dict[Component, FallbackFormulaMetricFetcher[Power] | None]:
        """Find primary and fallback components and create fallback formulas.

        The primary component is the one that will be used to calculate the consumer power.
        However, if it is not available, the fallback formula will be used instead.
        Fallback formulas calculate the consumer power using the fallback components.
        Fallback formulas are wrapped in `FallbackFormulaMetricFetcher` to allow
        for lazy initialization.

        Args:
            components: The producer components.

        Returns:
            A dictionary mapping primary components to their FallbackFormulaMetricFetcher.
        """
        fallbacks = self._get_metric_fallback_components(components)

        fallback_formulas: dict[
            Component, FallbackFormulaMetricFetcher[Power] | None
        ] = {}

        for primary_component, fallback_components in fallbacks.items():
            if len(fallback_components) == 0:
                fallback_formulas[primary_component] = None
                continue

            fallback_ids = [c.component_id for c in fallback_components]
            generator = SimplePowerFormula(
                f"{self._namespace}_fallback_{fallback_ids}",
                self._channel_registry,
                self._resampler_subscription_sender,
                FormulaGeneratorConfig(
                    component_ids=set(fallback_ids),
                    allow_fallback=False,
                ),
            )

            fallback_formulas[primary_component] = FallbackFormulaMetricFetcher(
                generator
            )

        return fallback_formulas



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_ev_charger_current_formula.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Formula generator from component graph for 3-phase Grid Current."""


import logging
from collections import abc

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentMetricId
from frequenz.quantities import Current

from .._formula_engine import FormulaEngine, FormulaEngine3Phase
from ._formula_generator import NON_EXISTING_COMPONENT_ID, FormulaGenerator

_logger = logging.getLogger(__name__)


class EVChargerCurrentFormula(FormulaGenerator[Current]):
    """Create a formula engine from the component graph for calculating grid current."""

    def generate(self) -> FormulaEngine3Phase[Current]:
        """Generate a formula for calculating total EV current for given component ids.

        Returns:
            A formula engine that calculates total 3-phase EV Charger current values.
        """
        component_ids = self._config.component_ids

        if not component_ids:
            _logger.warning(
                "No EV Charger component IDs specified. "
                "Subscribing to the resampling actor with a non-existing "
                "component id, so that `0` values are sent from the formula."
            )
            # If there are no EV Chargers, we have to send 0 values as the same
            # frequency as the other streams.  So we subscribe with a non-existing
            # component id, just to get a `None` message at the resampling interval.
            builder = self._get_builder(
                "ev-current", ComponentMetricId.ACTIVE_POWER, Current.from_amperes
            )
            builder.push_component_metric(
                NON_EXISTING_COMPONENT_ID, nones_are_zeros=True
            )
            engine = builder.build()
            return FormulaEngine3Phase(
                "ev-current",
                Current.from_amperes,
                (engine, engine, engine),
            )

        return FormulaEngine3Phase(
            "ev-current",
            Current.from_amperes,
            (
                (
                    self._gen_phase_formula(
                        component_ids, ComponentMetricId.CURRENT_PHASE_1
                    )
                ),
                (
                    self._gen_phase_formula(
                        component_ids, ComponentMetricId.CURRENT_PHASE_2
                    )
                ),
                (
                    self._gen_phase_formula(
                        component_ids, ComponentMetricId.CURRENT_PHASE_3
                    )
                ),
            ),
        )

    def _gen_phase_formula(
        self,
        component_ids: abc.Set[ComponentId],
        metric_id: ComponentMetricId,
    ) -> FormulaEngine[Current]:
        builder = self._get_builder("ev-current", metric_id, Current.from_amperes)

        # generate a formula that just adds values from all EV Chargers.
        for idx, component_id in enumerate(component_ids):
            if idx > 0:
                builder.push_oper("+")

            builder.push_component_metric(component_id, nones_are_zeros=True)

        return builder.build()



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_ev_charger_power_formula.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Formula generator from component graph for Grid Power."""

import logging

from frequenz.client.microgrid import ComponentMetricId
from frequenz.quantities import Power

from .._formula_engine import FormulaEngine
from ._formula_generator import NON_EXISTING_COMPONENT_ID, FormulaGenerator

_logger = logging.getLogger(__name__)


class EVChargerPowerFormula(FormulaGenerator[Power]):
    """Create a formula engine from the component graph for calculating grid power."""

    def generate(self) -> FormulaEngine[Power]:
        """Generate a formula for calculating total EV power for given component ids.

        Returns:
            A formula engine that calculates total EV Charger power values.
        """
        builder = self._get_builder(
            "ev-power", ComponentMetricId.ACTIVE_POWER, Power.from_watts
        )

        component_ids = self._config.component_ids
        if not component_ids:
            _logger.warning(
                "No EV Charger component IDs specified. "
                "Subscribing to the resampling actor with a non-existing "
                "component id, so that `0` values are sent from the formula."
            )
            # If there are no EV Chargers, we have to send 0 values as the same
            # frequency as the other streams. So we subscribe with a non-existing
            # component id, just to get a `None` message at the resampling interval.
            builder.push_component_metric(
                NON_EXISTING_COMPONENT_ID, nones_are_zeros=True
            )
            return builder.build()

        for idx, component_id in enumerate(component_ids):
            if idx > 0:
                builder.push_oper("+")
            builder.push_component_metric(component_id, nones_are_zeros=True)

        return builder.build()



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_fallback_formula_metric_fetcher.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""FallbackMetricFetcher implementation that uses formula generator."""

from frequenz.channels import Receiver

from ..._base_types import QuantityT, Sample
from .. import FormulaEngine
from .._formula_steps import FallbackMetricFetcher
from ._formula_generator import FormulaGenerator


# This is done as a separate module to avoid circular imports.
class FallbackFormulaMetricFetcher(FallbackMetricFetcher[QuantityT]):
    """A metric fetcher that uses a formula generator.

    The formula engine is generated lazily, meaning it is created only when
    the `start` or `fetch_next` method is called for the first time.
    Once the formula engine is initialized, it subscribes to its components
    and begins calculating and sending the formula results.
    """

    def __init__(self, formula_generator: FormulaGenerator[QuantityT]):
        """Create a `FallbackFormulaMetricFetcher` instance.

        Args:
            formula_generator: A formula generator that generates
                a formula engine with fallback components.
        """
        super().__init__()
        self._name = formula_generator.namespace
        self._formula_generator: FormulaGenerator[QuantityT] = formula_generator
        self._formula_engine: FormulaEngine[QuantityT] | None = None
        self._receiver: Receiver[Sample[QuantityT]] | None = None

    @property
    def name(self) -> str:
        """Get the name of the fetcher."""
        return self._name

    @property
    def is_running(self) -> bool:
        """Check whether the formula engine is running."""
        return self._receiver is not None

    def start(self) -> None:
        """Initialize the formula engine and start fetching samples."""
        engine = self._formula_generator.generate()
        # We need this assert because generate() can return a FormulaEngine
        # or FormulaEngine3Phase, but in this case we know it will return a
        # FormulaEngine. This helps to silence `mypy` and also to verify our
        # assumptions are still true at runtime
        assert isinstance(engine, FormulaEngine)
        self._formula_engine = engine
        self._receiver = self._formula_engine.new_receiver()

    async def ready(self) -> bool:
        """Wait until the receiver is ready with a message or an error.

        Once a call to `ready()` has finished, the message should be read with
        a call to `consume()` (`receive()` or iterated over).

        Returns:
            Whether the receiver is still active.
        """
        if self._receiver is None:
            self.start()

        assert self._receiver is not None
        return await self._receiver.ready()

    def consume(self) -> Sample[QuantityT]:
        """Return the latest message once `ready()` is complete."""
        assert (
            self._receiver is not None
        ), f"Fallback metric fetcher: {self.name} was not started"

        return self._receiver.consume()

    async def stop(self) -> None:
        """Stop fallback metric fetcher, by closing the connection."""
        if self._formula_engine is not None:
            await self._formula_engine.stop()
            self._formula_engine = None

        if self._receiver is not None:
            self._receiver.close()
            self._receiver = None



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_formula_generator.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Base class for formula generators that use the component graphs."""

from __future__ import annotations

import sys
from abc import ABC, abstractmethod
from collections import abc
from collections.abc import Callable
from dataclasses import dataclass
from typing import Generic

from frequenz.channels import Sender
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import Component, ComponentCategory, ComponentMetricId

from ...._internal._channels import ChannelRegistry
from ....microgrid import connection_manager
from ....microgrid._data_sourcing import ComponentMetricRequest
from ..._base_types import QuantityT
from .._formula_engine import FormulaEngine, FormulaEngine3Phase
from .._resampled_formula_builder import ResampledFormulaBuilder


class FormulaGenerationError(Exception):
    """An error encountered during formula generation from the component graph."""


class ComponentNotFound(FormulaGenerationError):
    """Indicates that a component required for generating a formula is not found."""


NON_EXISTING_COMPONENT_ID = ComponentId(sys.maxsize)
"""The component ID for non-existent components in the components graph.

The non-existing component ID is commonly used in scenarios where a formula
engine requires a component ID but there are no available components in the
graph to associate with it. Thus, the non-existing component ID is subscribed
instead so that the formula engine can send `None` or `0` values at the same
frequency as the other streams.
"""


@dataclass(frozen=True)
class FormulaGeneratorConfig:
    """Config for formula generators."""

    component_ids: abc.Set[ComponentId] | None = None
    """The component IDs to use for generating the formula."""

    allow_fallback: bool = True


class FormulaGenerator(ABC, Generic[QuantityT]):
    """A class for generating formulas from the component graph."""

    def __init__(
        self,
        namespace: str,
        channel_registry: ChannelRegistry,
        resampler_subscription_sender: Sender[ComponentMetricRequest],
        config: FormulaGeneratorConfig,
    ) -> None:
        """Create a `FormulaGenerator` instance.

        Args:
            namespace: A namespace to use with the data-pipeline.
            channel_registry: A channel registry instance shared with the resampling
                actor.
            resampler_subscription_sender: A sender for sending metric requests to the
                resampling actor.
            config: configs for the formula generator.
        """
        self._channel_registry: ChannelRegistry = channel_registry
        self._resampler_subscription_sender: Sender[ComponentMetricRequest] = (
            resampler_subscription_sender
        )
        self._namespace: str = namespace
        self._config: FormulaGeneratorConfig = config

    @property
    def namespace(self) -> str:
        """Get the namespace for the formula generator."""
        return self._namespace

    def _get_builder(
        self,
        name: str,
        component_metric_id: ComponentMetricId,
        create_method: Callable[[float], QuantityT],
    ) -> ResampledFormulaBuilder[QuantityT]:
        builder = ResampledFormulaBuilder(
            namespace=self._namespace,
            formula_name=name,
            channel_registry=self._channel_registry,
            resampler_subscription_sender=self._resampler_subscription_sender,
            metric_id=component_metric_id,
            create_method=create_method,
        )
        return builder

    def _get_grid_component(self) -> Component:
        """
        Get the grid component in the component graph.

        Returns:
            The first grid component found in the graph.

        Raises:
            ComponentNotFound: If the grid component is not found in the component graph.
        """
        component_graph = connection_manager.get().component_graph
        grid_component = next(
            iter(
                component_graph.components(
                    component_categories={ComponentCategory.GRID}
                )
            ),
            None,
        )
        if grid_component is None:
            raise ComponentNotFound("Grid component not found in the component graph.")

        return grid_component

    def _get_grid_component_successors(self) -> set[Component]:
        """Get the set of grid component successors in the component graph.

        Returns:
            A set of grid component successors.

        Raises:
            ComponentNotFound: If no successor components are found in the component graph.
        """
        grid_component = self._get_grid_component()
        component_graph = connection_manager.get().component_graph
        grid_successors = component_graph.successors(grid_component.component_id)

        if not grid_successors:
            raise ComponentNotFound("No components found in the component graph.")

        return grid_successors

    @abstractmethod
    def generate(
        self,
    ) -> FormulaEngine[QuantityT] | FormulaEngine3Phase[QuantityT]:
        """Generate a formula engine, based on the component graph."""

    def _get_metric_fallback_components(
        self, components: set[Component]
    ) -> dict[Component, set[Component]]:
        """Get primary and fallback components within a given set of components.

        When a meter is positioned before one or more components of the same type (e.g., inverters),
        it is considered the primary component, and the components that follow are treated
        as fallback components.
        If the non-meter component has no meter in front of it, then it is the primary component
        and has no fallbacks.

        The method iterates through the provided components and assesses their roles as primary
        or fallback components.
        If a component:
         * can act as a primary component (e.g., a meter), then it finds its
        fallback components and pairs them together.
         * can act as a fallback (e.g., an inverter or EV charger), then it finds
        the primary component for it (usually a meter) and pairs them together.
         * has no fallback (e.g., an inverter that has no meter attached), then it
        returns an empty set for that component. This means that the component
        is a primary component and has no fallbacks.

        Args:
            components: The components to be analyzed.

        Returns:
            A dictionary where:
                * The keys are primary components.
                * The values are sets of fallback components.
        """
        graph = connection_manager.get().component_graph
        fallbacks: dict[Component, set[Component]] = {}

        for component in components:
            if component.category == ComponentCategory.METER:
                fallbacks[component] = self._get_meter_fallback_components(component)
            else:
                predecessors = graph.predecessors(component.component_id)
                if len(predecessors) == 1:
                    predecessor = predecessors.pop()
                    if self._is_primary_fallback_pair(predecessor, component):
                        # predecessor is primary component and the component is one of the
                        # fallbacks components.
                        fallbacks.setdefault(predecessor, set()).add(component)
                        continue

                # This component is primary component with no fallbacks.
                fallbacks[component] = set()
        return fallbacks

    def _get_meter_fallback_components(self, meter: Component) -> set[Component]:
        """Get the fallback components for a given meter.

        Args:
            meter: The meter to find the fallback components for.

        Returns:
            A set of fallback components for the given meter.
            An empty set is returned if the meter has no fallbacks.
        """
        assert meter.category == ComponentCategory.METER

        graph = connection_manager.get().component_graph
        successors = graph.successors(meter.component_id)

        # All fallbacks has to be of the same type and category.
        if (
            all(graph.is_pv_inverter(c) for c in successors)
            or all(graph.is_battery_inverter(c) for c in successors)
            or all(graph.is_ev_charger(c) for c in successors)
        ):
            return successors
        return set()

    def _is_primary_fallback_pair(
        self,
        primary_candidate: Component,
        fallback_candidate: Component,
    ) -> bool:
        """Determine if a given component can act as a primary-fallback pair.

        This method checks:
         * whether the `fallback_candidate` is of a type that can have the `primary_candidate`,
         * if `primary_candidate` is the primary measuring point of the `fallback_candidate`.

        Args:
            primary_candidate: The component to be checked as a primary measuring device.
            fallback_candidate: The component to be checked as a fallback measuring device.

        Returns:
            bool: True if the provided components are a primary-fallback pair, False otherwise.
        """
        graph = connection_manager.get().component_graph

        # reassign to decrease the length of the line and make code readable
        fallback = fallback_candidate
        primary = primary_candidate

        # fmt: off
        return (
            graph.is_pv_inverter(fallback) and graph.is_pv_meter(primary)
            or graph.is_chp(fallback) and graph.is_chp_meter(primary)
            or graph.is_ev_charger(fallback) and graph.is_ev_charger_meter(primary)
            or graph.is_battery_inverter(fallback) and graph.is_battery_meter(primary)
        )
        # fmt: on



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_grid_current_formula.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Formula generator from component graph for 3-phase Grid Current."""

from frequenz.client.microgrid import Component, ComponentCategory, ComponentMetricId
from frequenz.quantities import Current

from .._formula_engine import FormulaEngine, FormulaEngine3Phase
from ._formula_generator import FormulaGenerator


class GridCurrentFormula(FormulaGenerator[Current]):
    """Create a formula engine from the component graph for calculating grid current."""

    def generate(  # noqa: DOC502
        # ComponentNotFound is raised indirectly by _get_grid_component_successors
        self,
    ) -> FormulaEngine3Phase[Current]:
        """Generate a formula for calculating grid current from the component graph.

        Returns:
            A formula engine that will calculate 3-phase grid current values.

        Raises:
            ComponentNotFound: when the component graph doesn't have a `GRID` component.
        """
        grid_successors = self._get_grid_component_successors()

        return FormulaEngine3Phase(
            "grid-current",
            Current.from_amperes,
            (
                self._gen_phase_formula(
                    grid_successors, ComponentMetricId.CURRENT_PHASE_1
                ),
                self._gen_phase_formula(
                    grid_successors, ComponentMetricId.CURRENT_PHASE_2
                ),
                self._gen_phase_formula(
                    grid_successors, ComponentMetricId.CURRENT_PHASE_3
                ),
            ),
        )

    def _gen_phase_formula(
        self,
        grid_successors: set[Component],
        metric_id: ComponentMetricId,
    ) -> FormulaEngine[Current]:
        builder = self._get_builder("grid-current", metric_id, Current.from_amperes)

        # generate a formula that just adds values from all components that are
        # directly connected to the grid.
        for idx, comp in enumerate(grid_successors):
            # When inverters or ev chargers produce `None` samples, those
            # inverters are excluded from the calculation by treating their
            # `None` values as `0`s.
            #
            # This is not possible for Meters, so when they produce `None`
            # values, those values get propagated as the output.
            if comp.category in (
                ComponentCategory.INVERTER,
                ComponentCategory.EV_CHARGER,
            ):
                nones_are_zeros = True
            elif comp.category == ComponentCategory.METER:
                nones_are_zeros = False
            else:
                continue

            if idx > 0:
                builder.push_oper("+")

            builder.push_component_metric(
                comp.component_id, nones_are_zeros=nones_are_zeros
            )

        return builder.build()



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_grid_power_3_phase_formula.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Formula generator from component graph for 3-phase Grid Power."""

from frequenz.client.microgrid import Component, ComponentCategory, ComponentMetricId
from frequenz.quantities import Power

from .._formula_engine import FormulaEngine, FormulaEngine3Phase
from ._formula_generator import FormulaGenerator


class GridPower3PhaseFormula(FormulaGenerator[Power]):
    """Create a formula engine for calculating the grid 3-phase power."""

    def generate(  # noqa: DOC502
        # ComponentNotFound is raised indirectly by _get_grid_component_successors
        self,
    ) -> FormulaEngine3Phase[Power]:
        """Generate a formula for calculating grid 3-phase power.

        Raises:
            ComponentNotFound: when the component graph doesn't have a `GRID` component.

        Returns:
            A formula engine that will calculate grid 3-phase power values.
        """
        grid_successors = self._get_grid_component_successors()

        return FormulaEngine3Phase(
            "grid-power-3-phase",
            Power.from_watts,
            (
                self._gen_phase_formula(
                    grid_successors, ComponentMetricId.ACTIVE_POWER_PHASE_1
                ),
                self._gen_phase_formula(
                    grid_successors, ComponentMetricId.ACTIVE_POWER_PHASE_2
                ),
                self._gen_phase_formula(
                    grid_successors, ComponentMetricId.ACTIVE_POWER_PHASE_3
                ),
            ),
        )

    def _gen_phase_formula(
        self,
        grid_successors: set[Component],
        metric_id: ComponentMetricId,
    ) -> FormulaEngine[Power]:
        """Generate a formula for calculating grid 3-phase power from the component graph.

        Generate a formula that adds values from all components that are directly
        connected to the grid.

        Args:
            grid_successors: The set of components that are directly connected to the grid.
            metric_id: The metric to use for the formula.

        Returns:
            A formula engine that will calculate grid 3-phase power values.
        """
        formula_builder = self._get_builder(
            "grid-power-3-phase", metric_id, Power.from_watts
        )

        for idx, comp in enumerate(grid_successors):
            # When inverters or EV chargers produce `None` samples, they are
            # excluded from the calculation by treating their `None` values
            # as `0`s.
            #
            # This is not possible for Meters, so when they produce `None`
            # values, those values get propagated as the output.
            if comp.category in (
                ComponentCategory.INVERTER,
                ComponentCategory.EV_CHARGER,
            ):
                nones_are_zeros = True
            elif comp.category == ComponentCategory.METER:
                nones_are_zeros = False
            else:
                continue

            if idx > 0:
                formula_builder.push_oper("+")

            formula_builder.push_component_metric(
                comp.component_id, nones_are_zeros=nones_are_zeros
            )

        return formula_builder.build()



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_grid_power_formula.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Formula generator from component graph for Grid Power."""


from frequenz.client.microgrid import Component, ComponentMetricId
from frequenz.quantities import Power

from .._formula_engine import FormulaEngine
from ._fallback_formula_metric_fetcher import FallbackFormulaMetricFetcher
from ._formula_generator import FormulaGeneratorConfig
from ._grid_power_formula_base import GridPowerFormulaBase
from ._simple_formula import SimplePowerFormula


class GridPowerFormula(GridPowerFormulaBase[Power]):
    """Creates a formula engine from the component graph for calculating grid power."""

    def generate(  # noqa: DOC502
        # * ComponentNotFound is raised indirectly by _get_grid_component_successors
        self,
    ) -> FormulaEngine[Power]:
        """Generate a formula for calculating grid power from the component graph.

        Returns:
            A formula engine that will calculate grid power values.

        Raises:
            ComponentNotFound: when the component graph doesn't have a `GRID` component.
        """
        builder = self._get_builder(
            "grid-power",
            ComponentMetricId.ACTIVE_POWER,
            Power.from_watts,
        )
        return self._generate(builder)

    def _get_fallback_formulas(
        self, components: set[Component]
    ) -> dict[Component, FallbackFormulaMetricFetcher[Power] | None]:
        """Find primary and fallback components and create fallback formulas.

        The primary component is the one that will be used to calculate the grid power.
        If it is not available, the fallback formula will be used instead.
        Fallback formulas calculate the grid power using the fallback components.
        Fallback formulas are wrapped in `FallbackFormulaMetricFetcher`.

        Args:
            components: The producer components.

        Returns:
            A dictionary mapping primary components to their FallbackFormulaMetricFetcher.
        """
        fallbacks = self._get_metric_fallback_components(components)

        fallback_formulas: dict[
            Component, FallbackFormulaMetricFetcher[Power] | None
        ] = {}

        for primary_component, fallback_components in fallbacks.items():
            if len(fallback_components) == 0:
                fallback_formulas[primary_component] = None
                continue

            fallback_ids = [c.component_id for c in fallback_components]
            generator = SimplePowerFormula(
                f"{self._namespace}_fallback_{fallback_ids}",
                self._channel_registry,
                self._resampler_subscription_sender,
                FormulaGeneratorConfig(
                    component_ids=set(fallback_ids),
                    allow_fallback=False,
                ),
            )

            fallback_formulas[primary_component] = FallbackFormulaMetricFetcher(
                generator
            )

        return fallback_formulas



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_grid_power_formula_base.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Base formula generator from component graph for Grid Power."""

from abc import ABC, abstractmethod

from frequenz.client.microgrid import Component, ComponentCategory

from ..._base_types import QuantityT
from .._formula_engine import FormulaEngine
from .._resampled_formula_builder import ResampledFormulaBuilder
from ._fallback_formula_metric_fetcher import FallbackFormulaMetricFetcher
from ._formula_generator import ComponentNotFound, FormulaGenerator


class GridPowerFormulaBase(FormulaGenerator[QuantityT], ABC):
    """Base class for grid power formula generators."""

    def _generate(
        self, builder: ResampledFormulaBuilder[QuantityT]
    ) -> FormulaEngine[QuantityT]:
        """Generate a formula for calculating grid power from the component graph.

        Args:
            builder: The builder to use to create the formula.

        Returns:
            A formula engine that will calculate grid power values.

        Raises:
            ComponentNotFound: when the component graph doesn't have a `GRID` component.
        """
        grid_successors = self._get_grid_component_successors()

        components = {
            c
            for c in grid_successors
            if c.category
            in {
                ComponentCategory.INVERTER,
                ComponentCategory.EV_CHARGER,
                ComponentCategory.METER,
            }
        }

        if not components:
            raise ComponentNotFound("No grid successors found")

        # generate a formula that just adds values from all components that are
        # directly connected to the grid.  If the requested formula type is
        # `PASSIVE_SIGN_CONVENTION`, there is nothing more to do.  If the requested
        # formula type is `PRODUCTION`, the formula output is negated, then clipped to
        # 0.  If the requested formula type is `CONSUMPTION`, the formula output is
        # already positive, so it is just clipped to 0.
        #
        # So the formulas would look like:
        #  - `PASSIVE_SIGN_CONVENTION`: `(grid-successor-1 + grid-successor-2 + ...)`
        #  - `PRODUCTION`: `max(0, -(grid-successor-1 + grid-successor-2 + ...))`
        #  - `CONSUMPTION`: `max(0, (grid-successor-1 + grid-successor-2 + ...))`
        if self._config.allow_fallback:
            fallbacks = self._get_fallback_formulas(components)

            for idx, (primary_component, fallback_formula) in enumerate(
                fallbacks.items()
            ):
                if idx > 0:
                    builder.push_oper("+")

                # should only be the case if the component is not a meter
                builder.push_component_metric(
                    primary_component.component_id,
                    nones_are_zeros=(
                        primary_component.category != ComponentCategory.METER
                    ),
                    fallback=fallback_formula,
                )
        else:
            for idx, comp in enumerate(components):
                if idx > 0:
                    builder.push_oper("+")

                builder.push_component_metric(
                    comp.component_id,
                    nones_are_zeros=(comp.category != ComponentCategory.METER),
                )

        return builder.build()

    @abstractmethod
    def _get_fallback_formulas(
        self, components: set[Component]
    ) -> dict[Component, FallbackFormulaMetricFetcher[QuantityT] | None]:
        """Find primary and fallback components and create fallback formulas.

        The primary component is the one that will be used to calculate the producer power.
        If it is not available, the fallback formula will be used instead.
        Fallback formulas calculate the grid power using the fallback components.
        Fallback formulas are wrapped in `FallbackFormulaMetricFetcher`.

        Args:
            components: The producer components.

        Returns:
            A dictionary mapping primary components to their FallbackFormulaMetricFetcher.
        """



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_grid_reactive_power_formula.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Formula generator from component graph for Grid Reactive Power."""


from frequenz.client.microgrid import Component, ComponentMetricId
from frequenz.quantities import ReactivePower

from .._formula_engine import FormulaEngine
from ._fallback_formula_metric_fetcher import FallbackFormulaMetricFetcher
from ._formula_generator import FormulaGeneratorConfig
from ._grid_power_formula_base import GridPowerFormulaBase
from ._simple_formula import SimpleReactivePowerFormula


class GridReactivePowerFormula(GridPowerFormulaBase[ReactivePower]):
    """Creates a formula engine from the component graph for calculating grid reactive power."""

    def generate(  # noqa: DOC502
        # * ComponentNotFound is raised indirectly by _get_grid_component_successors
        self,
    ) -> FormulaEngine[ReactivePower]:
        """Generate a formula for calculating grid reactive power from the component graph.

        Returns:
            A formula engine that will calculate grid reactive power values.

        Raises:
            ComponentNotFound: when the component graph doesn't have a `GRID` component.
        """
        builder = self._get_builder(
            "grid_reactive_power_formula",
            ComponentMetricId.REACTIVE_POWER,
            ReactivePower.from_volt_amperes_reactive,
        )
        return self._generate(builder)

    def _get_fallback_formulas(
        self, components: set[Component]
    ) -> dict[Component, FallbackFormulaMetricFetcher[ReactivePower] | None]:
        """Find primary and fallback components and create fallback formulas.

        The primary component is the one that will be used to calculate the grid reactive power.
        If it is not available, the fallback formula will be used instead.
        Fallback formulas calculate the grid power using the fallback components.
        Fallback formulas are wrapped in `FallbackFormulaMetricFetcher`.

        Args:
            components: The producer components.

        Returns:
            A dictionary mapping primary components to their FallbackFormulaMetricFetcher.
        """
        fallbacks = self._get_metric_fallback_components(components)

        fallback_formulas: dict[
            Component, FallbackFormulaMetricFetcher[ReactivePower] | None
        ] = {}

        for primary_component, fallback_components in fallbacks.items():
            if len(fallback_components) == 0:
                fallback_formulas[primary_component] = None
                continue

            fallback_ids = [c.component_id for c in fallback_components]
            generator = SimpleReactivePowerFormula(
                f"{self._namespace}_fallback_{fallback_ids}",
                self._channel_registry,
                self._resampler_subscription_sender,
                FormulaGeneratorConfig(
                    component_ids=set(fallback_ids),
                    allow_fallback=False,
                ),
            )

            fallback_formulas[primary_component] = FallbackFormulaMetricFetcher(
                generator
            )

        return fallback_formulas



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_producer_power_formula.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Formula generator from component graph for Producer Power."""

import logging
from typing import Callable

from frequenz.client.microgrid import Component, ComponentCategory, ComponentMetricId
from frequenz.quantities import Power

from ....microgrid import connection_manager
from .._formula_engine import FormulaEngine
from ._fallback_formula_metric_fetcher import FallbackFormulaMetricFetcher
from ._formula_generator import (
    NON_EXISTING_COMPONENT_ID,
    FormulaGenerator,
    FormulaGeneratorConfig,
)
from ._simple_formula import SimplePowerFormula

_logger = logging.getLogger(__name__)


class ProducerPowerFormula(FormulaGenerator[Power]):
    """Formula generator from component graph for calculating the Producer Power.

    The producer power is calculated by summing up the power of all power producers,
    which are CHP and PV.
    """

    def generate(  # noqa: DOC502
        # * ComponentNotFound is raised indirectly by _get_grid_component()
        # * RuntimeError is raised indirectly by connection_manager.get()
        self,
    ) -> FormulaEngine[Power]:
        """Generate formula for calculating producer power from the component graph.

        Returns:
            A formula engine that will calculate the producer power.

        Raises:
            ComponentNotFound: If the component graph does not contain a producer power
                component.
            RuntimeError: If the grid component has a single successor that is not a
                meter.
        """
        builder = self._get_builder(
            "producer_power", ComponentMetricId.ACTIVE_POWER, Power.from_watts
        )

        component_graph = connection_manager.get().component_graph
        # if in the future we support additional producers, we need to add them to the lambda
        producer_components = component_graph.dfs(
            self._get_grid_component(),
            set(),
            lambda component: component_graph.is_pv_chain(component)
            or component_graph.is_chp_chain(component),
        )

        if not producer_components:
            _logger.warning(
                "Unable to find any producer components in the component graph. "
                "Subscribing to the resampling actor with a non-existing "
                "component id, so that `0` values are sent from the formula."
            )
            # If there are no producer components, we have to send 0 values at the same
            # frequency as the other streams.  So we subscribe with a non-existing
            # component id, just to get a `None` message at the resampling interval.
            builder.push_component_metric(
                NON_EXISTING_COMPONENT_ID, nones_are_zeros=True
            )
            return builder.build()

        is_not_meter: Callable[[Component], bool] = (
            lambda component: component.category != ComponentCategory.METER
        )

        if self._config.allow_fallback:
            fallbacks = self._get_fallback_formulas(producer_components)

            for idx, (primary_component, fallback_formula) in enumerate(
                fallbacks.items()
            ):
                if idx > 0:
                    builder.push_oper("+")

                # should only be the case if the component is not a meter
                builder.push_component_metric(
                    primary_component.component_id,
                    nones_are_zeros=is_not_meter(primary_component),
                    fallback=fallback_formula,
                )
        else:
            for idx, component in enumerate(producer_components):
                if idx > 0:
                    builder.push_oper("+")

                builder.push_component_metric(
                    component.component_id,
                    nones_are_zeros=is_not_meter(component),
                )

        return builder.build()

    def _get_fallback_formulas(
        self, components: set[Component]
    ) -> dict[Component, FallbackFormulaMetricFetcher[Power] | None]:
        """Find primary and fallback components and create fallback formulas.

        The primary component is the one that will be used to calculate the producer power.
        However, if it is not available, the fallback formula will be used instead.
        Fallback formulas calculate the producer power using the fallback components.
        Fallback formulas are wrapped in `FallbackFormulaMetricFetcher`.

        Args:
            components: The producer components.

        Returns:
            A dictionary mapping primary components to their FallbackFormulaMetricFetcher.
        """
        fallbacks = self._get_metric_fallback_components(components)

        fallback_formulas: dict[
            Component, FallbackFormulaMetricFetcher[Power] | None
        ] = {}

        for primary_component, fallback_components in fallbacks.items():
            if len(fallback_components) == 0:
                fallback_formulas[primary_component] = None
                continue

            fallback_ids = [c.component_id for c in fallback_components]
            generator = SimplePowerFormula(
                f"{self._namespace}_fallback_{fallback_ids}",
                self._channel_registry,
                self._resampler_subscription_sender,
                FormulaGeneratorConfig(
                    component_ids=set(fallback_ids),
                    allow_fallback=False,
                ),
            )

            fallback_formulas[primary_component] = FallbackFormulaMetricFetcher(
                generator
            )

        return fallback_formulas



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_pv_power_formula.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Formula generator for PV Power, from the component graph."""

import logging

from frequenz.client.microgrid import Component, ComponentCategory, ComponentMetricId
from frequenz.quantities import Power

from ....microgrid import connection_manager
from .._formula_engine import FormulaEngine
from ._fallback_formula_metric_fetcher import FallbackFormulaMetricFetcher
from ._formula_generator import (
    NON_EXISTING_COMPONENT_ID,
    FormulaGenerator,
    FormulaGeneratorConfig,
)

_logger = logging.getLogger(__name__)


class PVPowerFormula(FormulaGenerator[Power]):
    """Creates a formula engine for calculating the PV power production."""

    def generate(  # noqa: DOC502
        # * ComponentNotFound is raised indirectly by _get_pv_power_components
        # * RuntimeError is also raised indirectly by _get_pv_power_components
        self,
    ) -> FormulaEngine[Power]:
        """Make a formula for the PV power production of a microgrid.

        Returns:
            A formula engine that will calculate PV power production values.

        Raises:
            ComponentNotFound: if there is a problem finding the needed components.
            RuntimeError: if the grid component has no PV inverters or meters as
                successors.
        """
        builder = self._get_builder(
            "pv-power", ComponentMetricId.ACTIVE_POWER, Power.from_watts
        )

        component_graph = connection_manager.get().component_graph
        component_ids = self._config.component_ids
        if component_ids:
            pv_components = component_graph.components(set(component_ids))
        else:
            pv_components = component_graph.dfs(
                self._get_grid_component(),
                set(),
                component_graph.is_pv_chain,
            )

        if not pv_components:
            _logger.warning(
                "Unable to find any PV components in the component graph. "
                "Subscribing to the resampling actor with a non-existing "
                "component id, so that `0` values are sent from the formula."
            )
            # If there are no PV components, we have to send 0 values at the same
            # frequency as the other streams.  So we subscribe with a non-existing
            # component id, just to get a `None` message at the resampling interval.
            builder.push_component_metric(
                NON_EXISTING_COMPONENT_ID,
                nones_are_zeros=True,
            )
            return builder.build()

        if self._config.allow_fallback:
            fallbacks = self._get_fallback_formulas(pv_components)

            for idx, (primary_component, fallback_formula) in enumerate(
                fallbacks.items()
            ):
                if idx > 0:
                    builder.push_oper("+")

                builder.push_component_metric(
                    primary_component.component_id,
                    nones_are_zeros=(
                        primary_component.category != ComponentCategory.METER
                    ),
                    fallback=fallback_formula,
                )
        else:
            for idx, component in enumerate(pv_components):
                if idx > 0:
                    builder.push_oper("+")

                builder.push_component_metric(
                    component.component_id,
                    nones_are_zeros=component.category != ComponentCategory.METER,
                )

        return builder.build()

    def _get_fallback_formulas(
        self, components: set[Component]
    ) -> dict[Component, FallbackFormulaMetricFetcher[Power] | None]:
        """Find primary and fallback components and create fallback formulas.

        The primary component is the one that will be used to calculate the PV power.
        If it is not available, the fallback formula will be used instead.
        Fallback formulas calculate the PV power using the fallback components.
        Fallback formulas are wrapped in `FallbackFormulaMetricFetcher`.

        Args:
            components: The PV components.

        Returns:
            A dictionary mapping primary components to their corresponding
                FallbackFormulaMetricFetcher.
        """
        fallbacks = self._get_metric_fallback_components(components)

        fallback_formulas: dict[
            Component, FallbackFormulaMetricFetcher[Power] | None
        ] = {}
        for primary_component, fallback_components in fallbacks.items():
            if len(fallback_components) == 0:
                fallback_formulas[primary_component] = None
                continue
            fallback_ids = [c.component_id for c in fallback_components]

            generator = PVPowerFormula(
                f"{self._namespace}_fallback_{fallback_ids}",
                self._channel_registry,
                self._resampler_subscription_sender,
                FormulaGeneratorConfig(
                    component_ids=set(fallback_ids),
                    allow_fallback=False,
                ),
            )

            fallback_formulas[primary_component] = FallbackFormulaMetricFetcher(
                generator
            )

        return fallback_formulas



================================================
FILE: src/frequenz/sdk/timeseries/formula_engine/_formula_generators/_simple_formula.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Formula generator from component graph."""

from frequenz.client.microgrid import ComponentCategory, ComponentMetricId
from frequenz.quantities import Power, ReactivePower

from ....microgrid import connection_manager
from ..._base_types import QuantityT
from .._formula_engine import FormulaEngine
from .._resampled_formula_builder import ResampledFormulaBuilder
from ._formula_generator import FormulaGenerator


class SimpleFormulaBase(FormulaGenerator[QuantityT]):
    """Base class for simple formula generators."""

    def _generate(
        self, builder: ResampledFormulaBuilder[QuantityT]
    ) -> FormulaEngine[QuantityT]:
        """Generate formula for calculating quantity from the component graph.

        Args:
            builder: The builder to use for generating the formula.

        Returns:
            A formula engine that will calculate the quantity.

        Raises:
            RuntimeError: If components ids in config are not specified
              or component graph does not contain all specified components.
        """
        component_graph = connection_manager.get().component_graph
        if self._config.component_ids is None:
            raise RuntimeError("Power formula without component ids is not supported.")

        components = component_graph.components(
            component_ids=set(self._config.component_ids)
        )

        not_found_components = self._config.component_ids - {
            c.component_id for c in components
        }
        if not_found_components:
            raise RuntimeError(
                f"Unable to find {not_found_components} components in the component graph. ",
            )

        for idx, component in enumerate(components):
            if idx > 0:
                builder.push_oper("+")

            builder.push_component_metric(
                component.component_id,
                nones_are_zeros=component.category != ComponentCategory.METER,
            )

        return builder.build()


class SimplePowerFormula(SimpleFormulaBase[Power]):
    """Formula generator from component graph for calculating sum of Power."""

    def generate(  # noqa: DOC502
        # * ComponentNotFound is raised indirectly by _get_grid_component()
        # * RuntimeError is raised indirectly by connection_manager.get()
        self,
    ) -> FormulaEngine[Power]:
        """Generate formula for calculating sum of power from the component graph.

        Returns:
            A formula engine that will calculate the power.

        Raises:
            RuntimeError: If components ids in config are not specified
                or component graph does not contain all specified components.
        """
        builder = self._get_builder(
            "simple_power_formula",
            ComponentMetricId.ACTIVE_POWER,
            Power.from_watts,
        )
        return self._generate(builder)


class SimpleReactivePowerFormula(SimpleFormulaBase[ReactivePower]):
    """Formula generator from component graph for calculating sum of reactive power."""

    def generate(  # noqa: DOC502
        # * ComponentNotFound is raised indirectly by _get_grid_component()
        # * RuntimeError is raised indirectly by connection_manager.get()
        self,
    ) -> FormulaEngine[ReactivePower]:
        """Generate formula for calculating sum of reactive power from the component graph.

        Returns:
            A formula engine that will calculate the power.

        Raises:
            RuntimeError: If components ids in config are not specified
                or component graph does not contain all specified components.
        """
        builder = self._get_builder(
            "simple_reactive_power_formula",
            ComponentMetricId.REACTIVE_POWER,
            ReactivePower.from_volt_amperes_reactive,
        )
        return self._generate(builder)



================================================
FILE: src/frequenz/sdk/timeseries/logical_meter/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""A logical meter for calculating high level metrics for a microgrid."""

from ._logical_meter import LogicalMeter

__all__ = ["LogicalMeter"]



================================================
FILE: src/frequenz/sdk/timeseries/logical_meter/_logical_meter.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""A logical meter for calculating high level metrics for a microgrid."""


import uuid

from frequenz.channels import Sender
from frequenz.quantities import Power, Quantity

from ..._internal._channels import ChannelRegistry
from ...microgrid._data_sourcing import ComponentMetricId, ComponentMetricRequest
from ..formula_engine import FormulaEngine
from ..formula_engine._formula_engine_pool import FormulaEnginePool
from ..formula_engine._formula_generators import CHPPowerFormula


class LogicalMeter:
    """A logical meter for calculating high level metrics in a microgrid.

    LogicalMeter provides methods for fetching power values from different points in the
    microgrid.  These methods return `FormulaReceiver` objects, which can be used like
    normal `Receiver`s, but can also be composed to form higher-order formula streams.

    !!! note
        `LogicalMeter` instances are not meant to be created directly by users.  Use the
        [`microgrid.logical_meter`][frequenz.sdk.microgrid.logical_meter] method for
        creating `LogicalMeter` instances.

    Example:
        ```python
        from datetime import timedelta

        from frequenz.sdk import microgrid
        from frequenz.sdk.timeseries import ResamplerConfig2
        from frequenz.client.microgrid import ComponentMetricId


        await microgrid.initialize(
            "grpc://microgrid.sandbox.api.frequenz.io:62060",
            ResamplerConfig2(resampling_period=timedelta(seconds=1)),
        )

        logical_meter = (
            microgrid.logical_meter()
            .start_formula("#1001 + #1002", ComponentMetricId.ACTIVE_POWER)
            .new_receiver()
        )

        async for power in logical_meter:
            print(power.value)
        ```
    """

    def __init__(
        self,
        channel_registry: ChannelRegistry,
        resampler_subscription_sender: Sender[ComponentMetricRequest],
    ) -> None:
        """Create a `LogicalMeter` instance.

        !!! note
            `LogicalMeter` instances are not meant to be created directly by users.  Use
            the [`microgrid.logical_meter`][frequenz.sdk.microgrid.logical_meter] method
            for creating `LogicalMeter` instances.

        Args:
            channel_registry: A channel registry instance shared with the resampling
                actor.
            resampler_subscription_sender: A sender for sending metric requests to the
                resampling actor.
        """
        self._channel_registry: ChannelRegistry = channel_registry
        self._resampler_subscription_sender: Sender[ComponentMetricRequest] = (
            resampler_subscription_sender
        )

        # Use a randomly generated uuid to create a unique namespace name for the local
        # meter to use when communicating with the resampling actor.
        self._namespace = f"logical-meter-{uuid.uuid4()}"
        self._formula_pool = FormulaEnginePool(
            self._namespace,
            self._channel_registry,
            self._resampler_subscription_sender,
        )

    def start_formula(
        self,
        formula: str,
        component_metric_id: ComponentMetricId,
        *,
        nones_are_zeros: bool = False,
    ) -> FormulaEngine[Quantity]:
        """Start execution of the given formula.

        Formulas can have Component IDs that are preceeded by a pound symbol("#"), and
        these operators: +, -, *, /, (, ).

        For example, the input string: "#20 + #5" is a formula for adding metrics from
        two components with ids 20 and 5.

        Args:
            formula: formula to execute.
            component_metric_id: The metric ID to use when fetching receivers from the
                resampling actor.
            nones_are_zeros: Whether to treat None values from the stream as 0s.  If
                False, the returned value will be a None.

        Returns:
            A FormulaEngine that applies the formula and streams values.
        """
        return self._formula_pool.from_string(
            formula, component_metric_id, nones_are_zeros=nones_are_zeros
        )

    @property
    def chp_power(self) -> FormulaEngine[Power]:
        """Fetch the CHP power production in the microgrid.

        This formula produces values that are in the Passive Sign Convention (PSC).

        If a formula engine to calculate CHP power production is not already running, it
        will be started.

        A receiver from the formula engine can be created using the `new_receiver`
        method.

        Returns:
            A FormulaEngine that will calculate and stream CHP power production.
        """
        engine = self._formula_pool.from_power_formula_generator(
            "chp_power",
            CHPPowerFormula,
        )
        assert isinstance(engine, FormulaEngine)
        return engine

    async def stop(self) -> None:
        """Stop all formula engines."""
        await self._formula_pool.stop()



================================================
FILE: src/frequenz/sdk/timeseries/pv_pool/__init__.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Interactions with PV inverters."""

from ._pv_pool import PVPool, PVPoolError
from ._result_types import PVPoolReport

__all__ = [
    "PVPool",
    "PVPoolError",
    "PVPoolReport",
]



================================================
FILE: src/frequenz/sdk/timeseries/pv_pool/_pv_pool.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Interactions with pools of PV inverters."""

import asyncio
import uuid
from collections import abc

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.quantities import Power

from ..._internal._channels import MappingReceiverFetcher, ReceiverFetcher
from ...microgrid import _power_distributing, _power_managing
from ...timeseries import Bounds
from .._base_types import SystemBounds
from ..formula_engine import FormulaEngine
from ..formula_engine._formula_generators import FormulaGeneratorConfig, PVPowerFormula
from ._pv_pool_reference_store import PVPoolReferenceStore
from ._result_types import PVPoolReport


class PVPoolError(Exception):
    """An error that occurred in any of the PVPool methods."""


class PVPool:
    """An interface for interaction with pools of PV inverters.

    Provides:
      - Aggregate [`power`][frequenz.sdk.timeseries.pv_pool.PVPool.power]
        measurements of the PV inverters in the pool.
    """

    def __init__(  # pylint: disable=too-many-arguments
        self,
        *,
        pool_ref_store: PVPoolReferenceStore,
        name: str | None,
        priority: int,
    ) -> None:
        """Initialize the instance.

        !!! note
            `PVPool` instances are not meant to be created directly by users. Use the
            [`microgrid.new_pv_pool`][frequenz.sdk.microgrid.new_pv_pool] method for
            creating `PVPool` instances.

        Args:
            pool_ref_store: The reference store for the PV pool.
            name: The name of the PV pool.
            priority: The priority of the PV pool.
        """
        self._pool_ref_store = pool_ref_store
        unique_id = uuid.uuid4()
        self._source_id = str(unique_id) if name is None else f"{name}-{unique_id}"
        self._priority = priority

    async def propose_power(
        self,
        power: Power | None,
        bounds: Bounds[Power | None] = Bounds(None, None),
    ) -> None:
        """Send a proposal to the power manager for the pool's set of PV inverters.

        This proposal is for the maximum power that can be set for the PV inverters in
        the pool.  The actual production might be lower.

        Power values need to follow the Passive Sign Convention (PSC). That is, positive
        values indicate charge power and negative values indicate discharge power.  Only
        discharge powers are allowed for PV inverters.

        Details on how the power manager handles proposals can be found in the
        [Microgrid][frequenz.sdk.microgrid--setting-power] documentation.

        Args:
            power: The power to propose for the PV inverters in the pool.  If `None`,
                this proposal will not have any effect on the target power, unless
                bounds are specified.  When speficied without bounds, bounds for lower
                priority actors will be shifted by this power.  If both are `None`, it
                is equivalent to not having a proposal or withdrawing a previous one.
            bounds: The power bounds for the proposal.  When specified, this will limit
                the bounds for lower priority actors.

        Raises:
            PVPoolError: If a charge power for PV inverters is requested.
        """
        if power is not None and power > Power.zero():
            raise PVPoolError("Charge powers for PV inverters is not supported.")
        await self._pool_ref_store.power_manager_requests_sender.send(
            _power_managing.Proposal(
                source_id=self._source_id,
                preferred_power=power,
                bounds=bounds,
                component_ids=self._pool_ref_store.component_ids,
                priority=self._priority,
                creation_time=asyncio.get_running_loop().time(),
            )
        )

    @property
    def component_ids(self) -> abc.Set[ComponentId]:
        """Return component IDs of all PV inverters managed by this PVPool.

        Returns:
            Set of managed component IDs.
        """
        return self._pool_ref_store.component_ids

    @property
    def power(self) -> FormulaEngine[Power]:
        """Fetch the total power for the PV Inverters in the pool.

        This formula produces values that are in the Passive Sign Convention (PSC).

        If a formula engine to calculate PV Inverter power is not already running, it
        will be started.

        A receiver from the formula engine can be created using the `new_receiver`
        method.

        Returns:
            A FormulaEngine that will calculate and stream the total power of all PV
                Inverters.
        """
        engine = self._pool_ref_store.formula_pool.from_power_formula_generator(
            "pv_power",
            PVPowerFormula,
            FormulaGeneratorConfig(
                component_ids=self._pool_ref_store.component_ids,
            ),
        )
        assert isinstance(engine, FormulaEngine)
        return engine

    @property
    def power_status(self) -> ReceiverFetcher[PVPoolReport]:
        """Get a receiver to receive new power status reports when they change.

        These include
          - the current inclusion/exclusion bounds available for the pool's priority,
          - the current target power for the pool's set of batteries,
          - the result of the last distribution request for the pool's set of batteries.

        Returns:
            A receiver that will stream power status reports for the pool's priority.
        """
        sub = _power_managing.ReportRequest(
            source_id=self._source_id,
            priority=self._priority,
            component_ids=self._pool_ref_store.component_ids,
        )
        self._pool_ref_store.power_bounds_subs[sub.get_channel_name()] = (
            asyncio.create_task(
                self._pool_ref_store.power_manager_bounds_subs_sender.send(sub)
            )
        )
        channel = self._pool_ref_store.channel_registry.get_or_create(
            _power_managing._Report,  # pylint: disable=protected-access
            sub.get_channel_name(),
        )
        channel.resend_latest = True

        return channel

    @property
    def power_distribution_results(self) -> ReceiverFetcher[_power_distributing.Result]:
        """Get a receiver to receive power distribution results.

        Returns:
            A receiver that will stream power distribution results for the pool's set of
            PV inverters.
        """
        return MappingReceiverFetcher(
            self._pool_ref_store.power_distribution_results_fetcher,
            lambda recv: recv.filter(
                lambda x: x.request.component_ids == self._pool_ref_store.component_ids
            ),
        )

    async def stop(self) -> None:
        """Stop all tasks and channels owned by the PVPool."""
        # This was closing the pool_ref_store, which is not correct, because those are
        # shared.
        #
        # This method will do until we have a mechanism to track the resources created
        # through it.  It can also eventually cleanup the pool_ref_store, when it is
        # holding the last reference to it.

    @property
    def _system_power_bounds(self) -> ReceiverFetcher[SystemBounds]:
        """Return a receiver fetcher for the system power bounds."""
        return self._pool_ref_store.bounds_channel



================================================
FILE: src/frequenz/sdk/timeseries/pv_pool/_pv_pool_reference_store.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Manages shared state/tasks for a set of PV inverters."""


import asyncio
import uuid
from collections import abc

from frequenz.channels import Broadcast, Receiver, Sender
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentCategory, InverterType

from ..._internal._channels import ChannelRegistry, ReceiverFetcher
from ...microgrid import connection_manager
from ...microgrid._data_sourcing import ComponentMetricRequest
from ...microgrid._power_distributing import ComponentPoolStatus, Result
from ...microgrid._power_managing._base_classes import Proposal, ReportRequest
from .._base_types import SystemBounds
from ..formula_engine._formula_engine_pool import FormulaEnginePool
from ._system_bounds_tracker import PVSystemBoundsTracker


class PVPoolReferenceStore:
    """A class for maintaining the shared state/tasks for a set of pool of PV inverters.

    This includes ownership of
    - the formula engine pool and metric calculators.
    - the tasks for calculating system bounds for the PV inverters.

    These are independent of the priority of the actors and can be shared between
    multiple users of the same set of PV inverters.

    They are exposed through the PVPool class.
    """

    def __init__(  # pylint: disable=too-many-arguments
        self,
        *,
        channel_registry: ChannelRegistry,
        resampler_subscription_sender: Sender[ComponentMetricRequest],
        status_receiver: Receiver[ComponentPoolStatus],
        power_manager_requests_sender: Sender[Proposal],
        power_manager_bounds_subs_sender: Sender[ReportRequest],
        power_distribution_results_fetcher: ReceiverFetcher[Result],
        component_ids: abc.Set[ComponentId] | None = None,
    ):
        """Initialize this instance.

        Args:
            channel_registry: A channel registry instance shared with the resampling
                actor.
            resampler_subscription_sender: A sender for sending metric requests to the
                resampling actor.
            status_receiver: A receiver that streams the status of the PV inverters in
                the pool.
            power_manager_requests_sender: A Channel sender for sending power
                requests to the power managing actor.
            power_manager_bounds_subs_sender: A Channel sender for sending power bounds
                subscription requests to the power managing actor.
            power_distribution_results_fetcher: A ReceiverFetcher for the results from
                the power distributing actor.
            component_ids: An optional list of component_ids belonging to this pool.  If
                not specified, IDs of all PV inverters in the microgrid will be fetched
                from the component graph.
        """
        self.channel_registry = channel_registry
        self.resampler_subscription_sender = resampler_subscription_sender
        self.status_receiver = status_receiver
        self.power_manager_requests_sender = power_manager_requests_sender
        self.power_manager_bounds_subs_sender = power_manager_bounds_subs_sender
        self.power_distribution_results_fetcher = power_distribution_results_fetcher

        if component_ids is not None:
            self.component_ids: frozenset[ComponentId] = frozenset(component_ids)
        else:
            graph = connection_manager.get().component_graph
            self.component_ids = frozenset(
                {
                    inv.component_id
                    for inv in graph.components(
                        component_categories={ComponentCategory.INVERTER}
                    )
                    if inv.type == InverterType.SOLAR
                }
            )

        self.power_bounds_subs: dict[str, asyncio.Task[None]] = {}

        self.namespace: str = f"pv-pool-{uuid.uuid4()}"
        self.formula_pool = FormulaEnginePool(
            self.namespace,
            self.channel_registry,
            self.resampler_subscription_sender,
        )
        self.bounds_channel: Broadcast[SystemBounds] = Broadcast(
            name=f"System Bounds for PV inverters: {component_ids}",
            resend_latest=True,
        )

        self.bounds_tracker: PVSystemBoundsTracker | None = None
        # In locations without PV inverters, the bounds tracker will not be started.
        if self.component_ids:
            self.bounds_tracker = PVSystemBoundsTracker(
                self.component_ids,
                self.status_receiver,
                self.bounds_channel.new_sender(),
            )
            self.bounds_tracker.start()

    async def stop(self) -> None:
        """Stop all tasks and channels owned by the PVInverterPool."""
        await self.formula_pool.stop()
        if self.bounds_tracker is not None:
            await self.bounds_tracker.stop()
        self.status_receiver.close()



================================================
FILE: src/frequenz/sdk/timeseries/pv_pool/_result_types.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Types for exposing PV pool reports."""

import typing

from frequenz.quantities import Power

from .._base_types import Bounds


class PVPoolReport(typing.Protocol):
    """A status report for a PV pool."""

    @property
    def target_power(self) -> Power | None:
        """The currently set power for the PV inverters."""

    @property
    def bounds(self) -> Bounds[Power] | None:
        """The usable bounds for the PV inverters.

        These bounds are adjusted to any restrictions placed by actors with higher
        priorities.
        """



================================================
FILE: src/frequenz/sdk/timeseries/pv_pool/_system_bounds_tracker.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""System bounds tracker for PV inverters."""

import asyncio
from collections import abc

from frequenz.channels import Receiver, Sender, merge, select, selected_from
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import InverterData
from frequenz.quantities import Power

from ..._internal._asyncio import run_forever
from ...actor import BackgroundService
from ...microgrid import connection_manager
from ...microgrid._power_distributing._component_status import ComponentPoolStatus
from .._base_types import Bounds, SystemBounds


class PVSystemBoundsTracker(BackgroundService):
    """Track the system bounds for PV inverters.

    System bounds are the aggregate bounds for the PV inverters in the pool that are in
    a working state.  They are calculated from the individual bounds received from the
    microgrid API.

    The system bounds are sent to the `bounds_sender` whenever they change.
    """

    def __init__(
        self,
        component_ids: abc.Set[ComponentId],
        status_receiver: Receiver[ComponentPoolStatus],
        bounds_sender: Sender[SystemBounds],
    ):
        """Initialize the system bounds tracker.

        Args:
            component_ids: The ids of the components to track.
            status_receiver: A receiver that streams the status of the PV inverters in
                the pool.
            bounds_sender: A sender to send the system bounds to.
        """
        super().__init__()

        self._component_ids = component_ids
        self._status_receiver = status_receiver
        self._bounds_sender = bounds_sender
        self._latest_component_data: dict[ComponentId, InverterData] = {}
        self._last_sent_bounds: SystemBounds | None = None
        self._component_pool_status = ComponentPoolStatus(set(), set())

    def start(self) -> None:
        """Start the PV inverter system bounds tracker."""
        self._tasks.add(asyncio.create_task(run_forever(self._run)))

    async def _send_bounds(self) -> None:
        """Calculate and send the aggregate system bounds if they have changed."""
        if not self._latest_component_data:
            return
        inclusion_bounds = Bounds(
            lower=Power.from_watts(
                sum(
                    data.active_power_inclusion_lower_bound
                    for data in self._latest_component_data.values()
                )
            ),
            upper=Power.from_watts(
                sum(
                    data.active_power_inclusion_upper_bound
                    for data in self._latest_component_data.values()
                )
            ),
        )
        exclusion_bounds = Bounds(
            lower=Power.from_watts(
                sum(
                    data.active_power_exclusion_lower_bound
                    for data in self._latest_component_data.values()
                )
            ),
            upper=Power.from_watts(
                sum(
                    data.active_power_exclusion_upper_bound
                    for data in self._latest_component_data.values()
                )
            ),
        )

        if (
            self._last_sent_bounds is None
            or self._last_sent_bounds.inclusion_bounds != inclusion_bounds
            or self._last_sent_bounds.exclusion_bounds != exclusion_bounds
        ):
            self._last_sent_bounds = SystemBounds(
                timestamp=max(
                    data.timestamp for data in self._latest_component_data.values()
                ),
                inclusion_bounds=inclusion_bounds,
                exclusion_bounds=exclusion_bounds,
            )
            await self._bounds_sender.send(self._last_sent_bounds)

    async def _run(self) -> None:
        """Run the system bounds tracker."""
        api_client = connection_manager.get().api_client
        status_rx = self._status_receiver
        pv_data_rx = merge(
            *(
                await asyncio.gather(
                    *(
                        api_client.inverter_data(component_id)
                        for component_id in self._component_ids
                    )
                )
            )
        )

        async for selected in select(status_rx, pv_data_rx):
            if selected_from(selected, status_rx):
                self._component_pool_status = selected.message
                to_remove: list[ComponentId] = []
                for comp_id in self._latest_component_data:
                    if (
                        comp_id not in self._component_pool_status.working
                        and comp_id not in self._component_pool_status.uncertain
                    ):
                        to_remove.append(comp_id)
                for comp_id in to_remove:
                    del self._latest_component_data[comp_id]
            elif selected_from(selected, pv_data_rx):
                data = selected.message
                comp_id = data.component_id
                if (
                    comp_id not in self._component_pool_status.working
                    and comp_id not in self._component_pool_status.uncertain
                ):
                    continue
                self._latest_component_data[data.component_id] = data

            await self._send_bounds()



================================================
FILE: tests/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Frequenz Python SDK Tests."""



================================================
FILE: tests/conftest.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Setup for all the tests."""
import contextlib
from collections.abc import Iterator
from datetime import timedelta

import pytest
import time_machine

from frequenz.sdk.actor import _actor

# Used to impose a hard time limit for some async tasks in tests so that tests don't
# run forever in case of a bug
SAFETY_TIMEOUT = timedelta(seconds=10.0)


@contextlib.contextmanager
def actor_restart_limit(limit: int) -> Iterator[None]:
    """Temporarily set the actor restart limit to a given value.

    Example:
        ```python
        with actor_restart_limit(0):  # No restart
            async with MyActor() as actor:
                # Do something with actor
        ```

    Args:
        limit: The new limit.
    """
    # pylint: disable=protected-access
    original_limit = _actor.Actor._restart_limit
    print(
        f"<actor_restart_limit> Changing the restart limit from {original_limit} to {limit}"
    )
    _actor.Actor._restart_limit = limit
    try:
        yield
    finally:
        print(f"<actor_restart_limit> Resetting restart limit to {original_limit}")
        _actor.Actor._restart_limit = original_limit


@pytest.fixture(scope="session", autouse=True)
def disable_actor_auto_restart() -> Iterator[None]:
    """Disable auto-restart of actors while running tests."""
    with actor_restart_limit(0):
        yield


@pytest.fixture
def actor_auto_restart_once() -> Iterator[None]:
    """Make actors restart only once."""
    with actor_restart_limit(1):
        yield


@contextlib.contextmanager
def actor_restart_delay(delay: float | timedelta) -> Iterator[None]:
    """Temporarily set the actor restart delay to a given value.

    Example:
        ```python
        with actor_restart_delay(0.0):  # No delay
            async with MyActor() as actor:
                # Do something with actor
        ```

    Args:
        delay: The new delay.
    """
    if isinstance(delay, float):
        delay = timedelta(seconds=delay)
    original_delay = _actor.Actor.RESTART_DELAY
    print(
        f"<actor_restart_delay> Changing the `RESTART_DELAY` from "
        f"{original_delay} to {delay}"
    )

    _actor.Actor.RESTART_DELAY = delay
    try:
        yield
    finally:
        print(f"<actor_restart_limit> Resetting restart limit to {original_delay}")
        _actor.Actor.RESTART_DELAY = original_delay


@pytest.fixture(scope="session", autouse=True)
def disable_actor_restart_delay() -> Iterator[None]:
    """Disable auto-restart of actors while running tests."""
    with actor_restart_delay(0.0):
        yield


@pytest.fixture
def fake_time() -> Iterator[time_machine.Coordinates]:
    """Replace real time with a time machine that doesn't automatically tick."""
    with time_machine.travel(0, tick=False) as traveller:
        yield traveller



================================================
FILE: tests/test_import_all_integration.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests to verify that all modules can be imported successfully."""

import importlib
import os

import pytest


@pytest.mark.integration
def test_all_modules_importable() -> None:
    """Test that all modules can be imported.

    This test is mainly to ensure that there are no circular imports.
    """
    src_path = "src"

    for dir_path, _, files in os.walk(src_path):
        for file in files:
            if file.endswith(".py") and not file.startswith("__"):
                module_path = os.path.join(dir_path, file)
                module_name = os.path.splitext(module_path.replace(os.sep, "."))[0]

                try:
                    importlib.import_module(module_name)
                except ImportError as error:
                    assert False, f"Failed to import {module_name}: {error}"



================================================
FILE: tests/actor/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Tests for the actor package."""



================================================
FILE: tests/actor/test_actor.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Simple test for the BaseActor."""

import asyncio
from datetime import timedelta

import pytest
from frequenz.channels import Broadcast, Receiver, Sender, select, selected_from

from frequenz.sdk.actor import Actor, run

from ..conftest import actor_restart_limit


class MyBaseException(BaseException):
    """A base exception for testing purposes."""


class BaseTestActor(Actor):
    """A base actor for testing purposes."""

    restart_count: int = -1

    def inc_restart_count(self) -> None:
        """Increment the restart count."""
        BaseTestActor.restart_count += 1

    @classmethod
    def reset_restart_count(cls) -> None:
        """Reset the restart count."""
        cls.restart_count = -1


@pytest.fixture(autouse=True)
def reset_restart_count() -> None:
    """Reset the restart count before each test."""
    BaseTestActor.reset_restart_count()


class NopActor(BaseTestActor):
    """An actor that does nothing."""

    def __init__(self) -> None:
        """Create an instance."""
        super().__init__(name="test")

    async def _run(self) -> None:
        """Start the actor and crash upon receiving a message."""
        print(f"{self} started")
        self.inc_restart_count()
        print(f"{self} done")


class RaiseExceptionOnCancelActor(BaseTestActor):
    """Actor that raises exception during stop."""

    def __init__(
        self, *, name: str | None = None, recv: Receiver[int], sender: Sender[int]
    ) -> None:
        """Create an instance."""
        super().__init__(name=name)
        self._recv = recv
        self._sender = sender

    async def _run(self) -> None:
        """Start the actor and raise exception after receiving CancelledError."""
        self.inc_restart_count()
        if BaseTestActor.restart_count == 1:
            # This actor should not restart
            # If it does we just return to avoid infinite await on `stop`
            return
        try:
            async for msg in self._recv:
                await self._sender.send(msg)
        except asyncio.CancelledError as exc:
            raise RuntimeError("Actor should stop.") from exc


class RaiseExceptionActor(BaseTestActor):
    """A faulty actor that raises an Exception as soon as it receives a message."""

    def __init__(
        self,
        recv: Receiver[int],
    ) -> None:
        """Create an instance.

        Args:
            recv: A channel receiver for int data.
        """
        super().__init__(name="test")
        self._recv = recv

    async def _run(self) -> None:
        """Start the actor and crash upon receiving a message."""
        print(f"{self} started")
        self.inc_restart_count()
        async for msg in self._recv:
            print(f"{self} is about to crash")
            _ = msg / 0
        print(f"{self} done (should not happen)")


ACTOR_INFO = ("frequenz.sdk.actor._actor", 20)
ACTOR_ERROR = ("frequenz.sdk.actor._actor", 40)
RUN_INFO = ("frequenz.sdk.actor._run_utils", 20)
RUN_ERROR = ("frequenz.sdk.actor._run_utils", 40)


class EchoActor(BaseTestActor):
    """An echo actor that whatever it receives into the output channel."""

    def __init__(
        self,
        name: str,
        recv1: Receiver[bool],
        recv2: Receiver[bool],
        output: Sender[bool],
    ) -> None:
        """Create an `EchoActor` instance.

        Args:
            name: Name of the actor.
            recv1: A channel receiver for test boolean data.
            recv2: A channel receiver for test boolean data.
            output: A channel sender for output test boolean data.
        """
        super().__init__(name=name)
        self._recv1 = recv1
        self._recv2 = recv2
        self._output = output

    async def _run(self) -> None:
        """Do computations depending on the selected input message."""
        print(f"{self} started")
        self.inc_restart_count()

        channel_1 = self._recv1
        channel_2 = self._recv2

        async for selected in select(channel_1, channel_2):
            print(f"{self} received message {selected.message!r}")
            if selected_from(selected, channel_1):
                print(f"{self} sending message received from channel_1")
                await self._output.send(selected.message)
            elif selected_from(selected, channel_2):
                print(f"{self} sending message received from channel_2")
                await self._output.send(selected.message)

        print(f"{self} done (should not happen)")


async def test_basic_actor(caplog: pytest.LogCaptureFixture) -> None:
    """Initialize the TestActor send a message and wait for the response."""
    caplog.set_level("DEBUG", logger="frequenz.sdk.actor._actor")

    input_chan_1: Broadcast[bool] = Broadcast(name="TestChannel1")
    input_chan_2: Broadcast[bool] = Broadcast(name="TestChannel2")

    echo_chan: Broadcast[bool] = Broadcast(name="echo output")
    echo_rx = echo_chan.new_receiver()

    async with EchoActor(
        "EchoActor",
        input_chan_1.new_receiver(),
        input_chan_2.new_receiver(),
        echo_chan.new_sender(),
    ) as actor:
        assert actor.is_running is True
        original_tasks = set(actor.tasks)

        # Start is a no-op if already started
        actor.start()
        assert actor.is_running is True
        assert original_tasks == set(actor.tasks)

        await input_chan_1.new_sender().send(True)
        msg = await echo_rx.receive()
        assert msg is True

        await input_chan_2.new_sender().send(False)
        msg = await echo_rx.receive()
        assert msg is False

        assert actor.is_running is True

    assert actor.is_running is False
    assert BaseTestActor.restart_count == 0
    assert caplog.record_tuples == [
        (*ACTOR_INFO, "Actor EchoActor[EchoActor]: Started."),
        (*ACTOR_INFO, "Actor EchoActor[EchoActor]: Cancelled."),
    ]


@pytest.mark.parametrize("restart_limit", [0, 1, 2, 10])
async def test_restart_on_unhandled_exception(
    restart_limit: int, caplog: pytest.LogCaptureFixture
) -> None:
    """Create a faulty actor and expect it to restart because it raises an exception.

    Also test this works with different restart limits.

    Args:
        restart_limit: The restart limit to use.
        caplog: The log capture fixture.
    """
    relevant_loggers = {"frequenz.sdk.actor._actor", "frequenz.sdk.actor._run_utils"}
    for logger in relevant_loggers:
        caplog.set_level("DEBUG", logger=logger)

    channel: Broadcast[int] = Broadcast(name="channel")

    # NB: We're adding 1.0s to the timeout to account for the time it takes to
    # run, crash, and restart the actor.
    expected_wait_time = timedelta(
        seconds=restart_limit * RaiseExceptionActor.RESTART_DELAY.total_seconds() + 1.0
    )
    async with asyncio.timeout(expected_wait_time.total_seconds()):
        with actor_restart_limit(restart_limit):
            actor = RaiseExceptionActor(
                channel.new_receiver(),
            )
            for i in range(restart_limit + 1):
                await channel.new_sender().send(i)

            await run(actor)
            await actor.wait()

    assert actor.is_running is False
    assert BaseTestActor.restart_count == restart_limit
    expected_log = [
        (*RUN_INFO, "Starting 1 actor(s)..."),
        (*RUN_INFO, "Actor RaiseExceptionActor[test]: Starting..."),
        (*ACTOR_INFO, "Actor RaiseExceptionActor[test]: Started."),
    ]
    restart_delay = Actor.RESTART_DELAY.total_seconds()
    for i in range(restart_limit):
        expected_log.extend(
            [
                (
                    *ACTOR_ERROR,
                    "Actor RaiseExceptionActor[test]: Raised an unhandled exception.",
                ),
                (
                    *ACTOR_INFO,
                    f"Actor RaiseExceptionActor[test]: Restarting ({i}/{restart_limit})...",
                ),
                (
                    *ACTOR_INFO,
                    f"Actor RaiseExceptionActor[test]: Waiting {restart_delay} seconds...",
                ),
            ]
        )
    expected_log.extend(
        [
            (
                *ACTOR_ERROR,
                "Actor RaiseExceptionActor[test]: Raised an unhandled exception.",
            ),
            (
                *ACTOR_INFO,
                "Actor RaiseExceptionActor[test]: Maximum restarts attempted "
                f"({restart_limit}/{restart_limit}), bailing out...",
            ),
            (
                *RUN_ERROR,
                "Actor RaiseExceptionActor[test]: Raised an exception while running.",
            ),
            (*RUN_INFO, "All 1 actor(s) finished."),
        ]
    )
    print("caplog.record_tuples:", caplog.record_tuples)
    # This is an ugly hack. There seem to be some issues with asyncio and caplog, maybe
    # pytest-asyncio, that reports some pending tasks from an unrelated test when tested
    # inside QEMU (suggesting also some timing issue when things run very slow).
    filtered_logs = [r for r in caplog.record_tuples if r[0] in relevant_loggers]
    print("filtered_logs:", filtered_logs)
    print("expected_log:", expected_log)
    assert filtered_logs == expected_log


async def test_does_not_restart_on_normal_exit(
    actor_auto_restart_once: None,  # pylint: disable=unused-argument
    caplog: pytest.LogCaptureFixture,
) -> None:
    """Create an actor that exists normally and expect it to not be restarted."""
    caplog.set_level("DEBUG", logger="frequenz.sdk.actor._actor")
    caplog.set_level("DEBUG", logger="frequenz.sdk.actor._run_utils")

    channel: Broadcast[int] = Broadcast(name="channel")

    actor = NopActor()

    async with asyncio.timeout(1.0):
        await channel.new_sender().send(1)
        await run(actor)

    assert BaseTestActor.restart_count == 0
    assert caplog.record_tuples == [
        (*RUN_INFO, "Starting 1 actor(s)..."),
        (*RUN_INFO, "Actor NopActor[test]: Starting..."),
        (*ACTOR_INFO, "Actor NopActor[test]: Started."),
        (*ACTOR_INFO, "Actor NopActor[test]: _run() returned without error."),
        (*ACTOR_INFO, "Actor NopActor[test]: Stopped."),
        (*RUN_INFO, "Actor NopActor[test]: Finished normally."),
        (*RUN_INFO, "All 1 actor(s) finished."),
    ]


async def test_does_not_restart_if_cancelled(
    actor_auto_restart_once: None,  # pylint: disable=unused-argument
    caplog: pytest.LogCaptureFixture,
) -> None:
    """Create a faulty actor and expect it not to restart when cancelled."""
    caplog.set_level("DEBUG", logger="frequenz.sdk.actor._actor")
    caplog.set_level("DEBUG", logger="frequenz.sdk.actor._run_utils")

    input_chan_1: Broadcast[bool] = Broadcast(name="TestChannel1")
    input_chan_2: Broadcast[bool] = Broadcast(name="TestChannel2")

    echo_chan: Broadcast[bool] = Broadcast(name="echo output")
    echo_rx = echo_chan.new_receiver()

    actor = EchoActor(
        "EchoActor",
        input_chan_1.new_receiver(),
        input_chan_2.new_receiver(),
        echo_chan.new_sender(),
    )

    async def cancel_actor() -> None:
        """Cancel the actor after a short delay."""
        await input_chan_1.new_sender().send(True)
        msg = await echo_rx.receive()
        assert msg is True
        assert actor.is_running is True

        await input_chan_2.new_sender().send(False)
        msg = await echo_rx.receive()
        assert msg is False

        actor.cancel()

    async with asyncio.timeout(1.0):
        async with asyncio.TaskGroup() as group:
            group.create_task(cancel_actor(), name="cancel")
            await run(actor)

    assert actor.is_running is False
    assert BaseTestActor.restart_count == 0
    assert caplog.record_tuples == [
        (*RUN_INFO, "Starting 1 actor(s)..."),
        (*RUN_INFO, "Actor EchoActor[EchoActor]: Starting..."),
        (*ACTOR_INFO, "Actor EchoActor[EchoActor]: Started."),
        (*ACTOR_INFO, "Actor EchoActor[EchoActor]: Cancelled."),
        (*RUN_INFO, "Actor EchoActor[EchoActor]: Cancelled while running."),
        (*RUN_INFO, "All 1 actor(s) finished."),
    ]


async def test_actor_stop_if_error_was_raised_during_cancel(
    actor_auto_restart_once: None,  # pylint: disable=unused-argument
    caplog: pytest.LogCaptureFixture,
) -> None:
    """If actor raises exception during cancellation it should stop.

    And throw unhandled exception to the user..
    """
    caplog.set_level("DEBUG", logger="frequenz.sdk.actor._actor")
    caplog.set_level("DEBUG", logger="frequenz.sdk.actor._run_utils")

    input_chan: Broadcast[int] = Broadcast(name="TestChannel")

    echo_chan: Broadcast[int] = Broadcast(name="echo output")
    echo_rx = echo_chan.new_receiver()

    actor = RaiseExceptionOnCancelActor(
        name="test",
        recv=input_chan.new_receiver(),
        sender=echo_chan.new_sender(),
    )

    # Start actor and make sure it is running
    actor.start()
    await input_chan.new_sender().send(5)
    msg = await echo_rx.receive()
    assert msg == 5
    assert actor.is_running is True

    await actor.stop()
    assert actor.restart_count == 0

    assert caplog.record_tuples == [
        (*ACTOR_INFO, "Actor RaiseExceptionOnCancelActor[test]: Started."),
        (
            *ACTOR_ERROR,
            "Actor RaiseExceptionOnCancelActor[test]: Raised an unhandled exception during stop.",
        ),
        (*ACTOR_INFO, "Actor RaiseExceptionOnCancelActor[test]: Stopped."),
    ]



================================================
FILE: tests/actor/test_background_service.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Simple test for the BaseActor."""
import asyncio
from typing import Literal, assert_never

import async_solipsism
import pytest

from frequenz.sdk.actor import BackgroundService


@pytest.fixture(autouse=True)
def event_loop_policy() -> async_solipsism.EventLoopPolicy:
    """Return an event loop policy that uses the async solipsism event loop."""
    return async_solipsism.EventLoopPolicy()


class FakeService(BackgroundService):
    """A background service that does nothing."""

    def __init__(
        self,
        *,
        name: str | None = None,
        sleep: float | None = None,
        exc: BaseException | None = None,
    ) -> None:
        """Initialize a new FakeService."""
        super().__init__(name=name)
        self._sleep = sleep
        self._exc = exc

    def start(self) -> None:
        """Start this service."""

        async def nop() -> None:
            if self._sleep is not None:
                await asyncio.sleep(self._sleep)
            if self._exc is not None:
                raise self._exc

        self._tasks.add(asyncio.create_task(nop(), name="nop"))


async def test_construction_defaults() -> None:
    """Test the construction of a background service with default arguments."""
    fake_service = FakeService()
    assert fake_service.name == str(id(fake_service))
    assert fake_service.tasks == set()
    assert fake_service.is_running is False
    assert str(fake_service) == f"FakeService[{fake_service.name}]"
    assert repr(fake_service) == f"FakeService(name={fake_service.name!r}, tasks=set())"


async def test_construction_custom() -> None:
    """Test the construction of a background service with a custom name."""
    fake_service = FakeService(name="test")
    assert fake_service.name == "test"
    assert fake_service.tasks == set()
    assert fake_service.is_running is False


async def test_start_await() -> None:
    """Test a background service starts and can be awaited."""
    fake_service = FakeService(name="test")
    assert fake_service.name == "test"
    assert fake_service.is_running is False

    # Is a no-op if the service is not running
    await fake_service.stop()
    assert fake_service.is_running is False

    fake_service.start()
    assert fake_service.is_running is True

    # Should stop immediately
    async with asyncio.timeout(1.0):
        await fake_service

    assert fake_service.is_running is False


async def test_start_stop() -> None:
    """Test a background service starts and stops correctly."""
    fake_service = FakeService(name="test", sleep=2.0)
    assert fake_service.name == "test"
    assert fake_service.is_running is False

    # Is a no-op if the service is not running
    await fake_service.stop()
    assert fake_service.is_running is False

    fake_service.start()
    assert fake_service.is_running is True

    await asyncio.sleep(1.0)
    assert fake_service.is_running is True

    await fake_service.stop()
    assert fake_service.is_running is False

    await fake_service.stop()
    assert fake_service.is_running is False


@pytest.mark.parametrize("method", ["await", "wait", "stop"])
async def test_start_and_crash(
    method: Literal["await"] | Literal["wait"] | Literal["stop"],
) -> None:
    """Test a background service reports when crashing."""
    exc = RuntimeError("error")
    fake_service = FakeService(name="test", exc=exc)
    assert fake_service.name == "test"
    assert fake_service.is_running is False

    fake_service.start()
    with pytest.raises(BaseExceptionGroup) as exc_info:
        match method:
            case "await":
                await fake_service
            case "wait":
                await fake_service.wait()
            case "stop":
                # Give the service some time to run and crash, otherwise stop() will
                # cancel it before it has a chance to crash
                await asyncio.sleep(1.0)
                await fake_service.stop()
            case _:
                assert_never(method)

    rt_errors, rest_errors = exc_info.value.split(RuntimeError)
    assert rt_errors is not None
    assert rest_errors is None
    assert len(rt_errors.exceptions) == 1
    assert rt_errors.exceptions[0] is exc


async def test_async_context_manager() -> None:
    """Test a background service works as an async context manager."""
    async with FakeService(name="test", sleep=1.0) as fake_service:
        assert fake_service.is_running is True
        # Is a no-op if the service is running
        fake_service.start()
        await asyncio.sleep(0)
        assert fake_service.is_running is True

    assert fake_service.is_running is False



================================================
FILE: tests/actor/test_channel_registry.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Tests for the ChannelRegistry."""

import pytest
from frequenz.channels import ReceiverError, SenderError

from frequenz.sdk._internal._channels import ChannelRegistry


async def test_channel_registry() -> None:
    """Tests for ChannelRegistry, with string as key type."""
    reg = ChannelRegistry(name="test-registry")

    assert "20-hello" not in reg
    assert "21-hello" not in reg

    chan20 = reg.get_or_create(int, "20-hello")
    assert "20-hello" in reg
    assert reg.message_type("20-hello") == int

    with pytest.raises(ValueError):
        reg.get_or_create(str, "20-hello")

    sender20 = chan20.new_sender()
    receiver20 = chan20.new_receiver()

    assert "21-hello" not in reg

    chan21 = reg.get_or_create(int, "21-hello")
    assert "21-hello" in reg
    assert reg.message_type("21-hello") == int

    sender21 = chan21.new_sender()
    receiver21 = chan21.new_receiver()

    await sender20.send(30)
    await sender21.send(31)

    rcvd = await receiver21.receive()
    assert rcvd == 31

    rcvd = await receiver20.receive()
    assert rcvd == 30

    await reg.close_and_remove("20-hello")
    assert "20-hello" not in reg
    assert chan20._closed  # pylint: disable=protected-access
    with pytest.raises(SenderError):
        await sender20.send(30)
    with pytest.raises(ReceiverError):
        await receiver20.receive()
    with pytest.raises(KeyError):
        reg.message_type("20-hello")
    with pytest.raises(KeyError):
        await reg.close_and_remove("20-hello")

    await reg.close_and_remove("21-hello")
    assert "21-hello" not in reg
    assert chan21._closed  # pylint: disable=protected-access
    with pytest.raises(SenderError):
        await sender21.send(30)
    with pytest.raises(ReceiverError):
        await receiver21.receive()
    with pytest.raises(KeyError):
        reg.message_type("21-hello")
    with pytest.raises(KeyError):
        await reg.close_and_remove("21-hello")



================================================
FILE: tests/actor/test_resampling.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Frequenz Python SDK resampling example."""
import asyncio
import dataclasses
from datetime import datetime, timedelta, timezone

import async_solipsism
import pytest
import time_machine
from frequenz.channels import Broadcast
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentMetricId
from frequenz.quantities import Quantity

from frequenz.sdk._internal._channels import ChannelRegistry
from frequenz.sdk.microgrid._data_sourcing import ComponentMetricRequest
from frequenz.sdk.microgrid._resampling import ComponentMetricsResamplingActor
from frequenz.sdk.timeseries import ResamplerConfig2, Sample


@pytest.fixture(autouse=True)
def event_loop_policy() -> async_solipsism.EventLoopPolicy:
    """Return an event loop policy that uses the async solipsism event loop."""
    return async_solipsism.EventLoopPolicy()


def _now() -> datetime:
    return datetime.now(timezone.utc)


async def _assert_resampling_works(
    channel_registry: ChannelRegistry,
    fake_time: time_machine.Coordinates,
    *,
    resampling_chan_name: str,
    data_source_chan_name: str,
) -> None:
    timeseries_receiver = channel_registry.get_or_create(
        Sample[Quantity], resampling_chan_name
    ).new_receiver()
    timeseries_sender = channel_registry.get_or_create(
        Sample[Quantity], data_source_chan_name
    ).new_sender()

    fake_time.shift(0.2)
    new_sample = await timeseries_receiver.receive()  # At 0.2s (timer)
    assert new_sample == Sample(_now(), None)

    fake_time.shift(0.1)
    sample = Sample(_now(), Quantity(3))  # ts = 0.3s
    await timeseries_sender.send(sample)

    fake_time.shift(0.1)
    new_sample = await timeseries_receiver.receive()  # At 0.4s (timer)
    assert new_sample is not None and new_sample.value is not None
    assert new_sample.value.base_value == 3
    assert new_sample.timestamp >= sample.timestamp
    assert new_sample.timestamp == _now()

    fake_time.shift(0.05)
    sample = Sample(_now(), Quantity(4))  # ts = 0.45s
    await timeseries_sender.send(sample)
    fake_time.shift(0.15)
    new_sample = await timeseries_receiver.receive()  # At 0.6s (timer)
    assert new_sample is not None and new_sample.value is not None
    assert new_sample.value.base_value == 3.5  # avg(3, 4)
    assert new_sample.timestamp >= sample.timestamp
    assert new_sample.timestamp == _now()

    fake_time.shift(0.05)
    await timeseries_sender.send(Sample(_now(), Quantity(8)))  # ts = 0.65s
    fake_time.shift(0.05)
    await timeseries_sender.send(Sample(_now(), Quantity(1)))  # ts = 0.7s
    fake_time.shift(0.05)
    sample = Sample(_now(), Quantity(9))  # ts = 0.75s
    await timeseries_sender.send(sample)
    fake_time.shift(0.05)
    new_sample = await timeseries_receiver.receive()  # At 0.8s (timer)
    assert new_sample is not None and new_sample.value is not None
    assert new_sample.value.base_value == 5.5  # avg(4, 8, 1, 9)
    assert new_sample.timestamp >= sample.timestamp
    assert new_sample.timestamp == _now()

    # No more samples sent
    fake_time.shift(0.2)
    new_sample = await timeseries_receiver.receive()  # At 1.0s (timer)
    assert new_sample is not None and new_sample.value is not None
    assert new_sample.value.base_value == 6  # avg(8, 1, 9)
    assert new_sample.timestamp >= sample.timestamp
    assert new_sample.timestamp == _now()

    # No more samples sent
    fake_time.shift(0.2)
    new_sample = await timeseries_receiver.receive()  # At 1.2s (timer)
    assert new_sample is not None
    assert new_sample.value is None
    assert new_sample.timestamp == _now()


async def test_single_request(
    fake_time: time_machine.Coordinates,
) -> None:
    """Run main functions that initializes and creates everything."""
    channel_registry = ChannelRegistry(name="test")
    data_source_req_chan = Broadcast[ComponentMetricRequest](name="data-source-req")
    data_source_req_recv = data_source_req_chan.new_receiver()
    resampling_req_chan = Broadcast[ComponentMetricRequest](name="resample-req")
    resampling_req_sender = resampling_req_chan.new_sender()

    async with ComponentMetricsResamplingActor(
        channel_registry=channel_registry,
        data_sourcing_request_sender=data_source_req_chan.new_sender(),
        resampling_request_receiver=resampling_req_chan.new_receiver(),
        config=ResamplerConfig2(
            resampling_period=timedelta(seconds=0.2),
            max_data_age_in_periods=2,
        ),
    ) as resampling_actor:
        subs_req = ComponentMetricRequest(
            namespace="Resampling",
            component_id=ComponentId(9),
            metric_id=ComponentMetricId.SOC,
            start_time=None,
        )

        await resampling_req_sender.send(subs_req)
        data_source_req = await data_source_req_recv.receive()
        assert data_source_req is not None
        assert data_source_req == dataclasses.replace(
            subs_req, namespace="Resampling:Source"
        )

        await _assert_resampling_works(
            channel_registry,
            fake_time,
            resampling_chan_name=subs_req.get_channel_name(),
            data_source_chan_name=data_source_req.get_channel_name(),
        )

        await resampling_actor._resampler.stop()  # pylint: disable=protected-access


async def test_duplicate_request(
    fake_time: time_machine.Coordinates,
) -> None:
    """Run main functions that initializes and creates everything."""
    channel_registry = ChannelRegistry(name="test")
    data_source_req_chan = Broadcast[ComponentMetricRequest](name="data-source-req")
    data_source_req_recv = data_source_req_chan.new_receiver()
    resampling_req_chan = Broadcast[ComponentMetricRequest](name="resample-req")
    resampling_req_sender = resampling_req_chan.new_sender()

    async with ComponentMetricsResamplingActor(
        channel_registry=channel_registry,
        data_sourcing_request_sender=data_source_req_chan.new_sender(),
        resampling_request_receiver=resampling_req_chan.new_receiver(),
        config=ResamplerConfig2(
            resampling_period=timedelta(seconds=0.2),
            max_data_age_in_periods=2,
        ),
    ) as resampling_actor:
        subs_req = ComponentMetricRequest(
            namespace="Resampling",
            component_id=ComponentId(9),
            metric_id=ComponentMetricId.SOC,
            start_time=None,
        )

        await resampling_req_sender.send(subs_req)
        data_source_req = await data_source_req_recv.receive()

        # Send duplicate request
        await resampling_req_sender.send(subs_req)
        with pytest.raises(asyncio.TimeoutError):
            await asyncio.wait_for(data_source_req_recv.receive(), timeout=0.1)

        await _assert_resampling_works(
            channel_registry,
            fake_time,
            resampling_chan_name=subs_req.get_channel_name(),
            data_source_chan_name=data_source_req.get_channel_name(),
        )

        await resampling_actor._resampler.stop()  # pylint: disable=protected-access



================================================
FILE: tests/actor/test_run_utils.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Simple tests for the actor runner."""

import asyncio
import time

import async_solipsism
import pytest
import time_machine

from frequenz.sdk.actor import Actor, run


@pytest.fixture(autouse=True)
def event_loop_policy() -> async_solipsism.EventLoopPolicy:
    """Return an event loop policy that uses the async solipsism event loop."""
    return async_solipsism.EventLoopPolicy()


class FaultyActor(Actor):
    """A test faulty actor."""

    def __init__(self, name: str) -> None:
        """Initialize the faulty actor.

        Args:
            name: the name of the faulty actor.
        """
        super().__init__(name=name)
        self.is_cancelled = False

    async def _run(self) -> None:
        """Run the faulty actor.

        Raises:
            asyncio.CancelledError: the exception causes the actor to be cancelled
        """
        self.is_cancelled = True
        raise asyncio.CancelledError(f"Faulty Actor {self.name} failed")


class SleepyActor(Actor):
    """A test actor that sleeps a short time."""

    def __init__(self, name: str, sleep_duration: float) -> None:
        """Initialize the sleepy actor.

        Args:
            name: the name of the sleepy actor.
            sleep_duration: the virtual duration to sleep while running.
        """
        super().__init__(name=name)
        self.sleep_duration = sleep_duration
        self.is_joined = False

    async def _run(self) -> None:
        """Run the sleepy actor."""
        while time.time() < self.sleep_duration:
            await asyncio.sleep(0.1)

        self.is_joined = True


# pylint: disable=redefined-outer-name
async def test_all_actors_done(fake_time: time_machine.Coordinates) -> None:
    """Test the completion of all actors."""
    sleepy_actor_1 = SleepyActor("sleepy_actor_1", sleep_duration=1.0)
    sleepy_actor_2 = SleepyActor("sleepy_actor_2", sleep_duration=2.0)

    test_task = asyncio.create_task(run(sleepy_actor_1, sleepy_actor_2))

    sleep_duration = time.time()

    assert sleep_duration == 0
    assert sleepy_actor_1.is_joined is False
    assert sleepy_actor_2.is_joined is False

    while not test_task.done():
        if sleep_duration < 1:
            assert sleepy_actor_1.is_joined is False
            assert sleepy_actor_2.is_joined is False
        elif sleep_duration < 2:
            assert sleepy_actor_1.is_joined is True
            assert sleepy_actor_2.is_joined is False
        elif sleep_duration == 2:
            assert sleepy_actor_1.is_joined is True
            assert sleepy_actor_2.is_joined is True

        fake_time.shift(0.5)
        sleep_duration = time.time()
        await asyncio.sleep(1)

    assert sleepy_actor_1.is_joined
    assert sleepy_actor_2.is_joined


async def test_actors_cancelled() -> None:
    """Test the completion of actors being cancelled."""
    faulty_actors = [FaultyActor(f"faulty_actor_{idx}") for idx in range(5)]

    await asyncio.wait_for(run(*faulty_actors), timeout=1.0)

    for faulty_actor in faulty_actors:
        assert faulty_actor.is_cancelled



================================================
FILE: tests/actor/_power_managing/__init__.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH



================================================
FILE: tests/actor/_power_managing/_sorted_set.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests for the sorted set."""

from frequenz.sdk.microgrid._power_managing import _sorted_set


def test_sorted_set() -> None:
    """Test the LLRB class."""
    tree = _sorted_set.SortedSet[str]()
    tree.insert("a")
    tree.insert("e")
    tree.insert("b")
    tree.insert("d")
    tree.insert("c")

    assert list(tree) == ["a", "b", "c", "d", "e"]
    assert tree.search("c") == "c"
    assert tree.search("f") is None
    tree.delete("c")
    assert list(tree) == ["a", "b", "d", "e"]
    assert tree.search("c") is None

    tree.insert("f")

    tree.delete("a")
    assert list(tree) == ["b", "d", "e", "f"]
    assert list(reversed(tree)) == ["f", "e", "d", "b"]
    tree.insert("c")
    tree.insert("c")

    tree.delete("f")
    assert list(tree) == ["b", "c", "d", "e"]

    tree.insert("q")

    assert list(tree) == ["b", "c", "d", "e", "q"]
    assert list(reversed(tree)) == ["q", "e", "d", "c", "b"]



================================================
FILE: tests/actor/_power_managing/test_matryoshka.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests for the Matryoshka power manager algorithm."""

import asyncio
import re
from datetime import datetime, timedelta, timezone

import pytest
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.quantities import Power

from frequenz.sdk import timeseries
from frequenz.sdk.microgrid._power_managing import Proposal
from frequenz.sdk.microgrid._power_managing._base_classes import DefaultPower
from frequenz.sdk.microgrid._power_managing._matryoshka import Matryoshka
from frequenz.sdk.timeseries import _base_types


class StatefulTester:
    """A stateful tester for the Matryoshka algorithm."""

    def __init__(
        self,
        batteries: frozenset[ComponentId],
        system_bounds: _base_types.SystemBounds,
    ) -> None:
        """Create a new instance of the stateful tester."""
        self._call_count = 0
        self._batteries = batteries
        self._system_bounds = system_bounds
        self.algorithm = Matryoshka(
            max_proposal_age=timedelta(seconds=60.0), default_power=DefaultPower.ZERO
        )

    def tgt_power(  # pylint: disable=too-many-arguments,too-many-positional-arguments
        self,
        priority: int,
        power: float | None,
        bounds: tuple[float | None, float | None],
        expected: float | None,
        creation_time: float | None = None,
        batteries: frozenset[ComponentId] | None = None,
    ) -> None:
        """Test the target power calculation."""
        self._call_count += 1
        tgt_power = self.algorithm.calculate_target_power(
            self._batteries if batteries is None else batteries,
            Proposal(
                component_ids=self._batteries if batteries is None else batteries,
                source_id=f"actor-{priority}",
                preferred_power=None if power is None else Power.from_watts(power),
                bounds=timeseries.Bounds(
                    None if bounds[0] is None else Power.from_watts(bounds[0]),
                    None if bounds[1] is None else Power.from_watts(bounds[1]),
                ),
                priority=priority,
                creation_time=(
                    creation_time
                    if creation_time is not None
                    else asyncio.get_event_loop().time()
                ),
            ),
            self._system_bounds,
        )
        assert tgt_power == (
            Power.from_watts(expected) if expected is not None else None
        )

    def bounds(
        self,
        priority: int,
        expected_power: float | None,
        expected_bounds: tuple[float, float],
    ) -> None:
        """Test the status report."""
        report = self.algorithm.get_status(
            self._batteries, priority, self._system_bounds
        )
        if expected_power is None:
            assert report.target_power is None
        else:
            assert report.target_power is not None
            assert report.target_power.as_watts() == expected_power
        # pylint: disable=protected-access
        assert report._inclusion_bounds is not None
        assert report._inclusion_bounds.lower.as_watts() == expected_bounds[0]
        assert report._inclusion_bounds.upper.as_watts() == expected_bounds[1]
        # pylint: enable=protected-access


async def test_matryoshka_no_excl() -> None:  # pylint: disable=too-many-statements
    """Tests for the power managing actor.

    With just inclusion bounds, and no exclusion bounds.
    """
    batteries = frozenset({ComponentId(2), ComponentId(5)})

    system_bounds = _base_types.SystemBounds(
        timestamp=datetime.now(tz=timezone.utc),
        inclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-200.0), upper=Power.from_watts(200.0)
        ),
        exclusion_bounds=timeseries.Bounds(lower=Power.zero(), upper=Power.zero()),
    )

    tester = StatefulTester(batteries, system_bounds)

    tester.tgt_power(priority=2, power=25.0, bounds=(25.0, 50.0), expected=25.0)
    tester.bounds(priority=2, expected_power=25.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=25.0, expected_bounds=(25.0, 50.0))

    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=25.0)
    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=25.0)
    tester.bounds(priority=1, expected_power=25.0, expected_bounds=(25.0, 50.0))

    tester.tgt_power(priority=3, power=10.0, bounds=(10.0, 15.0), expected=15.0)
    tester.bounds(priority=3, expected_power=15.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=15.0, expected_bounds=(10.0, 15.0))
    tester.bounds(priority=1, expected_power=15.0, expected_bounds=(10.0, 15.0))

    tester.tgt_power(priority=3, power=10.0, bounds=(10.0, 22.0), expected=22.0)
    tester.bounds(priority=3, expected_power=22.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=22.0, expected_bounds=(10.0, 22.0))
    tester.bounds(priority=1, expected_power=22.0, expected_bounds=(10.0, 22.0))

    tester.tgt_power(priority=1, power=30.0, bounds=(20.0, 50.0), expected=22.0)
    tester.bounds(priority=1, expected_power=22.0, expected_bounds=(10.0, 22.0))

    tester.tgt_power(priority=3, power=10.0, bounds=(10.0, 50.0), expected=30.0)
    tester.bounds(priority=3, expected_power=30.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=30.0, expected_bounds=(10.0, 50.0))
    tester.bounds(priority=1, expected_power=30.0, expected_bounds=(25.0, 50.0))

    tester.tgt_power(priority=2, power=40.0, bounds=(40.0, None), expected=40.0)
    tester.bounds(priority=3, expected_power=40.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=40.0, expected_bounds=(10.0, 50.0))
    tester.bounds(priority=1, expected_power=40.0, expected_bounds=(40.0, 50.0))

    tester.tgt_power(priority=2, power=0.0, bounds=(None, None), expected=30.0)
    tester.bounds(priority=4, expected_power=30.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=3, expected_power=30.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=30.0, expected_bounds=(10.0, 50.0))
    tester.bounds(priority=1, expected_power=30.0, expected_bounds=(10.0, 50.0))

    tester.tgt_power(priority=4, power=-50.0, bounds=(None, -50.0), expected=-50.0)
    tester.bounds(priority=4, expected_power=-50.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=3, expected_power=-50.0, expected_bounds=(-200.0, -50.0))
    tester.bounds(priority=2, expected_power=-50.0, expected_bounds=(-200.0, -50.0))
    tester.bounds(priority=1, expected_power=-50.0, expected_bounds=(-200.0, -50.0))

    tester.tgt_power(priority=3, power=-0.0, bounds=(-200.0, 200.0), expected=-50.0)
    tester.bounds(priority=1, expected_power=-50.0, expected_bounds=(-200.0, -50.0))

    tester.tgt_power(priority=1, power=-150.0, bounds=(-200.0, -150.0), expected=-150.0)
    tester.bounds(priority=2, expected_power=-150.0, expected_bounds=(-200.0, -50.0))
    tester.bounds(priority=1, expected_power=-150.0, expected_bounds=(-200.0, -50.0))

    tester.tgt_power(priority=4, power=-180.0, bounds=(-200.0, -50.0), expected=-150.0)
    tester.bounds(priority=1, expected_power=-150.0, expected_bounds=(-200.0, -50.0))

    tester.tgt_power(priority=4, power=50.0, bounds=(50.0, None), expected=50.0)
    tester.bounds(priority=4, expected_power=50.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=3, expected_power=50.0, expected_bounds=(50.0, 200.0))
    tester.bounds(priority=2, expected_power=50.0, expected_bounds=(50.0, 200.0))
    tester.bounds(priority=1, expected_power=50.0, expected_bounds=(50.0, 200.0))

    tester.tgt_power(priority=4, power=0.0, bounds=(-200.0, 200.0), expected=-150.0)
    tester.bounds(priority=4, expected_power=-150.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=3, expected_power=-150.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=-150.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=-150.0, expected_bounds=(-200.0, 200.0))

    tester.tgt_power(priority=3, power=0.0, bounds=(-200.0, 200.0), expected=-150.0)
    tester.bounds(priority=3, expected_power=-150.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=-150.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=-150.0, expected_bounds=(-200.0, 200.0))

    tester.tgt_power(priority=2, power=50.0, bounds=(-100, 100), expected=-100.0)
    tester.bounds(priority=3, expected_power=-100.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=-100.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=-100.0, expected_bounds=(-100.0, 100.0))

    tester.tgt_power(priority=1, power=100.0, bounds=(100, 200), expected=100.0)
    tester.bounds(priority=1, expected_power=100.0, expected_bounds=(-100.0, 100.0))

    tester.tgt_power(priority=1, power=50.0, bounds=(50, 200), expected=50.0)
    tester.bounds(priority=1, expected_power=50.0, expected_bounds=(-100.0, 100.0))

    tester.tgt_power(priority=1, power=200.0, bounds=(50, 200), expected=100.0)
    tester.bounds(priority=1, expected_power=100.0, expected_bounds=(-100.0, 100.0))

    tester.tgt_power(priority=1, power=0.0, bounds=(-200, 200), expected=0.0)
    tester.bounds(priority=1, expected_power=0.0, expected_bounds=(-100.0, 100.0))

    tester.tgt_power(priority=1, power=None, bounds=(-200, 200), expected=50.0)
    tester.bounds(priority=1, expected_power=50.0, expected_bounds=(-100.0, 100.0))


async def test_matryoshka_with_excl_1() -> None:
    """Tests for the power managing actor.

    With inclusion bounds, and exclusion bounds -30.0 to 0.0.
    """
    batteries = frozenset({ComponentId(2), ComponentId(5)})

    system_bounds = _base_types.SystemBounds(
        timestamp=datetime.now(tz=timezone.utc),
        inclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-200.0), upper=Power.from_watts(200.0)
        ),
        exclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-30.0), upper=Power.zero()
        ),
    )

    tester = StatefulTester(batteries, system_bounds)

    tester.tgt_power(priority=2, power=25.0, bounds=(25.0, 50.0), expected=25.0)
    tester.bounds(priority=2, expected_power=25.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=25.0, expected_bounds=(25.0, 50.0))

    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=25.0)
    tester.bounds(priority=1, expected_power=25.0, expected_bounds=(25.0, 50.0))

    tester.tgt_power(priority=2, power=-10.0, bounds=(-10.0, 50.0), expected=20.0)
    tester.bounds(priority=1, expected_power=20.0, expected_bounds=(0.0, 50.0))
    tester.bounds(priority=0, expected_power=20.0, expected_bounds=(20.0, 50.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, 50.0), expected=0.0)
    tester.bounds(priority=0, expected_power=0.0, expected_bounds=(0.0, 50.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, 20.0), expected=0.0)
    tester.bounds(priority=0, expected_power=0.0, expected_bounds=(0.0, 20.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, -5.0), expected=0.0)
    tester.bounds(priority=0, expected_power=0.0, expected_bounds=(0.0, 50.0))

    tester.tgt_power(priority=2, power=-10.0, bounds=(-200.0, -5.0), expected=-30.0)
    tester.bounds(priority=1, expected_power=-30.0, expected_bounds=(-200.0, -30.0))
    tester.bounds(priority=0, expected_power=-30.0, expected_bounds=(-200.0, -30.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-100.0, -5.0), expected=-30.0)
    tester.bounds(priority=0, expected_power=-30.0, expected_bounds=(-100.0, -30.0))

    tester.tgt_power(priority=1, power=-40.0, bounds=(-100.0, -35.0), expected=-40.0)
    tester.bounds(priority=0, expected_power=-40.0, expected_bounds=(-100.0, -35.0))


async def test_matryoshka_with_excl_2() -> None:
    """Tests for the power managing actor.

    With inclusion bounds, and exclusion bounds 0.0 to 30.0.
    """
    batteries = frozenset({ComponentId(2), ComponentId(5)})

    system_bounds = _base_types.SystemBounds(
        timestamp=datetime.now(tz=timezone.utc),
        inclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-200.0), upper=Power.from_watts(200.0)
        ),
        exclusion_bounds=timeseries.Bounds(
            lower=Power.zero(), upper=Power.from_watts(30.0)
        ),
    )

    tester = StatefulTester(batteries, system_bounds)

    tester.tgt_power(priority=2, power=25.0, bounds=(25.0, 50.0), expected=30.0)
    tester.bounds(priority=2, expected_power=30.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=30.0, expected_bounds=(30.0, 50.0))

    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=30.0)
    tester.bounds(priority=1, expected_power=30.0, expected_bounds=(30.0, 50.0))

    tester.tgt_power(priority=1, power=10.0, bounds=(5.0, 10.0), expected=30.0)
    tester.bounds(priority=0, expected_power=30.0, expected_bounds=(30, 50.0))

    tester.tgt_power(priority=2, power=-10.0, bounds=(-10.0, 50.0), expected=0.0)
    tester.bounds(priority=1, expected_power=0.0, expected_bounds=(-10.0, 50.0))
    tester.bounds(priority=0, expected_power=0.0, expected_bounds=(-10.0, 50.0))

    tester.tgt_power(priority=0, power=40, bounds=(None, None), expected=40.0)
    tester.tgt_power(priority=0, power=-10, bounds=(None, None), expected=-10.0)
    tester.tgt_power(priority=0, power=10, bounds=(None, None), expected=0.0)
    tester.tgt_power(priority=0, power=20, bounds=(None, None), expected=30.0)
    tester.tgt_power(priority=0, power=None, bounds=(None, None), expected=0.0)

    tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, 50.0), expected=-10.0)
    tester.bounds(priority=0, expected_power=-10.0, expected_bounds=(-10.0, 50.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, 20.0), expected=-10.0)
    tester.bounds(priority=0, expected_power=-10.0, expected_bounds=(-10.0, 0.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, -5.0), expected=-10.0)
    tester.bounds(priority=0, expected_power=-10.0, expected_bounds=(-10.0, -5.0))

    tester.tgt_power(priority=2, power=-10.0, bounds=(-200.0, -5.0), expected=-10.0)
    tester.bounds(priority=1, expected_power=-10.0, expected_bounds=(-200.0, -5.0))
    tester.bounds(priority=0, expected_power=-10.0, expected_bounds=(-10.0, -5.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-100.0, -5.0), expected=-10.0)
    tester.bounds(priority=0, expected_power=-10.0, expected_bounds=(-100.0, -5.0))

    tester.tgt_power(priority=1, power=-40.0, bounds=(-100.0, -35.0), expected=-40.0)
    tester.bounds(priority=0, expected_power=-40.0, expected_bounds=(-100.0, -35.0))


async def test_matryoshka_with_excl_3() -> None:
    """Tests for the power managing actor.

    With inclusion bounds, and exclusion bounds -30.0 to 30.0.
    """
    batteries = frozenset({ComponentId(2), ComponentId(5)})

    system_bounds = _base_types.SystemBounds(
        timestamp=datetime.now(tz=timezone.utc),
        inclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-200.0), upper=Power.from_watts(200.0)
        ),
        exclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-30.0), upper=Power.from_watts(30.0)
        ),
    )

    tester = StatefulTester(batteries, system_bounds)
    tester.tgt_power(priority=2, power=10.0, bounds=(None, None), expected=30.0)
    tester.tgt_power(priority=2, power=-10.0, bounds=(None, None), expected=-30.0)
    tester.tgt_power(priority=2, power=0.0, bounds=(None, None), expected=0.0)
    tester.tgt_power(priority=3, power=20.0, bounds=(None, None), expected=0.0)
    tester.tgt_power(priority=1, power=-20.0, bounds=(None, None), expected=-30.0)
    tester.tgt_power(priority=3, power=None, bounds=(None, None), expected=-30.0)
    tester.tgt_power(priority=1, power=None, bounds=(None, None), expected=0.0)

    tester.tgt_power(priority=2, power=25.0, bounds=(25.0, 50.0), expected=30.0)
    tester.bounds(priority=2, expected_power=30.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=30.0, expected_bounds=(30.0, 50.0))

    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=30.0)
    tester.bounds(priority=1, expected_power=30.0, expected_bounds=(30.0, 50.0))

    tester.tgt_power(priority=1, power=10.0, bounds=(5.0, 10.0), expected=30.0)
    tester.bounds(priority=0, expected_power=30.0, expected_bounds=(30, 50.0))

    tester.tgt_power(priority=2, power=-10.0, bounds=(-10.0, 50.0), expected=30.0)
    tester.bounds(priority=1, expected_power=30.0, expected_bounds=(30.0, 50.0))
    tester.bounds(priority=0, expected_power=30.0, expected_bounds=(30.0, 50.0))

    tester.tgt_power(priority=1, power=40.0, bounds=(-10.0, 50.0), expected=40.0)
    tester.bounds(priority=0, expected_power=40.0, expected_bounds=(30.0, 50.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, 20.0), expected=30.0)
    tester.bounds(priority=0, expected_power=30.0, expected_bounds=(30.0, 50.0))

    tester.tgt_power(priority=2, power=-10.0, bounds=(-200.0, -5.0), expected=-30.0)
    tester.bounds(priority=1, expected_power=-30.0, expected_bounds=(-200.0, -30.0))
    tester.bounds(priority=0, expected_power=-30.0, expected_bounds=(-200.0, -30.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-100.0, -5.0), expected=-30.0)
    tester.bounds(priority=0, expected_power=-30.0, expected_bounds=(-100.0, -30.0))

    tester.tgt_power(priority=1, power=-40.0, bounds=(-100.0, -35.0), expected=-40.0)
    tester.bounds(priority=0, expected_power=-40.0, expected_bounds=(-100.0, -35.0))


async def test_matryoshka_drop_old_proposals() -> None:
    """Tests for the power managing actor.

    With inclusion bounds, and exclusion bounds -30.0 to 30.0.
    """
    batteries = frozenset({ComponentId(2), ComponentId(5)})
    overlapping_batteries = frozenset({ComponentId(5), ComponentId(8)})

    system_bounds = _base_types.SystemBounds(
        timestamp=datetime.now(tz=timezone.utc),
        inclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-200.0), upper=Power.from_watts(200.0)
        ),
        exclusion_bounds=timeseries.Bounds(lower=Power.zero(), upper=Power.zero()),
    )

    tester = StatefulTester(batteries, system_bounds)

    now = asyncio.get_event_loop().time()

    tester.tgt_power(priority=3, power=22.0, bounds=(22.0, 30.0), expected=22.0)

    # When a proposal is too old and hasn't been updated, it is dropped.
    tester.tgt_power(
        priority=2,
        power=25.0,
        bounds=(25.0, 50.0),
        creation_time=now - 70.0,
        expected=25.0,
    )

    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=25.0)
    tester.algorithm.drop_old_proposals(now)
    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=22.0)

    # When overwritten by a newer proposal, that proposal is not dropped.
    tester.tgt_power(
        priority=2,
        power=25.0,
        bounds=(25.0, 50.0),
        creation_time=now - 70.0,
        expected=25.0,
    )
    tester.tgt_power(
        priority=2,
        power=25.0,
        bounds=(25.0, 50.0),
        creation_time=now - 30.0,
        expected=25.0,
    )

    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=25.0)
    tester.algorithm.drop_old_proposals(now)
    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=25.0)

    # When all proposals are too old, they are dropped, and the buckets are dropped as
    # well.  After that, sending a request for a different but overlapping bucket will
    # succeed.  And it will fail until then.
    with pytest.raises(
        NotImplementedError,
        match=re.escape(
            "PowerManagingActor: CID5, CID8 are already part of another bucket. "
            "Overlapping buckets are not yet supported."
        ),
    ):
        tester.tgt_power(
            priority=1,
            power=25.0,
            bounds=(25.0, 50.0),
            expected=25.0,
            batteries=overlapping_batteries,
        )

    tester.tgt_power(
        priority=1,
        power=25.0,
        bounds=(25.0, 50.0),
        creation_time=now - 70.0,
        expected=25.0,
    )
    tester.tgt_power(
        priority=2,
        power=25.0,
        bounds=(25.0, 50.0),
        creation_time=now - 70.0,
        expected=25.0,
    )
    tester.tgt_power(
        priority=3,
        power=25.0,
        bounds=(25.0, 50.0),
        creation_time=now - 70.0,
        expected=25.0,
    )

    tester.algorithm.drop_old_proposals(now)

    tester.tgt_power(
        priority=1,
        power=25.0,
        bounds=(25.0, 50.0),
        expected=25.0,
        batteries=overlapping_batteries,
    )


async def test_matryoshka_none_proposals() -> None:
    """Tests for the power managing actor.

    When a `None` proposal is received, is source id should be dropped from the bucket.
    Then if the bucket becomes empty, it should be dropped as well.
    """
    batteries = frozenset({ComponentId(2), ComponentId(5)})
    overlapping_batteries = frozenset({ComponentId(5), ComponentId(8)})

    system_bounds = _base_types.SystemBounds(
        timestamp=datetime.now(tz=timezone.utc),
        inclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-200.0), upper=Power.from_watts(200.0)
        ),
        exclusion_bounds=timeseries.Bounds(lower=Power.zero(), upper=Power.zero()),
    )

    def ensure_overlapping_bucket_request_fails() -> None:
        with pytest.raises(
            NotImplementedError,
            match=re.escape(
                "PowerManagingActor: CID5, CID8 are already part of another bucket. "
                "Overlapping buckets are not yet supported."
            ),
        ):
            tester.tgt_power(
                priority=1,
                power=None,
                bounds=(20.0, 50.0),
                expected=None,
                batteries=overlapping_batteries,
            )

    tester = StatefulTester(batteries, system_bounds)

    tester.tgt_power(priority=3, power=22.0, bounds=(22.0, 30.0), expected=22.0)
    tester.tgt_power(priority=2, power=25.0, bounds=(25.0, 50.0), expected=25.0)
    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=25.0)

    ensure_overlapping_bucket_request_fails()
    tester.tgt_power(priority=1, power=None, bounds=(None, None), expected=25.0)
    ensure_overlapping_bucket_request_fails()
    tester.tgt_power(priority=3, power=None, bounds=(None, None), expected=25.0)
    ensure_overlapping_bucket_request_fails()
    tester.tgt_power(priority=2, power=None, bounds=(None, None), expected=0.0)

    # Overlapping battery bucket is dropped.
    tester.tgt_power(
        priority=1,
        power=20.0,
        bounds=(20.0, 50.0),
        expected=20.0,
        batteries=overlapping_batteries,
    )



================================================
FILE: tests/actor/_power_managing/test_report.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests for methods provided by the PowerManager's reports."""

from frequenz.quantities import Power

from frequenz.sdk.microgrid._power_managing import _Report
from frequenz.sdk.timeseries import Bounds


class BoundsTester:
    """Test the bounds adjustment."""

    def __init__(
        self,
        inclusion_bounds: tuple[float, float] | None,
        exclusion_bounds: tuple[float, float] | None,
    ) -> None:
        """Initialize the tester."""
        self._report = _Report(
            target_power=None,
            _inclusion_bounds=(
                Bounds(  # pylint: disable=protected-access
                    Power.from_watts(inclusion_bounds[0]),
                    Power.from_watts(inclusion_bounds[1]),
                )
                if inclusion_bounds is not None
                else None
            ),
            _exclusion_bounds=(
                Bounds(  # pylint: disable=protected-access
                    Power.from_watts(exclusion_bounds[0]),
                    Power.from_watts(exclusion_bounds[1]),
                )
                if exclusion_bounds is not None
                else None
            ),
        )

    def case(
        self,
        desired_power: float,
        expected_lower: float | None,
        expected_upper: float | None,
    ) -> None:
        """Test a case."""
        lower, upper = self._report.adjust_to_bounds(Power.from_watts(desired_power))

        tgt_lower = (
            Power.from_watts(expected_lower) if expected_lower is not None else None
        )
        assert lower == tgt_lower
        tgt_upper = (
            Power.from_watts(expected_upper) if expected_upper is not None else None
        )
        assert upper == tgt_upper


def test_adjust_to_bounds() -> None:
    """Test that desired powers are adjusted to bounds correctly."""
    tester = BoundsTester(
        inclusion_bounds=(-200.0, 200.0),
        exclusion_bounds=(-30.0, 30.0),
    )
    tester.case(0.0, 0.0, 0.0)
    tester.case(-210.0, -200.0, None)
    tester.case(220.0, None, 200.0)
    tester.case(-20.0, -30.0, 30.0)
    tester.case(-30.0, -30.0, -30.0)
    tester.case(30.0, 30.0, 30.0)
    tester.case(50.0, 50.0, 50.0)
    tester.case(-200.0, -200.0, -200.0)
    tester.case(200.0, 200.0, 200.0)

    tester = BoundsTester(
        inclusion_bounds=(-200.0, 200.0),
        exclusion_bounds=(-210.0, 0.0),
    )
    tester.case(0.0, 0.0, 0.0)
    tester.case(-210.0, None, 0.0)
    tester.case(220.0, None, 200.0)
    tester.case(-20.0, None, 0.0)
    tester.case(20.0, 20.0, 20.0)

    tester = BoundsTester(
        inclusion_bounds=(-200.0, 200.0),
        exclusion_bounds=(0.0, 210.0),
    )
    tester.case(0.0, 0.0, 0.0)
    tester.case(-210.0, -200.0, None)
    tester.case(220.0, 0.0, None)
    tester.case(-20.0, -20.0, -20.0)
    tester.case(20.0, 0.0, None)

    tester = BoundsTester(
        inclusion_bounds=(-200.0, 200.0),
        exclusion_bounds=None,
    )
    tester.case(0.0, 0.0, 0.0)
    tester.case(-210.0, -200.0, None)
    tester.case(220.0, None, 200.0)
    tester.case(-20.0, -20.0, -20.0)
    tester.case(20.0, 20.0, 20.0)

    tester = BoundsTester(
        inclusion_bounds=(-200.0, 200.0),
        exclusion_bounds=(-210.0, 210.0),
    )
    tester.case(0.0, None, None)
    tester.case(-210.0, None, None)
    tester.case(220.0, None, None)
    tester.case(-20.0, None, None)
    tester.case(20.0, None, None)

    tester = BoundsTester(
        inclusion_bounds=None,
        exclusion_bounds=(-210.0, 210.0),
    )
    tester.case(0.0, None, None)
    tester.case(-210.0, None, None)
    tester.case(220.0, None, None)
    tester.case(-20.0, None, None)
    tester.case(20.0, None, None)



================================================
FILE: tests/actor/_power_managing/test_shifting_matryoshka.py
================================================
# License: MIT
# Copyright © 2025 Frequenz Energy-as-a-Service GmbH

"""Tests for the Shifting Matryoshka power manager algorithm."""

# pylint: disable=duplicate-code

import asyncio
import re
from datetime import datetime, timedelta, timezone

import pytest
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.quantities import Power

from frequenz.sdk import timeseries
from frequenz.sdk.microgrid._power_managing import Proposal
from frequenz.sdk.microgrid._power_managing._base_classes import DefaultPower
from frequenz.sdk.microgrid._power_managing._shifting_matryoshka import (
    ShiftingMatryoshka,
)
from frequenz.sdk.timeseries import _base_types


class StatefulTester:
    """A stateful tester for the Matryoshka algorithm."""

    def __init__(
        self,
        batteries: frozenset[ComponentId],
        system_bounds: _base_types.SystemBounds,
    ) -> None:
        """Create a new instance of the stateful tester."""
        self._call_count = 0
        self._batteries = batteries
        self._system_bounds = system_bounds
        self.algorithm = ShiftingMatryoshka(
            max_proposal_age=timedelta(seconds=60.0), default_power=DefaultPower.ZERO
        )

    def tgt_power(  # pylint: disable=too-many-arguments,too-many-positional-arguments
        self,
        priority: int,
        power: float | None,
        bounds: tuple[float | None, float | None],
        expected: float | None,
        creation_time: float | None = None,
        batteries: frozenset[ComponentId] | None = None,
    ) -> None:
        """Test the target power calculation."""
        self._call_count += 1
        tgt_power = self.algorithm.calculate_target_power(
            self._batteries if batteries is None else batteries,
            Proposal(
                component_ids=self._batteries if batteries is None else batteries,
                source_id=f"actor-{priority}",
                preferred_power=None if power is None else Power.from_watts(power),
                bounds=timeseries.Bounds(
                    None if bounds[0] is None else Power.from_watts(bounds[0]),
                    None if bounds[1] is None else Power.from_watts(bounds[1]),
                ),
                priority=priority,
                creation_time=(
                    creation_time
                    if creation_time is not None
                    else asyncio.get_event_loop().time()
                ),
            ),
            self._system_bounds,
        )
        assert tgt_power == (
            Power.from_watts(expected) if expected is not None else None
        )

    def bounds(
        self,
        priority: int,
        expected_power: float | None,
        expected_bounds: tuple[float, float],
    ) -> None:
        """Test the status report."""
        report = self.algorithm.get_status(
            self._batteries, priority, self._system_bounds
        )
        if expected_power is None:
            assert report.target_power is None
        else:
            assert report.target_power is not None
            assert report.target_power.as_watts() == expected_power
        # pylint: disable=protected-access
        assert report._inclusion_bounds is not None
        assert report._inclusion_bounds.lower.as_watts() == expected_bounds[0]
        assert report._inclusion_bounds.upper.as_watts() == expected_bounds[1]
        # pylint: enable=protected-access


async def test_matryoshka_no_excl() -> None:  # pylint: disable=too-many-statements
    """Tests for the power managing actor.

    With just inclusion bounds, and no exclusion bounds.
    """
    batteries = frozenset({ComponentId(2), ComponentId(5)})

    system_bounds = _base_types.SystemBounds(
        timestamp=datetime.now(tz=timezone.utc),
        inclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-200.0), upper=Power.from_watts(200.0)
        ),
        exclusion_bounds=timeseries.Bounds(lower=Power.zero(), upper=Power.zero()),
    )

    tester = StatefulTester(batteries, system_bounds)

    tester.tgt_power(priority=2, power=25.0, bounds=(25.0, 50.0), expected=25.0)
    tester.bounds(priority=2, expected_power=25.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=25.0, expected_bounds=(0.0, 25.0))

    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=45.0)
    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=45.0)
    tester.bounds(priority=1, expected_power=45.0, expected_bounds=(0.0, 25.0))

    tester.tgt_power(priority=3, power=10.0, bounds=(10.0, 15.0), expected=15.0)
    tester.bounds(priority=3, expected_power=15.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=15.0, expected_bounds=(0.0, 5.0))
    tester.bounds(priority=1, expected_power=15.0, expected_bounds=(0.0, 0.0))

    tester.tgt_power(priority=3, power=10.0, bounds=(10.0, 22.0), expected=22.0)
    tester.bounds(priority=3, expected_power=22.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=22.0, expected_bounds=(0.0, 12.0))
    tester.bounds(priority=1, expected_power=22.0, expected_bounds=(0.0, 0.0))

    tester.tgt_power(priority=1, power=30.0, bounds=(20.0, 50.0), expected=22.0)
    tester.bounds(priority=1, expected_power=22.0, expected_bounds=(0.0, 0.0))

    tester.tgt_power(priority=3, power=10.0, bounds=(10.0, 50.0), expected=50.0)
    tester.bounds(priority=3, expected_power=50.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=50.0, expected_bounds=(0.0, 40.0))
    tester.bounds(priority=1, expected_power=50.0, expected_bounds=(0.0, 15.0))

    tester.tgt_power(priority=2, power=40.0, bounds=(40.0, None), expected=50.0)
    tester.bounds(priority=3, expected_power=50.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=50.0, expected_bounds=(0.0, 40.0))
    tester.bounds(priority=1, expected_power=50.0, expected_bounds=(0.0, 0.0))

    tester.tgt_power(priority=2, power=0.0, bounds=(-200.0, 200.0), expected=40.0)
    tester.bounds(priority=4, expected_power=40.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=3, expected_power=40.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=40.0, expected_bounds=(0.0, 40.0))
    tester.bounds(priority=1, expected_power=40.0, expected_bounds=(0.0, 40.0))

    tester.tgt_power(priority=4, power=-50.0, bounds=(None, -50.0), expected=-50.0)
    tester.bounds(priority=4, expected_power=-50.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=3, expected_power=-50.0, expected_bounds=(-150.0, 0.0))
    tester.bounds(priority=2, expected_power=-50.0, expected_bounds=(0.0, 0.0))
    tester.bounds(priority=1, expected_power=-50.0, expected_bounds=(0.0, 0.0))

    tester.tgt_power(priority=3, power=0.0, bounds=(-200.0, 200.0), expected=-50.0)
    tester.bounds(priority=1, expected_power=-50.0, expected_bounds=(-150.0, 0.0))

    tester.tgt_power(priority=1, power=-150.0, bounds=(-200.0, -150.0), expected=-200.0)
    tester.bounds(priority=2, expected_power=-200.0, expected_bounds=(-150.0, 0.0))
    tester.bounds(priority=1, expected_power=-200.0, expected_bounds=(-150.0, 0.0))

    tester.tgt_power(priority=4, power=-180.0, bounds=(-200.0, -50.0), expected=-200.0)
    tester.bounds(priority=1, expected_power=-200.0, expected_bounds=(-20.0, 130.0))

    tester.tgt_power(priority=4, power=50.0, bounds=(50.0, None), expected=50.0)
    tester.bounds(priority=4, expected_power=50.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=3, expected_power=50.0, expected_bounds=(0.0, 150.0))
    tester.bounds(priority=2, expected_power=50.0, expected_bounds=(0.0, 150.0))
    tester.bounds(priority=1, expected_power=50.0, expected_bounds=(0.0, 150.0))

    tester.tgt_power(priority=4, power=0.0, bounds=(-200.0, 200.0), expected=-150.0)
    tester.bounds(priority=4, expected_power=-150.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=3, expected_power=-150.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=-150.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=-150.0, expected_bounds=(-200.0, 200.0))

    tester.tgt_power(priority=3, power=0.0, bounds=(-200.0, 200.0), expected=-150.0)
    tester.bounds(priority=3, expected_power=-150.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=-150.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=-150.0, expected_bounds=(-200.0, 200.0))

    tester.tgt_power(priority=2, power=50.0, bounds=(-100, 100), expected=-100.0)
    tester.bounds(priority=3, expected_power=-100.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=2, expected_power=-100.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=-100.0, expected_bounds=(-150.0, 50.0))

    tester.tgt_power(priority=1, power=100.0, bounds=(100, 200), expected=100.0)
    tester.bounds(priority=1, expected_power=100.0, expected_bounds=(-150.0, 50.0))

    tester.tgt_power(priority=1, power=50.0, bounds=(50, 200), expected=100.0)
    tester.bounds(priority=1, expected_power=100.0, expected_bounds=(-150.0, 50.0))

    tester.tgt_power(priority=1, power=10.0, bounds=(10, 200), expected=60.0)
    tester.bounds(priority=1, expected_power=60.0, expected_bounds=(-150.0, 50.0))

    tester.tgt_power(priority=1, power=0.0, bounds=(-200, 200), expected=50.0)
    tester.bounds(priority=1, expected_power=50.0, expected_bounds=(-150.0, 50.0))

    tester.tgt_power(priority=1, power=None, bounds=(-200, 200), expected=50.0)
    tester.bounds(priority=1, expected_power=50.0, expected_bounds=(-150.0, 50.0))


async def test_matryoshka_simple() -> None:
    """Tests for the power managing actor.

    With inclusion bounds, and exclusion bounds -30.0 to 0.0.
    """
    batteries = frozenset({ComponentId(2), ComponentId(5)})

    system_bounds = _base_types.SystemBounds(
        timestamp=datetime.now(tz=timezone.utc),
        inclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-200.0), upper=Power.from_watts(200.0)
        ),
        exclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-30.0), upper=Power.from_watts(30.0)
        ),
    )

    tester = StatefulTester(batteries, system_bounds)
    tester.tgt_power(priority=3, power=None, bounds=(-200.0, 200.0), expected=0.0)
    tester.bounds(priority=2, expected_power=0.0, expected_bounds=(-200.0, 200.0))
    tester.tgt_power(priority=1, power=None, bounds=(-200.0, 200.0), expected=0.0)
    tester.bounds(priority=2, expected_power=0.0, expected_bounds=(-200.0, 200.0))
    tester.tgt_power(priority=2, power=25.0, bounds=(25.0, 50.0), expected=30.0)
    # tester.tgt_power(priority=2, power=-10.0, bounds=(-10.0, 50.0), expected=-10.0)
    # tester.bounds(priority=1, expected_power=-10.0, expected_bounds=(0.0, 60.0))
    # tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, 20.0), expected=-10.0)
    # tester.bounds(priority=0, expected_power=-10.0, expected_bounds=(0.0, 20.0))


async def test_matryoshka_with_excl_1() -> None:
    """Tests for the power managing actor.

    With inclusion bounds, and exclusion bounds -30.0 to 0.0.
    """
    batteries = frozenset({ComponentId(2), ComponentId(5)})

    system_bounds = _base_types.SystemBounds(
        timestamp=datetime.now(tz=timezone.utc),
        inclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-200.0), upper=Power.from_watts(200.0)
        ),
        exclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-30.0), upper=Power.zero()
        ),
    )

    tester = StatefulTester(batteries, system_bounds)

    tester.tgt_power(priority=2, power=25.0, bounds=(25.0, 50.0), expected=25.0)
    tester.bounds(priority=2, expected_power=25.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=25.0, expected_bounds=(0.0, 25.0))

    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=45.0)
    tester.bounds(priority=1, expected_power=45.0, expected_bounds=(0.0, 25.0))

    tester.tgt_power(priority=2, power=-10.0, bounds=(-10.0, 50.0), expected=10.0)
    tester.bounds(priority=1, expected_power=10.0, expected_bounds=(0.0, 60.0))
    tester.bounds(priority=0, expected_power=10.0, expected_bounds=(0.0, 30.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, 50.0), expected=0.0)
    tester.bounds(priority=0, expected_power=0.0, expected_bounds=(0.0, 50.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, -5.0), expected=0.0)
    tester.bounds(priority=0, expected_power=0.0, expected_bounds=(0.0, 0.0))

    tester.tgt_power(priority=2, power=-10.0, bounds=(-200.0, -5.0), expected=-30.0)
    tester.bounds(priority=1, expected_power=-30.0, expected_bounds=(-190.0, 5.0))
    tester.bounds(priority=0, expected_power=-30.0, expected_bounds=(0.0, 5.0))

    tester.tgt_power(priority=2, power=None, bounds=(None, None), expected=0.0)
    tester.bounds(priority=2, expected_power=0.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=0.0, expected_bounds=(-200.0, 200.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, -5.0), expected=0.0)
    tester.bounds(priority=0, expected_power=0.0, expected_bounds=(0.0, 5.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-100.0, -5.0), expected=-30.0)
    tester.bounds(priority=0, expected_power=-30.0, expected_bounds=(-90.0, 5.0))

    tester.tgt_power(priority=1, power=-40.0, bounds=(-100.0, -35.0), expected=-40.0)
    tester.bounds(priority=0, expected_power=-40.0, expected_bounds=(-60.0, 5.0))


async def test_matryoshka_with_excl_2() -> None:
    """Tests for the power managing actor.

    With inclusion bounds, and exclusion bounds 0.0 to 30.0.
    """
    batteries = frozenset({ComponentId(2), ComponentId(5)})

    system_bounds = _base_types.SystemBounds(
        timestamp=datetime.now(tz=timezone.utc),
        inclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-200.0), upper=Power.from_watts(200.0)
        ),
        exclusion_bounds=timeseries.Bounds(
            lower=Power.zero(), upper=Power.from_watts(30.0)
        ),
    )

    tester = StatefulTester(batteries, system_bounds)

    tester.tgt_power(priority=2, power=25.0, bounds=(25.0, 50.0), expected=30.0)
    tester.bounds(priority=2, expected_power=30.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=30.0, expected_bounds=(0.0, 25.0))

    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=45.0)
    tester.bounds(priority=1, expected_power=45.0, expected_bounds=(0.0, 25.0))

    tester.tgt_power(priority=1, power=10.0, bounds=(5.0, 10.0), expected=35.0)
    tester.bounds(priority=0, expected_power=35.0, expected_bounds=(-5.0, 0.0))

    tester.tgt_power(priority=2, power=-10.0, bounds=(-10.0, 50.0), expected=0.0)
    tester.bounds(priority=1, expected_power=0.0, expected_bounds=(0.0, 60.0))
    tester.bounds(priority=0, expected_power=0.0, expected_bounds=(-5.0, 0.0))

    tester.tgt_power(priority=0, power=40, bounds=(None, None), expected=0.0)

    tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, 50.0), expected=30.0)
    tester.bounds(priority=0, expected_power=30.0, expected_bounds=(0.0, 50.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, 20.0), expected=0.0)
    tester.bounds(priority=0, expected_power=0.0, expected_bounds=(0.0, 20.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, -5.0), expected=-10.0)
    tester.bounds(priority=0, expected_power=-10.0, expected_bounds=(0.0, 0.0))

    tester.tgt_power(priority=2, power=-10.0, bounds=(-200.0, -5.0), expected=-15.0)
    tester.bounds(priority=1, expected_power=-15.0, expected_bounds=(-190.0, 5.0))
    tester.bounds(priority=0, expected_power=-15.0, expected_bounds=(0.0, 5.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-100.0, -5.0), expected=-15.0)
    tester.bounds(priority=0, expected_power=-15.0, expected_bounds=(-90.0, 5.0))

    tester.tgt_power(priority=1, power=-40.0, bounds=(-100.0, -35.0), expected=-45.0)
    tester.bounds(priority=0, expected_power=-45.0, expected_bounds=(-60.0, 5.0))


async def test_matryoshka_with_excl_3() -> None:
    """Tests for the power managing actor.

    With inclusion bounds, and exclusion bounds -30.0 to 30.0.
    """
    batteries = frozenset({ComponentId(2), ComponentId(5)})

    system_bounds = _base_types.SystemBounds(
        timestamp=datetime.now(tz=timezone.utc),
        inclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-200.0), upper=Power.from_watts(200.0)
        ),
        exclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-30.0), upper=Power.from_watts(30.0)
        ),
    )

    tester = StatefulTester(batteries, system_bounds)
    tester.tgt_power(priority=2, power=10.0, bounds=(-200.0, 200.0), expected=30.0)
    tester.tgt_power(priority=2, power=-10.0, bounds=(-200.0, 200.0), expected=-30.0)
    tester.tgt_power(priority=2, power=0.0, bounds=(-200.0, 200.0), expected=0.0)
    tester.tgt_power(priority=3, power=20.0, bounds=(-200.0, 200.0), expected=30.0)
    tester.tgt_power(priority=1, power=-20.0, bounds=(-200.0, 200.0), expected=0.0)
    tester.tgt_power(priority=3, power=None, bounds=(-200.0, 200.0), expected=-30.0)
    tester.tgt_power(priority=1, power=None, bounds=(-200.0, 200.0), expected=0.0)

    tester.tgt_power(priority=2, power=25.0, bounds=(25.0, 50.0), expected=30.0)
    tester.bounds(priority=2, expected_power=30.0, expected_bounds=(-200.0, 200.0))
    tester.bounds(priority=1, expected_power=30.0, expected_bounds=(0.0, 25.0))

    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=45.0)
    tester.bounds(priority=1, expected_power=45.0, expected_bounds=(0.0, 25.0))

    tester.tgt_power(priority=1, power=10.0, bounds=(5.0, 10.0), expected=35.0)
    tester.bounds(priority=0, expected_power=35.0, expected_bounds=(-5.0, 0.0))

    tester.tgt_power(priority=2, power=-10.0, bounds=(-10.0, 50.0), expected=30.0)
    tester.bounds(priority=1, expected_power=30.0, expected_bounds=(0.0, 60.0))
    tester.bounds(priority=0, expected_power=30.0, expected_bounds=(-5.0, 0.0))

    tester.tgt_power(priority=1, power=40.0, bounds=(-10.0, 50.0), expected=30.0)
    tester.bounds(priority=0, expected_power=30.0, expected_bounds=(-40.0, 10.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-10.0, 20.0), expected=30.0)
    tester.bounds(priority=0, expected_power=30.0, expected_bounds=(0.0, 20.0))

    tester.tgt_power(priority=2, power=-10.0, bounds=(-200.0, -5.0), expected=-30.0)
    tester.bounds(priority=1, expected_power=-30.0, expected_bounds=(-190.0, 5.0))
    tester.bounds(priority=0, expected_power=-30.0, expected_bounds=(0.0, 15.0))

    tester.tgt_power(priority=1, power=-10.0, bounds=(-100.0, -5.0), expected=-30.0)
    tester.bounds(priority=0, expected_power=-30.0, expected_bounds=(-90.0, 5.0))

    tester.tgt_power(priority=1, power=-40.0, bounds=(-100.0, -35.0), expected=-50.0)
    tester.bounds(priority=0, expected_power=-50.0, expected_bounds=(-60.0, 5.0))


async def test_matryoshka_drop_old_proposals() -> None:
    """Tests for the power managing actor.

    With inclusion bounds, and exclusion bounds -30.0 to 30.0.
    """
    batteries = frozenset({ComponentId(2), ComponentId(5)})
    overlapping_batteries = frozenset({ComponentId(5), ComponentId(8)})

    system_bounds = _base_types.SystemBounds(
        timestamp=datetime.now(tz=timezone.utc),
        inclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-200.0), upper=Power.from_watts(200.0)
        ),
        exclusion_bounds=timeseries.Bounds(lower=Power.zero(), upper=Power.zero()),
    )

    tester = StatefulTester(batteries, system_bounds)

    now = asyncio.get_event_loop().time()

    tester.tgt_power(priority=3, power=22.0, bounds=(22.0, 100.0), expected=22.0)

    # When a proposal is too old and hasn't been updated, it is dropped.
    tester.tgt_power(
        priority=2,
        power=25.0,
        bounds=(25.0, 50.0),
        creation_time=now - 70.0,
        expected=47.0,
    )

    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=67.0)
    tester.algorithm.drop_old_proposals(now)
    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=42.0)

    # When overwritten by a newer proposal, that proposal is not dropped.
    tester.tgt_power(
        priority=2,
        power=25.0,
        bounds=(25.0, 50.0),
        creation_time=now - 70.0,
        expected=67.0,
    )
    tester.tgt_power(
        priority=2,
        power=25.0,
        bounds=(25.0, 50.0),
        creation_time=now - 30.0,
        expected=67.0,
    )

    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=67.0)
    tester.algorithm.drop_old_proposals(now)
    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=67.0)

    # When all proposals are too old, they are dropped, and the buckets are dropped as
    # well.  After that, sending a request for a different but overlapping bucket will
    # succeed.  And it will fail until then.
    with pytest.raises(
        NotImplementedError,
        match=re.escape(
            "PowerManagingActor: CID5, CID8 are already part of another bucket. "
            + " Overlapping buckets are not yet supported."
        ),
    ):
        tester.tgt_power(
            priority=1,
            power=25.0,
            bounds=(25.0, 50.0),
            expected=25.0,
            batteries=overlapping_batteries,
        )

    tester.tgt_power(
        priority=1,
        power=25.0,
        bounds=(25.0, 100.0),
        creation_time=now - 70.0,
        expected=72.0,
    )
    tester.tgt_power(
        priority=2,
        power=25.0,
        bounds=(25.0, 100.0),
        creation_time=now - 70.0,
        expected=72.0,
    )
    tester.tgt_power(
        priority=3,
        power=25.0,
        bounds=(25.0, 100.0),
        creation_time=now - 70.0,
        expected=75.0,
    )

    tester.algorithm.drop_old_proposals(now)

    tester.tgt_power(
        priority=1,
        power=25.0,
        bounds=(25.0, 50.0),
        expected=25.0,
        batteries=overlapping_batteries,
    )


async def test_matryoshka_none_proposals() -> None:
    """Tests for the power managing actor.

    When a `None` proposal is received, is source id should be dropped from the bucket.
    Then if the bucket becomes empty, it should be dropped as well.
    """
    batteries = frozenset({ComponentId(2), ComponentId(5)})
    overlapping_batteries = frozenset({ComponentId(5), ComponentId(8)})

    system_bounds = _base_types.SystemBounds(
        timestamp=datetime.now(tz=timezone.utc),
        inclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-200.0), upper=Power.from_watts(200.0)
        ),
        exclusion_bounds=timeseries.Bounds(lower=Power.zero(), upper=Power.zero()),
    )

    def ensure_overlapping_bucket_request_fails() -> None:
        with pytest.raises(
            NotImplementedError,
            match=re.escape(
                "PowerManagingActor: CID5, CID8 are already part of another bucket. "
                + " Overlapping buckets are not yet supported."
            ),
        ):
            tester.tgt_power(
                priority=1,
                power=None,
                bounds=(20.0, 50.0),
                expected=None,
                batteries=overlapping_batteries,
            )

    tester = StatefulTester(batteries, system_bounds)

    tester.tgt_power(priority=3, power=22.0, bounds=(22.0, 30.0), expected=22.0)
    tester.tgt_power(priority=2, power=25.0, bounds=(25.0, 50.0), expected=30.0)
    tester.tgt_power(priority=1, power=20.0, bounds=(20.0, 50.0), expected=30.0)

    ensure_overlapping_bucket_request_fails()
    tester.tgt_power(priority=1, power=None, bounds=(None, None), expected=30.0)
    ensure_overlapping_bucket_request_fails()
    tester.tgt_power(priority=3, power=None, bounds=(None, None), expected=25.0)
    ensure_overlapping_bucket_request_fails()
    tester.tgt_power(priority=2, power=None, bounds=(None, None), expected=0.0)

    # Overlapping battery bucket is dropped.
    tester.tgt_power(
        priority=1,
        power=20.0,
        bounds=(20.0, 50.0),
        expected=20.0,
        batteries=overlapping_batteries,
    )


async def test_matryoshka_shifting_limiting() -> None:
    """Tests for the power managing actor.

    With the following scenario:

    | Actor | System Limits     | Specified Limits | Desired | Adjusted | Aggregate |
    | Prio  |                   |                  | Power   | Power    | Power     |
    |-------|-------------------|------------------|---------|----------|-----------|
    | 7     | -100 kW .. 100 kW | None             | 10 kW   | 10 kW    | 10 kW     |
    | 6     | -110 kW .. 90 kW  | -110 kW .. 80 kW | 10 kW   | 10 kW    | 20 kW     |
    | 5     | -120 kW .. 70 kW  | -100 kW .. 80 kW | 80 kW   | 70 kW    | 90 kW     |
    | 4     | -170 kW .. 0 kW   | None             | -120 kW | -120 kW  | -30 kW    |
    | 3     | -50 kW .. 120 kW  | None             | 60 kW   | 60 kW    | 30 kW     |
    | 2     | -110 kW .. 60 kW  | -40 kW .. 30 kW  | 20 kW   | 20 kW    | 50 kW     |
    | 1     | -60 kW .. 10 kW   | -50 kW .. 40 kW  | 25 kW   | 10 kW    | 60 kW     |
    | 0     | -60 kW .. 0 kW    | None             | 12 kW   | 0 kW     | 60 kW     |
    | -1    | -60 kW .. 0 kW    | -40 kW .. -10 kW | -10 kW  | -10 kW   | 50 kW     |
    |-------|-------------------|------------------|---------|----------|-----------|
    |       |                   |                  |         | Power    |           |
    |       |                   |                  |         | Setpoint | 50 kW     |
    """
    batteries = frozenset({ComponentId(2), ComponentId(5)})

    system_bounds = _base_types.SystemBounds(
        timestamp=datetime.now(tz=timezone.utc),
        inclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-100.0), upper=Power.from_watts(100.0)
        ),
        exclusion_bounds=timeseries.Bounds(
            lower=Power.from_watts(-0.0), upper=Power.from_watts(0.0)
        ),
    )

    tester = StatefulTester(batteries, system_bounds)
    tester.tgt_power(priority=7, power=10.0, bounds=(None, None), expected=10.0)
    tester.bounds(priority=7, expected_power=10.0, expected_bounds=(-100.0, 100.0))
    tester.bounds(priority=6, expected_power=10.0, expected_bounds=(-110.0, 90.0))

    tester.tgt_power(priority=6, power=10.0, bounds=(-110.0, 80.0), expected=20.0)
    tester.bounds(priority=5, expected_power=20.0, expected_bounds=(-120.0, 70.0))

    tester.tgt_power(priority=5, power=80.0, bounds=(-100.0, 80.0), expected=90.0)
    tester.bounds(priority=4, expected_power=90.0, expected_bounds=(-170.0, 0.0))

    tester.tgt_power(priority=4, power=-120.0, bounds=(None, None), expected=-30.0)
    tester.bounds(priority=3, expected_power=-30.0, expected_bounds=(-50.0, 120.0))

    tester.tgt_power(priority=3, power=60.0, bounds=(None, None), expected=30.0)
    tester.bounds(priority=2, expected_power=30.0, expected_bounds=(-110.0, 60.0))

    tester.tgt_power(priority=2, power=20.0, bounds=(-40.0, 30.0), expected=50.0)
    tester.bounds(priority=1, expected_power=50.0, expected_bounds=(-60.0, 10.0))

    tester.tgt_power(priority=1, power=25.0, bounds=(-50.0, 40.0), expected=60.0)
    tester.bounds(priority=0, expected_power=60.0, expected_bounds=(-60.0, 0.0))

    tester.tgt_power(priority=0, power=12.0, bounds=(None, None), expected=60.0)
    tester.bounds(priority=-1, expected_power=60.0, expected_bounds=(-60.0, 0.0))

    tester.tgt_power(priority=-1, power=-10.0, bounds=(-40.0, -10.0), expected=50.0)



================================================
FILE: tests/config/test_actor.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Test for ConfigManager."""
import os
import pathlib
from collections import defaultdict
from collections.abc import Mapping, MutableMapping
from dataclasses import dataclass
from typing import Any
from unittest.mock import MagicMock

import pytest
from frequenz.channels import Broadcast
from frequenz.channels.file_watcher import Event, EventType
from pytest_mock import MockerFixture

from frequenz.sdk.config import ConfigManagingActor
from frequenz.sdk.config._managing_actor import _recursive_update


class Item:
    """Test item."""

    item_id: int
    name: str


def create_content(number: int) -> str:
    """Create content to be written to a config file."""
    return f"""
    logging_lvl = "ERROR"
    var1 = "0"
    var2 = "{number}"
    """


class TestActorConfigManager:
    """Test for ConfigManager."""

    conf_path = "sdk/config.toml"
    conf_content = """
    logging_lvl = 'DEBUG'
    var1 = "1"
    var_int = "5"
    var_float = "3.14"
    var_bool = "true"
    list_int = "[1,2,3]"
    list_float = "[1,2.0,3.5]"
    var_off = "off"
    list_non_strict_bool = ["false", "0", "true", "1"]
    item_data = '[{"item_id": 1, "name": "My Item"}]'
    var_none = 'null'
    [dict_str_int]
    a = 1
    b = 2
    c = 3
    """

    @pytest.fixture()
    def config_file(self, tmp_path: pathlib.Path) -> pathlib.Path:
        """Create a test config file."""
        file_path = tmp_path / TestActorConfigManager.conf_path
        if not file_path.exists():
            file_path.parent.mkdir()
            file_path.touch()
        file_path.write_text(TestActorConfigManager.conf_content)
        return file_path

    async def test_update(self, config_file: pathlib.Path) -> None:
        """Test ConfigManager.

        Check if:

        - the initial content of the content file is correct
        - the config file modifications are picked up and the new content is correct
        """
        config_channel: Broadcast[Mapping[str, Any]] = Broadcast(
            name="Config Channel", resend_latest=True
        )
        config_receiver = config_channel.new_receiver()

        async with ConfigManagingActor(
            [config_file], config_channel.new_sender(), force_polling=False
        ):
            config = await config_receiver.receive()
            assert config is not None
            assert config.get("logging_lvl") == "DEBUG"
            assert config.get("var1") == "1"
            assert config.get("var2") is None
            assert config.get("var3") is None

            number = 5
            config_file.write_text(create_content(number=number))

            config = await config_receiver.receive()
            assert config is not None
            assert config.get("logging_lvl") == "ERROR"
            assert config.get("var1") == "0"
            assert config.get("var2") == str(number)
            assert config.get("var3") is None
            assert config_file.read_text() == create_content(number=number)

    async def test_update_relative_path(self, config_file: pathlib.Path) -> None:
        """Test ConfigManagingActor with a relative path."""
        config_channel: Broadcast[Mapping[str, Any]] = Broadcast(
            name="Config Channel", resend_latest=True
        )
        config_receiver = config_channel.new_receiver()

        current_dir = pathlib.Path.cwd()
        relative_path = os.path.relpath(config_file, current_dir)

        async with ConfigManagingActor(
            [relative_path], config_channel.new_sender(), force_polling=False
        ):
            config = await config_receiver.receive()
            assert config is not None
            assert config.get("var2") is None

            number = 8
            config_file.write_text(create_content(number=number))

            config = await config_receiver.receive()
            assert config is not None
            assert config.get("var2") == str(number)
            assert config_file.read_text() == create_content(number=number)

    async def test_update_multiple_files(self, config_file: pathlib.Path) -> None:
        """Test ConfigManagingActor with multiple files."""
        config_channel: Broadcast[Mapping[str, Any]] = Broadcast(
            name="Config Channel", resend_latest=True
        )
        config_receiver = config_channel.new_receiver()

        config_file2 = config_file.parent / "config2.toml"
        config_file2.write_text(
            """
            logging_lvl = 'ERROR'
            var1 = "0"
            var2 = "15"
            var_float = "3.14"
            list_int = "[1,3]"
            list_float = "[1,2.0,3.8,12.3]"
            var_off = "off"
            item_data = '[{"item_id": 1, "name": "My Item 2"}]'
            [dict_str_int]
            a = 1
            b = 2
            c = 4
            d = 3
            """
        )

        async with ConfigManagingActor(
            [config_file, config_file2],
            config_channel.new_sender(),
            force_polling=False,
        ):
            config = await config_receiver.receive()
            assert config is not None
            # Both config files are read and merged, and config_file2 overrides
            # the variables present in config_file.
            assert config == {
                "logging_lvl": "ERROR",
                "var1": "0",
                "var2": "15",
                "var_int": "5",
                "var_float": "3.14",
                "var_bool": "true",
                "list_int": "[1,3]",
                "list_float": "[1,2.0,3.8,12.3]",
                "var_off": "off",
                "list_non_strict_bool": ["false", "0", "true", "1"],
                "item_data": '[{"item_id": 1, "name": "My Item 2"}]',
                "dict_str_int": {"a": 1, "b": 2, "c": 4, "d": 3},
                "var_none": "null",
            }

            # We overwrite config_file with just two variables
            config_file.write_text(
                """
                logging_lvl = 'INFO'
                list_non_strict_bool = ["false", "0", "true"]
                """
            )

            config = await config_receiver.receive()
            assert config is not None
            # The config no longer contains variables that were only present in
            # config_file. config_file2 still takes precedence, so the result remains
            # unchanged except for the removal of variables unique to the old
            # config_file.
            assert config == {
                "logging_lvl": "ERROR",
                "var1": "0",
                "var2": "15",
                "var_float": "3.14",
                "list_int": "[1,3]",
                "list_float": "[1,2.0,3.8,12.3]",
                "var_off": "off",
                "item_data": '[{"item_id": 1, "name": "My Item 2"}]',
                "list_non_strict_bool": ["false", "0", "true"],
                "dict_str_int": {"a": 1, "b": 2, "c": 4, "d": 3},
            }

            # Now we only update logging_lvl in config_file2, it still takes precedence
            config_file2.write_text(
                """
                    logging_lvl = 'DEBUG'
                    var1 = "0"
                    var2 = "15"
                    var_float = "3.14"
                    list_int = "[1,3]"
                    list_float = "[1,2.0,3.8,12.3]"
                    var_off = "off"
                    item_data = '[{"item_id": 1, "name": "My Item 2"}]'
                    [dict_str_int]
                    a = 1
                    b = 2
                    c = 4
                    d = 3
                    """
            )

            config = await config_receiver.receive()
            assert config is not None
            assert config == {
                "logging_lvl": "DEBUG",
                "var1": "0",
                "var2": "15",
                "var_float": "3.14",
                "list_int": "[1,3]",
                "list_float": "[1,2.0,3.8,12.3]",
                "var_off": "off",
                "item_data": '[{"item_id": 1, "name": "My Item 2"}]',
                "list_non_strict_bool": ["false", "0", "true"],
                "dict_str_int": {"a": 1, "b": 2, "c": 4, "d": 3},
            }

            # Now add one variable to config_file not present in config_file2 and remove
            # a bunch of variables from config_file2 too, and update a few
            config_file.write_text(
                """
                logging_lvl = 'INFO'
                var10 = "10"
                """
            )
            config_file2.write_text(
                """
                    logging_lvl = 'DEBUG'
                    var1 = "3"
                    var_off = "on"
                    [dict_str_int]
                    a = 1
                    b = 2
                    c = 4
                    """
            )

            config = await config_receiver.receive()
            assert config is not None
            assert config == {
                "logging_lvl": "DEBUG",
                "var1": "3",
                "var10": "10",
                "var_off": "on",
                "dict_str_int": {"a": 1, "b": 2, "c": 4},
            }

    async def test_actor_works_if_not_all_config_files_exist(
        self, config_file: pathlib.Path
    ) -> None:
        """Test ConfigManagingActor works if not all config files exist."""
        config_channel: Broadcast[Mapping[str, Any]] = Broadcast(
            name="Config Channel", resend_latest=True
        )
        config_receiver = config_channel.new_receiver()

        # This file does not exist
        config_file2 = config_file.parent / "config2.toml"

        async with ConfigManagingActor(
            [config_file, config_file2],
            config_channel.new_sender(),
            force_polling=False,
        ):
            config = await config_receiver.receive()
            assert config is not None
            assert config.get("var2") is None

            number = 5
            config_file.write_text(create_content(number=number))

            config = await config_receiver.receive()
            assert config is not None
            assert config.get("var2") == str(number)

            # Create second config file that overrides the value from the first one
            number = 42
            config_file2.write_text(create_content(number=number))

            config = await config_receiver.receive()
            assert config is not None
            assert config.get("var2") == str(number)

    async def test_actor_does_not_crash_if_file_is_deleted(
        self, config_file: pathlib.Path, mocker: MockerFixture
    ) -> None:
        """Test ConfigManagingActor does not crash if a file is deleted."""
        config_channel: Broadcast[Mapping[str, Any]] = Broadcast(
            name="Config Channel", resend_latest=True
        )
        config_receiver = config_channel.new_receiver()

        number = 5
        config_file2 = config_file.parent / "config2.toml"
        config_file2.write_text(create_content(number=number))

        # Not config file but existing in the same directory
        any_file = config_file.parent / "any_file.txt"
        any_file.write_text("content")

        file_watcher_mock = MagicMock()
        file_watcher_mock.__anext__.side_effect = [
            Event(EventType.DELETE, any_file),
            Event(EventType.MODIFY, config_file2),
            Event(EventType.MODIFY, config_file),
        ]

        mocker.patch(
            "frequenz.channels.file_watcher.FileWatcher", return_value=file_watcher_mock
        )

        async with ConfigManagingActor(
            [config_file, config_file2],
            config_channel.new_sender(),
            force_polling=False,
        ) as actor:
            send_config_spy = mocker.spy(actor, "send_config")

            config = await config_receiver.receive()
            assert config is not None
            assert config.get("var2") == str(number)
            send_config_spy.assert_called_once()
            send_config_spy.reset_mock()

            # Remove file and send DELETE events
            any_file.unlink()
            config_file2.unlink()
            number = 101
            config_file.write_text(create_content(number=number))

            config = await config_receiver.receive()
            assert config is not None
            assert config.get("var2") == str(number)
            # Config should be updated only once on MODIFY event
            # DELETE events are ignored
            send_config_spy.assert_called_once()


@dataclass(frozen=True, kw_only=True)
class RecursiveUpdateTestCase:
    """A test case for the recursive_update function."""

    title: str
    d1: dict[str, Any]
    d2: MutableMapping[str, Any]
    expected: dict[str, Any]


# Define all test cases as instances of TestCase
recursive_update_test_cases = [
    RecursiveUpdateTestCase(
        title="Basic Update",
        d1={"a": 1, "b": 2, "c": {"d": 3, "e": 4}},
        d2={"b": 5, "h": 10, "c": {"d": 30}},
        expected={"a": 1, "b": 5, "h": 10, "c": {"d": 30, "e": 4}},
    ),
    RecursiveUpdateTestCase(
        title="Nested Update",
        d1={"a": {"b": 1, "c": {"d": 2}}, "e": 3},
        d2={"a": {"c": {"d": 20, "f": 4}, "g": 5}, "h": 6},
        expected={"a": {"b": 1, "c": {"d": 20, "f": 4}, "g": 5}, "e": 3, "h": 6},
    ),
    RecursiveUpdateTestCase(
        title="Non-Dict Overwrite",
        d1={"a": {"b": 1}, "c": 2},
        d2={"a": 10, "c": {"d": 3}},
        expected={"a": 10, "c": {"d": 3}},
    ),
    RecursiveUpdateTestCase(
        title="Empty d2",
        d1={"a": 1, "b": {"c": 2}},
        d2={},
        expected={"a": 1, "b": {"c": 2}},
    ),
    RecursiveUpdateTestCase(
        title="Empty d1",
        d1={},
        d2={"a": 1, "b": {"c": 2}},
        expected={"a": 1, "b": {"c": 2}},
    ),
    RecursiveUpdateTestCase(
        title="Non-Mapping Values Overwrite",
        d1={"a": {"b": [1, 2, 3]}, "c": "hello"},
        d2={"a": {"b": [4, 5]}, "c": "world"},
        expected={"a": {"b": [4, 5]}, "c": "world"},
    ),
    RecursiveUpdateTestCase(
        title="Multiple Data Types",
        d1={"int": 1, "float": 1.0, "str": "one", "list": [1, 2, 3], "dict": {"a": 1}},
        d2={"int": 2, "float": 2.0, "str": "two", "list": [4, 5], "dict": {"b": 2}},
        expected={
            "int": 2,
            "float": 2.0,
            "str": "two",
            "list": [4, 5],
            "dict": {"a": 1, "b": 2},
        },
    ),
    RecursiveUpdateTestCase(
        title="Defaultdict Handling",
        d1=defaultdict(dict, {"a": {"b": 1}, "c": 2}),
        d2={"a": {"c": 3}, "d": 4},
        expected={"a": {"b": 1, "c": 3}, "c": 2, "d": 4},
    ),
    # Additional Test Cases
    RecursiveUpdateTestCase(
        title="Overwrite with Nested Non-Mapping",
        d1={"x": {"y": {"z": 1}}},
        d2={"x": {"y": 2}},
        expected={"x": {"y": 2}},
    ),
    RecursiveUpdateTestCase(
        title="Overwrite with Nested Mapping",
        d1={"x": {"y": 2}},
        d2={"x": {"y": {"z": 3}}},
        expected={"x": {"y": {"z": 3}}},
    ),
]


@pytest.mark.parametrize(
    "test_case", recursive_update_test_cases, ids=lambda tc: tc.title
)
def test_recursive_update(test_case: RecursiveUpdateTestCase) -> None:
    """Test the recursive_update function with various input dictionaries."""
    assert _recursive_update(test_case.d1, test_case.d2) == test_case.expected



================================================
FILE: tests/config/test_logging_actor.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Tests for logging config updater."""

import asyncio
import logging
from typing import Any

import pytest
from frequenz.channels import Broadcast
from marshmallow import ValidationError
from pytest_mock import MockerFixture

from frequenz.sdk.config import (
    LoggerConfig,
    LoggingConfig,
    LoggingConfigUpdatingActor,
    RootLoggerConfig,
    load_config,
)


def test_logging_config() -> None:
    """Test if logging config is correctly loaded."""
    config_raw = {
        "root_logger": {"level": "DEBUG"},
        "loggers": {
            "actor": {"name": "frequenz.sdk.actor", "level": "INFO"},
            "timeseries": {"name": "frequenz.sdk.timeseries", "level": "ERROR"},
        },
    }
    config = LoggingConfig(
        root_logger=RootLoggerConfig(level="DEBUG"),
        loggers={
            "actor": LoggerConfig(name="frequenz.sdk.actor", level="INFO"),
            "timeseries": LoggerConfig(name="frequenz.sdk.timeseries", level="ERROR"),
        },
    )

    assert load_config(LoggingConfig, config_raw) == config

    config_raw = {}
    config = LoggingConfig()
    assert load_config(LoggingConfig, config_raw) == config

    config_raw = {"root_logger": {"level": "UNKNOWN"}}
    with pytest.raises(ValidationError):
        load_config(LoggingConfig, config_raw)

    config_raw = {"unknown": {"frequenz.sdk.actor": {"level": "DEBUG"}}}
    assert load_config(LoggingConfig, config_raw) == config


@pytest.fixture
def cleanup_logs() -> Any:
    """Reset logging to default.

    Python doesn't cleanup logging configuration after tests, so we need to do it manually.
    """
    yield

    logging.getLogger("frequenz.sdk.actor").setLevel(logging.NOTSET)
    logging.getLogger("frequenz.sdk.timeseries").setLevel(logging.NOTSET)


async def test_logging_config_updating_actor(
    mocker: MockerFixture,
    cleanup_logs: Any,
) -> None:
    """Test if logging is configured and updated correctly."""
    # Mock method that sets root level logging.
    # Python doesn't cleanup logging configuration after tests.
    # Overriding logging.basicConfig would mess up other tests, so we mock it.
    # This is just for extra safety because changing root logging level in unit tests
    # is not working anyway - python ignores it.
    mocker.patch("frequenz.sdk.config._logging_actor.logging.basicConfig")

    # Mock ConfigManager
    mock_config_manager = mocker.Mock()
    mock_config_manager.config_channel = Broadcast[LoggingConfig | Exception | None](
        name="config"
    )
    mock_config_manager.new_receiver = mocker.Mock(
        return_value=mock_config_manager.config_channel.new_receiver()
    )

    async with LoggingConfigUpdatingActor(mock_config_manager) as actor:
        assert logging.getLogger("frequenz.sdk.actor").level == logging.NOTSET
        assert logging.getLogger("frequenz.sdk.timeseries").level == logging.NOTSET

        update_logging_spy = mocker.spy(actor, "_update_logging")

        # Send first config
        expected_config = LoggingConfig(
            root_logger=RootLoggerConfig(level="ERROR"),
            loggers={
                "actor": LoggerConfig(name="frequenz.sdk.actor", level="DEBUG"),
                "timeseries": LoggerConfig(
                    name="frequenz.sdk.timeseries", level="ERROR"
                ),
            },
        )
        await mock_config_manager.config_channel.new_sender().send(expected_config)
        await asyncio.sleep(0.01)
        update_logging_spy.assert_called_once_with(expected_config)
        assert logging.getLogger("frequenz.sdk.actor").level == logging.DEBUG
        assert logging.getLogger("frequenz.sdk.timeseries").level == logging.ERROR
        update_logging_spy.reset_mock()

        # Send an exception and verify the previous config is maintained
        await mock_config_manager.config_channel.new_sender().send(
            ValueError("Test error")
        )
        await asyncio.sleep(0.01)
        update_logging_spy.assert_not_called()  # Should not try to update logging
        # Previous config should be maintained
        assert logging.getLogger("frequenz.sdk.actor").level == logging.DEBUG
        assert logging.getLogger("frequenz.sdk.timeseries").level == logging.ERROR
        assert (
            actor._current_config == expected_config  # pylint: disable=protected-access
        )  # pylint: disable=protected-access
        update_logging_spy.reset_mock()

        # Update config
        expected_config = LoggingConfig(
            root_logger=RootLoggerConfig(level="WARNING"),
            loggers={
                "actor": LoggerConfig(name="frequenz.sdk.actor", level="INFO"),
            },
        )
        await mock_config_manager.config_channel.new_sender().send(expected_config)
        await asyncio.sleep(0.01)
        update_logging_spy.assert_called_once_with(expected_config)
        assert logging.getLogger("frequenz.sdk.actor").level == logging.INFO
        assert logging.getLogger("frequenz.sdk.timeseries").level == logging.NOTSET
        update_logging_spy.reset_mock()

        # Send a None config to make sure actor doesn't crash and configures a default logging
        await mock_config_manager.config_channel.new_sender().send(None)
        await asyncio.sleep(0.01)
        update_logging_spy.assert_called_once_with(LoggingConfig())
        assert (
            actor._current_config == LoggingConfig()  # pylint: disable=protected-access
        )
        update_logging_spy.reset_mock()



================================================
FILE: tests/config/test_manager.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Tests for the config manager module."""


import asyncio
import dataclasses
import logging
import pathlib
from collections.abc import Mapping, Sequence
from dataclasses import dataclass
from datetime import timedelta
from typing import Any, assert_never

import marshmallow
import pytest
import pytest_mock

from frequenz.sdk.config import ConfigManager, InvalidValueForKeyError, wait_for_first
from frequenz.sdk.config._manager import _get_key


@dataclass
class SimpleConfig:
    """A simple configuration class for testing."""

    name: str = dataclasses.field(metadata={"validate": lambda s: s.startswith("test")})
    value: int


@dataclass(frozen=True, kw_only=True)
class ReceiverTestCase:
    """A test case for testing new_receiver configurations."""

    title: str
    key: str | tuple[str, ...]
    config_class: type[SimpleConfig]
    input_config: dict[str, Any]
    expected_output: Any | None
    base_schema: type[marshmallow.Schema] | None = None
    marshmallow_load_kwargs: dict[str, Any] | None = None


# Test cases for new_receiver
receiver_test_cases = [
    ReceiverTestCase(
        title="Basic Config",
        key="test",
        config_class=SimpleConfig,
        input_config={"test": {"name": "test1", "value": 42}},
        expected_output=SimpleConfig(name="test1", value=42),
    ),
    ReceiverTestCase(
        title="Nested Key Config",
        key=("nested", "config"),
        config_class=SimpleConfig,
        input_config={"nested": {"config": {"name": "test2", "value": 43}}},
        expected_output=SimpleConfig(name="test2", value=43),
    ),
    ReceiverTestCase(
        title="Validation Error",
        key="test",
        config_class=SimpleConfig,
        input_config={"test": {"name": "no-test1", "value": 42}},
        expected_output="{'name': ['Invalid value.']}",
    ),
    ReceiverTestCase(
        title="Invalid Value Type",
        key="test",
        config_class=SimpleConfig,
        input_config={"test": "not a mapping"},
        expected_output="Value for key ['test'] is not a mapping: 'not a mapping'",
    ),
    ReceiverTestCase(
        title="Raise on unknown",
        key="test",
        config_class=SimpleConfig,
        marshmallow_load_kwargs={"unknown": marshmallow.RAISE},
        input_config={"test": {"name": "test3", "value": 44, "not_allowed": 42}},
        expected_output="{'not_allowed': ['Unknown field.']}",
    ),
    ReceiverTestCase(
        title="Missing Key",
        key="missing",
        config_class=SimpleConfig,
        input_config={"test": {"name": "test3", "value": 44}},
        expected_output=None,
    ),
]


@pytest.mark.parametrize("test_case", receiver_test_cases, ids=lambda tc: tc.title)
async def test_new_receiver_configurations(
    test_case: ReceiverTestCase, mocker: pytest_mock.MockFixture
) -> None:
    """Test different configurations for new_receiver."""
    mocker.patch("frequenz.sdk.config._manager.ConfigManagingActor")
    config_manager = ConfigManager([pathlib.Path("dummy.toml")])
    await config_manager.config_channel.new_sender().send(test_case.input_config)
    receiver = config_manager.new_receiver(
        test_case.key,
        test_case.config_class,
        base_schema=test_case.base_schema,
        marshmallow_load_kwargs=test_case.marshmallow_load_kwargs,
    )

    async with asyncio.timeout(1):
        result = await receiver.receive()
    match result:
        case SimpleConfig() | None:
            assert result == test_case.expected_output
        case Exception():
            assert str(result) == str(test_case.expected_output)
        case unexpected:
            assert_never(unexpected)


async def test_warn_on_unknown_key(
    mocker: pytest_mock.MockerFixture, caplog: pytest.LogCaptureFixture
) -> None:
    """Test that a warning is logged when an unknown key is received."""
    mocker.patch("frequenz.sdk.config._manager.ConfigManagingActor")
    config_manager = ConfigManager([pathlib.Path("dummy.toml")])
    await config_manager.config_channel.new_sender().send(
        {"test": {"name": "test3", "value": 44, "not_allowed": 42}}
    )
    receiver = config_manager.new_receiver("test", SimpleConfig)

    async with asyncio.timeout(1):
        await receiver.receive()

    expected_log_entry = (
        "frequenz.sdk.config._manager",
        logging.WARNING,
        "The configuration for key 'test' has extra fields that will be ignored: "
        "{'not_allowed': ['Unknown field.']}",
    )
    assert expected_log_entry in caplog.record_tuples


async def test_skip_config_update_bursts(mocker: pytest_mock.MockerFixture) -> None:
    """Test that a burst of updates will only send the last update."""
    mocker.patch("frequenz.sdk.config._manager.ConfigManagingActor")
    config_manager = ConfigManager([pathlib.Path("dummy.toml")])
    sender = config_manager.config_channel.new_sender()
    receiver = config_manager.new_receiver(
        "test",
        SimpleConfig,
        skip_unchanged=True,
    )

    await sender.send({"test": {"name": "test1", "value": 42}})
    await sender.send({"test": {"name": "test2", "value": 43}})
    await sender.send({"test": {"name": "test3", "value": 44}})

    # Should only receive one orig_config and then the changed_config
    async with asyncio.timeout(1):
        result = await receiver.receive()
    assert result == SimpleConfig(name="test3", value=44)

    # There should be no more messages
    with pytest.raises(asyncio.TimeoutError):
        async with asyncio.timeout(0.1):
            await receiver.receive()


async def test_skip_unchanged_config(mocker: pytest_mock.MockerFixture) -> None:
    """Test that unchanged configurations are skipped when skip_unchanged is True."""
    mocker.patch("frequenz.sdk.config._manager.ConfigManagingActor")
    config_manager = ConfigManager([pathlib.Path("dummy.toml")])
    sender = config_manager.config_channel.new_sender()
    receiver = config_manager.new_receiver(
        "test",
        SimpleConfig,
        skip_unchanged=True,
    )

    # A first config should be received
    orig_config = {"test": {"name": "test1", "value": 42}}
    await sender.send(orig_config)
    async with asyncio.timeout(1):
        result = await receiver.receive()
    assert result == SimpleConfig(name="test1", value=42)

    # An unchanged config should be skipped (no message received)
    await sender.send(orig_config)
    with pytest.raises(asyncio.TimeoutError):
        async with asyncio.timeout(0.1):
            await receiver.receive()

    # A changed config should be received
    changed_config = {"test": {"name": "test2", "value": 43}}
    await sender.send(changed_config)
    async with asyncio.timeout(1):
        result = await receiver.receive()
    assert result == SimpleConfig(name="test2", value=43)

    # There should be no more messages
    with pytest.raises(asyncio.TimeoutError):
        async with asyncio.timeout(0.1):
            await receiver.receive()


async def test_wait_for_first(mocker: pytest_mock.MockerFixture) -> None:
    """Test wait_for_first function."""
    mocker.patch("frequenz.sdk.config._manager.ConfigManagingActor")
    config_manager = ConfigManager([pathlib.Path("dummy.toml")])

    receiver = config_manager.new_receiver(
        "test",
        SimpleConfig,
    )

    async with asyncio.timeout(0.2):
        with pytest.raises(asyncio.TimeoutError):
            await wait_for_first(receiver, timeout=timedelta(seconds=0.1))

    # Test successful wait
    await config_manager.config_channel.new_sender().send(
        {"test": {"name": "test1", "value": 42}}
    )
    async with asyncio.timeout(0.2):
        result = await wait_for_first(receiver, timeout=timedelta(seconds=0.1))
    assert result == SimpleConfig(name="test1", value=42)


def test_unknown_include_not_supported() -> None:
    """Test that unknown marshmallow load kwargs are not supported."""
    with pytest.raises(ValueError):
        ConfigManager([pathlib.Path("dummy.toml")]).new_receiver(
            "test",
            SimpleConfig,
            marshmallow_load_kwargs={"unknown": marshmallow.INCLUDE},
        )


@pytest.mark.integration
class TestConfigManagerIntegration:
    """Integration tests for ConfigManager."""

    @pytest.fixture
    def config_file(self, tmp_path: pathlib.Path) -> pathlib.Path:
        """Create a temporary config file for testing."""
        config_file = tmp_path / "config.toml"
        config_file.write_text(
            """
            [test]
            name = "test1"
            value = 42

            [logging.loggers.test]
            name = "test"
            level = "DEBUG"
            """
        )
        return config_file

    async def test_full_config_flow(self, config_file: pathlib.Path) -> None:
        """Test the complete flow of configuration management."""
        async with (
            # Disabling force_polling is a hack because of a bug in watchfiles not
            # detecting sub-second changes when using polling.
            ConfigManager([config_file], force_polling=False) as config_manager,
            asyncio.timeout(1),
        ):
            receiver = config_manager.new_receiver("test", SimpleConfig)
            first_config = await wait_for_first(receiver)
            assert first_config == SimpleConfig(name="test1", value=42)
            assert logging.getLogger("test").level == logging.DEBUG

            # Update config file
            config_file.write_text(
                """
                [test]
                name = "test2"
                value = 43

                [logging.loggers.test]
                name = "test"
                level = "INFO"
                """
            )

            # Check updated config
            config = await receiver.receive()
            assert config == SimpleConfig(name="test2", value=43)

            # Check updated logging config
            assert logging.getLogger("test").level == logging.INFO

    async def test_full_config_flow_without_logging(
        self, config_file: pathlib.Path
    ) -> None:
        """Test the complete flow of configuration management without logging."""
        logging.getLogger("test").setLevel(logging.WARNING)
        async with (
            # Disabling force_polling is a hack because of a bug in watchfiles not
            # detecting sub-second changes when using polling.
            ConfigManager(
                [config_file], logging_config_key=None, force_polling=False
            ) as config_manager,
            asyncio.timeout(1),
        ):
            receiver = config_manager.new_receiver("test", SimpleConfig)
            first_config = await wait_for_first(receiver)
            assert first_config == SimpleConfig(name="test1", value=42)
            assert logging.getLogger("test").level == logging.WARNING

            # Update config file
            config_file.write_text(
                """
                [test]
                name = "test2"
                value = 43

                [logging.loggers.test]
                name = "test"
                level = "DEBUG"
                """
            )

            # Check updated config
            config = await receiver.receive()
            assert config == SimpleConfig(name="test2", value=43)

            # Check updated logging config
            assert logging.getLogger("test").level == logging.WARNING


@dataclass(frozen=True)
class GetKeyTestCase:
    """Test case for _get_key function."""

    title: str
    config: dict[str, Any]
    key: str | Sequence[str]
    expected_result: Mapping[str, Any] | None | type[InvalidValueForKeyError]
    expected_error_key: list[str] | None = None
    expected_error_value: Any | None = None


_get_key_test_cases = [
    # Simple string key tests
    GetKeyTestCase(
        title="Simple string key - exists",
        config={"a": {"b": 1}},
        key="a",
        expected_result={"b": 1},
    ),
    GetKeyTestCase(
        title="Simple string key - doesn't exist",
        config={"a": {"b": 1}},
        key="x",
        expected_result=None,
    ),
    GetKeyTestCase(
        title="Simple string key - invalid value type",
        config={"a": 42},
        key="a",
        expected_result=InvalidValueForKeyError,
        expected_error_key=["a"],
        expected_error_value=42,
    ),
    # Sequence key tests
    GetKeyTestCase(
        title="Sequence key - all exist",
        config={"a": {"b": {"c": {"d": 1}}}},
        key=["a", "b", "c"],
        expected_result={"d": 1},
    ),
    GetKeyTestCase(
        title="Sequence key - middle doesn't exist",
        config={"a": {"b": {"c": 1}}},
        key=["a", "x", "c"],
        expected_result=None,
    ),
    GetKeyTestCase(
        title="Sequence key - invalid value in middle",
        config={"a": {"b": 42, "c": 1}},
        key=["a", "b", "c"],
        expected_result=InvalidValueForKeyError,
        expected_error_key=["a", "b"],
        expected_error_value=42,
    ),
    GetKeyTestCase(
        title="Empty sequence key",
        config={"a": 1},
        key=[],
        expected_result={"a": 1},
    ),
    # Empty string tests
    GetKeyTestCase(
        title="Empty string key",
        config={"": {"a": 1}},
        key="",
        expected_result={"a": 1},
    ),
    GetKeyTestCase(
        title="Empty string in sequence",
        config={"": {"": {"a": 1}}},
        key=["", ""],
        expected_result={"a": 1},
    ),
    # None value tests
    GetKeyTestCase(
        title="Value is None",
        config={"a": None},
        key="a",
        expected_result=None,
    ),
    GetKeyTestCase(
        title="Nested None value",
        config={"a": {"b": None}},
        key=["a", "b", "c"],
        expected_result=None,
    ),
    # Special string key tests to verify string handling
    GetKeyTestCase(
        title="String that looks like a sequence",
        config={"key": {"e": 1}},
        key="key",
        expected_result={"e": 1},
    ),
    GetKeyTestCase(
        title="String with special characters",
        config={"a.b": {"c": 1}},
        key="a.b",
        expected_result={"c": 1},
    ),
    # Nested mapping tests
    GetKeyTestCase(
        title="Deeply nested valid path",
        config={"a": {"b": {"c": {"d": {"e": {"f": 1}}}}}},
        key=["a", "b", "c", "d", "e"],
        expected_result={"f": 1},
    ),
    GetKeyTestCase(
        title="Mixed type nested invalid",
        config={"a": {"b": [1, 2, 3]}},
        key=["a", "b"],
        expected_result=InvalidValueForKeyError,
        expected_error_key=["a", "b"],
        expected_error_value=[1, 2, 3],
    ),
]


@pytest.mark.parametrize("test_case", _get_key_test_cases, ids=lambda tc: tc.title)
def test_get_key(test_case: GetKeyTestCase) -> None:
    """Test the _get_key function with various inputs.

    Args:
        test_case: The test case to run.
    """
    if test_case.expected_result is InvalidValueForKeyError:
        with pytest.raises(InvalidValueForKeyError) as exc_info:
            _get_key(test_case.config, test_case.key)

        # Verify the error details
        assert exc_info.value.key == test_case.expected_error_key
        assert exc_info.value.value == test_case.expected_error_value
    else:
        result = _get_key(test_case.config, test_case.key)
        assert result == test_case.expected_result



================================================
FILE: tests/config/test_util.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Tests for the config utilities."""

import dataclasses
from typing import Any

import marshmallow
import marshmallow_dataclass
import pytest
from pytest_mock import MockerFixture

from frequenz.sdk.config._util import load_config


@dataclasses.dataclass
class SimpleConfig:
    """A simple configuration class for testing."""

    name: str = dataclasses.field(metadata={"validate": lambda s: s.startswith("test")})
    value: int


@marshmallow_dataclass.dataclass
class MmSimpleConfig:
    """A simple marshmallow_dataclass configuration class for testing."""

    name: str = dataclasses.field(metadata={"validate": lambda s: s.startswith("test")})
    value: int


@pytest.mark.parametrize(
    "config_class",
    [SimpleConfig, MmSimpleConfig],
    ids=["dataclass", "marshmallow_dataclass"],
)
def test_load_config_dataclass(
    config_class: type[SimpleConfig] | type[MmSimpleConfig],
) -> None:
    """Test that load_config loads a configuration into a configuration class."""
    config: dict[str, Any] = {"name": "test", "value": 42}

    loaded_config = load_config(config_class, config)
    assert loaded_config == config_class(name="test", value=42)

    config["name"] = "not test"
    with pytest.raises(marshmallow.ValidationError):
        _ = load_config(config_class, config)


@pytest.mark.parametrize(
    "config_class",
    [SimpleConfig, MmSimpleConfig],
    ids=["dataclass", "marshmallow_dataclass"],
)
@pytest.mark.parametrize("config", [dict(), None], ids=["empty", "none"])
def test_load_config_load_empty(
    config_class: type[SimpleConfig] | type[MmSimpleConfig],
    config: dict[str, Any] | None,
) -> None:
    """Test that load_config raises ValidationError if the configuration is empty."""
    with pytest.raises(marshmallow.ValidationError):
        _ = load_config(config_class, config)  # type: ignore[arg-type]


@pytest.mark.parametrize(
    "config_class",
    [SimpleConfig, MmSimpleConfig],
    ids=["dataclass", "marshmallow_dataclass"],
)
def test_load_config_with_base_schema(
    config_class: type[SimpleConfig] | type[MmSimpleConfig],
) -> None:
    """Test that load_config loads a configuration using a base schema."""

    class _MyBaseSchema(marshmallow.Schema):
        """A base schema for testing."""

        class Meta:
            """Meta options for the schema."""

            unknown = marshmallow.RAISE

    config: dict[str, Any] = {"name": "test", "value": 42, "extra": "extra"}

    loaded_config = load_config(config_class, config)
    assert loaded_config == config_class(name="test", value=42)

    with pytest.raises(marshmallow.ValidationError):
        _ = load_config(config_class, config, base_schema=_MyBaseSchema)


def test_load_config_type_hints(mocker: MockerFixture) -> None:
    """Test that load_config loads a configuration into a configuration class."""
    mock_class_schema = mocker.Mock()
    mock_class_schema.return_value.load.return_value = {"name": "test", "value": 42}
    mocker.patch(
        "frequenz.sdk.config._util.class_schema", return_value=mock_class_schema
    )
    config: dict[str, Any] = {}

    # We add the type hint to test that the return type (hint) is correct
    _: SimpleConfig = load_config(
        SimpleConfig, config, marshmallow_load_kwargs={"marshmallow_arg": 1}
    )
    mock_class_schema.return_value.load.assert_called_once_with(
        config, marshmallow_arg=1
    )



================================================
FILE: tests/microgrid/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Tests for the microgrid package."""



================================================
FILE: tests/microgrid/fixtures.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Mock fixture for mocking the microgrid along with streaming."""

from __future__ import annotations

import asyncio
import dataclasses
from contextlib import asynccontextmanager
from datetime import timedelta
from typing import AsyncIterator

from frequenz.channels import Sender
from frequenz.client.microgrid import ComponentCategory
from pytest_mock import MockerFixture

from frequenz.sdk import microgrid
from frequenz.sdk.microgrid._power_distributing import ComponentPoolStatus
from frequenz.sdk.microgrid.component_graph import _MicrogridComponentGraph
from frequenz.sdk.timeseries import ResamplerConfig2

from ..timeseries.mock_microgrid import MockMicrogrid
from ..utils.component_data_streamer import MockComponentDataStreamer


@dataclasses.dataclass(frozen=True)
class _Mocks:
    """Mocks for the tests."""

    microgrid: MockMicrogrid
    """A mock microgrid instance."""

    streamer: MockComponentDataStreamer
    """A mock component data streamer."""

    component_status_sender: Sender[ComponentPoolStatus]
    """Sender for sending status of the components being tested."""

    @classmethod
    async def new(
        cls,
        component_category: ComponentCategory,
        mocker: MockerFixture,
        graph: _MicrogridComponentGraph | None = None,
        grid_meter: bool | None = None,
    ) -> _Mocks:
        """Initialize the mocks."""
        mockgrid = MockMicrogrid(graph=graph, grid_meter=grid_meter, mocker=mocker)
        if not graph:
            mockgrid.add_batteries(3)
        await mockgrid.start()

        # pylint: disable=protected-access
        if microgrid._data_pipeline._DATA_PIPELINE is not None:
            microgrid._data_pipeline._DATA_PIPELINE = None
        await microgrid._data_pipeline.initialize(
            ResamplerConfig2(resampling_period=timedelta(seconds=0.1))
        )
        streamer = MockComponentDataStreamer(mockgrid.mock_client)

        dp = microgrid._data_pipeline._DATA_PIPELINE
        assert dp is not None

        if component_category == ComponentCategory.BATTERY:
            return cls(
                mockgrid,
                streamer,
                dp._battery_power_wrapper.status_channel.new_sender(),
            )
        if component_category == ComponentCategory.EV_CHARGER:
            return cls(
                mockgrid,
                streamer,
                dp._ev_power_wrapper.status_channel.new_sender(),
            )
        raise ValueError(f"Unsupported component category: {component_category}")

    async def stop(self) -> None:
        """Stop the mocks."""
        # pylint: disable=protected-access
        assert microgrid._data_pipeline._DATA_PIPELINE is not None
        await asyncio.gather(
            *[
                microgrid._data_pipeline._DATA_PIPELINE._stop(),
                self.streamer.stop(),
                self.microgrid.cleanup(),
            ]
        )
        # pylint: enable=protected-access


@asynccontextmanager
async def _mocks(
    mocker: MockerFixture,
    component_category: ComponentCategory,
    *,
    graph: _MicrogridComponentGraph | None = None,
    grid_meter: bool | None = None,
) -> AsyncIterator[_Mocks]:
    """Initialize the mocks."""
    mocks = await _Mocks.new(
        component_category, mocker, graph=graph, grid_meter=grid_meter
    )
    try:
        yield mocks
    finally:
        await mocks.stop()



================================================
FILE: tests/microgrid/test_data_sourcing.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Tests for the DataSourcingActor."""

import asyncio
from collections.abc import AsyncIterator, Callable
from datetime import datetime, timezone
from typing import TypeVar
from unittest import mock

import pytest
import pytest_mock
from frequenz.channels import Broadcast
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    BatteryComponentState,
    BatteryData,
    BatteryRelayState,
    Component,
    ComponentCategory,
    ComponentData,
    ComponentMetricId,
    EVChargerCableState,
    EVChargerComponentState,
    EVChargerData,
    InverterComponentState,
    InverterData,
    MeterData,
)
from frequenz.quantities import Quantity

from frequenz.sdk._internal._channels import ChannelRegistry
from frequenz.sdk.microgrid._data_sourcing import (
    ComponentMetricRequest,
    DataSourcingActor,
)
from frequenz.sdk.timeseries import Sample

T = TypeVar("T", bound=ComponentData)


@pytest.fixture
def mock_connection_manager(mocker: pytest_mock.MockFixture) -> mock.Mock:
    """Fixture for getting a mock connection manager."""
    mock_client = mock.MagicMock(name="connection_manager.get().api_client")
    mock_client.components = mock.AsyncMock(
        name="components()",
        return_value=[
            Component(component_id=ComponentId(4), category=ComponentCategory.METER),
            Component(component_id=ComponentId(6), category=ComponentCategory.INVERTER),
            Component(component_id=ComponentId(9), category=ComponentCategory.BATTERY),
            Component(
                component_id=ComponentId(12), category=ComponentCategory.EV_CHARGER
            ),
        ],
    )
    mock_client.meter_data = _new_meter_data_mock(ComponentId(4), starting_value=100.0)
    mock_client.inverter_data = _new_inverter_data_mock(
        ComponentId(6), starting_value=0.0
    )
    mock_client.battery_data = _new_battery_data_mock(
        ComponentId(9), starting_value=9.0
    )
    mock_client.ev_charger_data = _new_ev_charger_data_mock(
        ComponentId(12), starting_value=-13.0
    )
    mock_conn_manager = mock.MagicMock(name="connection_manager")
    mocker.patch(
        "frequenz.sdk.microgrid._data_sourcing"
        ".microgrid_api_source.connection_manager.get",
        return_value=mock_conn_manager,
    )
    mock_conn_manager.api_client = mock_client
    return mock_conn_manager


async def test_data_sourcing_actor(  # pylint: disable=too-many-locals
    mock_connection_manager: mock.Mock,  # pylint: disable=redefined-outer-name,unused-argument
) -> None:
    """Tests for the DataSourcingActor."""
    req_chan = Broadcast[ComponentMetricRequest](name="data_sourcing_requests")
    req_sender = req_chan.new_sender()

    registry = ChannelRegistry(name="test-registry")

    async with DataSourcingActor(req_chan.new_receiver(), registry):
        active_power_request_4 = ComponentMetricRequest(
            "test-namespace", ComponentId(4), ComponentMetricId.ACTIVE_POWER, None
        )
        active_power_recv_4 = registry.get_or_create(
            Sample[Quantity], active_power_request_4.get_channel_name()
        ).new_receiver()
        await req_sender.send(active_power_request_4)

        reactive_power_request_4 = ComponentMetricRequest(
            "test-namespace", ComponentId(4), ComponentMetricId.REACTIVE_POWER, None
        )
        reactive_power_recv_4 = registry.get_or_create(
            Sample[Quantity], reactive_power_request_4.get_channel_name()
        ).new_receiver()
        await req_sender.send(reactive_power_request_4)

        active_power_request_6 = ComponentMetricRequest(
            "test-namespace", ComponentId(6), ComponentMetricId.ACTIVE_POWER, None
        )
        active_power_recv_6 = registry.get_or_create(
            Sample[Quantity], active_power_request_6.get_channel_name()
        ).new_receiver()
        await req_sender.send(active_power_request_6)

        soc_request_9 = ComponentMetricRequest(
            "test-namespace", ComponentId(9), ComponentMetricId.SOC, None
        )
        soc_recv_9 = registry.get_or_create(
            Sample[Quantity], soc_request_9.get_channel_name()
        ).new_receiver()
        await req_sender.send(soc_request_9)

        soc2_request_9 = ComponentMetricRequest(
            "test-namespace", ComponentId(9), ComponentMetricId.SOC, None
        )
        soc2_recv_9 = registry.get_or_create(
            Sample[Quantity], soc2_request_9.get_channel_name()
        ).new_receiver()
        await req_sender.send(soc2_request_9)

        active_power_request_12 = ComponentMetricRequest(
            "test-namespace", ComponentId(12), ComponentMetricId.ACTIVE_POWER, None
        )
        active_power_recv_12 = registry.get_or_create(
            Sample[Quantity], active_power_request_12.get_channel_name()
        ).new_receiver()
        await req_sender.send(active_power_request_12)

        for i in range(3):
            sample = await active_power_recv_4.receive()
            assert sample.value is not None
            assert 100.0 + i == sample.value.base_value

            sample = await reactive_power_recv_4.receive()
            assert sample.value is not None
            assert 100.0 + i == sample.value.base_value

            sample = await active_power_recv_6.receive()
            assert sample.value is not None
            assert 0.0 + i == sample.value.base_value

            sample = await soc_recv_9.receive()
            assert sample.value is not None
            assert 9.0 + i == sample.value.base_value

            sample = await soc2_recv_9.receive()
            assert sample.value is not None
            assert 9.0 + i == sample.value.base_value

            sample = await active_power_recv_12.receive()
            assert sample.value is not None
            assert -13.0 + i == sample.value.base_value


def _new_meter_data(
    component_id: ComponentId, timestamp: datetime, value: float
) -> MeterData:
    return MeterData(
        component_id=component_id,
        timestamp=timestamp,
        active_power=value,
        active_power_per_phase=(value, value, value),
        current_per_phase=(value, value, value),
        frequency=value,
        reactive_power=value,
        reactive_power_per_phase=(value, value, value),
        voltage_per_phase=(value, value, value),
    )


def _new_inverter_data(
    component_id: ComponentId, timestamp: datetime, value: float
) -> InverterData:
    return InverterData(
        component_id=component_id,
        timestamp=timestamp,
        active_power=value,
        reactive_power=value,
        frequency=value,
        reactive_power_per_phase=(value, value, value),
        active_power_per_phase=(value, value, value),
        current_per_phase=(value, value, value),
        voltage_per_phase=(value, value, value),
        active_power_exclusion_lower_bound=value,
        active_power_exclusion_upper_bound=value,
        active_power_inclusion_lower_bound=value,
        active_power_inclusion_upper_bound=value,
        component_state=InverterComponentState.UNSPECIFIED,
        errors=[],
    )


def _new_battery_data(
    component_id: ComponentId, timestamp: datetime, value: float
) -> BatteryData:
    return BatteryData(
        component_id=component_id,
        timestamp=timestamp,
        soc=value,
        temperature=value,
        component_state=BatteryComponentState.UNSPECIFIED,
        errors=[],
        soc_lower_bound=value,
        soc_upper_bound=value,
        capacity=value,
        power_exclusion_lower_bound=value,
        power_exclusion_upper_bound=value,
        power_inclusion_lower_bound=value,
        power_inclusion_upper_bound=value,
        relay_state=BatteryRelayState.UNSPECIFIED,
    )


def _new_ev_charger_data(
    component_id: ComponentId, timestamp: datetime, value: float
) -> EVChargerData:
    return EVChargerData(
        component_id=component_id,
        timestamp=timestamp,
        active_power=value,
        active_power_per_phase=(value, value, value),
        current_per_phase=(value, value, value),
        frequency=value,
        reactive_power=value,
        reactive_power_per_phase=(value, value, value),
        voltage_per_phase=(value, value, value),
        active_power_exclusion_lower_bound=value,
        active_power_exclusion_upper_bound=value,
        active_power_inclusion_lower_bound=value,
        active_power_inclusion_upper_bound=value,
        cable_state=EVChargerCableState.UNSPECIFIED,
        component_state=EVChargerComponentState.UNSPECIFIED,
    )


def _new_streamer_mock(
    name: str,
    constructor: Callable[[ComponentId, datetime, float], T],
    component_id: ComponentId,
    starting_value: float,
) -> mock.AsyncMock:
    """Get a mock streamer."""

    async def generate_data(starting_value: float) -> AsyncIterator[T]:
        value = starting_value
        while True:
            yield constructor(component_id, datetime.now(timezone.utc), value)
            await asyncio.sleep(0)  # Let other tasks run
            value += 1.0

    return mock.AsyncMock(name=name, return_value=generate_data(starting_value))


def _new_meter_data_mock(
    component_id: ComponentId, starting_value: float
) -> mock.AsyncMock:
    """Get a mock streamer for meter data."""
    return _new_streamer_mock(
        f"meter_data_mock(id={component_id}, starting_value={starting_value})",
        _new_meter_data,
        component_id,
        starting_value,
    )


def _new_inverter_data_mock(
    component_id: ComponentId, starting_value: float
) -> mock.AsyncMock:
    """Get a mock streamer for inverter data."""
    return _new_streamer_mock(
        f"inverter_data_mock(id={component_id}, starting_value={starting_value})",
        _new_inverter_data,
        component_id,
        starting_value,
    )


def _new_battery_data_mock(
    component_id: ComponentId, starting_value: float
) -> mock.AsyncMock:
    """Get a mock streamer for battery data."""
    return _new_streamer_mock(
        f"battery_data_mock(id={component_id}, starting_value={starting_value})",
        _new_battery_data,
        component_id,
        starting_value,
    )


def _new_ev_charger_data_mock(
    component_id: ComponentId, starting_value: float
) -> mock.AsyncMock:
    """Get a mock streamer for EV charger data."""
    return _new_streamer_mock(
        f"ev_charger_data_mock(id={component_id}, starting_value={starting_value})",
        _new_ev_charger_data,
        component_id,
        starting_value,
    )



================================================
FILE: tests/microgrid/test_datapipeline.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Basic tests for the DataPipeline."""

import asyncio
from datetime import timedelta

import async_solipsism
import pytest
import time_machine
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    Component,
    ComponentCategory,
    Connection,
    InverterType,
)
from pytest_mock import MockerFixture

from frequenz.sdk.microgrid._data_pipeline import _DataPipeline
from frequenz.sdk.timeseries import ResamplerConfig2

from ..utils.mock_microgrid_client import MockMicrogridClient


@pytest.fixture(autouse=True)
def event_loop_policy() -> async_solipsism.EventLoopPolicy:
    """Return an event loop policy that uses the async solipsism event loop."""
    return async_solipsism.EventLoopPolicy()


# loop time is advanced but not the system time
async def test_actors_started(
    fake_time: time_machine.Coordinates, mocker: MockerFixture
) -> None:
    """Test that the datasourcing, resampling and power distributing actors are started."""
    datapipeline = _DataPipeline(
        resampler_config=ResamplerConfig2(resampling_period=timedelta(seconds=1))
    )
    await asyncio.sleep(1)

    # pylint: disable=protected-access
    assert datapipeline._data_sourcing_actor is None
    assert datapipeline._resampling_actor is None
    assert datapipeline._battery_power_wrapper._power_distributing_actor is None

    datapipeline.logical_meter()

    assert datapipeline._data_sourcing_actor is not None
    assert datapipeline._data_sourcing_actor.actor is not None
    await asyncio.sleep(1)
    fake_time.shift(timedelta(seconds=1))
    assert datapipeline._data_sourcing_actor.actor.is_running

    assert datapipeline._resampling_actor is not None
    assert datapipeline._resampling_actor.actor is not None
    assert datapipeline._resampling_actor.actor.is_running

    assert datapipeline._battery_power_wrapper._power_distributing_actor is None

    mock_client = MockMicrogridClient(
        {
            Component(ComponentId(1), ComponentCategory.GRID),
            Component(ComponentId(4), ComponentCategory.INVERTER, InverterType.BATTERY),
            Component(ComponentId(15), ComponentCategory.BATTERY),
        },
        connections={
            Connection(ComponentId(1), ComponentId(4)),
            Connection(ComponentId(4), ComponentId(15)),
        },
    )
    mock_client.initialize(mocker)

    datapipeline.new_battery_pool(priority=5)

    assert datapipeline._battery_power_wrapper._power_distributing_actor is not None
    await asyncio.sleep(1)
    assert datapipeline._battery_power_wrapper._power_distributing_actor.is_running

    await datapipeline._stop()



================================================
FILE: tests/microgrid/test_grid.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests for the `Grid` module."""

from contextlib import AsyncExitStack

import frequenz.client.microgrid as client
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentCategory
from frequenz.quantities import Current, Power, Quantity, ReactivePower
from pytest_mock import MockerFixture

import frequenz.sdk.microgrid.component_graph as gr
from frequenz.sdk import microgrid
from frequenz.sdk.timeseries import Fuse
from tests.utils.graph_generator import GraphGenerator

from ..timeseries._formula_engine.utils import equal_float_lists, get_resampled_stream
from ..timeseries.mock_microgrid import MockMicrogrid


async def test_grid_1(mocker: MockerFixture) -> None:
    """Test the grid connection module."""
    # The tests here need to be in this exact sequence, because the grid connection
    # is a singleton. Once it gets created, it stays in memory for the duration of
    # the tests, unless we explicitly delete it.

    # validate that islands with no grid connection are accepted.
    components = {
        client.Component(ComponentId(1), client.ComponentCategory.NONE),
        client.Component(ComponentId(2), client.ComponentCategory.METER),
    }
    connections = {
        client.Connection(ComponentId(1), ComponentId(2)),
    }

    graph = gr._MicrogridComponentGraph(  # pylint: disable=protected-access
        components=components, connections=connections
    )

    async with MockMicrogrid(graph=graph, mocker=mocker), AsyncExitStack() as stack:
        grid = microgrid.grid()
        assert grid is not None
        stack.push_async_callback(grid.stop)

        assert grid
        assert grid.fuse
        assert grid.fuse.max_current == Current.from_amperes(0.0)


async def test_grid_2(mocker: MockerFixture) -> None:
    """Validate that microgrids with one grid connection are accepted."""
    components = {
        client.Component(
            ComponentId(1),
            client.ComponentCategory.GRID,
            None,
            client.ComponentMetadata(fuse=client.Fuse(max_current=123.0)),
        ),
        client.Component(ComponentId(2), client.ComponentCategory.METER),
    }
    connections = {
        client.Connection(ComponentId(1), ComponentId(2)),
    }

    graph = gr._MicrogridComponentGraph(  # pylint: disable=protected-access
        components=components, connections=connections
    )

    async with MockMicrogrid(graph=graph, mocker=mocker), AsyncExitStack() as stack:
        grid = microgrid.grid()
        assert grid is not None
        stack.push_async_callback(grid.stop)

        assert grid.fuse == Fuse(max_current=Current.from_amperes(123.0))


async def test_grid_3(mocker: MockerFixture) -> None:
    """Validate that microgrids with a grid connection without a fuse are instantiated."""
    components = {
        client.Component(
            ComponentId(1),
            client.ComponentCategory.GRID,
            None,
            client.GridMetadata(None),
        ),
        client.Component(ComponentId(2), client.ComponentCategory.METER),
    }
    connections = {client.Connection(ComponentId(1), ComponentId(2))}

    graph = gr._MicrogridComponentGraph(  # pylint: disable=protected-access
        components=components, connections=connections
    )

    async with MockMicrogrid(graph=graph, mocker=mocker), AsyncExitStack() as stack:
        grid = microgrid.grid()
        assert grid is not None
        stack.push_async_callback(grid.stop)
        assert grid.fuse is None


async def test_grid_power_1(mocker: MockerFixture) -> None:
    """Test the grid power formula with a grid side meter."""
    mockgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
    mockgrid.add_batteries(2)
    mockgrid.add_solar_inverters(1)

    results = []
    grid_meter_data = []
    async with mockgrid, AsyncExitStack() as stack:
        grid = microgrid.grid()
        assert grid, "Grid is not initialized"
        stack.push_async_callback(grid.stop)

        grid_power_recv = grid.power.new_receiver()

        grid_meter_recv = get_resampled_stream(
            grid._formula_pool._namespace,  # pylint: disable=protected-access
            mockgrid.meter_ids[0],
            client.ComponentMetricId.ACTIVE_POWER,
            Power.from_watts,
        )

        for count in range(10):
            await mockgrid.mock_resampler.send_meter_power(
                [20.0 + count, 12.0, -13.0, -5.0]
            )
            val = await grid_meter_recv.receive()
            assert (
                val is not None
                and val.value is not None
                and val.value.as_watts() != 0.0
            )
            grid_meter_data.append(val.value)

            val = await grid_power_recv.receive()
            assert val is not None and val.value is not None
            results.append(val.value)

    assert equal_float_lists(results, grid_meter_data)


async def test_grid_power_2(mocker: MockerFixture) -> None:
    """Test the grid power formula without a grid side meter."""
    mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
    mockgrid.add_consumer_meters(1)
    mockgrid.add_batteries(1, no_meter=False)
    mockgrid.add_batteries(1, no_meter=True)
    mockgrid.add_solar_inverters(1)

    results: list[Quantity] = []
    meter_sums: list[Quantity] = []
    async with mockgrid, AsyncExitStack() as stack:
        grid = microgrid.grid()
        assert grid, "Grid is not initialized"
        stack.push_async_callback(grid.stop)

        grid_power_recv = grid.power.new_receiver()

        component_receivers = [
            get_resampled_stream(
                grid._formula_pool._namespace,  # pylint: disable=protected-access
                component_id,
                client.ComponentMetricId.ACTIVE_POWER,
                Power.from_watts,
            )
            for component_id in [
                *mockgrid.meter_ids,
                # The last battery has no meter, so we get the power from the inverter
                mockgrid.battery_inverter_ids[-1],
            ]
        ]

        for count in range(10):
            await mockgrid.mock_resampler.send_meter_power([20.0 + count, 12.0, -13.0])
            await mockgrid.mock_resampler.send_bat_inverter_power([0.0, -5.0])
            meter_sum = 0.0
            for recv in component_receivers:
                val = await recv.receive()
                assert (
                    val is not None
                    and val.value is not None
                    and val.value.as_watts() != 0.0
                )
                meter_sum += val.value.as_watts()

            val = await grid_power_recv.receive()
            assert val is not None and val.value is not None
            results.append(val.value)
            meter_sums.append(Quantity(meter_sum))

    assert len(results) == 10
    assert equal_float_lists(results, meter_sums)


async def test_grid_reactive_power_1(mocker: MockerFixture) -> None:
    """Test the grid power formula with a grid side meter."""
    mockgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
    mockgrid.add_batteries(2)
    mockgrid.add_solar_inverters(1)

    results = []
    grid_meter_data = []
    async with mockgrid, AsyncExitStack() as stack:
        grid = microgrid.grid()
        assert grid, "Grid is not initialized"
        stack.push_async_callback(grid.stop)

        grid_power_recv = grid.reactive_power.new_receiver()

        grid_meter_recv = get_resampled_stream(
            grid._formula_pool._namespace,  # pylint: disable=protected-access
            mockgrid.meter_ids[0],
            client.ComponentMetricId.REACTIVE_POWER,
            ReactivePower.from_volt_amperes_reactive,
        )

        for count in range(10):
            await mockgrid.mock_resampler.send_meter_reactive_power(
                [20.0 + count, 12.0, -13.0, -5.0]
            )
            val = await grid_meter_recv.receive()
            assert (
                val is not None
                and val.value is not None
                and val.value.as_volt_amperes_reactive() != 0.0
            )
            grid_meter_data.append(val.value)

            val = await grid_power_recv.receive()
            assert val is not None and val.value is not None
            results.append(val.value)

    assert equal_float_lists(results, grid_meter_data)


async def test_grid_reactive_power_2(mocker: MockerFixture) -> None:
    """Test the grid power formula without a grid side meter."""
    mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
    mockgrid.add_consumer_meters(1)
    mockgrid.add_batteries(1, no_meter=False)
    mockgrid.add_batteries(1, no_meter=True)
    mockgrid.add_solar_inverters(1)

    results: list[Quantity] = []
    meter_sums: list[Quantity] = []
    async with mockgrid, AsyncExitStack() as stack:
        grid = microgrid.grid()
        assert grid, "Grid is not initialized"
        stack.push_async_callback(grid.stop)

        grid_power_recv = grid.reactive_power.new_receiver()

        component_receivers = [
            get_resampled_stream(
                grid._formula_pool._namespace,  # pylint: disable=protected-access
                component_id,
                client.ComponentMetricId.REACTIVE_POWER,
                ReactivePower.from_volt_amperes_reactive,
            )
            for component_id in [
                *mockgrid.meter_ids,
                # The last battery has no meter, so we get the power from the inverter
                mockgrid.battery_inverter_ids[-1],
            ]
        ]

        for count in range(10):
            await mockgrid.mock_resampler.send_meter_reactive_power(
                [20.0 + count, 12.0, -13.0]
            )
            await mockgrid.mock_resampler.send_bat_inverter_reactive_power([0.0, -5.0])
            meter_sum = 0.0
            for recv in component_receivers:
                val = await recv.receive()
                assert (
                    val is not None
                    and val.value is not None
                    and val.value.as_volt_amperes_reactive() != 0.0
                )
                meter_sum += val.value.as_volt_amperes_reactive()

            val = await grid_power_recv.receive()
            assert val is not None and val.value is not None
            results.append(val.value)
            meter_sums.append(Quantity(meter_sum))

    assert len(results) == 10
    assert equal_float_lists(results, meter_sums)


async def test_grid_power_3_phase_side_meter(mocker: MockerFixture) -> None:
    """Test the grid 3-phase power with a grid side meter."""
    mockgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
    mockgrid.add_batteries(1, no_meter=True)
    mockgrid.add_batteries(1, no_meter=False)

    async with mockgrid, AsyncExitStack() as stack:
        grid = microgrid.grid()
        assert grid, "Grid is not initialized"
        stack.push_async_callback(grid.stop)

        grid_power_per_phase_recv = (
            grid._power_per_phase.new_receiver()  # pylint: disable=protected-access
        )

        for count in range(10):
            watts_delta = 1 if count % 2 == 0 else -1
            watts_phases: list[float | None] = [
                220.0 * watts_delta,
                219.8 * watts_delta,
                220.2 * watts_delta,
            ]

            await mockgrid.mock_resampler.send_meter_power_3_phase(
                [watts_phases, watts_phases]
            )

            val = await grid_power_per_phase_recv.receive()
            assert val is not None
            assert val.value_p1 and val.value_p2 and val.value_p3
            assert val.value_p1.as_watts() == watts_phases[0]
            assert val.value_p2.as_watts() == watts_phases[1]
            assert val.value_p3.as_watts() == watts_phases[2]


async def test_grid_power_3_phase_none_values(mocker: MockerFixture) -> None:
    """Test the grid 3-phase power with None values."""
    mockgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
    mockgrid.add_batteries(2, no_meter=False)

    async with mockgrid, AsyncExitStack() as stack:
        grid = microgrid.grid()
        assert grid, "Grid is not initialized"
        stack.push_async_callback(grid.stop)

        grid_power_per_phase_recv = (
            grid._power_per_phase.new_receiver()  # pylint: disable=protected-access
        )

        for count in range(10):
            watts_delta = 1 if count % 2 == 0 else -1
            watts_phases: list[float | None] = [
                220.0 * watts_delta,
                219.8 * watts_delta,
                220.2 * watts_delta,
            ]

            await mockgrid.mock_resampler.send_meter_power_3_phase(
                [watts_phases, [None, None, None], [None, 219.8, 220.2]]
            )

            val = await grid_power_per_phase_recv.receive()
            assert val is not None
            assert val.value_p1 and val.value_p2 and val.value_p3
            assert val.value_p1.as_watts() == watts_phases[0]
            assert val.value_p2.as_watts() == watts_phases[1]
            assert val.value_p3.as_watts() == watts_phases[2]


async def test_grid_production_consumption_power_consumer_meter(
    mocker: MockerFixture,
) -> None:
    """Test the grid production and consumption power formulas."""
    mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
    mockgrid.add_consumer_meters()
    mockgrid.add_batteries(2)
    mockgrid.add_solar_inverters(1)

    async with mockgrid, AsyncExitStack() as stack:
        grid = microgrid.grid()
        assert grid, "Grid is not initialized"
        stack.push_async_callback(grid.stop)

        grid_recv = grid.power.new_receiver()

        await mockgrid.mock_resampler.send_meter_power([1.0, 2.0, 3.0, 4.0])
        assert (await grid_recv.receive()).value == Power.from_watts(10.0)

        await mockgrid.mock_resampler.send_meter_power([1.0, 2.0, -3.0, -4.0])
        assert (await grid_recv.receive()).value == Power.from_watts(-4.0)


async def test_grid_production_consumption_power_no_grid_meter(
    mocker: MockerFixture,
) -> None:
    """Test the grid production and consumption power formulas."""
    mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
    mockgrid.add_batteries(2)
    mockgrid.add_solar_inverters(1)

    async with mockgrid, AsyncExitStack() as stack:
        grid = microgrid.grid()
        assert grid, "Grid is not initialized"
        stack.push_async_callback(grid.stop)

        grid_recv = grid.power.new_receiver()

        await mockgrid.mock_resampler.send_meter_power([2.5, 3.5, 4.0])
        assert (await grid_recv.receive()).value == Power.from_watts(10.0)

        await mockgrid.mock_resampler.send_meter_power([3.0, -3.0, -4.0])
        assert (await grid_recv.receive()).value == Power.from_watts(-4.0)


async def test_consumer_power_2_grid_meters(mocker: MockerFixture) -> None:
    """Test the grid power formula with two grid meters."""
    mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
    # with no further successor these will be detected as grid meters
    mockgrid.add_consumer_meters(2)

    async with mockgrid, AsyncExitStack() as stack:
        grid = microgrid.grid()
        assert grid, "Grid is not initialized"
        stack.push_async_callback(grid.stop)

        grid_recv = grid.power.new_receiver()

        await mockgrid.mock_resampler.send_meter_power([1.0, 2.0])
        assert (await grid_recv.receive()).value == Power.from_watts(3.0)


async def test_grid_fallback_formula_without_grid_meter(mocker: MockerFixture) -> None:
    """Test the grid power formula without a grid meter."""
    gen = GraphGenerator()
    mockgrid = MockMicrogrid(
        graph=gen.to_graph(
            (
                [
                    ComponentCategory.METER,  # Consumer meter
                    (
                        ComponentCategory.METER,  # meter with 2 inverters
                        [
                            (
                                ComponentCategory.INVERTER,
                                [ComponentCategory.BATTERY],
                            ),
                            (
                                ComponentCategory.INVERTER,
                                [ComponentCategory.BATTERY, ComponentCategory.BATTERY],
                            ),
                        ],
                    ),
                    (ComponentCategory.INVERTER, ComponentCategory.BATTERY),
                ]
            )
        ),
        mocker=mocker,
    )

    async with mockgrid, AsyncExitStack() as stack:
        grid = microgrid.grid()
        stack.push_async_callback(grid.stop)
        consumer_power_receiver = grid.power.new_receiver()

        # Note: GridPowerFormula has a "nones-are-zero" rule, that says:
        # * if the meter value is None, it should be treated as None.
        # * for other components None is treated as 0.

        # fmt: off
        expected_input_output: list[
            tuple[list[float | None], list[float | None],  Power | None]
        ] = [
            # ([consumer_meter, bat1_meter], [bat1_1_inv, bat1_2_inv, bat2_inv], expected_power)
            ([100, -200], [-300, -300, 50], Power.from_watts(-50)),
            ([500, 100], [100, 1000, -200,], Power.from_watts(400)),
            # Consumer meter is invalid - consumer meter has no fallback.
            # Formula should return None as defined in nones-are-zero rule.
            ([None, 100], [100, 1000, -200,], None),
            ([None, -50], [100, 100, -200,], None),
            ([500, 100], [100, 50, -200,], Power.from_watts(400)),
            # bat1_inv is invalid.
            # Return None and subscribe for fallback devices.
            # Next call should return formula result with pv_inv value.
            ([500, None], [100, 1000, -200,], None),
            ([500, None], [100, -1000, -200,], Power.from_watts(-600)),
            ([500, None], [-100, 200, 50], Power.from_watts(650)),
            # Second Battery inverter is invalid. This component has no fallback.
            # return 0 instead of None as defined in nones-are-zero rule.
            ([2000, None], [-200, 1000, None], Power.from_watts(2800)),
            ([2000, 1000], [-200, 1000, None], Power.from_watts(3000)),
            # battery start working
            ([2000, 10], [-200, 1000, 100], Power.from_watts(2110)),
            # No primary value, start fallback formula
            ([2000, None], [-200, 1000, 100], None),
            ([2000, None], [-200, 1000, 100], Power.from_watts(2900)),
        ]
        # fmt: on

        for idx, (
            meter_power,
            bat_inv_power,
            expected_power,
        ) in enumerate(expected_input_output):
            await mockgrid.mock_resampler.send_meter_power(meter_power)
            await mockgrid.mock_resampler.send_bat_inverter_power(bat_inv_power)
            mockgrid.mock_resampler.next_ts()

            result = await consumer_power_receiver.receive()
            assert result.value == expected_power, (
                f"Test case {idx} failed:"
                + f" meter_power: {meter_power}"
                + f" bat_inverter_power {bat_inv_power}"
                + f" expected_power: {expected_power}"
                + f" actual_power: {result.value}"
            )


async def test_grid_fallback_formula_with_grid_meter(mocker: MockerFixture) -> None:
    """Test the grid power formula without a grid meter."""
    gen = GraphGenerator()
    mockgrid = MockMicrogrid(
        graph=gen.to_graph(
            (
                ComponentCategory.METER,  # Grid meter
                [
                    (
                        ComponentCategory.METER,  # meter with 2 inverters
                        [
                            (
                                ComponentCategory.INVERTER,
                                [ComponentCategory.BATTERY],
                            ),
                            (
                                ComponentCategory.INVERTER,
                                [ComponentCategory.BATTERY, ComponentCategory.BATTERY],
                            ),
                        ],
                    ),
                    (ComponentCategory.INVERTER, ComponentCategory.BATTERY),
                ],
            )
        ),
        mocker=mocker,
    )

    async with mockgrid, AsyncExitStack() as stack:
        grid = microgrid.grid()
        stack.push_async_callback(grid.stop)
        consumer_power_receiver = grid.power.new_receiver()

        # Note: GridPowerFormula has a "nones-are-zero" rule, that says:
        # * if the meter value is None, it should be treated as None.
        # * for other components None is treated as 0.

        # fmt: off
        expected_input_output: list[
            tuple[list[float | None], list[float | None],  Power | None]
        ] = [
            # ([grid_meter, bat1_meter], [bat1_1_inv, bat1_2_inv, bat2_inv], expected_power)
            ([100, -200], [-300, -300, 50], Power.from_watts(100)),
            ([-100, 100], [100, 1000, -200,], Power.from_watts(-100)),
            ([None, 100], [100, 1000, -200,], None),
            ([None, -50], [100, 100, -200,], None),
            ([500, 100], [100, 50, -200,], Power.from_watts(500)),
        ]
        # fmt: on

        for idx, (
            meter_power,
            bat_inv_power,
            expected_power,
        ) in enumerate(expected_input_output):
            await mockgrid.mock_resampler.send_meter_power(meter_power)
            await mockgrid.mock_resampler.send_bat_inverter_power(bat_inv_power)
            mockgrid.mock_resampler.next_ts()

            result = await consumer_power_receiver.receive()
            assert result.value == expected_power, (
                f"Test case {idx} failed:"
                + f" meter_power: {meter_power}"
                + f" bat_inverter_power {bat_inv_power}"
                + f" expected_power: {expected_power}"
                + f" actual_power: {result.value}"
            )



================================================
FILE: tests/microgrid/test_microgrid_api.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Tests of MicrogridApi."""

import asyncio
from asyncio.tasks import ALL_COMPLETED
from unittest import mock
from unittest.mock import AsyncMock, MagicMock

import pytest
from frequenz.client.common.microgrid import MicrogridId
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    Component,
    ComponentCategory,
    Connection,
    Location,
    Metadata,
)

from frequenz.sdk.microgrid import connection_manager


class TestMicrogridApi:
    """Test for MicropgridApi."""

    # ignore mypy: Untyped decorator makes function "components" untyped
    @pytest.fixture
    def components(self) -> list[list[Component]]:
        """Get components in the graph.

        Override this method to create a graph with different components.

        Returns:
            list of components in graph

        """
        components = [
            [
                Component(ComponentId(1), ComponentCategory.GRID),
                Component(ComponentId(4), ComponentCategory.METER),
                Component(ComponentId(5), ComponentCategory.METER),
                Component(ComponentId(7), ComponentCategory.METER),
                Component(ComponentId(8), ComponentCategory.INVERTER),
                Component(ComponentId(9), ComponentCategory.BATTERY),
                Component(ComponentId(10), ComponentCategory.METER),
                Component(ComponentId(11), ComponentCategory.INVERTER),
                Component(ComponentId(12), ComponentCategory.BATTERY),
            ],
            [
                Component(ComponentId(1), ComponentCategory.GRID),
                Component(ComponentId(4), ComponentCategory.METER),
                Component(ComponentId(7), ComponentCategory.METER),
                Component(ComponentId(8), ComponentCategory.INVERTER),
                Component(ComponentId(9), ComponentCategory.BATTERY),
            ],
        ]
        return components

    # ignore mypy: Untyped decorator makes function "components" untyped
    @pytest.fixture
    def connections(self) -> list[list[Connection]]:
        """Get connections between components in the graph.

        Override this method to create a graph with different connections.

        Returns:
            list of connections between components in graph

        """
        connections = [
            [
                Connection(ComponentId(1), ComponentId(4)),
                Connection(ComponentId(1), ComponentId(5)),
                Connection(ComponentId(1), ComponentId(7)),
                Connection(ComponentId(7), ComponentId(8)),
                Connection(ComponentId(8), ComponentId(9)),
                Connection(ComponentId(1), ComponentId(10)),
                Connection(ComponentId(10), ComponentId(11)),
                Connection(ComponentId(11), ComponentId(12)),
            ],
            [
                Connection(ComponentId(1), ComponentId(4)),
                Connection(ComponentId(1), ComponentId(7)),
                Connection(ComponentId(7), ComponentId(8)),
                Connection(ComponentId(8), ComponentId(9)),
            ],
        ]
        return connections

    @pytest.fixture
    def metadata(self) -> Metadata:
        """Fetch the microgrid metadata.

        Returns:
            the microgrid metadata.
        """
        return Metadata(
            microgrid_id=MicrogridId(8),
            location=Location(
                latitude=52.520008,
                longitude=13.404954,
            ),
        )

    @mock.patch("grpc.aio.insecure_channel")
    async def test_connection_manager(
        self,
        _insecure_channel_mock: MagicMock,
        components: list[list[Component]],
        connections: list[list[Connection]],
        metadata: Metadata,
    ) -> None:
        """Test microgrid api.

        Args:
            _insecure_channel_mock: insecure channel mock from `mock.patch`
            components: components
            connections: connections
            metadata: the metadata of the microgrid
        """
        microgrid_client = MagicMock()
        microgrid_client.components = AsyncMock(side_effect=components)
        microgrid_client.connections = AsyncMock(side_effect=connections)
        microgrid_client.metadata = AsyncMock(return_value=metadata)

        with mock.patch(
            "frequenz.sdk.microgrid.connection_manager.MicrogridApiClient",
            return_value=microgrid_client,
        ):
            # Get instance without initializing git first.
            with pytest.raises(RuntimeError):
                connection_manager.get()

            tasks = [
                asyncio.create_task(
                    connection_manager.initialize("grpc://127.0.0.1:10001")
                ),
                asyncio.create_task(
                    connection_manager.initialize("grpc://127.0.0.1:10001")
                ),
            ]
            initialize_task = asyncio.wait(tasks, return_when=ALL_COMPLETED)

            # Check if we can get connection_manager after not full initialization
            with pytest.raises(RuntimeError):
                connection_manager.get()

            done, pending = await initialize_task
            assert len(pending) == 0
            assert len(done) == 2
            assertion_counter = 0
            for result in done:
                try:
                    result.result()
                except AssertionError:
                    assertion_counter += 1
            assert assertion_counter == 1

            # Initialization is over we should now get api
            api = connection_manager.get()
            assert api.api_client is microgrid_client

            graph = api.component_graph
            assert set(graph.components()) == set(components[0])
            assert set(graph.connections()) == set(connections[0])

            assert api.microgrid_id == metadata.microgrid_id
            assert api.location == metadata.location

            # It should not be possible to initialize method once again
            with pytest.raises(AssertionError):
                await connection_manager.initialize("grpc://127.0.0.1:10001")

            api2 = connection_manager.get()

            assert api is api2
            graph = api2.component_graph
            assert set(graph.components()) == set(components[0])
            assert set(graph.connections()) == set(connections[0])

            assert api.microgrid_id == metadata.microgrid_id
            assert api.location == metadata.location

    @mock.patch("grpc.aio.insecure_channel")
    async def test_connection_manager_another_method(
        self,
        _insecure_channel_mock: MagicMock,
        components: list[list[Component]],
        connections: list[list[Connection]],
        metadata: Metadata,
    ) -> None:
        """Test if the api was not deallocated.

        Args:
            _insecure_channel_mock: insecure channel mock
            components: components
            connections: connections
            metadata: the metadata of the microgrid
        """
        microgrid_client = MagicMock()
        microgrid_client.components = AsyncMock(return_value=[])
        microgrid_client.connections = AsyncMock(return_value=[])
        microgrid_client.get_metadata = AsyncMock(return_value=None)

        api = connection_manager.get()
        graph = api.component_graph
        assert set(graph.components()) == set(components[0])
        assert set(graph.connections()) == set(connections[0])

        assert api.microgrid_id == metadata.microgrid_id
        assert api.location == metadata.location



================================================
FILE: tests/microgrid/power_distributing/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Tests for the power distributing actor and algorithm."""



================================================
FILE: tests/microgrid/power_distributing/_component_status/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Tests for component status tracking for the power distributing actor."""



================================================
FILE: tests/microgrid/power_distributing/_component_status/test_battery_pool_status.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH
"""Tests for BatteryPoolStatus."""

import asyncio
from contextlib import AsyncExitStack
from datetime import timedelta

from frequenz.channels import Broadcast
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentCategory
from pytest_mock import MockerFixture

from frequenz.sdk.microgrid._power_distributing._component_pool_status_tracker import (
    ComponentPoolStatusTracker,
)
from frequenz.sdk.microgrid._power_distributing._component_status import (
    BatteryStatusTracker,
    ComponentPoolStatus,
)
from tests.timeseries.mock_microgrid import MockMicrogrid

from .test_battery_status import battery_data, inverter_data


# pylint: disable=protected-access
class TestBatteryPoolStatus:
    """Tests for BatteryPoolStatus."""

    async def test_batteries_status(self, mocker: MockerFixture) -> None:
        """Basic tests for BatteryPoolStatus.

        BatteryStatusTracker is more tested in its own unit tests.

        Args:
            mocker: Pytest mocker fixture.
        """
        mock_microgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
        mock_microgrid.add_batteries(3)

        async with AsyncExitStack() as stack:
            await stack.enter_async_context(mock_microgrid)
            batteries = {
                battery.component_id
                for battery in mock_microgrid.mock_client.component_graph.components(
                    component_categories={ComponentCategory.BATTERY}
                )
            }
            battery_status_channel = Broadcast[ComponentPoolStatus](
                name="battery_status"
            )
            battery_status_recv = battery_status_channel.new_receiver(limit=1)
            batteries_status = ComponentPoolStatusTracker(
                component_ids=batteries,
                component_status_sender=battery_status_channel.new_sender(),
                max_data_age=timedelta(seconds=5),
                max_blocking_duration=timedelta(seconds=30),
                component_status_tracker_type=BatteryStatusTracker,
            )
            stack.push_async_callback(batteries_status.stop)
            await asyncio.sleep(0.1)

            expected_working: set[ComponentId] = set()
            assert (
                batteries_status.get_working_components(batteries) == expected_working
            )

            batteries_list = list(batteries)

            await mock_microgrid.mock_client.send(
                battery_data(component_id=batteries_list[0])
            )
            await asyncio.sleep(0.1)
            assert (
                batteries_status.get_working_components(batteries) == expected_working
            )

            expected_working.add(batteries_list[0])
            await mock_microgrid.mock_client.send(
                inverter_data(component_id=ComponentId(int(batteries_list[0]) - 1))
            )
            await asyncio.sleep(0.1)
            assert (
                batteries_status.get_working_components(batteries) == expected_working
            )
            msg = await asyncio.wait_for(battery_status_recv.receive(), timeout=0.2)
            assert msg == batteries_status._current_status

            await mock_microgrid.mock_client.send(
                inverter_data(component_id=ComponentId(int(batteries_list[1]) - 1))
            )
            await mock_microgrid.mock_client.send(
                battery_data(component_id=batteries_list[1])
            )

            await mock_microgrid.mock_client.send(
                inverter_data(component_id=ComponentId(int(batteries_list[2]) - 1))
            )
            await mock_microgrid.mock_client.send(
                battery_data(component_id=batteries_list[2])
            )

            expected_working = set(batteries_list)
            await asyncio.sleep(0.1)
            assert (
                batteries_status.get_working_components(batteries) == expected_working
            )
            msg = await asyncio.wait_for(battery_status_recv.receive(), timeout=0.2)
            assert msg == batteries_status._current_status

            await batteries_status.update_status(
                succeeded_components={ComponentId(9)},
                failed_components={ComponentId(19), ComponentId(29)},
            )
            await asyncio.sleep(0.1)
            assert batteries_status.get_working_components(batteries) == {
                ComponentId(9)
            }

            await batteries_status.update_status(
                succeeded_components={ComponentId(9), ComponentId(19)},
                failed_components=set(),
            )
            await asyncio.sleep(0.1)
            assert batteries_status.get_working_components(batteries) == {
                ComponentId(9),
                ComponentId(19),
            }



================================================
FILE: tests/microgrid/power_distributing/_component_status/test_battery_status.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests for BatteryStatusTracker."""

# pylint: disable=too-many-lines

import asyncio
import math
from collections.abc import AsyncIterator, Iterable
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Generic, TypeVar

import async_solipsism
import pytest
from frequenz.channels import Broadcast, Receiver
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    BatteryComponentState,
    BatteryData,
    BatteryError,
    BatteryErrorCode,
    BatteryRelayState,
    ErrorLevel,
    InverterComponentState,
    InverterData,
    InverterError,
    InverterErrorCode,
)
from pytest_mock import MockerFixture
from time_machine import TimeMachineFixture

from frequenz.sdk.microgrid._power_distributing._component_status import (
    BatteryStatusTracker,
    ComponentStatus,
    ComponentStatusEnum,
    SetPowerResult,
)
from tests.timeseries.mock_microgrid import MockMicrogrid

from ....utils.component_data_wrapper import BatteryDataWrapper, InverterDataWrapper
from ....utils.receive_timeout import Timeout, receive_timeout


@pytest.fixture
def event_loop_policy() -> async_solipsism.EventLoopPolicy:
    """Event loop policy."""
    return async_solipsism.EventLoopPolicy()


def battery_data(  # pylint: disable=too-many-arguments,too-many-positional-arguments
    component_id: ComponentId,
    timestamp: datetime | None = None,
    relay_state: BatteryRelayState = BatteryRelayState.CLOSED,
    component_state: BatteryComponentState = BatteryComponentState.CHARGING,
    errors: Iterable[BatteryError] | None = None,
    capacity: float = 0,
) -> BatteryData:
    """Create BatteryData with given arguments.

    By default function creates BatteryData correct for BatteryPoolStatus with specified
        default arguments.
    If other arguments are given, then it creates BatteryData with that arguments.

    Args:
        component_id: component id
        timestamp: Timestamp of the component message.
            Defaults to datetime.now(tz=timezone.utc).
        relay_state: Battery relay state.
            Defaults to BatteryRelayState.CLOSED.
        component_state: Component state.
            Defaults to BatteryComponentState.CHARGING.
        errors: List of the components error. By default empty list will be created.
        capacity: Battery capacity.

    Returns:
        BatteryData with given arguments.
    """
    return BatteryDataWrapper(
        component_id=component_id,
        capacity=capacity,
        timestamp=datetime.now(tz=timezone.utc) if timestamp is None else timestamp,
        relay_state=relay_state,
        component_state=component_state,
        errors=list(errors) if errors is not None else [],
    )


def inverter_data(
    component_id: ComponentId,
    timestamp: datetime | None = None,
    component_state: InverterComponentState = InverterComponentState.CHARGING,
    errors: list[InverterError] | None = None,
) -> InverterData:
    """Create InverterData with given arguments.

    By default function creates BatteryData correct for BatteryPoolStatus with specified
        default arguments.
    If other arguments are given, then it creates BatteryData with that arguments.

    Args:
        component_id: component id
        timestamp: Timestamp of the component message.
            Defaults to datetime.now(tz=timezone.utc).
        component_state: Component state.
            Defaults to InverterComponentState.CHARGING.
        errors: List of the components error. By default empty list will be created.

    Returns:
        InverterData with given arguments.
    """
    return InverterDataWrapper(
        component_id=component_id,
        timestamp=datetime.now(tz=timezone.utc) if timestamp is None else timestamp,
        component_state=component_state,
        errors=errors,
    )


T = TypeVar("T")


@dataclass
class Message(Generic[T]):
    """Helper class to store FakeSelect data in the `inner` attribute."""

    inner: T


BATTERY_ID = ComponentId(9)
INVERTER_ID = ComponentId(8)


# pylint: disable=protected-access, unused-argument
class TestBatteryStatus:
    """Tests BatteryStatusTracker."""

    async def test_sync_update_status_with_messages(
        self, mocker: MockerFixture, time_machine: TimeMachineFixture
    ) -> None:
        """Test if messages changes battery status.

        Tests uses FakeSelect to test status in sync way.
        Otherwise we would have lots of async calls and waiting.

        Args:
            mocker: Pytest mocker fixture.
            time_machine: Pytest time_machine fixture.
        """
        mock_microgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
        mock_microgrid.add_batteries(3)

        status_channel = Broadcast[ComponentStatus](name="battery_status")
        set_power_result_channel = Broadcast[SetPowerResult](name="set_power_result")

        async with (
            mock_microgrid,
            BatteryStatusTracker(
                component_id=BATTERY_ID,
                max_data_age=timedelta(seconds=5),
                max_blocking_duration=timedelta(seconds=30),
                status_sender=status_channel.new_sender(),
                set_power_result_receiver=set_power_result_channel.new_receiver(),
            ) as tracker,
        ):
            time_machine.move_to("2022-01-01 00:00 UTC", tick=False)
            assert tracker.battery_id == BATTERY_ID
            assert tracker._last_status == ComponentStatusEnum.NOT_WORKING

            tracker._handle_status_inverter(inverter_data(component_id=INVERTER_ID))
            assert tracker._get_new_status_if_changed() is None

            tracker._handle_status_battery(battery_data(component_id=BATTERY_ID))
            assert tracker._get_new_status_if_changed() is ComponentStatusEnum.WORKING

            # --- Send correct message once again, status should not change ---
            tracker._handle_status_inverter(inverter_data(component_id=INVERTER_ID))
            assert tracker._get_new_status_if_changed() is None

            tracker._handle_status_battery(battery_data(component_id=BATTERY_ID))
            assert tracker._get_new_status_if_changed() is None

            # --- Send outdated message ---
            tracker._handle_status_inverter(
                inverter_data(
                    component_id=INVERTER_ID,
                    timestamp=datetime.now(tz=timezone.utc) - timedelta(seconds=31),
                )
            )
            assert (
                tracker._get_new_status_if_changed() is ComponentStatusEnum.NOT_WORKING
            )

            # --- BatteryRelayState is invalid. ---
            tracker._handle_status_battery(
                battery_data(
                    component_id=BATTERY_ID,
                    relay_state=BatteryRelayState.OPENED,
                )
            )
            assert tracker._get_new_status_if_changed() is None

            # --- Inverter started sending data, but battery relays state are still invalid ---
            tracker._handle_status_inverter(inverter_data(component_id=INVERTER_ID))
            assert tracker._get_new_status_if_changed() is None

            tracker._handle_status_battery(battery_data(component_id=BATTERY_ID))
            assert tracker._get_new_status_if_changed() is ComponentStatusEnum.WORKING

            # --- Inverter started sending data, but battery relays state are still invalid ---
            tracker._handle_status_inverter(
                inverter_data(
                    component_id=INVERTER_ID,
                    component_state=InverterComponentState.SWITCHING_OFF,
                )
            )
            assert (
                tracker._get_new_status_if_changed() is ComponentStatusEnum.NOT_WORKING
            )

            inverter_critical_error = InverterError(
                code=InverterErrorCode.UNSPECIFIED,
                level=ErrorLevel.CRITICAL,
                message="",
            )

            inverter_warning_error = InverterError(
                code=InverterErrorCode.UNSPECIFIED,
                level=ErrorLevel.WARN,
                message="",
            )

            tracker._handle_status_inverter(
                inverter_data(
                    component_id=INVERTER_ID,
                    component_state=InverterComponentState.SWITCHING_OFF,
                    errors=[inverter_critical_error, inverter_warning_error],
                )
            )

            assert tracker._get_new_status_if_changed() is None

            tracker._handle_status_inverter(
                inverter_data(
                    component_id=INVERTER_ID,
                    errors=[inverter_critical_error, inverter_warning_error],
                )
            )

            assert tracker._get_new_status_if_changed() is None

            tracker._handle_status_inverter(
                inverter_data(component_id=INVERTER_ID, errors=[inverter_warning_error])
            )

            assert tracker._get_new_status_if_changed() is ComponentStatusEnum.WORKING

            battery_critical_error = BatteryError(
                code=BatteryErrorCode.UNSPECIFIED,
                level=ErrorLevel.CRITICAL,
                message="",
            )

            battery_warning_error = BatteryError(
                code=BatteryErrorCode.UNSPECIFIED,
                level=ErrorLevel.WARN,
                message="",
            )

            tracker._handle_status_battery(
                battery_data(component_id=BATTERY_ID, errors=[battery_warning_error])
            )

            assert tracker._get_new_status_if_changed() is None

            tracker._handle_status_battery(
                battery_data(
                    component_id=BATTERY_ID,
                    errors=[battery_warning_error, battery_critical_error],
                )
            )

            assert (
                tracker._get_new_status_if_changed() is ComponentStatusEnum.NOT_WORKING
            )

            tracker._handle_status_battery(
                battery_data(
                    component_id=BATTERY_ID,
                    component_state=BatteryComponentState.ERROR,
                    errors=[battery_warning_error, battery_critical_error],
                )
            )

            assert tracker._get_new_status_if_changed() is None

            # Check if NaN capacity changes the battery status.
            tracker._handle_status_battery(battery_data(component_id=BATTERY_ID))

            assert tracker._get_new_status_if_changed() is ComponentStatusEnum.WORKING

            tracker._handle_status_battery(
                battery_data(component_id=BATTERY_ID, capacity=math.nan)
            )

            assert (
                tracker._get_new_status_if_changed() is ComponentStatusEnum.NOT_WORKING
            )

    async def test_sync_blocking_feature(self, mocker: MockerFixture) -> None:
        """Test if status changes when SetPowerResult message is received.

        Tests uses FakeSelect to test status in sync way.
        Otherwise we would have lots of async calls and waiting.

        Args:
            mocker: Pytest mocker fixture.
        """
        import time_machine  # pylint: disable=import-outside-toplevel

        mock_microgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
        mock_microgrid.add_batteries(3)

        status_channel = Broadcast[ComponentStatus](name="battery_status")
        set_power_result_channel = Broadcast[SetPowerResult](name="set_power_result")

        async with (
            mock_microgrid,
            BatteryStatusTracker(
                # increase max_data_age_sec for blocking tests.
                # Otherwise it will block blocking.
                component_id=BATTERY_ID,
                max_data_age=timedelta(seconds=500),
                max_blocking_duration=timedelta(seconds=30),
                status_sender=status_channel.new_sender(),
                set_power_result_receiver=set_power_result_channel.new_receiver(),
            ) as tracker,
        ):
            with time_machine.travel("2022-01-01 00:00 UTC", tick=False) as time:
                tracker._handle_status_inverter(inverter_data(component_id=INVERTER_ID))

                assert tracker._get_new_status_if_changed() is None

                tracker._handle_status_battery(
                    battery_data(
                        component_id=BATTERY_ID,
                        component_state=BatteryComponentState.ERROR,
                    )
                )

                assert tracker._get_new_status_if_changed() is None

                # message is not correct, component should not block.
                tracker._handle_status_set_power_result(
                    SetPowerResult(succeeded={ComponentId(1)}, failed={BATTERY_ID})
                )

                assert tracker._get_new_status_if_changed() is None

                tracker._handle_status_battery(battery_data(component_id=BATTERY_ID))

                assert (
                    tracker._get_new_status_if_changed() is ComponentStatusEnum.WORKING
                )

                expected_blocking_timeout = [1, 2, 4, 8, 16, 30, 30]

                for timeout in expected_blocking_timeout:
                    # message is not correct, component should not block.
                    tracker._handle_status_set_power_result(
                        SetPowerResult(succeeded={ComponentId(1)}, failed={BATTERY_ID})
                    )

                    assert (
                        tracker._get_new_status_if_changed()
                        is ComponentStatusEnum.UNCERTAIN
                    )

                    # Battery should be still blocked, nothing should happen
                    time.shift(timeout - 1)
                    tracker._handle_status_set_power_result(
                        SetPowerResult(succeeded={ComponentId(1)}, failed={BATTERY_ID})
                    )

                    assert tracker._get_new_status_if_changed() is None

                    tracker._handle_status_battery(
                        battery_data(component_id=BATTERY_ID)
                    )

                    assert tracker._get_new_status_if_changed() is None

                    time.shift(1)
                    tracker._handle_status_battery(
                        battery_data(component_id=BATTERY_ID)
                    )

                    assert (
                        tracker._get_new_status_if_changed()
                        is ComponentStatusEnum.WORKING
                    )

                # should block for 30 sec
                tracker._handle_status_set_power_result(
                    SetPowerResult(succeeded={ComponentId(1)}, failed={BATTERY_ID})
                )

                assert (
                    tracker._get_new_status_if_changed()
                    is ComponentStatusEnum.UNCERTAIN
                )
                time.shift(28)

                tracker._handle_status_battery(
                    battery_data(
                        component_id=BATTERY_ID,
                        component_state=BatteryComponentState.ERROR,
                    )
                )

                assert (
                    tracker._get_new_status_if_changed()
                    is ComponentStatusEnum.NOT_WORKING
                )

                # Message that changed status to correct should unblock the battery.
                tracker._handle_status_battery(battery_data(component_id=BATTERY_ID))
                assert (
                    tracker._get_new_status_if_changed() is ComponentStatusEnum.WORKING
                )

                # should block for 30 sec
                tracker._handle_status_set_power_result(
                    SetPowerResult(succeeded={ComponentId(1)}, failed={BATTERY_ID})
                )
                assert (
                    tracker._get_new_status_if_changed()
                    is ComponentStatusEnum.UNCERTAIN
                )
                time.shift(28)

                # If battery succeed, then it should unblock.
                tracker._handle_status_set_power_result(
                    SetPowerResult(succeeded={BATTERY_ID}, failed={ComponentId(19)})
                )
                assert (
                    tracker._get_new_status_if_changed() is ComponentStatusEnum.WORKING
                )

    async def test_sync_blocking_interrupted_with_with_max_data(
        self, mocker: MockerFixture, time_machine: TimeMachineFixture
    ) -> None:
        """Test if status changes when SetPowerResult message is received.

        Tests uses FakeSelect to test status in sync way.
        Otherwise we would have lots of async calls and waiting.

        Args:
            mocker: Pytest mocker fixture.
            time_machine: Pytest time_machine fixture.
        """
        mock_microgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
        mock_microgrid.add_batteries(3)

        status_channel = Broadcast[ComponentStatus](name="battery_status")
        set_power_result_channel = Broadcast[SetPowerResult](name="set_power_result")

        async with (
            mock_microgrid,
            BatteryStatusTracker(
                component_id=BATTERY_ID,
                max_data_age=timedelta(seconds=5),
                max_blocking_duration=timedelta(seconds=30),
                status_sender=status_channel.new_sender(),
                set_power_result_receiver=set_power_result_channel.new_receiver(),
            ) as tracker,
        ):
            start = datetime(2022, 1, 1, tzinfo=timezone.utc)
            time_machine.move_to(start, tick=False)

            tracker._handle_status_inverter(inverter_data(component_id=INVERTER_ID))
            assert tracker._get_new_status_if_changed() is None

            tracker._handle_status_battery(battery_data(component_id=BATTERY_ID))
            assert tracker._get_new_status_if_changed() is ComponentStatusEnum.WORKING

            tracker._handle_status_set_power_result(
                SetPowerResult(succeeded={ComponentId(1)}, failed={BATTERY_ID})
            )
            assert tracker._get_new_status_if_changed() is ComponentStatusEnum.UNCERTAIN

            expected_blocking_timeout = [1, 2, 4]
            for timeout in expected_blocking_timeout:
                # message is not correct, component should not block.
                tracker._handle_status_set_power_result(
                    SetPowerResult(succeeded={ComponentId(1)}, failed={BATTERY_ID})
                )
                assert tracker._get_new_status_if_changed() is None
                time_machine.move_to(start + timedelta(seconds=timeout))

    async def test_sync_blocking_interrupted_with_invalid_message(
        self, mocker: MockerFixture, time_machine: TimeMachineFixture
    ) -> None:
        """Test if status changes when SetPowerResult message is received.

        Tests uses FakeSelect to test status in sync way.
        Otherwise we would have lots of async calls and waiting.

        Args:
            mocker: Pytest mocker fixture.
            time_machine: Pytest time_machine fixture.
        """
        mock_microgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
        mock_microgrid.add_batteries(3)

        status_channel = Broadcast[ComponentStatus](name="battery_status")
        set_power_result_channel = Broadcast[SetPowerResult](name="set_power_result")

        async with (
            mock_microgrid,
            BatteryStatusTracker(
                component_id=BATTERY_ID,
                max_data_age=timedelta(seconds=5),
                max_blocking_duration=timedelta(seconds=30),
                status_sender=status_channel.new_sender(),
                set_power_result_receiver=set_power_result_channel.new_receiver(),
            ) as tracker,
        ):
            time_machine.move_to("2022-01-01 00:00 UTC", tick=False)

            tracker._handle_status_inverter(inverter_data(component_id=INVERTER_ID))
            assert tracker._get_new_status_if_changed() is None

            tracker._handle_status_battery(battery_data(component_id=BATTERY_ID))
            assert tracker._get_new_status_if_changed() is ComponentStatusEnum.WORKING

            tracker._handle_status_set_power_result(
                SetPowerResult(succeeded={ComponentId(1)}, failed={BATTERY_ID})
            )
            assert tracker._get_new_status_if_changed() is ComponentStatusEnum.UNCERTAIN

            tracker._handle_status_inverter(
                inverter_data(
                    component_id=INVERTER_ID,
                    component_state=InverterComponentState.ERROR,
                )
            )
            assert (
                tracker._get_new_status_if_changed() is ComponentStatusEnum.NOT_WORKING
            )

            tracker._handle_status_set_power_result(
                SetPowerResult(succeeded={ComponentId(1)}, failed={BATTERY_ID})
            )
            assert tracker._get_new_status_if_changed() is None

            tracker._handle_status_set_power_result(
                SetPowerResult(succeeded={BATTERY_ID}, failed=set())
            )
            assert tracker._get_new_status_if_changed() is None

            tracker._handle_status_inverter(inverter_data(component_id=INVERTER_ID))
            assert tracker._get_new_status_if_changed() is ComponentStatusEnum.WORKING

    async def test_timers(
        self, mocker: MockerFixture, time_machine: TimeMachineFixture
    ) -> None:
        """Test if messages changes battery status.

        Tests uses FakeSelect to test status in sync way.
        Otherwise we would have lots of async calls and waiting.

        Args:
            mocker: Pytest mocker fixture.
            time_machine: Pytest time_machine fixture.
        """
        mock_microgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
        mock_microgrid.add_batteries(3)

        status_channel = Broadcast[ComponentStatus](name="battery_status")
        set_power_result_channel = Broadcast[SetPowerResult](name="set_power_result")

        async with (
            mock_microgrid,
            BatteryStatusTracker(
                component_id=BATTERY_ID,
                max_data_age=timedelta(seconds=5),
                max_blocking_duration=timedelta(seconds=30),
                status_sender=status_channel.new_sender(),
                set_power_result_receiver=set_power_result_channel.new_receiver(),
            ) as tracker,
        ):
            time_machine.move_to("2022-01-01 00:00 UTC", tick=False)

            battery_timer_spy = mocker.spy(tracker._battery.data_recv_timer, "reset")
            inverter_timer_spy = mocker.spy(tracker._inverter.data_recv_timer, "reset")

            assert tracker.battery_id == BATTERY_ID
            assert tracker._last_status == ComponentStatusEnum.NOT_WORKING

            tracker._handle_status_inverter(inverter_data(component_id=INVERTER_ID))
            assert tracker._get_new_status_if_changed() is None

            tracker._handle_status_battery(battery_data(component_id=BATTERY_ID))
            assert tracker._get_new_status_if_changed() is ComponentStatusEnum.WORKING

            assert battery_timer_spy.call_count == 1

            tracker._handle_status_battery_timer()
            assert (
                tracker._get_new_status_if_changed() is ComponentStatusEnum.NOT_WORKING
            )

            assert battery_timer_spy.call_count == 1

            tracker._handle_status_battery(battery_data(component_id=BATTERY_ID))
            assert tracker._get_new_status_if_changed() is ComponentStatusEnum.WORKING

            assert battery_timer_spy.call_count == 2

            tracker._handle_status_inverter_timer()
            assert (
                tracker._get_new_status_if_changed() is ComponentStatusEnum.NOT_WORKING
            )

            tracker._handle_status_battery_timer()
            assert tracker._get_new_status_if_changed() is None

            tracker._handle_status_battery(battery_data(component_id=BATTERY_ID))
            assert tracker._get_new_status_if_changed() is None

            tracker._handle_status_inverter(inverter_data(component_id=INVERTER_ID))
            assert tracker._get_new_status_if_changed() is ComponentStatusEnum.WORKING

            assert inverter_timer_spy.call_count == 2

    async def test_async_battery_status(self, mocker: MockerFixture) -> None:
        """Test if status changes.

        Args:
            mocker: Pytest mocker fixture.
        """
        mock_microgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
        mock_microgrid.add_batteries(3)

        status_channel = Broadcast[ComponentStatus](name="battery_status")
        set_power_result_channel = Broadcast[SetPowerResult](name="set_power_result")

        status_receiver = status_channel.new_receiver()
        set_power_result_sender = set_power_result_channel.new_sender()

        async with (
            mock_microgrid,
            BatteryStatusTracker(
                component_id=BATTERY_ID,
                max_data_age=timedelta(seconds=5),
                max_blocking_duration=timedelta(seconds=30),
                status_sender=status_channel.new_sender(),
                set_power_result_receiver=set_power_result_channel.new_receiver(),
            ),
        ):
            import time_machine  # pylint: disable=import-outside-toplevel

            await asyncio.sleep(0.01)

            with time_machine.travel("2022-01-01 00:00 UTC", tick=False) as time:
                await mock_microgrid.mock_client.send(
                    inverter_data(component_id=INVERTER_ID)
                )
                await mock_microgrid.mock_client.send(
                    battery_data(component_id=BATTERY_ID)
                )
                status = await asyncio.wait_for(status_receiver.receive(), timeout=0.1)
                assert status.value is ComponentStatusEnum.WORKING

                await set_power_result_sender.send(
                    SetPowerResult(succeeded=set(), failed={BATTERY_ID})
                )
                status = await asyncio.wait_for(status_receiver.receive(), timeout=0.1)
                assert status.value is ComponentStatusEnum.UNCERTAIN

                time.shift(2)

                await mock_microgrid.mock_client.send(
                    battery_data(component_id=BATTERY_ID)
                )
                status = await asyncio.wait_for(status_receiver.receive(), timeout=0.1)
                assert status.value is ComponentStatusEnum.WORKING

                await mock_microgrid.mock_client.send(
                    inverter_data(
                        component_id=INVERTER_ID,
                        timestamp=datetime.now(tz=timezone.utc) - timedelta(seconds=7),
                    )
                )
                status = await asyncio.wait_for(status_receiver.receive(), timeout=0.1)
                assert status.value is ComponentStatusEnum.NOT_WORKING

                await set_power_result_sender.send(
                    SetPowerResult(succeeded=set(), failed={BATTERY_ID})
                )
                time.shift(10)
                await asyncio.sleep(0.3)
                with pytest.raises(asyncio.TimeoutError):
                    await asyncio.wait_for(status_receiver.receive(), timeout=0.1)

                await mock_microgrid.mock_client.send(
                    inverter_data(component_id=INVERTER_ID)
                )
                status = await asyncio.wait_for(status_receiver.receive(), timeout=0.1)
                assert status.value is ComponentStatusEnum.WORKING


class TestBatteryStatusRecovery:
    """Test battery status recovery.

    The following cases are tested:

    - battery/inverter data missing
    - battery/inverter bad state
    - battery/inverter warning/critical error
    - battery capacity missing
    - received stale battery/inverter data
    """

    @pytest.fixture
    async def setup_tracker(
        self,
        mocker: MockerFixture,
    ) -> AsyncIterator[tuple[MockMicrogrid, Receiver[ComponentStatus]]]:
        """Set a BatteryStatusTracker instance up to run tests with."""
        mock_microgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
        mock_microgrid.add_batteries(1)

        status_channel = Broadcast[ComponentStatus](name="battery_status")
        set_power_result_channel = Broadcast[SetPowerResult](name="set_power_result")

        status_receiver = status_channel.new_receiver()

        async with (
            mock_microgrid,
            BatteryStatusTracker(
                component_id=BATTERY_ID,
                max_data_age=timedelta(seconds=0.2),
                max_blocking_duration=timedelta(seconds=1),
                status_sender=status_channel.new_sender(),
                set_power_result_receiver=set_power_result_channel.new_receiver(),
            ),
        ):
            await asyncio.sleep(0.05)
            yield (mock_microgrid, status_receiver)

    async def _send_healthy_battery(
        self, mock_microgrid: MockMicrogrid, timestamp: datetime | None = None
    ) -> None:
        await mock_microgrid.mock_client.send(
            battery_data(
                timestamp=timestamp,
                component_id=BATTERY_ID,
                component_state=BatteryComponentState.IDLE,
                relay_state=BatteryRelayState.CLOSED,
            )
        )

    async def _send_battery_missing_capacity(
        self, mock_microgrid: MockMicrogrid
    ) -> None:
        await mock_microgrid.mock_client.send(
            battery_data(
                component_id=BATTERY_ID,
                component_state=BatteryComponentState.IDLE,
                relay_state=BatteryRelayState.CLOSED,
                capacity=math.nan,
            )
        )

    async def _send_healthy_inverter(
        self, mock_microgrid: MockMicrogrid, timestamp: datetime | None = None
    ) -> None:
        await mock_microgrid.mock_client.send(
            inverter_data(
                timestamp=timestamp,
                component_id=INVERTER_ID,
                component_state=InverterComponentState.IDLE,
            )
        )

    async def _send_bad_state_battery(self, mock_microgrid: MockMicrogrid) -> None:
        await mock_microgrid.mock_client.send(
            battery_data(
                component_id=BATTERY_ID,
                component_state=BatteryComponentState.ERROR,
                relay_state=BatteryRelayState.CLOSED,
            )
        )

    async def _send_bad_state_inverter(self, mock_microgrid: MockMicrogrid) -> None:
        await mock_microgrid.mock_client.send(
            inverter_data(
                component_id=INVERTER_ID,
                component_state=InverterComponentState.ERROR,
            )
        )

    async def _send_critical_error_battery(self, mock_microgrid: MockMicrogrid) -> None:
        battery_critical_error = BatteryError(
            code=BatteryErrorCode.BLOCK_ERROR,
            level=ErrorLevel.CRITICAL,
            message="",
        )
        await mock_microgrid.mock_client.send(
            battery_data(
                component_id=BATTERY_ID,
                component_state=BatteryComponentState.IDLE,
                relay_state=BatteryRelayState.CLOSED,
                errors=[battery_critical_error],
            )
        )

    async def _send_warning_error_battery(self, mock_microgrid: MockMicrogrid) -> None:
        battery_warning_error = BatteryError(
            code=BatteryErrorCode.HIGH_HUMIDITY,
            level=ErrorLevel.WARN,
            message="",
        )
        await mock_microgrid.mock_client.send(
            battery_data(
                component_id=BATTERY_ID,
                component_state=BatteryComponentState.IDLE,
                relay_state=BatteryRelayState.CLOSED,
                errors=[battery_warning_error],
            )
        )

    async def _send_critical_error_inverter(
        self, mock_microgrid: MockMicrogrid
    ) -> None:
        inverter_critical_error = InverterError(
            code=InverterErrorCode.UNSPECIFIED,
            level=ErrorLevel.CRITICAL,
            message="",
        )
        await mock_microgrid.mock_client.send(
            inverter_data(
                component_id=INVERTER_ID,
                component_state=InverterComponentState.IDLE,
                errors=[inverter_critical_error],
            )
        )

    async def _send_warning_error_inverter(self, mock_microgrid: MockMicrogrid) -> None:
        inverter_warning_error = InverterError(
            code=InverterErrorCode.UNSPECIFIED,
            level=ErrorLevel.WARN,
            message="",
        )
        await mock_microgrid.mock_client.send(
            inverter_data(
                component_id=INVERTER_ID,
                component_state=InverterComponentState.IDLE,
                errors=[inverter_warning_error],
            )
        )

    async def test_missing_data(
        self,
        setup_tracker: tuple[MockMicrogrid, Receiver[ComponentStatus]],
    ) -> None:
        """Test recovery after missing data."""
        mock_microgrid, status_receiver = setup_tracker

        await self._send_healthy_battery(mock_microgrid)
        await self._send_healthy_inverter(mock_microgrid)
        assert (await status_receiver.receive()).value is ComponentStatusEnum.WORKING

        # --- missing battery data ---
        await self._send_healthy_inverter(mock_microgrid)
        assert (
            await status_receiver.receive()
        ).value is ComponentStatusEnum.NOT_WORKING

        await self._send_healthy_battery(mock_microgrid)
        await self._send_healthy_inverter(mock_microgrid)
        assert (await status_receiver.receive()).value is ComponentStatusEnum.WORKING

        # --- missing inverter data ---
        await self._send_healthy_battery(mock_microgrid)
        assert (
            await status_receiver.receive()
        ).value is ComponentStatusEnum.NOT_WORKING

        await self._send_healthy_battery(mock_microgrid)
        await self._send_healthy_inverter(mock_microgrid)
        assert (await status_receiver.receive()).value is ComponentStatusEnum.WORKING

    async def test_bad_state(
        self,
        setup_tracker: tuple[MockMicrogrid, Receiver[ComponentStatus]],
    ) -> None:
        """Test recovery after bad component state."""
        mock_microgrid, status_receiver = setup_tracker

        await self._send_healthy_battery(mock_microgrid)
        await self._send_healthy_inverter(mock_microgrid)
        assert (await status_receiver.receive()).value is ComponentStatusEnum.WORKING

        # --- bad battery state ---
        await self._send_healthy_inverter(mock_microgrid)
        await self._send_bad_state_battery(mock_microgrid)
        assert (
            await status_receiver.receive()
        ).value is ComponentStatusEnum.NOT_WORKING

        await self._send_healthy_battery(mock_microgrid)
        await self._send_healthy_inverter(mock_microgrid)
        assert (await status_receiver.receive()).value is ComponentStatusEnum.WORKING

        # --- bad inverter state ---
        await self._send_bad_state_inverter(mock_microgrid)
        await self._send_healthy_battery(mock_microgrid)
        assert (
            await status_receiver.receive()
        ).value is ComponentStatusEnum.NOT_WORKING

        await self._send_healthy_battery(mock_microgrid)
        await self._send_healthy_inverter(mock_microgrid)
        assert (await status_receiver.receive()).value is ComponentStatusEnum.WORKING

    async def test_critical_error(
        self,
        setup_tracker: tuple[MockMicrogrid, Receiver[ComponentStatus]],
    ) -> None:
        """Test recovery after critical error."""
        mock_microgrid, status_receiver = setup_tracker

        await self._send_healthy_inverter(mock_microgrid)
        await self._send_healthy_battery(mock_microgrid)
        assert (await status_receiver.receive()).value is ComponentStatusEnum.WORKING

        # --- battery warning error (keeps working) ---
        await self._send_healthy_inverter(mock_microgrid)
        await self._send_warning_error_battery(mock_microgrid)
        assert await receive_timeout(status_receiver, timeout=0.1) is Timeout

        await self._send_healthy_battery(mock_microgrid)
        await self._send_healthy_inverter(mock_microgrid)

        # --- battery critical error ---
        await self._send_healthy_inverter(mock_microgrid)
        await self._send_critical_error_battery(mock_microgrid)
        assert (
            await status_receiver.receive()
        ).value is ComponentStatusEnum.NOT_WORKING

        await self._send_healthy_battery(mock_microgrid)
        await self._send_healthy_inverter(mock_microgrid)
        assert (await status_receiver.receive()).value is ComponentStatusEnum.WORKING

        # --- inverter warning error (keeps working) ---
        await self._send_healthy_battery(mock_microgrid)
        await self._send_warning_error_inverter(mock_microgrid)
        assert await receive_timeout(status_receiver, timeout=0.1) is Timeout

        await self._send_healthy_battery(mock_microgrid)
        await self._send_healthy_inverter(mock_microgrid)

        # --- inverter critical error ---
        await self._send_healthy_battery(mock_microgrid)
        await self._send_critical_error_inverter(mock_microgrid)
        assert (
            await status_receiver.receive()
        ).value is ComponentStatusEnum.NOT_WORKING

        await self._send_healthy_battery(mock_microgrid)
        await self._send_healthy_inverter(mock_microgrid)
        assert (await status_receiver.receive()).value is ComponentStatusEnum.WORKING

    async def test_missing_capacity(
        self,
        setup_tracker: tuple[MockMicrogrid, Receiver[ComponentStatus]],
    ) -> None:
        """Test recovery after missing capacity."""
        mock_microgrid, status_receiver = setup_tracker

        await self._send_healthy_battery(mock_microgrid)
        await self._send_healthy_inverter(mock_microgrid)
        assert (await status_receiver.receive()).value is ComponentStatusEnum.WORKING

        await self._send_healthy_inverter(mock_microgrid)
        await self._send_battery_missing_capacity(mock_microgrid)
        assert (
            await status_receiver.receive()
        ).value is ComponentStatusEnum.NOT_WORKING

        await self._send_healthy_battery(mock_microgrid)
        await self._send_healthy_inverter(mock_microgrid)
        assert (await status_receiver.receive()).value is ComponentStatusEnum.WORKING

    async def test_stale_data(
        self,
        setup_tracker: tuple[MockMicrogrid, Receiver[ComponentStatus]],
    ) -> None:
        """Test recovery after stale data."""
        import time_machine  # pylint: disable=import-outside-toplevel

        with time_machine.travel("2022-01-01 00:00 UTC", tick=False) as time:
            mock_microgrid, status_receiver = setup_tracker

            timestamp = datetime.now(timezone.utc)
            await self._send_healthy_battery(mock_microgrid, timestamp)
            await self._send_healthy_inverter(mock_microgrid)
            assert (
                await status_receiver.receive()
            ).value is ComponentStatusEnum.WORKING

            # --- stale battery data ---
            await self._send_healthy_inverter(mock_microgrid)
            await self._send_healthy_battery(mock_microgrid, timestamp)
            assert await receive_timeout(status_receiver) is Timeout

            await self._send_healthy_inverter(mock_microgrid)
            await self._send_healthy_battery(mock_microgrid, timestamp)
            time.shift(0.3)
            assert await receive_timeout(status_receiver, 0.3) == ComponentStatus(
                BATTERY_ID, ComponentStatusEnum.NOT_WORKING
            )

            timestamp = datetime.now(timezone.utc)
            await self._send_healthy_battery(mock_microgrid, timestamp)
            await self._send_healthy_inverter(mock_microgrid, timestamp)
            assert (
                await status_receiver.receive()
            ).value is ComponentStatusEnum.WORKING

            # --- stale inverter data ---
            await self._send_healthy_battery(mock_microgrid)
            await self._send_healthy_inverter(mock_microgrid, timestamp)
            assert await receive_timeout(status_receiver) is Timeout

            await self._send_healthy_battery(mock_microgrid)
            await self._send_healthy_inverter(mock_microgrid, timestamp)
            time.shift(0.3)
            assert await receive_timeout(status_receiver, 0.3) == ComponentStatus(
                BATTERY_ID, ComponentStatusEnum.NOT_WORKING
            )

            await self._send_healthy_battery(mock_microgrid)
            await self._send_healthy_inverter(mock_microgrid)
            assert (
                await status_receiver.receive()
            ).value is ComponentStatusEnum.WORKING



================================================
FILE: tests/microgrid/power_distributing/_component_status/test_ev_charger_status.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests for EVChargerStatusTracker."""

import asyncio
from datetime import datetime, timedelta, timezone

from frequenz.channels import Broadcast
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import EVChargerCableState, EVChargerComponentState
from pytest_mock import MockerFixture

from frequenz.sdk._internal._asyncio import cancel_and_await
from frequenz.sdk.microgrid._power_distributing._component_status import (
    ComponentStatus,
    ComponentStatusEnum,
    EVChargerStatusTracker,
    SetPowerResult,
)

from ....timeseries.mock_microgrid import MockMicrogrid
from ....utils.component_data_wrapper import EvChargerDataWrapper
from ....utils.receive_timeout import Timeout, receive_timeout

_EV_CHARGER_ID = ComponentId(6)


class TestEVChargerStatusTracker:
    """Tests for EVChargerStatusTracker."""

    async def test_status_changes(self, mocker: MockerFixture) -> None:
        """Test that the status changes as expected."""
        mock_microgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
        mock_microgrid.add_ev_chargers(3)

        status_channel = Broadcast[ComponentStatus](name="battery_status")
        set_power_result_channel = Broadcast[SetPowerResult](name="set_power_result")
        set_power_result_sender = set_power_result_channel.new_sender()

        async with (
            mock_microgrid,
            EVChargerStatusTracker(
                component_id=_EV_CHARGER_ID,
                max_data_age=timedelta(seconds=0.2),
                max_blocking_duration=timedelta(seconds=1),
                status_sender=status_channel.new_sender(),
                set_power_result_receiver=set_power_result_channel.new_receiver(),
            ),
        ):
            status_receiver = status_channel.new_receiver()
            # The status is initially not working.
            assert (
                await status_receiver.receive()
            ).value == ComponentStatusEnum.NOT_WORKING

            # When an EV is plugged, it is working
            await mock_microgrid.mock_client.send(
                EvChargerDataWrapper(
                    _EV_CHARGER_ID,
                    datetime.now(tz=timezone.utc),
                    active_power=0.0,
                    component_state=EVChargerComponentState.READY,
                    cable_state=EVChargerCableState.EV_PLUGGED,
                )
            )
            assert await receive_timeout(status_receiver) == ComponentStatus(
                _EV_CHARGER_ID, ComponentStatusEnum.WORKING
            )

            # When an EV is locked, no change in status
            await mock_microgrid.mock_client.send(
                EvChargerDataWrapper(
                    _EV_CHARGER_ID,
                    datetime.now(tz=timezone.utc),
                    active_power=0.0,
                    component_state=EVChargerComponentState.READY,
                    cable_state=EVChargerCableState.EV_LOCKED,
                )
            )
            assert await receive_timeout(status_receiver) is Timeout

            # When an EV is unplugged, it is not working
            await mock_microgrid.mock_client.send(
                EvChargerDataWrapper(
                    _EV_CHARGER_ID,
                    datetime.now(tz=timezone.utc),
                    active_power=0.0,
                    component_state=EVChargerComponentState.READY,
                    cable_state=EVChargerCableState.UNPLUGGED,
                )
            )
            assert await receive_timeout(status_receiver) == ComponentStatus(
                _EV_CHARGER_ID, ComponentStatusEnum.NOT_WORKING
            )

            # Get it back to working again
            await mock_microgrid.mock_client.send(
                EvChargerDataWrapper(
                    _EV_CHARGER_ID,
                    datetime.now(tz=timezone.utc),
                    active_power=0.0,
                    component_state=EVChargerComponentState.READY,
                    cable_state=EVChargerCableState.EV_LOCKED,
                )
            )
            assert await receive_timeout(status_receiver) == ComponentStatus(
                _EV_CHARGER_ID, ComponentStatusEnum.WORKING
            )

            # When there's no new data, it should become not working
            assert await receive_timeout(status_receiver, 0.1) is Timeout
            assert await receive_timeout(status_receiver, 0.2) == ComponentStatus(
                _EV_CHARGER_ID, ComponentStatusEnum.NOT_WORKING
            )

            # Get it back to working again
            await asyncio.sleep(0.1)
            await mock_microgrid.mock_client.send(
                EvChargerDataWrapper(
                    _EV_CHARGER_ID,
                    datetime.now(tz=timezone.utc),
                    active_power=0.0,
                    component_state=EVChargerComponentState.READY,
                    cable_state=EVChargerCableState.EV_LOCKED,
                )
            )
            assert await receive_timeout(status_receiver) == ComponentStatus(
                _EV_CHARGER_ID, ComponentStatusEnum.WORKING
            )

            async def keep_sending_healthy_message() -> None:
                while True:
                    await mock_microgrid.mock_client.send(
                        EvChargerDataWrapper(
                            _EV_CHARGER_ID,
                            datetime.now(tz=timezone.utc),
                            active_power=0.0,
                            component_state=EVChargerComponentState.READY,
                            cable_state=EVChargerCableState.EV_LOCKED,
                        )
                    )
                    await asyncio.sleep(0.1)

            _keep_sending_healthy_message_task = asyncio.create_task(
                keep_sending_healthy_message()
            )

            # when there's a PowerDistributor failure for the component, status should
            # become uncertain.
            await set_power_result_sender.send(
                SetPowerResult(
                    succeeded=set(),
                    failed={_EV_CHARGER_ID},
                )
            )
            assert await receive_timeout(status_receiver) == ComponentStatus(
                _EV_CHARGER_ID, ComponentStatusEnum.UNCERTAIN
            )

            # After the blocking duration, it should become working again.
            assert await receive_timeout(status_receiver) is Timeout
            assert await receive_timeout(status_receiver, 1.0) == ComponentStatus(
                _EV_CHARGER_ID, ComponentStatusEnum.WORKING
            )
            await cancel_and_await(_keep_sending_healthy_message_task)



================================================
FILE: tests/microgrid/power_distributing/_component_status/test_pv_inverter_status.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Tests for PVInverterStatusTracker."""


import asyncio
from datetime import datetime, timedelta, timezone

from frequenz.channels import Broadcast
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import InverterComponentState
from pytest_mock import MockerFixture

from frequenz.sdk._internal._asyncio import cancel_and_await
from frequenz.sdk.microgrid._power_distributing._component_status import (
    ComponentStatus,
    ComponentStatusEnum,
    PVInverterStatusTracker,
    SetPowerResult,
)

from ....timeseries.mock_microgrid import MockMicrogrid
from ....utils.component_data_wrapper import InverterDataWrapper
from ....utils.receive_timeout import Timeout, receive_timeout

_PV_INVERTER_ID = ComponentId(8)


class TestPVInverterStatusTracker:
    """Tests for PVInverterStatusTracker."""

    async def test_status_changes(self, mocker: MockerFixture) -> None:
        """Test that the status changes as expected."""
        mock_microgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
        mock_microgrid.add_solar_inverters(1)

        status_channel = Broadcast[ComponentStatus](name="pv_inverter_status")
        set_power_result_channel = Broadcast[SetPowerResult](name="set_power_result")
        set_power_result_sender = set_power_result_channel.new_sender()

        async with (
            mock_microgrid,
            PVInverterStatusTracker(
                component_id=_PV_INVERTER_ID,
                max_data_age=timedelta(seconds=0.2),
                max_blocking_duration=timedelta(seconds=1),
                status_sender=status_channel.new_sender(),
                set_power_result_receiver=set_power_result_channel.new_receiver(),
            ),
        ):
            status_receiver = status_channel.new_receiver()
            # The status is initially not working.
            assert (
                await status_receiver.receive()
            ).value == ComponentStatusEnum.NOT_WORKING

            # When there's healthy inverter data, status should be working.
            await mock_microgrid.mock_client.send(
                InverterDataWrapper(
                    _PV_INVERTER_ID,
                    datetime.now(tz=timezone.utc),
                    active_power=0.0,
                    component_state=InverterComponentState.IDLE,
                )
            )
            assert await receive_timeout(status_receiver) == ComponentStatus(
                _PV_INVERTER_ID, ComponentStatusEnum.WORKING
            )

            # When it is discharging, there should be no change in status
            await mock_microgrid.mock_client.send(
                InverterDataWrapper(
                    _PV_INVERTER_ID,
                    datetime.now(tz=timezone.utc),
                    active_power=0.0,
                    component_state=InverterComponentState.DISCHARGING,
                )
            )
            assert await receive_timeout(status_receiver) is Timeout

            # When there an error message, status should be not working
            await mock_microgrid.mock_client.send(
                InverterDataWrapper(
                    _PV_INVERTER_ID,
                    datetime.now(tz=timezone.utc),
                    active_power=0.0,
                    component_state=InverterComponentState.ERROR,
                )
            )
            assert await receive_timeout(status_receiver) == ComponentStatus(
                _PV_INVERTER_ID, ComponentStatusEnum.NOT_WORKING
            )

            # Get it back to working again
            await mock_microgrid.mock_client.send(
                InverterDataWrapper(
                    _PV_INVERTER_ID,
                    datetime.now(tz=timezone.utc),
                    active_power=0.0,
                    component_state=InverterComponentState.IDLE,
                )
            )
            assert await receive_timeout(status_receiver) == ComponentStatus(
                _PV_INVERTER_ID, ComponentStatusEnum.WORKING
            )

            # When there's no new data, status should be not working
            assert await receive_timeout(status_receiver, 0.1) is Timeout
            assert await receive_timeout(status_receiver, 0.2) == ComponentStatus(
                _PV_INVERTER_ID, ComponentStatusEnum.NOT_WORKING
            )

            # Get it back to working again
            await mock_microgrid.mock_client.send(
                InverterDataWrapper(
                    _PV_INVERTER_ID,
                    datetime.now(tz=timezone.utc),
                    active_power=0.0,
                    component_state=InverterComponentState.IDLE,
                )
            )
            assert await receive_timeout(status_receiver) == ComponentStatus(
                _PV_INVERTER_ID, ComponentStatusEnum.WORKING
            )

            async def keep_sending_healthy_message() -> None:
                """Keep sending healthy messages."""
                while True:
                    await mock_microgrid.mock_client.send(
                        InverterDataWrapper(
                            _PV_INVERTER_ID,
                            datetime.now(tz=timezone.utc),
                            active_power=0.0,
                            component_state=InverterComponentState.IDLE,
                        )
                    )
                    await asyncio.sleep(0.1)

            _keep_sending_healthy_message_task = asyncio.create_task(
                keep_sending_healthy_message()
            )
            # when there's a PowerDistributor failure for the component, status should
            # become uncertain.
            await set_power_result_sender.send(
                SetPowerResult(
                    succeeded=set(),
                    failed={_PV_INVERTER_ID},
                )
            )
            assert await receive_timeout(status_receiver) == ComponentStatus(
                _PV_INVERTER_ID, ComponentStatusEnum.UNCERTAIN
            )

            # After the blocking duration, it should become working again.
            assert await receive_timeout(status_receiver) is Timeout
            assert await receive_timeout(status_receiver, 1.0) == ComponentStatus(
                _PV_INVERTER_ID, ComponentStatusEnum.WORKING
            )
            await cancel_and_await(_keep_sending_healthy_message_task)



================================================
FILE: tests/timeseries/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Timeseries tests."""



================================================
FILE: tests/timeseries/mock_microgrid.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""A configurable mock microgrid for testing logical meter formulas."""

from __future__ import annotations

import asyncio
from collections.abc import Callable
from datetime import datetime, timedelta, timezone
from types import TracebackType
from typing import Coroutine

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    Component,
    ComponentCategory,
    ComponentData,
    Connection,
    EVChargerCableState,
    EVChargerComponentState,
    Fuse,
    GridMetadata,
    InverterType,
)
from pytest_mock import MockerFixture

from frequenz.sdk import microgrid
from frequenz.sdk._internal._asyncio import cancel_and_await
from frequenz.sdk.microgrid import _data_pipeline
from frequenz.sdk.microgrid.component_graph import _MicrogridComponentGraph
from frequenz.sdk.timeseries import ResamplerConfig2

from ..utils import MockMicrogridClient
from ..utils.component_data_wrapper import (
    BatteryDataWrapper,
    EvChargerDataWrapper,
    InverterDataWrapper,
    MeterDataWrapper,
)
from .mock_resampler import MockResampler


class MockMicrogrid:  # pylint: disable=too-many-instance-attributes
    """Setup a MockApi instance with multiple component layouts for tests."""

    grid_id = ComponentId(1)
    _grid_meter_id = ComponentId(4)

    chp_id_suffix = 5
    evc_id_suffix = 6
    meter_id_suffix = 7
    inverter_id_suffix = 8
    battery_id_suffix = 9

    mock_client: MockMicrogridClient
    mock_resampler: MockResampler

    def __init__(  # pylint: disable=too-many-arguments,too-many-positional-arguments
        self,
        grid_meter: bool | None = None,
        api_client_streaming: bool = False,
        num_values: int = 2000,
        sample_rate_s: float = 0.01,
        num_namespaces: int = 1,
        fuse: Fuse | None = Fuse(10_000.0),
        graph: _MicrogridComponentGraph | None = None,
        mocker: MockerFixture | None = None,
    ):
        """Create a new instance.

        Args:
            grid_meter: optional, whether there is a meter successor of the GRID component.
            api_client_streaming: whether the mock client should be configured to stream
                raw data from the API client.
            num_values: number of values to generate for each component.
            sample_rate_s: sample rate in seconds.
            num_namespaces: number of namespaces that each metric should be available
                to.  Useful in tests where multiple namespaces (logical_meter,
                battery_pool, etc) are used, and the same metric is used by formulas in
                different namespaces.
            fuse: optional, the fuse to use for the grid connection.
            graph: optional, a graph of components to use instead of the default grid
                layout. If specified, grid_meter must be None.
            mocker: optional, a mocker to pass to the mock client and mock resampler.

        Raises:
            ValueError: if both grid_meter and graph are specified.
        """
        self._mocker = mocker
        if grid_meter is not None and graph is not None:
            raise ValueError("grid_meter and graph are mutually exclusive")

        self._components: set[Component] = (
            {
                Component(
                    ComponentId(1), ComponentCategory.GRID, None, GridMetadata(fuse)
                ),
            }
            if graph is None
            else graph.components()
        )

        self._connections: set[Connection] = (
            set() if graph is None else graph.connections()
        )

        self._id_increment = 0 if graph is None else len(self._components)
        self._api_client_streaming = api_client_streaming
        self._num_values = num_values
        self._sample_rate_s = sample_rate_s
        self._namespaces = num_namespaces

        self._connect_to = self.grid_id

        def filter_comp(category: ComponentCategory) -> list[ComponentId]:
            if graph is None:
                return []
            return sorted(
                list(
                    map(
                        lambda c: c.component_id,
                        graph.components(component_categories={category}),
                    )
                )
            )

        def inverters(comp_type: InverterType) -> list[ComponentId]:
            if graph is None:
                return []

            return sorted(
                [
                    c.component_id
                    for c in graph.components(
                        component_categories={ComponentCategory.INVERTER}
                    )
                    if c.type == comp_type
                ]
            )

        self.chp_ids: list[ComponentId] = filter_comp(ComponentCategory.CHP)
        self.battery_ids: list[ComponentId] = filter_comp(ComponentCategory.BATTERY)
        self.evc_ids: list[ComponentId] = filter_comp(ComponentCategory.EV_CHARGER)
        self.meter_ids: list[ComponentId] = filter_comp(ComponentCategory.METER)

        self.battery_inverter_ids: list[ComponentId] = inverters(InverterType.BATTERY)
        self.pv_inverter_ids: list[ComponentId] = inverters(InverterType.SOLAR)

        self.bat_inv_map: dict[ComponentId, ComponentId] = (
            {}
            if graph is None
            else {
                # Hacky, ignores multiple batteries behind one inverter
                list(graph.successors(c.component_id))[0].component_id: c.component_id
                for c in graph.components(
                    component_categories={ComponentCategory.INVERTER}
                )
                if c.type == InverterType.BATTERY
            }
        )

        self.evc_component_states: dict[ComponentId, EVChargerComponentState] = {}
        self.evc_cable_states: dict[ComponentId, EVChargerCableState] = {}

        self._streaming_coros: list[tuple[ComponentId, Coroutine[None, None, None]]] = (
            []
        )
        """The streaming coroutines for each component.

        The tuple stores the component id we are streaming for as the first item and the
        coroutine as the second item.
        """

        self._streaming_tasks: dict[ComponentId, asyncio.Task[None]] = {}
        """The streaming tasks for each component.

        The key is the component id we are streaming for in this task.
        """

        if grid_meter:
            self._connect_to = self._grid_meter_id
            self._connections.add(Connection(self.grid_id, self._grid_meter_id))
            self._components.add(
                Component(self._grid_meter_id, ComponentCategory.METER)
            )
            self.meter_ids.append(self._grid_meter_id)
            self._start_meter_streaming(self._grid_meter_id)

    async def start(self, mocker: MockerFixture | None = None) -> None:
        """Init the mock microgrid client and start the mock resampler."""
        # Return if it is already started
        if hasattr(self, "mock_client") or hasattr(self, "mock_resampler"):
            return

        if mocker is None:
            mocker = self._mocker
        assert mocker is not None, "A mocker must be set at init or start time"

        # This binding to a local is needed because Python uses late binding for
        # closures and `mocker` could be bound to `None` again after the lambda is
        # created. See:
        # https://mypy.readthedocs.io/en/stable/common_issues.html#narrowing-and-inner-functions
        local_mocker = mocker
        self.init_mock_client(lambda mock_client: mock_client.initialize(local_mocker))
        self.mock_resampler = MockResampler(
            mocker,
            ResamplerConfig2(timedelta(seconds=self._sample_rate_s)),
            bat_inverter_ids=self.battery_inverter_ids,
            pv_inverter_ids=self.pv_inverter_ids,
            evc_ids=self.evc_ids,
            meter_ids=self.meter_ids,
            chp_ids=self.chp_ids,
            namespaces=self._namespaces,
        )

    def init_mock_client(
        self, initialize_cb: Callable[[MockMicrogridClient], None]
    ) -> None:
        """Set up the mock client. Does not start the streaming tasks."""
        self.mock_client = MockMicrogridClient(self._components, self._connections)
        initialize_cb(self.mock_client)

    def start_mock_client(
        self, initialize_cb: Callable[[MockMicrogridClient], None]
    ) -> MockMicrogridClient:
        """Start the mock client.

        Creates the microgrid mock client, initializes it, and starts the streaming
        tasks.

        For unittests, users should use the `start()` method.

        Args:
            initialize_cb: callback to initialize the mock client.

        Returns:
            A MockMicrogridClient instance.
        """
        self.init_mock_client(initialize_cb)

        def _done_callback(task: asyncio.Task[None]) -> None:
            try:
                task.result()
            except (asyncio.CancelledError, Exception) as exc:
                raise SystemExit(
                    f"Streaming task {task.get_name()!r} failed: {exc}"
                ) from exc

        for component_id, coro in self._streaming_coros:
            task = asyncio.create_task(coro, name=f"component-id:{component_id}")
            self._streaming_tasks[component_id] = task
            task.add_done_callback(_done_callback)

        return self.mock_client

    async def _comp_data_send_task(
        self,
        comp_id: ComponentId,
        make_comp_data: Callable[[int, datetime], ComponentData],
    ) -> None:
        for value in range(1, self._num_values + 1):
            timestamp = datetime.now(tz=timezone.utc)
            val_to_send = value + int(int(comp_id) / 10)
            # for inverters with component_id > 100, send only half the messages.
            if int(comp_id) % 10 == self.inverter_id_suffix:
                if int(comp_id) < 100 or value <= 5:
                    await self.mock_client.send(make_comp_data(val_to_send, timestamp))
            else:
                await self.mock_client.send(make_comp_data(val_to_send, timestamp))
            await asyncio.sleep(self._sample_rate_s)

        await self.mock_client.close_channel(comp_id)

    def _start_meter_streaming(self, meter_id: ComponentId) -> None:
        if not self._api_client_streaming:
            return
        self._streaming_coros.append(
            (
                meter_id,
                self._comp_data_send_task(
                    meter_id,
                    lambda value, ts: MeterDataWrapper(
                        component_id=meter_id,
                        timestamp=ts,
                        reactive_power=2 * value,
                        active_power=value,
                        current_per_phase=(value + 100.0, value + 101.0, value + 102.0),
                        voltage_per_phase=(value + 200.0, value + 199.8, value + 200.2),
                    ),
                ),
            )
        )

    def _start_battery_streaming(self, bat_id: ComponentId) -> None:
        if not self._api_client_streaming:
            return
        self._streaming_coros.append(
            (
                bat_id,
                self._comp_data_send_task(
                    bat_id,
                    lambda value, ts: BatteryDataWrapper(
                        component_id=bat_id, timestamp=ts, soc=value
                    ),
                ),
            )
        )

    def _start_inverter_streaming(self, inv_id: ComponentId) -> None:
        if not self._api_client_streaming:
            return
        self._streaming_coros.append(
            (
                inv_id,
                self._comp_data_send_task(
                    inv_id,
                    lambda value, ts: InverterDataWrapper(
                        component_id=inv_id,
                        timestamp=ts,
                        active_power=value,
                        reactive_power=2 * value,
                    ),
                ),
            )
        )

    def _start_ev_charger_streaming(self, evc_id: ComponentId) -> None:
        if not self._api_client_streaming:
            return
        self._streaming_coros.append(
            (
                evc_id,
                self._comp_data_send_task(
                    evc_id,
                    lambda value, ts: EvChargerDataWrapper(
                        component_id=evc_id,
                        timestamp=ts,
                        active_power=value,
                        reactive_power=2 * value,
                        current_per_phase=(value + 10.0, value + 11.0, value + 12.0),
                        component_state=self.evc_component_states[evc_id],
                        cable_state=self.evc_cable_states[evc_id],
                    ),
                ),
            )
        )

    def add_consumer_meters(self, count: int = 1) -> None:
        """Add consumer meters to the mock microgrid.

        A consumer meter is a meter with unknown successors
        that draw a certain amount of power.

        We use it to calculate the total power consumption
        at the grid connection point.

        Args:
            count: number of consumer meters to add.
        """
        for _ in range(count):
            meter_id = ComponentId(self._id_increment * 10 + self.meter_id_suffix)
            self._id_increment += 1
            self.meter_ids.append(meter_id)
            self._components.add(
                Component(
                    meter_id,
                    ComponentCategory.METER,
                )
            )
            self._connections.add(Connection(self._connect_to, meter_id))
            self._start_meter_streaming(meter_id)

    def add_chps(self, count: int, no_meters: bool = False) -> None:
        """Add CHPs with connected meters to the mock microgrid.

        Args:
            count: number of CHPs to add.
            no_meters: if True, do not add a meter for each CHP.
        """
        for _ in range(count):
            chp_id = ComponentId(self._id_increment * 10 + self.chp_id_suffix)
            self.chp_ids.append(chp_id)
            self._components.add(
                Component(
                    chp_id,
                    ComponentCategory.CHP,
                )
            )
            if no_meters:
                self._connections.add(Connection(self._connect_to, chp_id))
            else:
                meter_id = ComponentId(self._id_increment * 10 + self.meter_id_suffix)
                self.meter_ids.append(meter_id)
                self._components.add(
                    Component(
                        meter_id,
                        ComponentCategory.METER,
                    )
                )
                self._start_meter_streaming(meter_id)
                self._connections.add(Connection(self._connect_to, meter_id))
                self._connections.add(Connection(meter_id, chp_id))

            self._id_increment += 1

    def add_batteries(self, count: int, no_meter: bool = False) -> None:
        """Add batteries with connected inverters and meters to the microgrid.

        Args:
            count: number of battery sets to add.
            no_meter: if True, do not add a meter for each battery set.
        """
        for _ in range(count):
            meter_id = ComponentId(self._id_increment * 10 + self.meter_id_suffix)
            inv_id = ComponentId(self._id_increment * 10 + self.inverter_id_suffix)
            bat_id = ComponentId(self._id_increment * 10 + self.battery_id_suffix)
            self._id_increment += 1

            self.battery_inverter_ids.append(inv_id)
            self.battery_ids.append(bat_id)
            self.bat_inv_map[bat_id] = inv_id

            self._components.add(
                Component(inv_id, ComponentCategory.INVERTER, InverterType.BATTERY)
            )
            self._components.add(
                Component(
                    bat_id,
                    ComponentCategory.BATTERY,
                )
            )
            self._start_battery_streaming(bat_id)
            self._start_inverter_streaming(inv_id)

            if no_meter:
                self._connections.add(Connection(self._connect_to, inv_id))
            else:
                self.meter_ids.append(meter_id)
                self._components.add(
                    Component(
                        meter_id,
                        ComponentCategory.METER,
                    )
                )
                self._start_meter_streaming(meter_id)
                self._connections.add(Connection(self._connect_to, meter_id))
                self._connections.add(Connection(meter_id, inv_id))
            self._connections.add(Connection(inv_id, bat_id))

    def add_solar_inverters(self, count: int, no_meter: bool = False) -> None:
        """Add pv inverters and connected pv meters to the microgrid.

        Args:
            count: number of inverters to add to the microgrid.
            no_meter: if True, do not add a meter for each inverter.
        """
        for _ in range(count):
            meter_id = ComponentId(self._id_increment * 10 + self.meter_id_suffix)
            inv_id = ComponentId(self._id_increment * 10 + self.inverter_id_suffix)
            self._id_increment += 1

            self.pv_inverter_ids.append(inv_id)

            self._components.add(
                Component(
                    inv_id,
                    ComponentCategory.INVERTER,
                    InverterType.SOLAR,
                )
            )
            self._start_inverter_streaming(inv_id)

            if no_meter:
                self._connections.add(Connection(self._connect_to, inv_id))
            else:
                self.meter_ids.append(meter_id)
                self._components.add(
                    Component(
                        meter_id,
                        ComponentCategory.METER,
                    )
                )
                self._start_meter_streaming(meter_id)
                self._connections.add(Connection(self._connect_to, meter_id))
                self._connections.add(Connection(meter_id, inv_id))

    def add_ev_chargers(self, count: int) -> None:
        """Add EV Chargers to the microgrid.

        Args:
            count: Number of ev chargers to add to the microgrid.
        """
        for _ in range(count):
            evc_id = ComponentId(self._id_increment * 10 + self.evc_id_suffix)
            self._id_increment += 1

            self.evc_ids.append(evc_id)
            self.evc_component_states[evc_id] = EVChargerComponentState.READY
            self.evc_cable_states[evc_id] = EVChargerCableState.UNPLUGGED

            self._components.add(
                Component(
                    evc_id,
                    ComponentCategory.EV_CHARGER,
                )
            )
            self._start_ev_charger_streaming(evc_id)
            self._connections.add(Connection(self._connect_to, evc_id))

    async def send_meter_data(self, values: list[float]) -> None:
        """Send raw meter data from the mock microgrid.

        Args:
            values: list of active power values for each meter.
        """
        assert len(values) == len(self.meter_ids)
        timestamp = datetime.now(tz=timezone.utc)
        for comp_id, value in zip(self.meter_ids, values):
            await self.mock_client.send(
                MeterDataWrapper(
                    component_id=comp_id,
                    timestamp=timestamp,
                    active_power=value,
                    current_per_phase=(
                        value + 100.0,
                        value + 101.0,
                        value + 102.0,
                    ),
                    voltage_per_phase=(
                        value + 200.0,
                        value + 199.8,
                        value + 200.2,
                    ),
                )
            )

    async def send_battery_data(self, socs: list[float]) -> None:
        """Send raw battery data from the mock microgrid.

        Args:
            socs: list of soc values for each battery.
        """
        assert len(socs) == len(self.battery_ids)
        timestamp = datetime.now(tz=timezone.utc)
        for comp_id, value in zip(self.battery_ids, socs):
            await self.mock_client.send(
                BatteryDataWrapper(component_id=comp_id, timestamp=timestamp, soc=value)
            )

    async def send_battery_inverter_data(self, values: list[float]) -> None:
        """Send raw battery inverter data from the mock microgrid.

        Args:
            values: list of active power values for each battery inverter.
        """
        assert len(values) == len(self.battery_inverter_ids)
        timestamp = datetime.now(tz=timezone.utc)
        for comp_id, value in zip(self.battery_inverter_ids, values):
            await self.mock_client.send(
                InverterDataWrapper(
                    component_id=comp_id, timestamp=timestamp, active_power=value
                )
            )

    async def send_pv_inverter_data(self, values: list[float]) -> None:
        """Send raw pv inverter data from the mock microgrid.

        Args:
            values: list of active power values for each pv inverter.
        """
        assert len(values) == len(self.pv_inverter_ids)
        timestamp = datetime.now(tz=timezone.utc)
        for comp_id, value in zip(self.pv_inverter_ids, values):
            await self.mock_client.send(
                InverterDataWrapper(
                    component_id=comp_id, timestamp=timestamp, active_power=value
                )
            )

    async def send_ev_charger_data(self, values: list[float]) -> None:
        """Send raw ev charger data from the mock microgrid.

        Args:
            values: list of active power values for each ev charger.
        """
        assert len(values) == len(self.evc_ids)
        timestamp = datetime.now(tz=timezone.utc)
        for comp_id, value in zip(self.evc_ids, values):
            await self.mock_client.send(
                EvChargerDataWrapper(
                    component_id=comp_id,
                    timestamp=timestamp,
                    active_power=value,
                    current_per_phase=(
                        value + 100.0,
                        value + 101.0,
                        value + 102.0,
                    ),
                    component_state=self.evc_component_states[comp_id],
                    cable_state=self.evc_cable_states[comp_id],
                )
            )

    async def cleanup(self) -> None:
        """Clean up after a test."""
        # pylint: disable=protected-access
        if _data_pipeline._DATA_PIPELINE:
            await _data_pipeline._DATA_PIPELINE._stop()

        await self.mock_resampler._stop()

        for _, coro in self._streaming_coros:
            coro.close()

        for task in self._streaming_tasks.values():
            await cancel_and_await(task)
        microgrid.connection_manager._CONNECTION_MANAGER = None
        # pylint: enable=protected-access

    async def __aenter__(self) -> MockMicrogrid:
        """Enter context manager."""
        await self.start()
        return self

    async def __aexit__(
        self,
        exc_type: type[BaseException] | None,
        exc_value: BaseException | None,
        traceback: TracebackType | None,
        /,
    ) -> None:
        """Exit context manager."""
        await self.cleanup()



================================================
FILE: tests/timeseries/mock_resampler.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Mock data_pipeline."""


import asyncio
import math
from datetime import datetime

from frequenz.channels import Broadcast, Receiver, Sender
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentMetricId
from frequenz.quantities import Quantity
from pytest_mock import MockerFixture

from frequenz.sdk._internal._asyncio import cancel_and_await
from frequenz.sdk.microgrid._data_pipeline import _DataPipeline
from frequenz.sdk.microgrid._data_sourcing import ComponentMetricRequest
from frequenz.sdk.timeseries import ResamplerConfig2, Sample
from frequenz.sdk.timeseries.formula_engine._formula_generators._formula_generator import (
    NON_EXISTING_COMPONENT_ID,
)

# pylint: disable=too-many-instance-attributes disable=too-many-locals


class MockResampler:
    """Mock resampler."""

    def __init__(  # pylint: disable=too-many-arguments,too-many-positional-arguments
        self,
        mocker: MockerFixture,
        resampler_config: ResamplerConfig2,
        bat_inverter_ids: list[ComponentId],
        pv_inverter_ids: list[ComponentId],
        evc_ids: list[ComponentId],
        chp_ids: list[ComponentId],
        meter_ids: list[ComponentId],
        namespaces: int,
    ) -> None:
        """Create a `MockDataPipeline` instance."""
        self._data_pipeline = _DataPipeline(resampler_config)

        self._channel_registry = self._data_pipeline._channel_registry
        self._resampler_request_channel = Broadcast[ComponentMetricRequest](
            name="resampler-request"
        )
        self._input_channels_receivers: dict[str, list[Receiver[Sample[Quantity]]]] = {}

        def metric_senders(
            comp_ids: list[ComponentId],
            metric_id: ComponentMetricId,
        ) -> list[Sender[Sample[Quantity]]]:
            senders: list[Sender[Sample[Quantity]]] = []
            for comp_id in comp_ids:
                name = f"{comp_id}:{metric_id}"
                senders.append(
                    self._channel_registry.get_or_create(
                        Sample[Quantity], name
                    ).new_sender()
                )
                self._input_channels_receivers[name] = [
                    self._channel_registry.get_or_create(
                        Sample[Quantity], name
                    ).new_receiver()
                    for _ in range(namespaces)
                ]
            return senders

        # Active power senders
        self._bat_inverter_power_senders = metric_senders(
            bat_inverter_ids, ComponentMetricId.ACTIVE_POWER
        )
        self._pv_inverter_power_senders = metric_senders(
            pv_inverter_ids, ComponentMetricId.ACTIVE_POWER
        )
        self._ev_power_senders = metric_senders(evc_ids, ComponentMetricId.ACTIVE_POWER)

        self._chp_power_senders = metric_senders(
            chp_ids, ComponentMetricId.ACTIVE_POWER
        )
        self._meter_power_senders = metric_senders(
            meter_ids, ComponentMetricId.ACTIVE_POWER
        )
        self._non_existing_component_sender = metric_senders(
            [NON_EXISTING_COMPONENT_ID], ComponentMetricId.ACTIVE_POWER
        )[0]

        # Frequency senders
        self._bat_inverter_frequency_senders = metric_senders(
            bat_inverter_ids, ComponentMetricId.FREQUENCY
        )
        self._meter_frequency_senders = metric_senders(
            meter_ids, ComponentMetricId.FREQUENCY
        )

        # Reactive power senders
        self._meter_reactive_power_senders = metric_senders(
            meter_ids, ComponentMetricId.REACTIVE_POWER
        )
        self._bat_inverter_reactive_power_senders = metric_senders(
            bat_inverter_ids, ComponentMetricId.REACTIVE_POWER
        )
        self._ev_reactive_power_senders = metric_senders(
            evc_ids, ComponentMetricId.REACTIVE_POWER
        )

        def multi_phase_senders(
            ids: list[ComponentId],
            metrics: tuple[ComponentMetricId, ComponentMetricId, ComponentMetricId],
        ) -> list[list[Sender[Sample[Quantity]]]]:
            senders: list[list[Sender[Sample[Quantity]]]] = []
            for comp_id in ids:
                p1_name = f"{comp_id}:{metrics[0]}"
                p2_name = f"{comp_id}:{metrics[1]}"
                p3_name = f"{comp_id}:{metrics[2]}"

                senders.append(
                    [
                        self._channel_registry.get_or_create(
                            Sample[Quantity], p1_name
                        ).new_sender(),
                        self._channel_registry.get_or_create(
                            Sample[Quantity], p2_name
                        ).new_sender(),
                        self._channel_registry.get_or_create(
                            Sample[Quantity], p3_name
                        ).new_sender(),
                    ]
                )
                self._input_channels_receivers[p1_name] = [
                    self._channel_registry.get_or_create(
                        Sample[Quantity], p1_name
                    ).new_receiver()
                    for _ in range(namespaces)
                ]
                self._input_channels_receivers[p2_name] = [
                    self._channel_registry.get_or_create(
                        Sample[Quantity], p2_name
                    ).new_receiver()
                    for _ in range(namespaces)
                ]
                self._input_channels_receivers[p3_name] = [
                    self._channel_registry.get_or_create(
                        Sample[Quantity], p3_name
                    ).new_receiver()
                    for _ in range(namespaces)
                ]
            return senders

        def current_senders(
            ids: list[ComponentId],
        ) -> list[list[Sender[Sample[Quantity]]]]:
            return multi_phase_senders(
                ids,
                (
                    ComponentMetricId.CURRENT_PHASE_1,
                    ComponentMetricId.CURRENT_PHASE_2,
                    ComponentMetricId.CURRENT_PHASE_3,
                ),
            )

        def voltage_senders(
            ids: list[ComponentId],
        ) -> list[list[Sender[Sample[Quantity]]]]:
            return multi_phase_senders(
                ids,
                (
                    ComponentMetricId.VOLTAGE_PHASE_1,
                    ComponentMetricId.VOLTAGE_PHASE_2,
                    ComponentMetricId.VOLTAGE_PHASE_3,
                ),
            )

        def power_3_phase_senders(
            ids: list[ComponentId],
        ) -> list[list[Sender[Sample[Quantity]]]]:
            return multi_phase_senders(
                ids,
                (
                    ComponentMetricId.ACTIVE_POWER_PHASE_1,
                    ComponentMetricId.ACTIVE_POWER_PHASE_2,
                    ComponentMetricId.ACTIVE_POWER_PHASE_3,
                ),
            )

        self._bat_inverter_current_senders = current_senders(bat_inverter_ids)
        self._pv_inverter_current_senders = current_senders(pv_inverter_ids)
        self._ev_current_senders = current_senders(evc_ids)
        self._chp_current_senders = current_senders(chp_ids)
        self._meter_current_senders = current_senders(meter_ids)

        self._meter_voltage_senders = voltage_senders(meter_ids)
        self._meter_power_3_phase_senders = power_3_phase_senders(meter_ids)

        self._next_ts = datetime.now()

        mocker.patch(
            "frequenz.sdk.microgrid._data_pipeline._DataPipeline"
            "._resampling_request_sender",
            self._resampling_request_sender,
        )
        mocker.patch(
            "frequenz.sdk.microgrid._data_pipeline._DATA_PIPELINE", self._data_pipeline
        )

        self._forward_tasks: dict[str, asyncio.Task[None]] = {}
        task = asyncio.create_task(self._handle_resampling_requests())
        task.add_done_callback(self._handle_task_done)
        self._request_handler_task = task

    def next_ts(self) -> None:
        """Increment the timestamp."""
        self._next_ts = datetime.now()

    def _handle_task_done(self, task: asyncio.Task[None]) -> None:
        if task.cancelled():
            return
        if exc := task.exception():
            raise SystemExit(f"Task {task.get_name()!r} failed: {exc}") from exc

    async def _stop(self) -> None:
        tasks_to_stop = [
            cancel_and_await(task)
            for task in list(self._forward_tasks.values())
            + [self._request_handler_task]
        ]
        await asyncio.gather(*tasks_to_stop)

    def _resampling_request_sender(self) -> Sender[ComponentMetricRequest]:
        return self._resampler_request_channel.new_sender()

    async def _channel_forward_messages(
        self, receiver: Receiver[Sample[Quantity]], sender: Sender[Sample[Quantity]]
    ) -> None:
        async for sample in receiver:
            await sender.send(sample)

    async def _handle_resampling_requests(self) -> None:
        async for request in self._resampler_request_channel.new_receiver():
            name = request.get_channel_name()
            if name in self._forward_tasks:
                continue
            input_chan_recv_name = f"{request.component_id}:{request.metric_id}"
            input_chan_recv = self._input_channels_receivers[input_chan_recv_name].pop()
            assert input_chan_recv is not None
            output_chan_sender: Sender[Sample[Quantity]] = (
                self._channel_registry.get_or_create(
                    Sample[Quantity], name
                ).new_sender()
            )
            task = asyncio.create_task(
                self._channel_forward_messages(
                    input_chan_recv,
                    output_chan_sender,
                ),
                name=name,
            )

            def _done_callback(task: asyncio.Task[None]) -> None:
                del self._forward_tasks[task.get_name()]
                self._handle_task_done(task)

            task.add_done_callback(_done_callback)
            self._forward_tasks[name] = task

    def make_sample(self, value: float | None) -> Sample[Quantity]:
        """Create a sample with the given value."""
        return Sample(
            self._next_ts,
            None if value is None or math.isnan(value) else Quantity(value),
        )

    async def send_meter_power(self, values: list[float | None]) -> None:
        """Send the given values as resampler output for meter power."""
        assert len(values) == len(self._meter_power_senders)
        for chan, value in zip(self._meter_power_senders, values):
            sample = self.make_sample(value)
            await chan.send(sample)

    async def send_chp_power(self, values: list[float | None]) -> None:
        """Send the given values as resampler output for CHP power."""
        assert len(values) == len(self._chp_power_senders)
        for chan, value in zip(self._chp_power_senders, values):
            sample = self.make_sample(value)
            await chan.send(sample)

    async def send_pv_inverter_power(self, values: list[float | None]) -> None:
        """Send the given values as resampler output for PV Inverter power."""
        assert len(values) == len(self._pv_inverter_power_senders)
        for chan, value in zip(self._pv_inverter_power_senders, values):
            sample = self.make_sample(value)
            await chan.send(sample)

    async def send_meter_frequency(self, values: list[float | None]) -> None:
        """Send the given values as resampler output for meter frequency."""
        assert len(values) == len(self._meter_frequency_senders)
        for sender, value in zip(self._meter_frequency_senders, values):
            sample = self.make_sample(value)
            await sender.send(sample)

    async def send_bat_inverter_frequency(self, values: list[float | None]) -> None:
        """Send the given values as resampler output for battery inverter frequency."""
        assert len(values) == len(self._bat_inverter_frequency_senders)
        for chan, value in zip(self._bat_inverter_frequency_senders, values):
            sample = self.make_sample(value)
            await chan.send(sample)

    async def send_evc_power(self, values: list[float | None]) -> None:
        """Send the given values as resampler output for EV Charger power."""
        assert len(values) == len(self._ev_power_senders)
        for chan, value in zip(self._ev_power_senders, values):
            sample = self.make_sample(value)
            await chan.send(sample)

    async def send_bat_inverter_power(self, values: list[float | None]) -> None:
        """Send the given values as resampler output for battery inverter power."""
        assert len(values) == len(self._bat_inverter_power_senders)
        for chan, value in zip(self._bat_inverter_power_senders, values):
            sample = self.make_sample(value)
            await chan.send(sample)

    async def send_meter_reactive_power(self, values: list[float | None]) -> None:
        """Send the given values as resampler output for meter reactive power."""
        assert len(values) == len(self._meter_reactive_power_senders)
        for chan, value in zip(self._meter_reactive_power_senders, values):
            sample = self.make_sample(value)
            await chan.send(sample)

    async def send_bat_inverter_reactive_power(
        self, values: list[float | None]
    ) -> None:
        """Send the given values as resampler output for battery inverter reactive power."""
        assert len(values) == len(self._bat_inverter_reactive_power_senders)
        for chan, value in zip(self._bat_inverter_reactive_power_senders, values):
            sample = self.make_sample(value)
            await chan.send(sample)

    async def send_evc_reactive_power(self, values: list[float | None]) -> None:
        """Send the given values as resampler output for EV Charger reactive power."""
        assert len(values) == len(self._ev_reactive_power_senders)
        for chan, value in zip(self._ev_reactive_power_senders, values):
            sample = self.make_sample(value)
            await chan.send(sample)

    async def send_non_existing_component_value(self) -> None:
        """Send a value for a non existing component."""
        sample = self.make_sample(None)
        await self._non_existing_component_sender.send(sample)

    async def send_evc_current(self, values: list[list[float | None]]) -> None:
        """Send the given values as resampler output for EV Charger current."""
        assert len(values) == len(self._ev_current_senders)
        for chan, evc_values in zip(self._ev_current_senders, values):
            assert len(evc_values) == 3  # 3 values for phases
            for phase, value in enumerate(evc_values):
                sample = self.make_sample(value)
                await chan[phase].send(sample)

    async def send_bat_inverter_current(self, values: list[list[float | None]]) -> None:
        """Send the given values as resampler output for battery inverter current."""
        assert len(values) == len(self._bat_inverter_current_senders)
        for chan, bat_values in zip(self._bat_inverter_current_senders, values):
            assert len(bat_values) == 3  # 3 values for phases
            for phase, value in enumerate(bat_values):
                sample = self.make_sample(value)
                await chan[phase].send(sample)

    async def send_meter_current(self, values: list[list[float | None]]) -> None:
        """Send the given values as resampler output for meter current."""
        assert len(values) == len(self._meter_current_senders)
        for chan, meter_values in zip(self._meter_current_senders, values):
            assert len(meter_values) == 3  # 3 values for phases
            for phase, value in enumerate(meter_values):
                sample = self.make_sample(value)
                await chan[phase].send(sample)

    async def send_meter_voltage(self, values: list[list[float | None]]) -> None:
        """Send the given values as resampler output for meter voltage."""
        assert len(values) == len(self._meter_voltage_senders)
        for chan, meter_values in zip(self._meter_voltage_senders, values):
            assert len(meter_values) == 3  # 3 values for phases
            for phase, value in enumerate(meter_values):
                sample = self.make_sample(value)
                await chan[phase].send(sample)

    async def send_meter_power_3_phase(self, values: list[list[float | None]]) -> None:
        """Send the given values as resampler output for meter active power."""
        assert len(values) == len(self._meter_power_3_phase_senders)
        for chan, meter_values in zip(self._meter_power_3_phase_senders, values):
            assert len(meter_values) == 3  # 3 values for phases
            for phase, value in enumerate(meter_values):
                sample = self.make_sample(value)
                await chan[phase].send(sample)



================================================
FILE: tests/timeseries/test_base_types.py
================================================
# License: MIT
# Copyright © 2024 Frequenz Energy-as-a-Service GmbH

"""Tests for timeseries base types."""


from datetime import datetime

from frequenz.quantities import Power

from frequenz.sdk.timeseries._base_types import Bounds, SystemBounds


def test_bounds_contains() -> None:
    """Tests with complete bounds."""
    bounds = Bounds(lower=Power.from_watts(10), upper=Power.from_watts(100))
    assert Power.from_watts(50) in bounds  # within
    assert Power.from_watts(10) in bounds  # at lower
    assert Power.from_watts(100) in bounds  # at upper
    assert Power.from_watts(9) not in bounds  # below lower
    assert Power.from_watts(101) not in bounds  # above upper


def test_bounds_contains_no_lower() -> None:
    """Tests without lower bound."""
    bounds_no_lower = Bounds(lower=None, upper=Power.from_watts(100))
    assert Power.from_watts(50) in bounds_no_lower  # within upper
    assert Power.from_watts(100) in bounds_no_lower  # at upper
    assert Power.from_watts(101) not in bounds_no_lower  # above upper


def test_bounds_contains_no_upper() -> None:
    """Tests without upper bound."""
    bounds_no_upper = Bounds(lower=Power.from_watts(10), upper=None)
    assert Power.from_watts(50) in bounds_no_upper  # within lower
    assert Power.from_watts(10) in bounds_no_upper  # at lower
    assert Power.from_watts(9) not in bounds_no_upper  # below lower


def test_bounds_contains_no_bounds() -> None:
    """Tests with no bounds."""
    bounds_no_bounds: Bounds[Power | None] = Bounds(lower=None, upper=None)
    assert Power.from_watts(50) in bounds_no_bounds  # any value within bounds


INCLUSION_BOUND = Bounds(lower=Power.from_watts(10), upper=Power.from_watts(100))
EXCLUSION_BOUND = Bounds(lower=Power.from_watts(40), upper=Power.from_watts(50))


def test_system_bounds_contains() -> None:
    """Tests with complete system bounds."""
    system_bounds = SystemBounds(
        timestamp=datetime.now(),
        inclusion_bounds=INCLUSION_BOUND,
        exclusion_bounds=EXCLUSION_BOUND,
    )

    assert Power.from_watts(30) in system_bounds  # within inclusion, not in exclusion
    assert Power.from_watts(45) not in system_bounds  # within inclusion and exclusion
    assert Power.from_watts(110) not in system_bounds  # outside inclusion


def test_system_bounds_contains_no_exclusion() -> None:
    """Tests with no exclusion bounds."""
    system_bounds_no_exclusion = SystemBounds(
        timestamp=datetime.now(),
        inclusion_bounds=INCLUSION_BOUND,
        exclusion_bounds=None,
    )
    assert Power.from_watts(30) in system_bounds_no_exclusion  # within inclusion
    assert Power.from_watts(110) not in system_bounds_no_exclusion  # outside inclusion


def test_system_bounds_contains_no_inclusion() -> None:
    """Tests with no inclusion bounds."""
    system_bounds_no_inclusion = SystemBounds(
        timestamp=datetime.now(),
        inclusion_bounds=None,
        exclusion_bounds=EXCLUSION_BOUND,
    )
    assert Power.from_watts(30) not in system_bounds_no_inclusion  # outside exclusion
    assert Power.from_watts(45) not in system_bounds_no_inclusion  # within exclusion


def test_system_bounds_contains_no_bounds() -> None:
    """Tests with no bounds."""
    system_bounds_no_bounds = SystemBounds(
        timestamp=datetime.now(),
        inclusion_bounds=None,
        exclusion_bounds=None,
    )
    assert Power.from_watts(30) not in system_bounds_no_bounds  # any value outside
    assert Power.from_watts(110) not in system_bounds_no_bounds  # any value outside



================================================
FILE: tests/timeseries/test_consumer.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Test the logical component for calculating high level consumer metrics."""

from contextlib import AsyncExitStack

from frequenz.quantities import Power
from pytest_mock import MockerFixture

from frequenz.sdk import microgrid

from .mock_microgrid import MockMicrogrid


class TestConsumer:
    """Tests for the consumer power formula."""

    async def test_consumer_power_grid_meter(self, mocker: MockerFixture) -> None:
        """Test the consumer power formula with a grid meter."""
        mockgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
        mockgrid.add_batteries(2)
        mockgrid.add_solar_inverters(2)

        async with mockgrid, AsyncExitStack() as stack:
            consumer = microgrid.consumer()
            stack.push_async_callback(consumer.stop)
            consumer_power_receiver = consumer.power.new_receiver()

            await mockgrid.mock_resampler.send_meter_power([20.0, 2.0, 3.0, 4.0, 5.0])
            assert (await consumer_power_receiver.receive()).value == Power.from_watts(
                6.0
            )

    async def test_consumer_power_no_grid_meter(self, mocker: MockerFixture) -> None:
        """Test the consumer power formula without a grid meter."""
        mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
        mockgrid.add_consumer_meters()
        mockgrid.add_batteries(2)
        mockgrid.add_solar_inverters(2)

        async with mockgrid, AsyncExitStack() as stack:
            consumer = microgrid.consumer()
            stack.push_async_callback(consumer.stop)
            consumer_power_receiver = consumer.power.new_receiver()

            await mockgrid.mock_resampler.send_meter_power([20.0, 2.0, 3.0, 4.0, 5.0])
            assert (await consumer_power_receiver.receive()).value == Power.from_watts(
                20.0
            )

    async def test_consumer_power_no_grid_meter_no_consumer_meter(
        self, mocker: MockerFixture
    ) -> None:
        """Test the consumer power formula without a grid meter."""
        mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
        mockgrid.add_batteries(2)
        mockgrid.add_solar_inverters(2)

        async with mockgrid, AsyncExitStack() as stack:
            consumer = microgrid.consumer()
            stack.push_async_callback(consumer.stop)
            consumer_power_receiver = consumer.power.new_receiver()

            await mockgrid.mock_resampler.send_non_existing_component_value()
            assert (await consumer_power_receiver.receive()).value == Power.from_watts(
                0.0
            )

    async def test_consumer_power_fallback_formula_with_grid_meter(
        self, mocker: MockerFixture
    ) -> None:
        """Test the consumer power formula with a grid meter."""
        mockgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
        mockgrid.add_batteries(1)
        mockgrid.add_solar_inverters(1)
        mockgrid.add_solar_inverters(1, no_meter=True)

        # formula is grid_meter - battery - pv1 - pv2

        async with mockgrid, AsyncExitStack() as stack:
            consumer = microgrid.consumer()
            stack.push_async_callback(consumer.stop)
            consumer_power_formula = consumer.power
            print(consumer_power_formula)
            consumer_power_receiver = consumer_power_formula.new_receiver()

            # Note: ConsumerPowerFormula has a "nones-are-zero" rule, that says:
            # * if the meter value is None, it should be treated as None.
            # * for other components None is treated as 0.

            # fmt: off
            expected_input_output: list[
                tuple[list[float | None], list[float | None], list[float | None], Power | None]
            ] = [
                # ([grid_meter, bat_meter, pv1_meter], [bat_inv], [pv1_inv, pv2_inv], expected_power) # noqa: E501
                ([100, 100, -50], [100], [-200, -300], Power.from_watts(350)),
                ([500, -200, -100], [100], [-200, -100], Power.from_watts(900)),
                # Case 2: The meter is unavailable (None).
                # Subscribe to the fallback inverter, but return None as the result,
                # according to the "nones-are-zero" rule
                ([500, None, -100], [100], [-200, -100], None),
                ([500, None, -100], [100], [-200, -100], Power.from_watts(600)),
                # Case 3: Second meter is unavailable (None).
                ([500, None, None], [100], [-200, -100], None),
                ([500, None, None], [100], [-200, -100], Power.from_watts(700)),
                # Case 3: pv2_inv is unavailable (None).
                # It has no fallback, so return 0 as its value according to
                # the "nones-are-zero" rule.
                ([500, None, None], [100], [-200, None], Power.from_watts(600)),
                # Case 4: Grid meter is unavailable (None).
                # It has no fallback, so return None according to the "nones-are-zero" rule.
                ([None, 100, -50], [100], [-200, -300], None),
                ([None, 200, -50], [100], [-200, -300], None),
                ([100, 100, -50], [100], [-200, -300], Power.from_watts(350)),
                # Case 5: Only grid meter is working, subscribe for fallback formula
                ([100, None, None], [None], [None, None], None),
                ([100, None, None], [None], [None, None], Power.from_watts(100)),
                ([-500, None, None], [None], [None, None], Power.from_watts(-500)),
                # Case 6: Nothing is working
                ([None, None, None], [None], [None, None], None),
            ]
            # fmt: on

            for idx, (
                meter_power,
                bat_inv_power,
                pv_inv_power,
                expected_power,
            ) in enumerate(expected_input_output):
                await mockgrid.mock_resampler.send_meter_power(meter_power)
                await mockgrid.mock_resampler.send_bat_inverter_power(bat_inv_power)
                await mockgrid.mock_resampler.send_pv_inverter_power(pv_inv_power)
                mockgrid.mock_resampler.next_ts()

                result = await consumer_power_receiver.receive()
                assert result.value == expected_power, (
                    f"Test case {idx} failed:"
                    + f" meter_power: {meter_power}"
                    + f" bat_inverter_power {bat_inv_power}"
                    + f" pv_inverter_power {pv_inv_power}"
                    + f" expected_power: {expected_power}"
                    + f" actual_power: {result.value}"
                )

    async def test_consumer_power_fallback_formula_without_grid_meter(
        self, mocker: MockerFixture
    ) -> None:
        """Test the consumer power formula with a grid meter."""
        mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
        mockgrid.add_consumer_meters(2)
        mockgrid.add_batteries(1)
        mockgrid.add_solar_inverters(1, no_meter=True)

        # formula is sum of consumer meters

        async with mockgrid, AsyncExitStack() as stack:
            consumer = microgrid.consumer()
            stack.push_async_callback(consumer.stop)
            consumer_power_receiver = consumer.power.new_receiver()

            # Note: ConsumerPowerFormula has a "nones-are-zero" rule, that says:
            # * if the meter value is None, it should be treated as None.
            # * for other components None is treated as 0.

            # fmt: off
            expected_input_output: list[
                tuple[list[float | None], list[float | None], list[float | None], Power | None]
            ] = [
                # ([consumer_meter1, consumer_meter2, bat_meter], [bat_inv], [pv_inv], expected_power) # noqa: E501
                ([100, 100, -50], [100], [-200,], Power.from_watts(200)),
                ([500, 100, -50], [100], [-200,], Power.from_watts(600)),
                # One of the meters is invalid - should return None according to none-are-zero rule
                ([None, 100, -50], [100], [-200,], None),
                ([None, None, -50], [100], [-200,], None),
                ([500, None, -50], [100], [-200,], None),
                ([2000, 1000, None], [None], [None], Power.from_watts(3000)),
            ]
            # fmt: on

            for idx, (
                meter_power,
                bat_inv_power,
                pv_inv_power,
                expected_power,
            ) in enumerate(expected_input_output):
                await mockgrid.mock_resampler.send_meter_power(meter_power)
                await mockgrid.mock_resampler.send_bat_inverter_power(bat_inv_power)
                await mockgrid.mock_resampler.send_pv_inverter_power(pv_inv_power)
                mockgrid.mock_resampler.next_ts()

                result = await consumer_power_receiver.receive()
                assert result.value == expected_power, (
                    f"Test case {idx} failed:"
                    + f" meter_power: {meter_power}"
                    + f" bat_inverter_power {bat_inv_power}"
                    + f" pv_inverter_power {pv_inv_power}"
                    + f" expected_power: {expected_power}"
                    + f" actual_power: {result.value}"
                )



================================================
FILE: tests/timeseries/test_formula_engine.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Tests for the FormulaEngine and the Tokenizer."""

import asyncio
from collections.abc import Callable
from datetime import datetime

from frequenz.channels import Broadcast, Receiver
from frequenz.quantities import Power, Quantity

from frequenz.sdk.timeseries import Sample
from frequenz.sdk.timeseries.formula_engine._formula_engine import (
    FormulaBuilder,
    FormulaEngine,
    HigherOrderFormulaBuilder,
)
from frequenz.sdk.timeseries.formula_engine._tokenizer import (
    Token,
    Tokenizer,
    TokenType,
)


class TestTokenizer:
    """Tests for the Tokenizer."""

    def test_1(self) -> None:
        """Test the tokenization of the formula: "#10 + #20 - (#5 * #4)"."""
        tokens = []
        lexer = Tokenizer("#10 + #20 - (#5 * #4)")
        for token in lexer:
            tokens.append(token)
        assert tokens == [
            Token(TokenType.COMPONENT_METRIC, "10"),
            Token(TokenType.OPER, "+"),
            Token(TokenType.COMPONENT_METRIC, "20"),
            Token(TokenType.OPER, "-"),
            Token(TokenType.OPER, "("),
            Token(TokenType.COMPONENT_METRIC, "5"),
            Token(TokenType.OPER, "*"),
            Token(TokenType.COMPONENT_METRIC, "4"),
            Token(TokenType.OPER, ")"),
        ]


class TestFormulaEngine:
    """Tests for the FormulaEngine."""

    async def run_test(  # pylint: disable=too-many-locals
        self,
        formula: str,
        postfix: str,
        io_pairs: list[tuple[list[float | None], float | None]],
        nones_are_zeros: bool = False,
    ) -> None:
        """Run a formula test."""
        channels: dict[str, Broadcast[Sample[Quantity]]] = {}
        builder = FormulaBuilder("test_formula", Quantity)
        for token in Tokenizer(formula):
            if token.type == TokenType.COMPONENT_METRIC:
                if token.value not in channels:
                    channels[token.value] = Broadcast(name=token.value)
                builder.push_metric(
                    f"#{token.value}",
                    channels[token.value].new_receiver(),
                    nones_are_zeros=nones_are_zeros,
                )
            elif token.type == TokenType.OPER:
                builder.push_oper(token.value)
        engine = builder.build()
        results_rx = engine.new_receiver()

        assert repr(builder._steps) == postfix  # pylint: disable=protected-access

        now = datetime.now()
        tests_passed = 0
        for io_pair in io_pairs:
            io_input, io_output = io_pair
            await asyncio.gather(
                *[
                    chan.new_sender().send(
                        Sample(now, None if not value else Quantity(value))
                    )
                    for chan, value in zip(channels.values(), io_input)
                ]
            )
            next_val = await results_rx.receive()
            if io_output is None:
                assert next_val.value is None
            else:
                assert (
                    next_val.value is not None
                    and next_val.value.base_value == io_output
                )
            tests_passed += 1
        await engine.stop()
        assert tests_passed == len(io_pairs)

    async def test_simple(self) -> None:
        """Test simple formulas."""
        await self.run_test(
            "#2 - #4 + #5",
            "[#2, #4, -, #5, +]",
            [
                ([10.0, 12.0, 15.0], 13.0),
                ([15.0, 17.0, 20.0], 18.0),
            ],
        )
        await self.run_test(
            "#2 + #4 - #5",
            "[#2, #4, #5, -, +]",
            [
                ([10.0, 12.0, 15.0], 7.0),
                ([15.0, 17.0, 20.0], 12.0),
            ],
        )
        await self.run_test(
            "#2 * #4 + #5",
            "[#2, #4, *, #5, +]",
            [
                ([10.0, 12.0, 15.0], 135.0),
                ([15.0, 17.0, 20.0], 275.0),
            ],
        )
        await self.run_test(
            "#2 * #4 / #5",
            "[#2, #4, #5, /, *]",
            [
                ([10.0, 12.0, 15.0], 8.0),
                ([15.0, 17.0, 20.0], 12.75),
            ],
        )
        await self.run_test(
            "#2 / #4 - #5",
            "[#2, #4, /, #5, -]",
            [
                ([6.0, 12.0, 15.0], -14.5),
                ([15.0, 20.0, 20.0], -19.25),
            ],
        )
        await self.run_test(
            "#2 - #4 - #5",
            "[#2, #4, -, #5, -]",
            [
                ([6.0, 12.0, 15.0], -21.0),
                ([15.0, 20.0, 20.0], -25.0),
            ],
        )
        await self.run_test(
            "#2 + #4 + #5",
            "[#2, #4, +, #5, +]",
            [
                ([6.0, 12.0, 15.0], 33.0),
                ([15.0, 20.0, 20.0], 55.0),
            ],
        )
        await self.run_test(
            "#2 / #4 / #5",
            "[#2, #4, /, #5, /]",
            [
                ([30.0, 3.0, 5.0], 2.0),
                ([15.0, 3.0, 2.0], 2.5),
            ],
        )

    async def test_compound(self) -> None:
        """Test compound formulas."""
        await self.run_test(
            "#2 + #4 - #5 * #6",
            "[#2, #4, #5, #6, *, -, +]",
            [
                ([10.0, 12.0, 15.0, 2.0], -8.0),
                ([15.0, 17.0, 20.0, 1.5], 2.0),
            ],
        )
        await self.run_test(
            "#2 + (#4 - #5) * #6",
            "[#2, #4, #5, -, #6, *, +]",
            [
                ([10.0, 12.0, 15.0, 2.0], 4.0),
                ([15.0, 17.0, 20.0, 1.5], 10.5),
            ],
        )
        await self.run_test(
            "#2 + (#4 - #5 * #6)",
            "[#2, #4, #5, #6, *, -, +]",
            [
                ([10.0, 12.0, 15.0, 2.0], -8.0),
                ([15.0, 17.0, 20.0, 1.5], 2.0),
            ],
        )
        await self.run_test(
            "#2 + (#4 - #5 - #6)",
            "[#2, #4, #5, -, #6, -, +]",
            [
                ([10.0, 12.0, 15.0, 2.0], 5.0),
                ([15.0, 17.0, 20.0, 1.5], 10.5),
            ],
        )
        await self.run_test(
            "#2 + #4 - #5 - #6",
            "[#2, #4, #5, -, #6, -, +]",
            [
                ([10.0, 12.0, 15.0, 2.0], 5.0),
                ([15.0, 17.0, 20.0, 1.5], 10.5),
            ],
        )
        await self.run_test(
            "#2 + #4 - (#5 - #6)",
            "[#2, #4, #5, #6, -, -, +]",
            [
                ([10.0, 12.0, 15.0, 2.0], 9.0),
                ([15.0, 17.0, 20.0, 1.5], 13.5),
            ],
        )
        await self.run_test(
            "(#2 + #4 - #5) * #6",
            "[#2, #4, #5, -, +, #6, *]",
            [
                ([10.0, 12.0, 15.0, 2.0], 14.0),
                ([15.0, 17.0, 20.0, 1.5], 18.0),
            ],
        )
        await self.run_test(
            "(#2 + #4 - #5) / #6",
            "[#2, #4, #5, -, +, #6, /]",
            [
                ([10.0, 12.0, 15.0, 2.0], 3.5),
                ([15.0, 17.0, 20.0, 1.5], 8.0),
            ],
        )
        await self.run_test(
            "#2 + #4 - (#5 / #6)",
            "[#2, #4, #5, #6, /, -, +]",
            [
                ([10.0, 12.0, 15.0, 2.0], 14.5),
                ([15.0, 17.0, 20.0, 5.0], 28.0),
            ],
        )

    async def test_nones_are_zeros(self) -> None:
        """Test that `None`s are treated as zeros when configured."""
        await self.run_test(
            "#2 - #4 + #5",
            "[#2, #4, -, #5, +]",
            [
                ([10.0, 12.0, 15.0], 13.0),
                ([None, 12.0, 15.0], 3.0),
                ([10.0, None, 15.0], 25.0),
                ([15.0, 17.0, 20.0], 18.0),
                ([15.0, None, None], 15.0),
            ],
            True,
        )

        await self.run_test(
            "#2 + #4 - (#5 * #6)",
            "[#2, #4, #5, #6, *, -, +]",
            [
                ([10.0, 12.0, 15.0, 2.0], -8.0),
                ([10.0, 12.0, 15.0, None], 22.0),
                ([10.0, None, 15.0, 2.0], -20.0),
                ([15.0, 17.0, 20.0, 5.0], -68.0),
                ([15.0, 17.0, None, 5.0], 32.0),
            ],
            True,
        )

    async def test_nones_are_not_zeros(self) -> None:
        """Test that calculated values are `None` on input `None`s."""
        await self.run_test(
            "#2 - #4 + #5",
            "[#2, #4, -, #5, +]",
            [
                ([10.0, 12.0, 15.0], 13.0),
                ([None, 12.0, 15.0], None),
                ([10.0, None, 15.0], None),
                ([15.0, 17.0, 20.0], 18.0),
                ([15.0, None, None], None),
            ],
            False,
        )

        await self.run_test(
            "#2 + #4 - (#5 * #6)",
            "[#2, #4, #5, #6, *, -, +]",
            [
                ([10.0, 12.0, 15.0, 2.0], -8.0),
                ([10.0, 12.0, 15.0, None], None),
                ([10.0, None, 15.0, 2.0], None),
                ([15.0, 17.0, 20.0, 5.0], -68.0),
                ([15.0, 17.0, None, 5.0], None),
            ],
            False,
        )


class TestFormulaEngineComposition:
    """Tests for formula channels."""

    def make_engine(
        self, stream_id: int, data: Receiver[Sample[Quantity]]
    ) -> FormulaEngine[Quantity]:
        """Make a basic FormulaEngine."""
        name = f"#{stream_id}"
        builder = FormulaBuilder(name, create_method=Quantity)
        builder.push_metric(
            name,
            data,
            nones_are_zeros=False,
        )
        return FormulaEngine(builder, create_method=Quantity)

    async def run_test(  # pylint: disable=too-many-locals
        self,
        num_items: int,
        make_builder: (
            Callable[
                [
                    FormulaEngine[Quantity],
                ],
                HigherOrderFormulaBuilder[Quantity],
            ]
            | Callable[
                [
                    FormulaEngine[Quantity],
                    FormulaEngine[Quantity],
                ],
                HigherOrderFormulaBuilder[Quantity],
            ]
            | Callable[
                [
                    FormulaEngine[Quantity],
                    FormulaEngine[Quantity],
                    FormulaEngine[Quantity],
                ],
                HigherOrderFormulaBuilder[Quantity],
            ]
            | Callable[
                [
                    FormulaEngine[Quantity],
                    FormulaEngine[Quantity],
                    FormulaEngine[Quantity],
                    FormulaEngine[Quantity],
                ],
                HigherOrderFormulaBuilder[Quantity],
            ]
        ),
        io_pairs: list[tuple[list[float | None], float | None]],
        nones_are_zeros: bool = False,
    ) -> None:
        """Run a test with the specs provided."""
        channels = [
            Broadcast[Sample[Quantity]](name=str(ctr)) for ctr in range(num_items)
        ]
        l1_engines = [
            self.make_engine(ctr, channels[ctr].new_receiver())
            for ctr in range(num_items)
        ]
        builder = make_builder(*l1_engines)
        engine = builder.build("l2 formula", nones_are_zeros=nones_are_zeros)
        result_chan = engine.new_receiver()

        now = datetime.now()
        tests_passed = 0
        for io_pair in io_pairs:
            io_input, io_output = io_pair
            await asyncio.gather(
                *[
                    chan.new_sender().send(
                        Sample(now, None if not value else Quantity(value))
                    )
                    for chan, value in zip(channels, io_input)
                ]
            )
            next_val = await result_chan.receive()
            if io_output is None:
                assert next_val.value is None
            else:
                assert (
                    next_val.value is not None
                    and next_val.value.base_value == io_output
                )
            tests_passed += 1
        await engine.stop()
        assert tests_passed == len(io_pairs)

    async def test_simple(self) -> None:
        """Test simple formulas."""
        await self.run_test(
            3,
            lambda c2, c4, c5: c2 - c4 + c5,
            [
                ([10.0, 12.0, 15.0], 13.0),
                ([15.0, 17.0, 20.0], 18.0),
            ],
        )
        await self.run_test(
            3,
            lambda c2, c4, c5: c2 + c4 - c5,
            [
                ([10.0, 12.0, 15.0], 7.0),
                ([15.0, 17.0, 20.0], 12.0),
            ],
        )
        await self.run_test(
            3,
            lambda c2, c4, c5: c2 * c4 + c5,
            [
                ([10.0, 12.0, 15.0], 135.0),
                ([15.0, 17.0, 20.0], 275.0),
            ],
        )
        await self.run_test(
            3,
            lambda c2, c4, c5: c2 * c4 / c5,
            [
                ([10.0, 12.0, 15.0], 8.0),
                ([15.0, 17.0, 20.0], 12.75),
            ],
        )
        await self.run_test(
            3,
            lambda c2, c4, c5: c2 / c4 - c5,
            [
                ([6.0, 12.0, 15.0], -14.5),
                ([15.0, 20.0, 20.0], -19.25),
            ],
        )
        await self.run_test(
            3,
            lambda c2, c4, c5: c2 - c4 - c5,
            [
                ([6.0, 12.0, 15.0], -21.0),
                ([15.0, 20.0, 20.0], -25.0),
            ],
        )
        await self.run_test(
            3,
            lambda c2, c4, c5: c2 + c4 + c5,
            [
                ([6.0, 12.0, 15.0], 33.0),
                ([15.0, 20.0, 20.0], 55.0),
            ],
        )
        await self.run_test(
            3,
            lambda c2, c4, c5: c2 / c4 / c5,
            [
                ([30.0, 3.0, 5.0], 2.0),
                ([15.0, 3.0, 2.0], 2.5),
            ],
        )

    async def test_min_max(self) -> None:
        """Test min and max functions in combination."""
        await self.run_test(
            3,
            lambda c2, c4, c5: c2.min(c4).max(c5),
            [
                ([4.0, 6.0, 5.0], 5.0),
            ],
        )

    async def test_max(self) -> None:
        """Test the max function."""
        await self.run_test(
            3,
            lambda c2, c4, c5: c2 * c4.max(c5),
            [
                ([10.0, 12.0, 15.0], 150.0),
            ],
        )
        await self.run_test(
            3,
            lambda c2, c4, c5: (c2 * c4).max(c5),
            [
                ([10.0, 12.0, 15.0], 120.0),
            ],
        )
        await self.run_test(
            3,
            lambda c2, c4, c5: (c2 + c4).max(c5),
            [
                ([10.0, 12.0, 15.0], 22.0),
            ],
        )
        await self.run_test(
            3,
            lambda c2, c4, c5: c2 + c4.max(c5),
            [
                ([10.0, 12.0, 15.0], 25.0),
            ],
        )

    async def test_min(self) -> None:
        """Test the min function."""
        await self.run_test(
            3,
            lambda c2, c4, c5: (c2 * c4).min(c5),
            [
                ([10.0, 12.0, 15.0], 15.0),
            ],
        )
        await self.run_test(
            3,
            lambda c2, c4, c5: c2 * c4.min(c5),
            [
                ([10.0, 12.0, 15.0], 120.0),
            ],
        )
        await self.run_test(
            3,
            lambda c2, c4, c5: (c2 + c4).min(c5),
            [
                ([10.0, 2.0, 15.0], 12.0),
            ],
        )
        await self.run_test(
            3,
            lambda c2, c4, c5: c2 + c4.min(c5),
            [
                ([10.0, 12.0, 15.0], 22.0),
            ],
        )

    async def test_compound(self) -> None:
        """Test compound formulas."""
        await self.run_test(
            4,
            lambda c2, c4, c5, c6: c2 + c4 - c5 * c6,
            [
                ([10.0, 12.0, 15.0, 2.0], -8.0),
                ([15.0, 17.0, 20.0, 1.5], 2.0),
            ],
        )
        await self.run_test(
            4,
            lambda c2, c4, c5, c6: c2 + (c4 - c5) * c6,
            [
                ([10.0, 12.0, 15.0, 2.0], 4.0),
                ([15.0, 17.0, 20.0, 1.5], 10.5),
            ],
        )
        await self.run_test(
            4,
            lambda c2, c4, c5, c6: c2 + (c4 - c5 * c6),
            [
                ([10.0, 12.0, 15.0, 2.0], -8.0),
                ([15.0, 17.0, 20.0, 1.5], 2.0),
            ],
        )
        await self.run_test(
            4,
            lambda c2, c4, c5, c6: c2 + (c4 - c5 - c6),
            [
                ([10.0, 12.0, 15.0, 2.0], 5.0),
                ([15.0, 17.0, 20.0, 1.5], 10.5),
            ],
        )
        await self.run_test(
            4,
            lambda c2, c4, c5, c6: c2 + c4 - c5 - c6,
            [
                ([10.0, 12.0, 15.0, 2.0], 5.0),
                ([15.0, 17.0, 20.0, 1.5], 10.5),
            ],
        )
        await self.run_test(
            4,
            lambda c2, c4, c5, c6: c2 + c4 - (c5 - c6),
            [
                ([10.0, 12.0, 15.0, 2.0], 9.0),
                ([15.0, 17.0, 20.0, 1.5], 13.5),
            ],
        )
        await self.run_test(
            4,
            lambda c2, c4, c5, c6: (c2 + c4 - c5) * c6,
            [
                ([10.0, 12.0, 15.0, 2.0], 14.0),
                ([15.0, 17.0, 20.0, 1.5], 18.0),
            ],
        )
        await self.run_test(
            4,
            lambda c2, c4, c5, c6: (c2 + c4 - c5) / c6,
            [
                ([10.0, 12.0, 15.0, 2.0], 3.5),
                ([15.0, 17.0, 20.0, 1.5], 8.0),
            ],
        )
        await self.run_test(
            4,
            lambda c2, c4, c5, c6: c2 + c4 - (c5 / c6),
            [
                ([10.0, 12.0, 15.0, 2.0], 14.5),
                ([15.0, 17.0, 20.0, 5.0], 28.0),
            ],
        )

    async def test_consumption(self) -> None:
        """Test the consumption operator."""
        await self.run_test(
            1,
            lambda c1: c1.consumption(),
            [
                ([10.0], 10.0),
                ([-10.0], 0.0),
            ],
        )

    async def test_production(self) -> None:
        """Test the production operator."""
        await self.run_test(
            1,
            lambda c1: c1.production(),
            [
                ([10.0], 0.0),
                ([-10.0], 10.0),
            ],
        )

    async def test_consumption_production(self) -> None:
        """Test the consumption and production operator combined."""
        await self.run_test(
            2,
            lambda c1, c2: c1.consumption() + c2.production(),
            [
                ([10.0, 12.0], 10.0),
                ([-12.0, -10.0], 10.0),
            ],
        )
        await self.run_test(
            2,
            lambda c1, c2: c1.consumption() + c2.consumption(),
            [
                ([10.0, -12.0], 10.0),
                ([-10.0, 12.0], 12.0),
            ],
        )
        await self.run_test(
            2,
            lambda c1, c2: c1.production() + c2.production(),
            [
                ([10.0, -12.0], 12.0),
                ([-10.0, 12.0], 10.0),
            ],
        )
        await self.run_test(
            2,
            lambda c1, c2: c1.min(c2).consumption(),
            [
                ([10.0, -12.0], 0.0),
                ([10.0, 12.0], 10.0),
            ],
        )
        await self.run_test(
            2,
            lambda c1, c2: c1.max(c2).consumption(),
            [
                ([10.0, -12.0], 10.0),
                ([-10.0, -12.0], 0.0),
            ],
        )
        await self.run_test(
            2,
            lambda c1, c2: c1.min(c2).production(),
            [
                ([10.0, -12.0], 12.0),
                ([10.0, 12.0], 0.0),
            ],
        )
        await self.run_test(
            2,
            lambda c1, c2: c1.max(c2).production(),
            [
                ([10.0, -12.0], 0.0),
                ([-10.0, -12.0], 10.0),
            ],
        )
        await self.run_test(
            2,
            lambda c1, c2: c1.production() + c2,
            [
                ([10.0, -12.0], -12.0),
                ([-10.0, -12.0], -2.0),
            ],
        )

    async def test_nones_are_zeros(self) -> None:
        """Test that `None`s are treated as zeros when configured."""
        await self.run_test(
            3,
            lambda c2, c4, c5: c2 - c4 + c5,
            [
                ([10.0, 12.0, 15.0], 13.0),
                ([None, 12.0, 15.0], 3.0),
                ([10.0, None, 15.0], 25.0),
                ([15.0, 17.0, 20.0], 18.0),
                ([15.0, None, None], 15.0),
            ],
            True,
        )

        await self.run_test(
            4,
            lambda c2, c4, c5, c6: c2 + c4 - (c5 * c6),
            [
                ([10.0, 12.0, 15.0, 2.0], -8.0),
                ([10.0, 12.0, 15.0, None], 22.0),
                ([10.0, None, 15.0, 2.0], -20.0),
                ([15.0, 17.0, 20.0, 5.0], -68.0),
                ([15.0, 17.0, None, 5.0], 32.0),
            ],
            True,
        )

    async def test_nones_are_not_zeros(self) -> None:
        """Test that calculated values are `None` on input `None`s."""
        await self.run_test(
            3,
            lambda c2, c4, c5: c2 - c4 + c5,
            [
                ([10.0, 12.0, 15.0], 13.0),
                ([None, 12.0, 15.0], None),
                ([10.0, None, 15.0], None),
                ([15.0, 17.0, 20.0], 18.0),
                ([15.0, None, None], None),
            ],
            False,
        )

        await self.run_test(
            4,
            lambda c2, c4, c5, c6: c2 + c4 - (c5 * c6),
            [
                ([10.0, 12.0, 15.0, 2.0], -8.0),
                ([10.0, 12.0, 15.0, None], None),
                ([10.0, None, 15.0, 2.0], None),
                ([15.0, 17.0, 20.0, 5.0], -68.0),
                ([15.0, 17.0, None, 5.0], None),
            ],
            False,
        )


class TestConstantValue:
    """Tests for the constant value step."""

    async def test_constant_value(self) -> None:
        """Test using constant values in formulas."""
        channel_1 = Broadcast[Sample[Quantity]](name="channel_1")
        channel_2 = Broadcast[Sample[Quantity]](name="channel_2")

        sender_1 = channel_1.new_sender()
        sender_2 = channel_2.new_sender()

        builder = FormulaBuilder("test_constant_value", create_method=Quantity)
        builder.push_metric(
            "channel_1", channel_1.new_receiver(), nones_are_zeros=False
        )
        builder.push_oper("+")
        builder.push_constant(2.0)
        builder.push_oper("*")
        builder.push_metric(
            "channel_2", channel_2.new_receiver(), nones_are_zeros=False
        )

        engine = builder.build()

        results_rx = engine.new_receiver()

        now = datetime.now()
        await sender_1.send(Sample(now, Quantity(10.0)))
        await sender_2.send(Sample(now, Quantity(15.0)))
        assert (await results_rx.receive()).value == Quantity(40.0)

        await sender_1.send(Sample(now, Quantity(-10.0)))
        await sender_2.send(Sample(now, Quantity(15.0)))
        assert (await results_rx.receive()).value == Quantity(20.0)

        builder = FormulaBuilder("test_constant_value", create_method=Quantity)
        builder.push_oper("(")
        builder.push_metric(
            "channel_1", channel_1.new_receiver(), nones_are_zeros=False
        )
        builder.push_oper("+")
        builder.push_constant(2.0)
        builder.push_oper(")")
        builder.push_oper("*")
        builder.push_metric(
            "channel_2", channel_2.new_receiver(), nones_are_zeros=False
        )

        engine = builder.build()

        results_rx = engine.new_receiver()

        now = datetime.now()
        await sender_1.send(Sample(now, Quantity(10.0)))
        await sender_2.send(Sample(now, Quantity(15.0)))
        assert (await results_rx.receive()).value == Quantity(180.0)

        await sender_1.send(Sample(now, Quantity(-10.0)))
        await sender_2.send(Sample(now, Quantity(15.0)))
        assert (await results_rx.receive()).value == Quantity(-120.0)


class TestClipper:
    """Tests for the clipper step."""

    async def test_clipper(self) -> None:
        """Test the usage of clipper in formulas."""
        channel_1 = Broadcast[Sample[Quantity]](name="channel_1")
        channel_2 = Broadcast[Sample[Quantity]](name="channel_2")

        sender_1 = channel_1.new_sender()
        sender_2 = channel_2.new_sender()

        builder = FormulaBuilder("test_clipper", create_method=Quantity)
        builder.push_metric(
            "channel_1", channel_1.new_receiver(), nones_are_zeros=False
        )
        builder.push_oper("+")
        builder.push_metric(
            "channel_2", channel_2.new_receiver(), nones_are_zeros=False
        )
        builder.push_clipper(0.0, 100.0)
        engine = builder.build()

        results_rx = engine.new_receiver()

        now = datetime.now()
        await sender_1.send(Sample(now, Quantity(10.0)))
        await sender_2.send(Sample(now, Quantity(150.0)))
        assert (await results_rx.receive()).value == Quantity(110.0)

        await sender_1.send(Sample(now, Quantity(200.0)))
        await sender_2.send(Sample(now, Quantity(-10.0)))
        assert (await results_rx.receive()).value == Quantity(200.0)

        await sender_1.send(Sample(now, Quantity(200.0)))
        await sender_2.send(Sample(now, Quantity(10.0)))
        assert (await results_rx.receive()).value == Quantity(210.0)

        builder = FormulaBuilder("test_clipper", create_method=Quantity)
        builder.push_oper("(")
        builder.push_metric(
            "channel_1", channel_1.new_receiver(), nones_are_zeros=False
        )
        builder.push_oper("+")
        builder.push_metric(
            "channel_2", channel_2.new_receiver(), nones_are_zeros=False
        )
        builder.push_oper(")")
        builder.push_clipper(0.0, 100.0)
        engine = builder.build()

        results_rx = engine.new_receiver()

        now = datetime.now()
        await sender_1.send(Sample(now, Quantity(10.0)))
        await sender_2.send(Sample(now, Quantity(150.0)))
        assert (await results_rx.receive()).value == Quantity(100.0)

        await sender_1.send(Sample(now, Quantity(200.0)))
        await sender_2.send(Sample(now, Quantity(-10.0)))
        assert (await results_rx.receive()).value == Quantity(100.0)

        await sender_1.send(Sample(now, Quantity(25.0)))
        await sender_2.send(Sample(now, Quantity(-10.0)))
        assert (await results_rx.receive()).value == Quantity(15.0)


class TestFormulaOutputTyping:
    """Tests for the typing of the output of formulas."""

    async def test_types(self) -> None:
        """Test the typing of the output of formulas."""
        channel_1 = Broadcast[Sample[Power]](name="channel_1")
        channel_2 = Broadcast[Sample[Power]](name="channel_2")

        sender_1 = channel_1.new_sender()
        sender_2 = channel_2.new_sender()

        builder = FormulaBuilder("test_typing", create_method=Power.from_watts)
        builder.push_metric(
            "channel_1", channel_1.new_receiver(), nones_are_zeros=False
        )
        builder.push_oper("+")
        builder.push_metric(
            "channel_2", channel_2.new_receiver(), nones_are_zeros=False
        )
        engine = builder.build()

        results_rx = engine.new_receiver()

        now = datetime.now()
        await sender_1.send(Sample(now, Power.from_watts(10.0)))
        await sender_2.send(Sample(now, Power.from_watts(150.0)))
        result = await results_rx.receive()
        assert result is not None and result.value is not None
        assert result.value.as_watts() == 160.0


class TestFromReceiver:
    """Test creating a formula engine from a receiver."""

    async def test_from_receiver(self) -> None:
        """Test creating a formula engine from a receiver."""
        channel = Broadcast[Sample[Power]](name="channel_1")
        sender = channel.new_sender()

        builder = FormulaBuilder("test_from_receiver", create_method=Power.from_watts)
        builder.push_metric("channel_1", channel.new_receiver(), nones_are_zeros=False)
        engine = builder.build()

        engine_from_receiver = FormulaEngine.from_receiver(
            "test_from_receiver", engine.new_receiver(), create_method=Power.from_watts
        )

        results_rx = engine_from_receiver.new_receiver()

        await sender.send(Sample(datetime.now(), Power.from_watts(10.0)))
        result = await results_rx.receive()
        assert result is not None and result.value is not None
        assert result.value.as_watts() == 10.0



================================================
FILE: tests/timeseries/test_formula_formatter.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests for the FormulaFormatter."""


from frequenz.channels import Broadcast
from frequenz.quantities import Quantity

from frequenz.sdk.timeseries import Sample
from frequenz.sdk.timeseries.formula_engine._formula_engine import FormulaBuilder
from frequenz.sdk.timeseries.formula_engine._formula_formatter import format_formula
from frequenz.sdk.timeseries.formula_engine._formula_steps import (
    Clipper,
    ConstantValue,
    FormulaStep,
    Maximizer,
    Minimizer,
)
from frequenz.sdk.timeseries.formula_engine._tokenizer import Tokenizer, TokenType


def build_formula(formula: str) -> list[FormulaStep]:
    """Parse the formula and returns the steps.

    Args:
        formula: The formula in infix notation.

    Returns:
        The formula in postfix notation steps.
    """
    channels: dict[str, Broadcast[Sample[Quantity]]] = {}
    builder = FormulaBuilder("test_formula", Quantity)
    nones_are_zeros = True

    for token in Tokenizer(formula):
        if token.type == TokenType.COMPONENT_METRIC:
            if token.value not in channels:
                channels[token.value] = Broadcast(name=token.value)
            builder.push_metric(
                name=f"#{token.value}",
                data_stream=channels[token.value].new_receiver(),
                nones_are_zeros=nones_are_zeros,
            )
        elif token.type == TokenType.OPER:
            builder.push_oper(token.value)
    steps, _ = builder.finalize()
    return steps


def reconstruct(formula: str) -> str:
    """Parse the formula and reconstructs it from the steps.

    Args:
        formula: The formula in infix notation.

    Returns:
        The reconstructed formula in infix notation.
    """
    steps = build_formula(formula)
    reconstructed = format_formula(steps)
    if formula != reconstructed:
        print(f"Formula: input {formula} != output {reconstructed}")
    return reconstructed


class TestFormulaFormatter:
    """Tests for the FormulaFormatter."""

    async def test_basic_precedence(self) -> None:
        """Test that the formula is wrapped in parentheses for operators of different precedence."""
        assert reconstruct("#2 + #3 * #4") == "#2 + #3 * #4"

    def test_all_same_precedence(self) -> None:
        """Test that the formula is not wrapped in parentheses for operators of same precedence."""
        assert reconstruct("#2 + #3 + #4") == "#2 + #3 + #4"

    def test_lhs_precedence(self) -> None:
        """Test that the left-hand side of a binary operation is wrapped in parentheses."""
        assert reconstruct("(#2 - #3) - #4") == "#2 - #3 - #4"
        assert reconstruct("#2 - #3 - #4") == "#2 - #3 - #4"
        assert reconstruct("(#2 - #3) * #4") == "(#2 - #3) * #4"

    def test_rhs_precedence(self) -> None:
        """Test that the right-hand side of a binary operation is wrapped in parentheses if needed."""
        assert reconstruct("#2 + #3") == "#2 + #3"
        assert reconstruct("#2 - #3") == "#2 - #3"
        assert reconstruct("#2 + #3 + #4") == "#2 + #3 + #4"
        assert reconstruct("#2 - #3 - #4") == "#2 - #3 - #4"
        assert reconstruct("#2 - #3 * #4") == "#2 - #3 * #4"
        assert reconstruct("#2 - (#3 * #4)") == "#2 - #3 * #4"
        assert reconstruct("#2 - (#3 - #4)") == "#2 - (#3 - #4)"
        assert reconstruct("#2 - (#3 + #4)") == "#2 - (#3 + #4)"

    def test_rhs_parenthesis(self) -> None:
        """Test that the right-hand side of a binary operation is wrapped in parentheses."""
        assert reconstruct("#2 / (#3 - #4)") == "#2 / (#3 - #4)"

    def test_functions(self) -> None:
        """Test that the functions are formatted correctly."""
        # For simplicity, we only test with constant values.
        # fmt: off
        # flake8: noqa: E501
        assert format_formula([ConstantValue(2), ConstantValue(3), Minimizer()]) == "min(2, 3)"
        assert format_formula([ConstantValue(2), ConstantValue(3), Maximizer()]) == "max(2, 3)"
        assert format_formula([ConstantValue(3.5), Clipper(0.0, 1.0)]) == "clip(0.0, 3.5, 1.0)"
        # flake8: enable
        # fmt: on

    async def test_higher_order_formula(self) -> None:
        """Test that higher-order formulas (formulas combining other formulas) are formatted correctly."""
        # Create two base formulas
        builder1 = FormulaBuilder("test_formula1", Quantity)
        builder2 = FormulaBuilder("test_formula2", Quantity)

        # Push metrics directly to the builders
        channel1 = Broadcast[Sample[Quantity]](name="channel1")
        channel2 = Broadcast[Sample[Quantity]](name="channel2")
        builder1.push_metric("#1", channel1.new_receiver(), nones_are_zeros=True)
        builder1.push_oper("+")
        builder1.push_metric("#2", channel2.new_receiver(), nones_are_zeros=True)

        channel3 = Broadcast[Sample[Quantity]](name="channel3")
        channel4 = Broadcast[Sample[Quantity]](name="channel4")
        builder2.push_metric("#3", channel3.new_receiver(), nones_are_zeros=True)
        builder2.push_oper("+")
        builder2.push_metric("#4", channel4.new_receiver(), nones_are_zeros=True)

        # Build individual formula engines first
        engine1 = builder1.build()
        engine2 = builder2.build()

        # Combine them into a higher-order formula
        composed_formula = (engine1 - engine2).build("higher_order_formula")

        # Check the string representation
        assert (
            str(composed_formula)
            == "[test_formula1](#1 + #2) - [test_formula2](#3 + #4)"
        )



================================================
FILE: tests/timeseries/test_frequency_streaming.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Tests for the logical meter."""


import asyncio
from datetime import datetime, timezone

from frequenz.quantities import Frequency
from pytest_mock import MockerFixture

from frequenz.sdk import microgrid
from tests.utils import component_data_wrapper

from ._formula_engine.utils import equal_float_lists
from .mock_microgrid import MockMicrogrid

# pylint: disable=protected-access


async def test_grid_frequency_none(mocker: MockerFixture) -> None:
    """Test the grid frequency formula."""
    mockgrid = MockMicrogrid(grid_meter=True)
    mockgrid.add_batteries(2)
    mockgrid.add_solar_inverters(1)
    await mockgrid.start(mocker)

    grid_freq = microgrid.frequency()
    grid_freq_recv = grid_freq.new_receiver()

    assert grid_freq._task is not None
    # We have to wait for the metric request to be sent
    await grid_freq._task
    # And consumed
    await asyncio.sleep(0)

    await mockgrid.mock_client.send(
        component_data_wrapper.MeterDataWrapper(
            mockgrid.meter_ids[0], datetime.now(tz=timezone.utc)
        )
    )

    val = await grid_freq_recv.receive()
    assert val is not None
    assert val.value is None
    await mockgrid.cleanup()


async def test_grid_frequency_1(mocker: MockerFixture) -> None:
    """Test the grid frequency formula."""
    mockgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
    mockgrid.add_batteries(2)
    mockgrid.add_solar_inverters(1)

    async with mockgrid:
        grid_freq = microgrid.frequency()
        grid_freq_recv = grid_freq.new_receiver()

        assert grid_freq._task is not None
        # We have to wait for the metric request to be sent
        await grid_freq._task
        # And consumed
        await asyncio.sleep(0)

        results = []
        grid_meter_data = []
        for count in range(10):
            freq = float(50.0 + (1 if count % 2 == 0 else -1) * 0.01)
            await mockgrid.mock_client.send(
                component_data_wrapper.MeterDataWrapper(
                    mockgrid.meter_ids[0], datetime.now(tz=timezone.utc), frequency=freq
                )
            )

            grid_meter_data.append(Frequency.from_hertz(freq))
            val = await grid_freq_recv.receive()
            assert val is not None and val.value is not None
            assert val.value.as_hertz() == freq
            results.append(val.value)

    assert equal_float_lists(results, grid_meter_data)


async def test_grid_frequency_no_grid_meter_no_consumer_meter(
    mocker: MockerFixture,
) -> None:
    """Test the grid frequency formula without a grid side meter."""
    mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
    mockgrid.add_consumer_meters()
    mockgrid.add_batteries(1, no_meter=False)
    mockgrid.add_batteries(1, no_meter=False)

    async with mockgrid:
        grid_freq = microgrid.frequency()

        grid_freq_recv = grid_freq.new_receiver()
        # We have to wait for the metric request to be sent
        assert grid_freq._task is not None
        await grid_freq._task
        # And consumed
        await asyncio.sleep(0)

        results = []
        meter_data = []
        for count in range(10):
            freq = float(50.0 + (1 if count % 2 == 0 else -1) * 0.01)
            await mockgrid.mock_client.send(
                component_data_wrapper.MeterDataWrapper(
                    mockgrid.meter_ids[0], datetime.now(tz=timezone.utc), frequency=freq
                )
            )
            meter_data.append(Frequency.from_hertz(freq))

            val = await grid_freq_recv.receive()
            assert val is not None and val.value is not None
            assert val.value.as_hertz() == freq
            results.append(val.value)

    assert equal_float_lists(results, meter_data)


async def test_grid_frequency_no_grid_meter(
    mocker: MockerFixture,
) -> None:
    """Test the grid frequency formula without a grid side meter."""
    mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
    mockgrid.add_batteries(1, no_meter=False)
    mockgrid.add_batteries(1, no_meter=True)

    async with mockgrid:
        grid_freq = microgrid.frequency()

        grid_freq_recv = grid_freq.new_receiver()
        # We have to wait for the metric request to be sent
        assert grid_freq._task is not None
        await grid_freq._task
        # And consumed
        await asyncio.sleep(0)

        results = []
        meter_data = []
        for count in range(10):
            freq = float(50.0 + (1 if count % 2 == 0 else -1) * 0.01)
            await mockgrid.mock_client.send(
                component_data_wrapper.MeterDataWrapper(
                    mockgrid.meter_ids[0], datetime.now(tz=timezone.utc), frequency=freq
                )
            )
            meter_data.append(Frequency.from_hertz(freq))

            val = await grid_freq_recv.receive()
            assert val is not None and val.value is not None
            assert val.value.as_hertz() == freq
            results.append(val.value)

    assert equal_float_lists(results, meter_data)


async def test_grid_frequency_only_inverter(
    mocker: MockerFixture,
) -> None:
    """Test the grid frequency formula without any meter but only inverters."""
    mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
    mockgrid.add_batteries(2, no_meter=True)

    async with mockgrid:
        grid_freq = microgrid.frequency()
        grid_freq_recv = grid_freq.new_receiver()
        # We have to wait for the metric request to be sent
        assert grid_freq._task is not None
        await grid_freq._task
        # And consumed
        await asyncio.sleep(0)

        results = []
        meter_data = []
        for count in range(10):
            freq = float(50.0 + (1 if count % 2 == 0 else -1) * 0.01)

            await mockgrid.mock_client.send(
                component_data_wrapper.InverterDataWrapper(
                    mockgrid.battery_inverter_ids[0],
                    datetime.now(tz=timezone.utc),
                    frequency=freq,
                )
            )

            meter_data.append(Frequency.from_hertz(freq))
            val = await grid_freq_recv.receive()
            assert val is not None and val.value is not None
            assert val.value.as_hertz() == freq
            results.append(val.value)

    assert equal_float_lists(results, meter_data)



================================================
FILE: tests/timeseries/test_logical_meter.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Tests for the logical meter."""


from contextlib import AsyncExitStack

from frequenz.quantities import Power
from pytest_mock import MockerFixture

from frequenz.sdk import microgrid

from .mock_microgrid import MockMicrogrid

# pylint: disable=too-many-locals
# pylint: disable=protected-access


class TestLogicalMeter:  # pylint: disable=too-many-public-methods
    """Tests for the logical meter."""

    async def test_chp_power(self, mocker: MockerFixture) -> None:
        """Test the chp power formula."""
        mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
        mockgrid.add_chps(1)
        mockgrid.add_batteries(2)

        async with mockgrid, AsyncExitStack() as stack:
            logical_meter = microgrid.logical_meter()
            stack.push_async_callback(logical_meter.stop)

            chp_power_receiver = logical_meter.chp_power.new_receiver()

            await mockgrid.mock_resampler.send_meter_power([2.0, 3.0, 4.0])
            assert (await chp_power_receiver.receive()).value == Power.from_watts(2.0)

            await mockgrid.mock_resampler.send_meter_power([-12.0, None, 10.2])
            assert (await chp_power_receiver.receive()).value == Power.from_watts(-12.0)

    async def test_pv_power(self, mocker: MockerFixture) -> None:
        """Test the pv power formula."""
        mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
        mockgrid.add_solar_inverters(2)

        async with mockgrid, AsyncExitStack() as stack:
            pv_pool = microgrid.new_pv_pool(priority=5)
            stack.push_async_callback(pv_pool.stop)
            pv_power_receiver = pv_pool.power.new_receiver()

            await mockgrid.mock_resampler.send_meter_power([-1.0, -2.0])
            await mockgrid.mock_resampler.send_pv_inverter_power([-10.0, -20.0])
            assert (await pv_power_receiver.receive()).value == Power.from_watts(-3.0)

    async def test_pv_power_no_meter(self, mocker: MockerFixture) -> None:
        """Test the pv power formula."""
        mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
        mockgrid.add_solar_inverters(2, no_meter=True)

        async with mockgrid, AsyncExitStack() as stack:
            pv_pool = microgrid.new_pv_pool(priority=5)
            stack.push_async_callback(pv_pool.stop)
            pv_power_receiver = pv_pool.power.new_receiver()

            await mockgrid.mock_resampler.send_pv_inverter_power([-1.0, -2.0])
            assert (await pv_power_receiver.receive()).value == Power.from_watts(-3.0)

    async def test_pv_power_no_pv_components(self, mocker: MockerFixture) -> None:
        """Test the pv power formula without having any pv components."""
        async with (
            MockMicrogrid(grid_meter=True, mocker=mocker) as mockgrid,
            AsyncExitStack() as stack,
        ):
            pv_pool = microgrid.new_pv_pool(priority=5)
            stack.push_async_callback(pv_pool.stop)
            pv_power_receiver = pv_pool.power.new_receiver()

            await mockgrid.mock_resampler.send_non_existing_component_value()
            assert (await pv_power_receiver.receive()).value == Power.zero()

    async def test_pv_power_with_failing_meter(self, mocker: MockerFixture) -> None:
        """Test the pv power formula."""
        mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
        mockgrid.add_solar_inverters(2)

        async with mockgrid, AsyncExitStack() as stack:
            pv_pool = microgrid.new_pv_pool(priority=5)
            stack.push_async_callback(pv_pool.stop)
            pv_power_receiver = pv_pool.power.new_receiver()

            # Note: PvPowerFormula has a "nones-are-zero" rule, that says:
            # * if the meter value is None, it should be treated as None.
            # * for other components None is treated as 0.

            expected_input_output: list[
                tuple[list[float | None], list[float | None], Power | None]
            ] = [
                # ([meter_power], [pv_inverter_power], expected_power)
                #
                # Case 1: Both meters are available, so inverters are not used.
                ([-1.0, -2.0], [None, -5.0], Power.from_watts(-3.0)),
                ([-1.0, -2.0], [-10.0, -20.0], Power.from_watts(-3.0)),
                # Case 2: The first meter is unavailable (None).
                # Subscribe to the fallback inverter, but return None as the result,
                # according to the "nones-are-zero" rule
                ([None, -2.0], [-10.0, -20.0], None),
                # Case 3: First meter is unavailable (None). Fallback inverter provides
                # a value.
                ([None, -2.0], [-10.0, -20.0], Power.from_watts(-12.0)),
                ([None, -2.0], [-11.0, -20.0], Power.from_watts(-13.0)),
                # Case 4: Both first meter and its fallback inverter are unavailable
                # (None). Return 0 according to the "nones-are-zero" rule.
                ([None, -2.0], [None, -20.0], Power.from_watts(-2.0)),
                ([None, -2.0], [-11.0, -20.0], Power.from_watts(-13.0)),
                # Case 5: Both meters are unavailable (None).
                # Subscribe to the fallback inverter, but return None as the result,
                # according "nones-are-zero" rule
                ([None, None], [-5.0, -20.0], None),
                # Case 6: Both meters are unavailable (None). Fallback inverter provides
                # a values.
                ([None, None], [-5.0, -20.0], Power.from_watts(-25.0)),
                # Case 7: All components are unavailable (None).
                # Return 0 according to the "nones-are-zero" rule.
                ([None, None], [None, None], Power.from_watts(0.0)),
                ([None, None], [-5.0, -20.0], Power.from_watts(-25.0)),
                # Case 8: Meters becomes available and inverter values are not used.
                ([-10.0, None], [-5.0, -20.0], Power.from_watts(-30.0)),
                ([-10.0, -2.0], [-5.0, -20.0], Power.from_watts(-12.0)),
            ]

            for idx, (meter_power, pv_inverter_power, expected_power) in enumerate(
                expected_input_output
            ):
                await mockgrid.mock_resampler.send_meter_power(meter_power)
                await mockgrid.mock_resampler.send_pv_inverter_power(pv_inverter_power)
                mockgrid.mock_resampler.next_ts()

                result = await pv_power_receiver.receive()
                assert result.value == expected_power, (
                    f"Test case {idx} failed:"
                    + f" meter_power: {meter_power}"
                    + f" pv_inverter_power {pv_inverter_power}"
                    + f" expected_power: {expected_power}"
                    + f" actual_power: {result.value}"
                )



================================================
FILE: tests/timeseries/test_moving_window.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests for the moving window."""

import asyncio
import re
from collections.abc import Sequence
from datetime import datetime, timedelta, timezone

import async_solipsism
import numpy as np
import pytest
import time_machine
from frequenz.channels import Broadcast, Sender
from frequenz.core.datetime import UNIX_EPOCH
from frequenz.quantities import Quantity

from frequenz.sdk.timeseries import (
    MovingWindow,
    ResamplerConfig,
    ResamplerConfig2,
    Sample,
)


@pytest.fixture(autouse=True)
def event_loop_policy() -> async_solipsism.EventLoopPolicy:
    """Return an event loop policy that uses the async solipsism event loop."""
    return async_solipsism.EventLoopPolicy()


async def push_logical_meter_data(
    sender: Sender[Sample[Quantity]],
    test_seq: Sequence[float | None],
    start_ts: datetime = UNIX_EPOCH,
    fake_time: time_machine.Coordinates | None = None,
) -> None:
    """Push data in the passed sender to mock `LogicalMeter` behaviour.

    Starting with UNIX_EPOCH.

    Args:
        sender: Sender for pushing resampled samples to the `MovingWindow`.
        test_seq: The Sequence that is pushed into the `MovingWindow`.
        start_ts: The start timestamp of the `MovingWindow`.
        fake_time: The fake time object to shift the time.
    """
    for i, j in zip(test_seq, range(0, len(test_seq))):
        timestamp = start_ts + timedelta(seconds=j)
        await sender.send(
            Sample(timestamp, Quantity(float(i)) if i is not None else None)
        )
        if fake_time is not None:
            # For the wall clock timer we need to make sure that the wall clock is
            # adjusted just before the timer wakes up from sleeping, and then we need to
            # make sure this function returns *after* the timer has woken up.
            await asyncio.sleep(0.999)
            fake_time.shift(1.0)
            await asyncio.sleep(0.001)

    await asyncio.sleep(0.0)


def init_moving_window(
    size: timedelta,
    resampler_config: ResamplerConfig | None = None,
) -> tuple[MovingWindow, Sender[Sample[Quantity]]]:
    """Initialize the moving window with given shape.

    Args:
        size: The size of the `MovingWindow`
        resampler_config: The resampler configuration.

    Returns:
        tuple[MovingWindow, Sender[Sample]]: A pair of sender and `MovingWindow`.
    """
    lm_chan = Broadcast[Sample[Quantity]](name="lm_net_power")
    lm_tx = lm_chan.new_sender()
    window = MovingWindow(
        size=size,
        resampled_data_recv=lm_chan.new_receiver(),
        input_sampling_period=timedelta(seconds=1),
        resampler_config=resampler_config,
    )
    return window, lm_tx


def dt(i: int) -> datetime:  # pylint: disable=invalid-name
    """Create a timestamp from the given index.

    Args:
        i: The index to create a timestamp from.

    Returns:
        The timestamp created from the index.
    """
    return datetime.fromtimestamp(i, tz=timezone.utc)


async def test_access_window_by_index() -> None:
    """Test indexing a window by integer index."""
    window, sender = init_moving_window(timedelta(seconds=2))
    async with window:
        await push_logical_meter_data(sender, [1, 2, 3])
        assert np.array_equal(window[0], 2.0)
        assert np.array_equal(window[1], 3.0)
        assert np.array_equal(window[-1], 3.0)
        assert np.array_equal(window[-2], 2.0)
        with pytest.raises(IndexError):
            _ = window[3]
        with pytest.raises(IndexError):
            _ = window[-3]


async def test_access_window_by_timestamp() -> None:
    """Test indexing a window by timestamp."""
    window, sender = init_moving_window(timedelta(seconds=2))
    async with window:
        await push_logical_meter_data(sender, [0, 1, 2])
        assert np.array_equal(window[dt(1)], 1.0)
        assert np.array_equal(window.at(dt(1)), 1.0)
        assert np.array_equal(window[dt(2)], 2.0)
        assert np.array_equal(window.at(dt(2)), 2.0)
        with pytest.raises(IndexError):
            _ = window[dt(0)]
        with pytest.raises(IndexError):
            _ = window.at(dt(0))
        with pytest.raises(IndexError):
            _ = window[dt(3)]
        with pytest.raises(IndexError):
            _ = window.at(dt(3))


async def test_access_window_by_int_slice() -> None:
    """Test accessing a subwindow with an integer slice.

    Note that the second test is overwriting the data of the first test.
    since the push_lm_data function is starting with the same initial timestamp.
    """
    window, sender = init_moving_window(timedelta(seconds=14))
    async with window:
        await push_logical_meter_data(sender, range(0, 5))
        assert np.array_equal(window[3:5], np.array([3.0, 4.0]))
        assert np.array_equal(window.window(3, 5), np.array([3.0, 4.0]))

        data = [1, 2, 2.5, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1]
        await push_logical_meter_data(sender, data)
        assert np.array_equal(window[5:14], np.array(data[5:14]))
        assert np.array_equal(window.window(5, 14), np.array(data[5:14]))

        # Test with step size (other than 1 not supported)
        assert np.array_equal(window[5:14:1], np.array(data[5:14]))
        assert np.array_equal(window[5:14:None], np.array(data[5:14]))
        with pytest.raises(ValueError):
            _ = window[5:14:2]
        with pytest.raises(ValueError):
            _ = window[14:5:-1]

    window, sender = init_moving_window(timedelta(seconds=5))

    def test_eq(expected: list[float], start: int | None, end: int | None) -> None:
        assert np.allclose(
            window.window(start, end), np.array(expected), equal_nan=True
        )
        assert np.allclose(window[start:end], np.array(expected), equal_nan=True)

    async with window:
        test_eq([], 0, 1)

        # Incomplete window
        await push_logical_meter_data(sender, [0.0, 1.0])
        test_eq([0.0, 1.0], 0, 2)
        test_eq([0.0, 1.0], 0, 9)
        test_eq([0.0, 1.0], 0, None)
        test_eq([0.0, 1.0], -9, None)
        test_eq([0.0, 1.0], None, None)
        test_eq([0.0], -2, -1)
        test_eq([1.0], -1, None)

        # Incomplete window with gap
        await push_logical_meter_data(
            sender, [3.0], start_ts=UNIX_EPOCH + timedelta(seconds=3)
        )
        test_eq([0.0, 1.0], 0, 2)
        # test gaps to be NaN
        test_eq([0.0, 1.0, np.nan, 3.0], 0, None)
        test_eq([np.nan, 3.0], -2, None)

        # Test fill_value
        assert np.allclose(
            np.array([0.0, 1.0, 2.0, 3.0]),
            window.window(0, None, fill_value=2.0),
            equal_nan=True,
        )

        # Complete window
        await push_logical_meter_data(sender, [0.0, 1.0, 2.0, 3.0, 4.0])
        test_eq([0.0, 1.0], 0, 2)
        test_eq([3.0, 4.0], -2, None)

        # Complete window with nan
        await push_logical_meter_data(sender, [0.0, 1.0, np.nan])
        test_eq([0.0, 1.0, np.nan], 0, 3)
        test_eq([np.nan, 3.0, 4.0], -3, None)


async def test_access_window_by_ts_slice() -> None:
    """Test accessing a subwindow with a timestamp slice."""
    window, sender = init_moving_window(timedelta(seconds=5))
    async with window:
        await push_logical_meter_data(sender, range(0, 5))
        time_start = UNIX_EPOCH + timedelta(seconds=3)
        time_end = time_start + timedelta(seconds=2)
        assert np.array_equal(window[time_start:time_end], np.array([3.0, 4.0]))  # type: ignore
        assert np.array_equal(window.window(dt(3), dt(5)), np.array([3.0, 4.0]))
        assert np.array_equal(window.window(dt(3), dt(3)), np.array([]))
        # Window also supports slicing with indices outside allowed range
        assert np.array_equal(window.window(dt(3), dt(1)), np.array([]))
        assert np.array_equal(window.window(dt(3), dt(6)), np.array([3, 4]))
        assert np.array_equal(window.window(dt(-1), dt(5)), np.array([0, 1, 2, 3, 4]))


async def test_access_empty_window() -> None:
    """Test accessing an empty window, should throw IndexError."""
    window, _ = init_moving_window(timedelta(seconds=5))
    async with window:
        with pytest.raises(IndexError, match=r"^The buffer is empty\.$"):
            _ = window[42]


async def test_window_size() -> None:  # pylint: disable=too-many-statements
    """Test the size of the window."""
    window, sender = init_moving_window(timedelta(seconds=10))
    async with window:

        def assert_valid_and_covered_counts(
            *,
            since: datetime | None = None,
            until: datetime | None = None,
            expected: int | None = None,
            expected_valid: int | None = None,
            expected_covered: int | None = None,
        ) -> None:
            if expected is not None:
                assert window.count_valid(since=since, until=until) == expected
                assert window.count_covered(since=since, until=until) == expected
                return

            assert window.count_valid(since=since, until=until) == expected_valid
            assert window.count_covered(since=since, until=until) == expected_covered

        assert window.capacity == 10, "Wrong window capacity"
        assert window.count_valid() == 0, "Window should be empty"
        assert window.count_covered() == 0, "Window should be empty"

        await push_logical_meter_data(sender, range(0, 2))
        assert window.capacity == 10, "Wrong window capacity"
        assert window.count_valid() == 2, "Window should be partially full"
        assert window.count_covered() == 2, "Window should be partially full"

        newest_ts = window.newest_timestamp
        assert newest_ts is not None and newest_ts == UNIX_EPOCH + timedelta(seconds=1)

        await push_logical_meter_data(sender, range(2, 5), start_ts=newest_ts)
        assert window.capacity == 10, "Wrong window capacity"
        assert window.count_valid() == 4, "Window should be partially full"
        assert window.count_covered() == 4, "Window should be partially full"

        newest_ts = window.newest_timestamp
        assert newest_ts is not None and newest_ts == UNIX_EPOCH + timedelta(seconds=3)

        await push_logical_meter_data(sender, range(5, 12), start_ts=newest_ts)
        assert window.capacity == 10, "Wrong window capacity"
        assert window.count_valid() == 10, "Window should be full"
        assert window.count_covered() == 10, "Window should be full"

        assert_valid_and_covered_counts(
            since=UNIX_EPOCH + timedelta(seconds=1), expected=9
        )
        assert_valid_and_covered_counts(
            until=UNIX_EPOCH + timedelta(seconds=2), expected=3
        )
        assert_valid_and_covered_counts(
            since=UNIX_EPOCH + timedelta(seconds=1),
            until=UNIX_EPOCH + timedelta(seconds=1),
            expected=1,
        )
        assert_valid_and_covered_counts(
            since=UNIX_EPOCH + timedelta(seconds=3),
            until=UNIX_EPOCH + timedelta(seconds=8),
            expected=6,
        )
        assert_valid_and_covered_counts(
            since=UNIX_EPOCH + timedelta(seconds=8),
            until=UNIX_EPOCH + timedelta(seconds=3),
            expected=0,
        )

        newest_ts = window.newest_timestamp
        assert newest_ts is not None and newest_ts == UNIX_EPOCH + timedelta(seconds=9)
        assert window.oldest_timestamp == UNIX_EPOCH

        await push_logical_meter_data(sender, range(5, 12), start_ts=newest_ts)
        assert window.capacity == 10, "Wrong window capacity"
        assert_valid_and_covered_counts(expected=10)

        newest_ts = window.newest_timestamp
        assert newest_ts is not None and newest_ts == UNIX_EPOCH + timedelta(seconds=15)
        assert window.oldest_timestamp == UNIX_EPOCH + timedelta(seconds=6)

        assert_valid_and_covered_counts(
            since=UNIX_EPOCH + timedelta(seconds=1),
            until=UNIX_EPOCH + timedelta(seconds=5),
            expected=0,
        )
        assert_valid_and_covered_counts(
            since=UNIX_EPOCH + timedelta(seconds=3),
            until=UNIX_EPOCH + timedelta(seconds=8),
            expected=3,
        )
        assert_valid_and_covered_counts(
            since=UNIX_EPOCH + timedelta(seconds=6),
            until=UNIX_EPOCH + timedelta(seconds=20),
            expected=10,
        )

        newest_ts = window.newest_timestamp
        assert newest_ts is not None and newest_ts == UNIX_EPOCH + timedelta(seconds=15)
        assert window.oldest_timestamp == UNIX_EPOCH + timedelta(seconds=6)

        await push_logical_meter_data(
            sender, [3, 4, None, None, 10, 12, None], start_ts=newest_ts
        )

        # After the last insertion, the moving window would look like this:
        #
        # +------------------------+----+----+-----+----+----+-----+-----+-----+-----+-----+
        # | MovingWindow timestamp |    |    |     |    |    |     |     |     |     |     |
        # | (seconds after EPOCH)  | 12 | 13 | 14  | 15 | 16 | 17  | 18  | 19  | 20  | 21  |
        # |------------------------+----+----+-----+----+----+-----+-----+-----+-----+-----|
        # | value in buffer        | 8. | 9. | 10. | 3. | 4. | nan | nan | 10. | 12. | nan |
        # +------------------------+----+----+-----+----+----+-----+-----+-----+-----+-----+

        newest_ts = window.newest_timestamp
        assert newest_ts is not None and newest_ts == UNIX_EPOCH + timedelta(seconds=21)
        assert window.oldest_timestamp == UNIX_EPOCH + timedelta(seconds=12)

        assert_valid_and_covered_counts(
            expected_valid=7,
            expected_covered=10,
        )
        assert_valid_and_covered_counts(
            since=UNIX_EPOCH + timedelta(seconds=15),
            expected_valid=4,
            expected_covered=7,
        )
        assert_valid_and_covered_counts(
            until=UNIX_EPOCH + timedelta(seconds=19),
            expected_valid=6,
            expected_covered=8,
        )
        assert_valid_and_covered_counts(
            since=UNIX_EPOCH + timedelta(seconds=12),
            until=UNIX_EPOCH + timedelta(seconds=15),
            expected_valid=4,
            expected_covered=4,
        )
        assert_valid_and_covered_counts(
            since=UNIX_EPOCH + timedelta(seconds=17),
            until=UNIX_EPOCH + timedelta(seconds=18),
            expected_valid=0,
            expected_covered=2,
        )
        assert_valid_and_covered_counts(
            since=UNIX_EPOCH + timedelta(seconds=16),
            until=UNIX_EPOCH + timedelta(seconds=20),
            expected_valid=3,
            expected_covered=5,
        )


async def test_wait_for_samples() -> None:
    """Test waiting for samples in the window."""
    window, sender = init_moving_window(timedelta(seconds=10))
    async with window:
        task = asyncio.create_task(window.wait_for_samples(5))
        await asyncio.sleep(0)
        assert not task.done()
        await push_logical_meter_data(sender, range(0, 5))
        await asyncio.sleep(0)
        # After pushing 5 values, the `wait_for_samples` task should be done.
        assert task.done()

        task = asyncio.create_task(window.wait_for_samples(5))
        await asyncio.sleep(0)
        await push_logical_meter_data(
            sender, [1, 2, 3, 4], start_ts=UNIX_EPOCH + timedelta(seconds=5)
        )
        await asyncio.sleep(0)
        # The task should not be done yet, since we have only pushed 4 values.
        assert not task.done()

        await push_logical_meter_data(
            sender, [1], start_ts=UNIX_EPOCH + timedelta(seconds=9)
        )
        await asyncio.sleep(0)
        # After pushing the last value, the task should be done.
        assert task.done()

        task = asyncio.create_task(window.wait_for_samples(-1))
        with pytest.raises(
            ValueError,
            match=re.escape("The number of samples to wait for must be 0 or greater."),
        ):
            await task

        task = asyncio.create_task(window.wait_for_samples(20))
        with pytest.raises(
            ValueError,
            match=re.escape(
                "The number of samples to wait for must be less than or equal to the "
                + "capacity of the MovingWindow (10)."
            ),
        ):
            await task

        task = asyncio.create_task(window.wait_for_samples(4))
        await asyncio.sleep(0)
        await push_logical_meter_data(
            sender, range(0, 10), start_ts=UNIX_EPOCH + timedelta(seconds=10)
        )
        await asyncio.sleep(0)
        assert task.done()

        task = asyncio.create_task(window.wait_for_samples(10))
        await asyncio.sleep(0)
        await push_logical_meter_data(
            sender, range(0, 5), start_ts=UNIX_EPOCH + timedelta(seconds=20)
        )
        await asyncio.sleep(0)
        assert not task.done()

        await push_logical_meter_data(
            sender, range(10, 15), start_ts=UNIX_EPOCH + timedelta(seconds=25)
        )
        await asyncio.sleep(0)
        assert task.done()

        task = asyncio.create_task(window.wait_for_samples(5))
        await asyncio.sleep(0)
        await push_logical_meter_data(
            sender, [1, 2, None, 4, None], start_ts=UNIX_EPOCH + timedelta(seconds=30)
        )
        await asyncio.sleep(0)
        # `None` values *are* counted towards the number of samples to wait for.
        assert task.done()


@pytest.mark.parametrize("config_class", [ResamplerConfig, ResamplerConfig2])
async def test_wait_for_samples_with_resampling(
    config_class: type[ResamplerConfig], fake_time: time_machine.Coordinates
) -> None:
    """Test waiting for samples in a moving window with resampling."""
    window, sender = init_moving_window(
        timedelta(seconds=20), config_class(resampling_period=timedelta(seconds=2))
    )
    async with window:
        task = asyncio.create_task(window.wait_for_samples(3))
        await asyncio.sleep(0)
        assert not task.done()
        await push_logical_meter_data(sender, range(0, 7), fake_time=fake_time)
        assert task.done()

        task = asyncio.create_task(window.wait_for_samples(10))
        await push_logical_meter_data(
            sender,
            range(0, 10),
            fake_time=fake_time,
            start_ts=UNIX_EPOCH + timedelta(seconds=7),
        )
        assert window.count_covered() == 8
        assert not task.done()

        await push_logical_meter_data(
            sender,
            range(0, 6),
            fake_time=fake_time,
            start_ts=UNIX_EPOCH + timedelta(seconds=17),
        )
        assert window.count_covered() == 10
        assert not task.done()

        await push_logical_meter_data(
            sender,
            range(0, 6),
            fake_time=fake_time,
            start_ts=UNIX_EPOCH + timedelta(seconds=23),
        )
        assert window.count_covered() == 10
        assert window.count_valid() == 10
        assert task.done()

        task = asyncio.create_task(window.wait_for_samples(5))
        await push_logical_meter_data(
            sender,
            [1, 2, None, None, None, None, None, None, None, None],
            fake_time=fake_time,
            start_ts=UNIX_EPOCH + timedelta(seconds=29),
        )
        assert window.count_covered() == 10
        assert window.count_valid() == 8
        assert task.done()

        task = asyncio.create_task(window.wait_for_samples(5))
        await push_logical_meter_data(
            sender,
            [None, 4, None, None, None, None, None, None, None, 5],
            fake_time=fake_time,
            start_ts=UNIX_EPOCH + timedelta(seconds=39),
        )
        assert window.count_covered() == 10
        # There is also an inconsistency here between the monotonic and wall clock,
        # the wall clock timer have less samples in the buffer at the end. This is
        # probably some border condition because of the way we fake time
        is_wall_clock = config_class is ResamplerConfig2
        assert window.count_valid() == (5 if is_wall_clock else 7)
        assert task.done()


# pylint: disable=redefined-outer-name
async def test_resampling_window(fake_time: time_machine.Coordinates) -> None:
    """Test resampling in MovingWindow."""
    channel = Broadcast[Sample[Quantity]](name="net_power")
    sender = channel.new_sender()

    window_size = timedelta(seconds=16)
    input_sampling = timedelta(seconds=1)
    output_sampling = timedelta(seconds=2)
    resampler_config = ResamplerConfig(resampling_period=output_sampling)

    async with MovingWindow(
        size=window_size,
        resampled_data_recv=channel.new_receiver(),
        input_sampling_period=input_sampling,
        resampler_config=resampler_config,
    ) as window:
        assert window.capacity == window_size / output_sampling, "Wrong window capacity"
        assert window.count_valid() == 0, "Window should be empty at the beginning"
        stream_values = [4.0, 8.0, 2.0, 6.0, 5.0] * 100
        for value in stream_values:
            timestamp = datetime.now(tz=timezone.utc)
            sample = Sample(timestamp, Quantity(float(value)))
            await sender.send(sample)
            await asyncio.sleep(0.1)
            fake_time.shift(0.1)

        assert window.count_valid() == window_size / output_sampling
        for value in window:  # type: ignore
            assert 4.9 < value < 5.1


async def test_timestamps() -> None:
    """Test indexing a window by timestamp."""
    window, sender = init_moving_window(timedelta(seconds=5))
    async with window:
        await push_logical_meter_data(
            sender, [1, 2], start_ts=UNIX_EPOCH + timedelta(seconds=1)
        )
        assert window.oldest_timestamp == UNIX_EPOCH + timedelta(seconds=1)
        assert window.newest_timestamp == UNIX_EPOCH + timedelta(seconds=2)



================================================
FILE: tests/timeseries/test_periodic_feature_extractor.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests for the timeseries averager."""

import contextlib
from collections.abc import AsyncIterator
from datetime import datetime, timedelta, timezone

import numpy as np
import pytest
from frequenz.channels import Broadcast
from frequenz.core.datetime import UNIX_EPOCH
from frequenz.quantities import Quantity

from frequenz.sdk.timeseries import (
    MovingWindow,
    PeriodicFeatureExtractor,
    Sample,
)
from tests.timeseries.test_moving_window import (
    init_moving_window,
    push_logical_meter_data,
)


@contextlib.asynccontextmanager
async def init_feature_extractor(
    data: list[float], period: timedelta
) -> AsyncIterator[PeriodicFeatureExtractor]:
    """
    Initialize a PeriodicFeatureExtractor with a `MovingWindow` that contains the data.

    Args:
        data: The data that is pushed into the moving window.
        period: The distance between two successive windows.

    Yields:
        PeriodicFeatureExtractor
    """
    window, sender = init_moving_window(timedelta(seconds=len(data)))
    async with window:
        await push_logical_meter_data(sender, data)
        yield PeriodicFeatureExtractor(moving_window=window, period=period)


@contextlib.asynccontextmanager
async def init_feature_extractor_no_data(
    period: int,
) -> AsyncIterator[PeriodicFeatureExtractor]:
    """
    Initialize a PeriodicFeatureExtractor with a `MovingWindow` that contains no data.

    Args:
        period: The distance between two successive windows.

    Yields:
        PeriodicFeatureExtractor
    """
    # We only need the moving window to initialize the PeriodicFeatureExtractor class.
    lm_chan = Broadcast[Sample[Quantity]](name="lm_net_power")
    moving_window = MovingWindow(
        size=timedelta(seconds=1),
        resampled_data_recv=lm_chan.new_receiver(),
        input_sampling_period=timedelta(seconds=1),
    )
    async with moving_window:
        await lm_chan.new_sender().send(
            Sample(datetime.now(tz=timezone.utc), Quantity(0))
        )

        # Initialize the PeriodicFeatureExtractor class with a period of period seconds.
        # This works since the sampling period is set to 1 second.
        yield PeriodicFeatureExtractor(moving_window, timedelta(seconds=period))


async def test_interval_shifting() -> None:
    """Test if a interval is properly shifted into a moving window."""
    async with init_feature_extractor(
        [1, 2, 2, 1, 1, 1, 2, 2, 1, 1], timedelta(seconds=5)
    ) as feature_extractor:
        # Test if the timestamp is not shifted
        timestamp = datetime(2023, 1, 1, 0, 0, 1, tzinfo=timezone.utc)
        index_not_shifted = (
            feature_extractor._timestamp_to_rel_index(  # pylint: disable=protected-access
                timestamp
            )
            % feature_extractor._period  # pylint: disable=protected-access
        )
        assert index_not_shifted == 1

        # Test if a timestamp in the window is shifted to the first appearance of the window
        timestamp = datetime(2023, 1, 1, 0, 0, 6, tzinfo=timezone.utc)
        index_shifted = (
            feature_extractor._timestamp_to_rel_index(  # pylint: disable=protected-access
                timestamp
            )
            % feature_extractor._period  # pylint: disable=protected-access
        )
        assert index_shifted == 1

        # Test if a timestamp outside the window is shifted
        timestamp = datetime(2023, 1, 1, 0, 0, 11, tzinfo=timezone.utc)
        index_shifted = (
            feature_extractor._timestamp_to_rel_index(  # pylint: disable=protected-access
                timestamp
            )
            % feature_extractor._period  # pylint: disable=protected-access
        )
        assert index_shifted == 1


async def test_feature_extractor() -> None:  # pylint: disable=too-many-statements
    """Test the feature extractor with a moving window that contains data."""
    start = UNIX_EPOCH + timedelta(seconds=1)
    end = start + timedelta(seconds=2)

    data: list[float] = [1, 2, 2.5, 1, 1, 1, 2, 2, 1, 1, 2, 2]

    async with init_feature_extractor(data, timedelta(seconds=3)) as feature_extractor:
        assert np.allclose(feature_extractor.avg(start, end), [5 / 3, 4 / 3])

    async with init_feature_extractor(data, timedelta(seconds=4)) as feature_extractor:
        assert np.allclose(feature_extractor.avg(start, end), [1, 2])

    data: list[float] = [1, 2, 2.5, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1]  # type: ignore[no-redef]

    async with init_feature_extractor(data, timedelta(seconds=5)) as feature_extractor:
        assert np.allclose(feature_extractor.avg(start, end), [1.5, 1.5])

    async def _test_fun(  # pylint: disable=too-many-arguments,too-many-positional-arguments
        data: list[float],
        period: int,
        start: int,
        end: int,
        expected: list[float],
        weights: list[float] | None = None,
    ) -> None:
        async with init_feature_extractor(
            data, timedelta(seconds=period)
        ) as feature_extractor:
            ret = feature_extractor.avg(
                UNIX_EPOCH + timedelta(seconds=start),
                UNIX_EPOCH + timedelta(seconds=end),
                weights=weights,
            )
            assert np.allclose(ret, expected)

    async def test_09(
        period: int,
        start: int,
        end: int,
        expected: list[float],
        weights: list[float] | None = None,
    ) -> None:
        data: list[float] = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
        await _test_fun(data, period, start, end, expected, weights)

    async def test_011(
        period: int,
        start: int,
        end: int,
        expected: list[float],
        weights: list[float] | None = None,
    ) -> None:
        data: list[float] = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
        await _test_fun(data, period, start, end, expected, weights)

    # empty time period
    with pytest.raises(ValueError):
        await test_09(period=5, start=1, end=1, expected=[4711])

    # moving window not multiple of period
    with pytest.raises(ValueError):
        await test_09(period=3, start=1, end=1, expected=[4711])

    # time period in moving window
    await test_09(period=2, start=0, end=1, expected=[5])
    await test_09(period=2, start=0, end=2, expected=[5, 6])
    await test_09(period=2, start=5, end=7, expected=[4, 5])
    await test_09(period=2, start=8, end=10, expected=[5, 6])
    await test_09(period=5, start=0, end=1, expected=[5])
    await test_09(period=5, start=5, end=6, expected=[5])

    # time period outside of moving window in future
    await test_09(period=5, start=10, end=11, expected=[5])
    # time period outside of moving window in past
    await test_09(period=5, start=-5, end=-4, expected=[5])

    await test_09(period=5, start=0, end=2, expected=[5, 6])
    await test_09(period=5, start=0, end=3, expected=[5, 6, 7])
    await test_09(period=5, start=0, end=4, expected=[5, 6, 7, 8])
    await test_09(period=5, start=1, end=5, expected=[6, 7, 8, 9])

    # No full time period in moving window, expect to throw
    await test_09(period=5, start=0, end=5, expected=[5, 6, 7, 8, 9])
    with pytest.raises(Exception):
        await test_09(period=5, start=0, end=6, expected=[5])
    with pytest.raises(Exception):
        await test_09(period=5, start=0, end=7, expected=[5, 6])
    with pytest.raises(Exception):
        await test_09(period=5, start=0, end=8, expected=[5, 6, 7])
    with pytest.raises(Exception):
        await test_09(period=5, start=0, end=9, expected=[5, 6, 7, 8])
    with pytest.raises(Exception):
        await test_09(period=5, start=0, end=10, expected=[4711])
    with pytest.raises(Exception):
        await test_09(period=5, start=0, end=11, expected=[5])
    with pytest.raises(Exception):
        await test_09(period=5, start=0, end=12, expected=[5, 6])

    # time period outside window but more matches
    await test_09(
        period=5, start=8, end=11, expected=[3, 4, 5]
    )  # First occurence [-2, -1, 0] partly inside window

    # time period larger than period
    with pytest.raises(Exception):
        await test_09(period=2, start=8, end=11, expected=[5])
    with pytest.raises(Exception):
        await test_09(period=2, start=0, end=3, expected=[5])

    # Weights
    await test_011(period=4, start=0, end=2, expected=[6, 7])
    await test_011(period=4, start=0, end=2, expected=[6, 7], weights=None)
    await test_011(period=4, start=0, end=2, expected=[6, 7], weights=[1, 1])
    await test_011(
        period=4, start=0, end=2, expected=[4, 5], weights=[1, 0]
    )  # oldest weight first
    await test_011(period=4, start=0, end=2, expected=[6, 7], weights=[1, 1])
    with pytest.raises(ValueError):
        await test_011(
            period=4, start=0, end=2, expected=[4711, 4711], weights=[1, 1, 1]
        )
    with pytest.raises(ValueError):
        await test_011(period=4, start=0, end=2, expected=[4711, 4711], weights=[1])


async def test_profiler_calculate_np() -> None:
    """Test calculating the average with numpy and a pure python version.

    Calculate the average using a numpy array and compare the run time against the pure
    python method with the same functionality.
    """
    data = np.array([2, 2.5, 1, 1, 1, 2])
    async with init_feature_extractor_no_data(4) as feature_extractor:
        window_size = 2
        reshaped = (
            feature_extractor._reshape_np_array(  # pylint: disable=protected-access
                data, window_size
            )
        )
        result = np.average(reshaped[:, :window_size], axis=0)
        assert np.allclose(result, np.array([1.5, 2.25]))

    data = np.array([2, 2, 1, 1, 2])
    async with init_feature_extractor_no_data(5) as feature_extractor:
        reshaped = (
            feature_extractor._reshape_np_array(  # pylint: disable=protected-access
                data, window_size
            )
        )
        result = np.average(reshaped[:, :window_size], axis=0)
        assert np.allclose(result, np.array([2, 2]))



================================================
FILE: tests/timeseries/test_producer.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Test the logical component for calculating high level producer metrics."""

from contextlib import AsyncExitStack

from frequenz.quantities import Power
from pytest_mock import MockerFixture

from frequenz.sdk import microgrid

from .mock_microgrid import MockMicrogrid


class TestProducer:
    """Tests for the producer power formula."""

    async def test_producer_power(self, mocker: MockerFixture) -> None:
        """Test the producer power formula."""
        mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
        mockgrid.add_solar_inverters(2)
        mockgrid.add_chps(2)

        async with mockgrid, AsyncExitStack() as stack:
            producer = microgrid.producer()
            stack.push_async_callback(producer.stop)
            producer_power_receiver = producer.power.new_receiver()

            await mockgrid.mock_resampler.send_meter_power([2.0, 3.0, 4.0, 5.0])
            assert (await producer_power_receiver.receive()).value == Power.from_watts(
                14.0
            )

    async def test_producer_power_no_chp(self, mocker: MockerFixture) -> None:
        """Test the producer power formula without a chp."""
        mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
        mockgrid.add_solar_inverters(2)

        async with mockgrid, AsyncExitStack() as stack:
            producer = microgrid.producer()
            stack.push_async_callback(producer.stop)
            producer_power_receiver = producer.power.new_receiver()

            await mockgrid.mock_resampler.send_meter_power([2.0, 3.0])
            assert (await producer_power_receiver.receive()).value == Power.from_watts(
                5.0
            )

    async def test_producer_power_no_pv_no_consumer_meter(
        self, mocker: MockerFixture
    ) -> None:
        """Test the producer power formula without pv and without consumer meter."""
        mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
        mockgrid.add_chps(1, True)

        async with mockgrid, AsyncExitStack() as stack:
            producer = microgrid.producer()
            stack.push_async_callback(producer.stop)
            producer_power_receiver = producer.power.new_receiver()

            await mockgrid.mock_resampler.send_chp_power([2.0])

            assert (await producer_power_receiver.receive()).value == Power.from_watts(
                2.0
            )

    async def test_producer_power_no_pv(self, mocker: MockerFixture) -> None:
        """Test the producer power formula without pv."""
        mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
        mockgrid.add_consumer_meters()
        mockgrid.add_chps(1)

        async with mockgrid, AsyncExitStack() as stack:
            producer = microgrid.producer()
            stack.push_async_callback(producer.stop)
            producer_power_receiver = producer.power.new_receiver()

            await mockgrid.mock_resampler.send_meter_power([20.0, 2.0])
            assert (await producer_power_receiver.receive()).value == Power.from_watts(
                2.0
            )

    async def test_no_producer_power(self, mocker: MockerFixture) -> None:
        """Test the producer power formula without producers."""
        async with (
            MockMicrogrid(grid_meter=True, mocker=mocker) as mockgrid,
            AsyncExitStack() as stack,
        ):
            producer = microgrid.producer()
            stack.push_async_callback(producer.stop)
            producer_power_receiver = producer.power.new_receiver()

            await mockgrid.mock_resampler.send_non_existing_component_value()
            assert (await producer_power_receiver.receive()).value == Power.from_watts(
                0.0
            )

    async def test_producer_fallback_formula(self, mocker: MockerFixture) -> None:
        """Test the producer power formula with fallback formulas."""
        mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
        mockgrid.add_solar_inverters(2)
        # CHP has no meter, so no fallback component
        mockgrid.add_chps(1, no_meters=True)

        async with mockgrid, AsyncExitStack() as stack:
            producer = microgrid.producer()
            stack.push_async_callback(producer.stop)
            producer_power_receiver = producer.power.new_receiver()

            # Note: ProducerPowerFormula has a "nones-are-zero" rule, that says:
            # * if the meter value is None, it should be treated as None.
            # * for other components None is treated as 0.

            expected_input_output: list[
                tuple[
                    float,
                    list[float | None],
                    list[float | None],
                    list[float | None],
                    Power | None,
                ]
            ]
            # fmt: off
            expected_input_output = [
                # (test number, [pv_meter_power], [pv_inverter_power], [chp_power], expected_power)
                # Step 1: All components are available
                # Add power from meters and chp
                (1.1, [-1.0, -2.0], [None, -200.0], [300], Power.from_watts(297.0)),
                (1.2, [-1.0, -10], [-100.0, -200.0], [400], Power.from_watts(389.0)),
                # Step 2: The first meter is unavailable (None).
                # Subscribe to the fallback inverter, but return None as the result,
                # according to the "nones-are-zero" rule
                (2.1, [None, -2.0], [-100, -200.0], [400], None),
                # Step 3: First meter is unavailable (None). Fallback inverter provides
                # a value.
                # Add second meter, first inverter and chp power
                (3.1, [None, -2.0], [-100, -200.0], [400], Power.from_watts(298.0)),
                (3.2, [None, -2.0], [-50, -200.0], [300], Power.from_watts(248.0)),
                # Step 4: Both first meter and its fallback inverter are unavailable
                # (None). Return 0 from failing component according to the
                # "nones-are-zero" rule.
                (4.1, [None, -2.0], [None, -200.0], [300], Power.from_watts(298.0)),
                (4.2, [None, -10.0], [-20.0, -200.0], [300], Power.from_watts(270.0)),
                # Step 5: CHP is unavailable. Return 0 from failing component
                # according to the "nones-are-zero" rule.
                (5.1, [None, -10.0], [-20.0, -200.0], [None], Power.from_watts(-30.0)),
                # Step 6: Both meters are unavailable (None). Subscribe for fallback inverter
                (6.1, [None, None], [-20.0, -200.0], [None], None),
                (6.2, [None, None], [-20.0, -200.0], [None], Power.from_watts(-220.0)),
                (6.3, [None, None], [None, -200.0], [None], Power.from_watts(-200.0)),
                # Step 7: All components are unavailable (None). Return 0 according to the
                # "nones-are-zero" rule.
                (7.1, [None, None], [None, None], [None], Power.from_watts(0)),
                (7.2, [None, None], [None, None], [None], Power.from_watts(0)),
                (7.3, [None, None], [None, None], [300.0], Power.from_watts(300.0)),
                (7.4, [-200.0, None], [None, -100.0], [50.0], Power.from_watts(-250.0)),
                (7.5, [-200.0, -200.0], [-10.0, -20.0], [50.0], Power.from_watts(-350.0)),
                # Step 8: Meter is unavailable, start fallback formula.
                (8.1, [None, -200.0], [-10.0, -100.0], [50.0], None),
                (8.2, [None, -200.0], [-10.0, -100.0], [50.0], Power.from_watts(-160)),

            ]
            # fmt: on

            for (
                idx,
                meter_power,
                pv_inverter_power,
                chp_power,
                expected_power,
            ) in expected_input_output:
                print("----------------------------------------------------")
                print(f"                Test step {idx}")
                print("----------------------------------------------------")
                await mockgrid.mock_resampler.send_chp_power(chp_power)
                await mockgrid.mock_resampler.send_meter_power(meter_power)
                await mockgrid.mock_resampler.send_pv_inverter_power(pv_inverter_power)
                mockgrid.mock_resampler.next_ts()

                result = await producer_power_receiver.receive()
                assert result.value == expected_power, (
                    f"Test step {idx} failed:"
                    + f" meter_power: {meter_power}"
                    + f" pv_inverter_power {pv_inverter_power}"
                    + f" chp_power {chp_power}"
                    + f" expected_power: {expected_power}"
                    + f" actual_power: {result.value}"
                )



================================================
FILE: tests/timeseries/test_ringbuffer.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests for the `OrderedRingbuffer` class."""


import random
from datetime import datetime, timedelta, timezone
from itertools import cycle, islice
from typing import Any

import numpy as np
import pytest
from frequenz.quantities import Quantity

from frequenz.sdk.timeseries import Sample
from frequenz.sdk.timeseries._ringbuffer import Gap, OrderedRingBuffer
from frequenz.sdk.timeseries._ringbuffer.buffer import FloatArray

FIVE_MINUTES = timedelta(minutes=5)
ONE_MINUTE = timedelta(minutes=1)
ONE_SECOND = timedelta(seconds=1)
TWO_HUNDRED_MS = timedelta(milliseconds=200)


@pytest.mark.parametrize(
    "buffer",
    [
        OrderedRingBuffer([0.0] * 1800, TWO_HUNDRED_MS),
        OrderedRingBuffer(
            np.empty(shape=(24 * 1800,), dtype=np.float64),
            TWO_HUNDRED_MS,
        ),
        OrderedRingBuffer(
            [0.0] * 1800, TWO_HUNDRED_MS, datetime(2000, 1, 1, tzinfo=timezone.utc)
        ),
    ],
)
def test_timestamp_ringbuffer(buffer: OrderedRingBuffer[Any]) -> None:
    """Test ordered ring buffer."""
    size = buffer.maxlen

    random.seed(0)

    resolution = buffer.sampling_period.total_seconds()

    # import pdb; pdb.set_trace()

    # Push in random order
    # for i in random.sample(range(size), size):
    for i in range(size):
        buffer.update(
            Sample(
                datetime.fromtimestamp(200 + i * resolution, tz=timezone.utc),
                Quantity(i),
            )
        )

    # Check all possible window sizes and start positions
    for i in range(0, size, 1000):
        for j in range(1, size - i, 987):
            assert i + j < size
            start = datetime.fromtimestamp(200 + i * resolution, tz=timezone.utc)
            end = datetime.fromtimestamp(200 + (j + i) * resolution, tz=timezone.utc)

            tmp = list(islice(cycle(range(0, size)), i, i + j))
            assert list(buffer.window(start, end)) == list(tmp)


@pytest.mark.parametrize(
    "buffer",
    [
        (OrderedRingBuffer([0.0] * 24, ONE_SECOND)),
        (OrderedRingBuffer(np.empty(shape=(24,), dtype=np.float64), ONE_SECOND)),
    ],
)
def test_timestamp_ringbuffer_overwrite(buffer: OrderedRingBuffer[Any]) -> None:
    """Test overwrite behavior and correctness."""
    size = buffer.maxlen

    random.seed(0)

    # Push in random order
    for i in random.sample(range(size), size):
        buffer.update(
            Sample(datetime.fromtimestamp(200 + i, tz=timezone.utc), Quantity(i))
        )

    # Push the same amount twice
    for i in random.sample(range(size), size):
        buffer.update(
            Sample(datetime.fromtimestamp(200 + i, tz=timezone.utc), Quantity(i * 2))
        )

    # Check all possible window sizes and start positions
    for i in range(size):
        for j in range(1, size - i):
            start = datetime.fromtimestamp(200 + i, tz=timezone.utc)
            end = datetime.fromtimestamp(200 + j + i, tz=timezone.utc)

            tmp = islice(cycle(range(0, size * 2, 2)), i, i + j)
            actual: float
            for actual, expectation in zip(buffer.window(start, end), tmp):
                assert actual == expectation

            assert j == len(buffer.window(start, end))


@pytest.mark.parametrize(
    "buffer",
    [
        (OrderedRingBuffer([0.0] * 24, ONE_SECOND)),
        (OrderedRingBuffer(np.empty(shape=(24,), dtype=np.float64), ONE_SECOND)),
    ],
)
def test_timestamp_ringbuffer_gaps(
    buffer: OrderedRingBuffer[Any],
) -> None:
    """Test force_copy command for window()."""
    size = buffer.maxlen
    random.seed(0)

    # Add initial data
    for i in random.sample(range(size), size):
        buffer.update(
            Sample(datetime.fromtimestamp(200 + i, tz=timezone.utc), Quantity(i))
        )

    # Request window of the data
    buffer.window(
        datetime.fromtimestamp(200, tz=timezone.utc),
        datetime.fromtimestamp(202, tz=timezone.utc),
    )

    # Add entry far in the future
    buffer.update(
        Sample(datetime.fromtimestamp(500 + size, tz=timezone.utc), Quantity(9999))
    )

    # Allow still to request old (empty) window
    buffer.window(
        datetime.fromtimestamp(200, tz=timezone.utc),
        datetime.fromtimestamp(202, tz=timezone.utc),
    )

    # Receive new window without exception
    buffer.window(
        datetime.fromtimestamp(501, tz=timezone.utc),
        datetime.fromtimestamp(500 + size, tz=timezone.utc),
    )


@pytest.mark.parametrize(
    "buffer",
    [
        OrderedRingBuffer([0.0] * 24 * int(FIVE_MINUTES.total_seconds()), FIVE_MINUTES),
        OrderedRingBuffer(
            np.empty(shape=(24 * int(FIVE_MINUTES.total_seconds())), dtype=np.float64),
            FIVE_MINUTES,
        ),
    ],
)
def test_timestamp_ringbuffer_missing_parameter(
    buffer: OrderedRingBuffer[Any],
) -> None:
    """Test ordered ring buffer."""
    buffer.update(Sample(datetime(2, 2, 2, 0, 0, tzinfo=timezone.utc), Quantity(0)))

    assert buffer.normalize_timestamp(buffer.gaps[0].start) == buffer.gaps[0].start

    # Expecting one gap now, made of all the previous entries of the one just
    # added.
    assert len(buffer.gaps) == 1
    assert buffer.gaps[0].end == datetime(2, 2, 2, tzinfo=timezone.utc)

    # Add entry so that a second gap appears
    assert buffer.normalize_timestamp(
        datetime(2, 2, 2, 0, 7, 31, tzinfo=timezone.utc)
    ) == datetime(2, 2, 2, 0, 10, tzinfo=timezone.utc)
    buffer.update(Sample(datetime(2, 2, 2, 0, 7, 31, tzinfo=timezone.utc), Quantity(0)))

    assert buffer.to_internal_index(
        datetime(2, 2, 2, 0, 7, 31, tzinfo=timezone.utc)
    ) == buffer.to_internal_index(datetime(2, 2, 2, 0, 10, tzinfo=timezone.utc))
    assert len(buffer.gaps) == 2

    # import pdb; pdb.set_trace()
    buffer.update(Sample(datetime(2, 2, 2, 0, 5, tzinfo=timezone.utc), Quantity(0)))
    assert len(buffer.gaps) == 1


def dt(i: int) -> datetime:  # pylint: disable=invalid-name
    """Create a timestamp from the given index.

    Args:
        i: The index to create a timestamp from.

    Returns:
        The timestamp created from the index.
    """
    return datetime.fromtimestamp(i, tz=timezone.utc)


def test_gaps() -> None:  # pylint: disable=too-many-statements
    """Test gap treatment in ordered ring buffer."""
    buffer = OrderedRingBuffer([0.0] * 5, ONE_SECOND)
    assert buffer.oldest_timestamp is None
    assert buffer.newest_timestamp is None
    assert buffer.count_valid() == 0
    assert buffer.count_covered() == 0
    assert len(buffer.gaps) == 0

    buffer.update(Sample(dt(0), Quantity(0)))
    assert buffer.oldest_timestamp == dt(0)
    assert buffer.newest_timestamp == dt(0)
    assert buffer.count_valid() == 1
    assert buffer.count_covered() == 1
    assert len(buffer.gaps) == 1

    buffer.update(Sample(dt(6), Quantity(0)))
    assert buffer.oldest_timestamp == dt(6)
    assert buffer.newest_timestamp == dt(6)
    assert buffer.count_valid() == 1
    assert buffer.count_covered() == 1
    assert len(buffer.gaps) == 1

    buffer.update(Sample(dt(2), Quantity(2)))
    buffer.update(Sample(dt(3), Quantity(3)))
    buffer.update(Sample(dt(4), Quantity(4)))
    assert buffer.oldest_timestamp == dt(2)
    assert buffer.newest_timestamp == dt(6)
    assert buffer.count_valid() == 4
    assert buffer.count_covered() == 5
    assert len(buffer.gaps) == 1

    buffer.update(Sample(dt(3), None))
    assert buffer.oldest_timestamp == dt(2)
    assert buffer.newest_timestamp == dt(6)
    assert buffer.count_valid() == 3
    assert buffer.count_covered() == 5
    assert len(buffer.gaps) == 2

    buffer.update(Sample(dt(3), Quantity(np.nan)))
    assert buffer.oldest_timestamp == dt(2)
    assert buffer.newest_timestamp == dt(6)
    assert buffer.count_valid() == 3
    assert buffer.count_covered() == 5
    assert len(buffer.gaps) == 2

    buffer.update(Sample(dt(2), Quantity(np.nan)))
    assert buffer.oldest_timestamp == dt(4)
    assert buffer.newest_timestamp == dt(6)
    assert buffer.count_valid() == 2
    assert buffer.count_covered() == 3
    assert len(buffer.gaps) == 2

    buffer.update(Sample(dt(3), Quantity(3)))
    assert buffer.oldest_timestamp == dt(3)
    assert buffer.newest_timestamp == dt(6)
    assert buffer.count_valid() == 3
    assert buffer.count_covered() == 4
    assert len(buffer.gaps) == 2

    buffer.update(Sample(dt(2), Quantity(2)))
    assert buffer.oldest_timestamp == dt(2)
    assert buffer.newest_timestamp == dt(6)
    assert buffer.count_valid() == 4
    assert buffer.count_covered() == 5
    assert len(buffer.gaps) == 1

    buffer.update(Sample(dt(5), Quantity(5)))
    assert buffer.oldest_timestamp == dt(2)
    assert buffer.newest_timestamp == dt(6)
    assert buffer.count_valid() == 5
    assert buffer.count_covered() == 5
    assert len(buffer.gaps) == 0

    # whole range gap
    buffer.update(Sample(dt(99), None))
    assert buffer.oldest_timestamp is None
    assert buffer.newest_timestamp is None
    assert buffer.count_valid() == 0
    assert buffer.count_covered() == 0
    assert len(buffer.gaps) == 1


@pytest.mark.parametrize(
    "buffer",
    [
        OrderedRingBuffer([0.0] * 10 * int(ONE_MINUTE.total_seconds()), ONE_MINUTE),
        OrderedRingBuffer(
            np.empty(shape=(12 * int(ONE_MINUTE.total_seconds()),), dtype=np.float64),
            ONE_MINUTE,
        ),
        OrderedRingBuffer([0.0] * 5 * int(FIVE_MINUTES.total_seconds()), FIVE_MINUTES),
        OrderedRingBuffer(
            np.empty(shape=(5 * int(FIVE_MINUTES.total_seconds())), dtype=np.float64),
            FIVE_MINUTES,
        ),
    ],
)
def test_timestamp_ringbuffer_missing_parameter_smoke(
    buffer: OrderedRingBuffer[Any],
) -> None:
    """Test ordered ring buffer."""
    size = buffer.maxlen
    resolution = int(buffer.sampling_period.total_seconds())

    random.seed(0)

    for _ in range(0, 10):
        expected_gaps_abstract = [(0.1, 0.2), (0.35, 0.4), (0.86, 1.0)]
        expected_gaps_concrete = list(
            map(lambda x: (int(size * x[0]), int(size * x[1])), expected_gaps_abstract)
        )

        # Push in random order
        # pylint: disable=cell-var-from-loop
        for j in random.sample(range(size), size):
            missing = any(map(lambda x: x[0] <= j < x[1], expected_gaps_concrete))
            buffer.update(
                Sample(
                    datetime.fromtimestamp(
                        200 + j * buffer.sampling_period.total_seconds(),
                        tz=timezone.utc,
                    ),
                    None if missing else Quantity(j),
                )
            )

        expected_gaps = list(
            map(
                lambda x: Gap(
                    start=buffer.normalize_timestamp(
                        datetime.fromtimestamp(200 + x[0] * resolution, tz=timezone.utc)
                    ),
                    end=buffer.normalize_timestamp(
                        datetime.fromtimestamp(200 + x[1] * resolution, tz=timezone.utc)
                    ),
                ),
                expected_gaps_concrete,
            )
        )

        def window_to_timestamp(window: Any) -> Any:
            return window.start.timestamp()

        assert len(expected_gaps) == len(buffer.gaps)
        assert expected_gaps == sorted(
            buffer.gaps,
            key=window_to_timestamp,
        )


def test_len_ringbuffer_samples_fit_buffer_size() -> None:
    """Test the length of ordered ring buffer.

    The number of samples fits the ordered ring buffer size.
    """
    min_samples = 1
    max_samples = 100

    for num_samples in range(
        min_samples, max_samples + 1
    ):  # Include max_samples in the range
        test_samples = range(num_samples)

        buffer = OrderedRingBuffer(
            np.empty(shape=len(test_samples), dtype=float),
            sampling_period=timedelta(seconds=1),
            align_to=datetime(1, 1, 1, tzinfo=timezone.utc),
        )

        start_ts: datetime = datetime(2023, 1, 1, tzinfo=timezone.utc)
        for index, sample_value in enumerate(test_samples):
            timestamp = start_ts + timedelta(seconds=index)
            buffer.update(Sample(timestamp, Quantity(float(sample_value))))

        assert buffer.count_valid() == len(test_samples)


def test_len_with_gaps() -> None:
    """Test the length when there are gaps in the buffer."""
    buffer = OrderedRingBuffer(
        np.empty(shape=10, dtype=float),
        sampling_period=timedelta(seconds=1),
        align_to=datetime(1, 1, 1, tzinfo=timezone.utc),
    )

    for i in range(10):
        buffer.update(
            Sample(datetime(2, 2, 2, 0, 0, i, tzinfo=timezone.utc), Quantity(float(i)))
        )
        assert buffer.count_valid() == i + 1


def test_len_ringbuffer_samples_overwrite_buffer() -> None:
    """Test the length of ordered ring buffer.

    The number of samples overwrites the ordered ring buffer.
    """
    min_samples = 2
    max_samples = 100

    for num_samples in range(
        min_samples, max_samples + 1
    ):  # Include max_samples in the range
        test_samples = range(num_samples)
        half_buffer_size = len(test_samples) // 2

        buffer = OrderedRingBuffer(
            np.empty(shape=half_buffer_size, dtype=float),
            sampling_period=timedelta(seconds=1),
            align_to=datetime(1, 1, 1, tzinfo=timezone.utc),
        )

        start_ts: datetime = datetime(2023, 1, 1, tzinfo=timezone.utc)
        for index, sample_value in enumerate(test_samples):
            timestamp = start_ts + timedelta(seconds=index)
            buffer.update(Sample(timestamp, Quantity(float(sample_value))))

        assert buffer.count_valid() == half_buffer_size


def test_ringbuffer_empty_buffer() -> None:
    """Test capacity ordered ring buffer."""
    empty_np_buffer = np.empty(shape=0, dtype=float)
    empty_list_buffer: list[float] = []

    assert len(empty_np_buffer) == len(empty_list_buffer) == 0

    with pytest.raises(AssertionError):
        OrderedRingBuffer(
            empty_np_buffer,
            sampling_period=timedelta(seconds=1),
            align_to=datetime(1, 1, 1),
        )
        OrderedRingBuffer(
            empty_list_buffer,
            sampling_period=timedelta(seconds=1),
            align_to=datetime(1, 1, 1),
        )


def test_off_by_one_gap_logic_bug() -> None:
    """Test off by one bug in the gap calculation."""
    buffer = OrderedRingBuffer(
        np.empty(shape=2, dtype=float),
        sampling_period=timedelta(seconds=1),
        align_to=datetime(1, 1, 1, tzinfo=timezone.utc),
    )

    base_time = datetime(2023, 1, 1, tzinfo=timezone.utc)

    times = [base_time, base_time + timedelta(seconds=1)]

    buffer.update(Sample(times[0], Quantity(1.0)))
    buffer.update(Sample(times[1], Quantity(2.0)))

    assert buffer.is_missing(times[0]) is False
    assert buffer.is_missing(times[1]) is False


def test_cleanup_oldest_gap_timestamp() -> None:
    """Test that gaps are updated such that they are fully contained in the buffer."""
    buffer = OrderedRingBuffer(
        np.empty(shape=15, dtype=float),
        sampling_period=timedelta(seconds=1),
        align_to=datetime(1, 1, 1, tzinfo=timezone.utc),
    )

    for i in range(10):
        buffer.update(
            Sample(datetime.fromtimestamp(200 + i, tz=timezone.utc), Quantity(i))
        )

    gap = Gap(
        datetime.fromtimestamp(195, tz=timezone.utc),
        datetime.fromtimestamp(200, tz=timezone.utc),
    )

    assert gap == buffer.gaps[0]


def test_delete_oudated_gap() -> None:
    """Test updating the buffer such that the gap is no longer valid.

    We introduce two gaps and check that the oldest is removed.
    """
    buffer = OrderedRingBuffer(
        np.empty(shape=3, dtype=float),
        sampling_period=timedelta(seconds=1),
        align_to=datetime(1, 1, 1, tzinfo=timezone.utc),
    )

    for i in range(2):
        buffer.update(
            Sample(datetime.fromtimestamp(200 + i, tz=timezone.utc), Quantity(i))
        )
    assert len(buffer.gaps) == 1

    buffer.update(Sample(datetime.fromtimestamp(202, tz=timezone.utc), Quantity(2)))

    assert len(buffer.gaps) == 0


def get_orb(data: FloatArray) -> OrderedRingBuffer[FloatArray]:
    """Get OrderedRingBuffer with data.

    Args:
        data: Data to fill the buffer with.

    Returns:
        OrderedRingBuffer with data.
    """
    buffer = OrderedRingBuffer(data, ONE_SECOND)
    for i, d in enumerate(data):  # pylint: disable=invalid-name
        buffer.update(Sample(dt(i), Quantity(d) if d is not None else None))
    return buffer


def test_window_datetime() -> None:
    """Test the window function with datetime."""
    buffer = get_orb(np.array([0, None, 2, 3, 4]))
    win = buffer.window(dt(0), dt(3), force_copy=False, fill_value=None)
    assert [0, np.nan, 2] == list(win)
    buffer._buffer[1] = 1  # pylint: disable=protected-access
    # Test whether the window is a view or a copy
    assert [0, 1, 2] == list(win)
    win = buffer.window(dt(0), dt(3), force_copy=False, fill_value=None)
    assert [0, 1, 2] == list(win)
    # Empty array
    assert 0 == buffer.window(dt(1), dt(1)).size

    buffer = get_orb([0.0, 1.0, 2.0, 3.0, 4.0])  # type: ignore
    assert [0, 1, 2] == buffer.window(dt(0), dt(3))
    assert [] == buffer.window(dt(0), dt(0))
    assert [] == buffer.window(dt(1), dt(1))


def test_window_index() -> None:
    """Test the window function with index."""
    buffer = get_orb([0.0, 1.0, 2.0, 3.0, 4.0])
    assert [0, 1, 2] == buffer.window(0, 3)
    assert [0, 1, 2, 3, 4] == buffer.window(0, 5)
    assert [0, 1, 2, 3, 4] == buffer.window(0, 99)
    assert [2, 3] == buffer.window(-3, -1)
    assert [2, 3, 4] == buffer.window(-3, 5)
    assert [0, 1, 2, 3] == buffer.window(-5, -1)
    assert [0, 1, 2, 3, 4] == buffer.window(-99, None)
    assert [0, 1, 2, 3, 4] == buffer.window(None, 99)
    # start >= end
    assert [] == buffer.window(0, 0)
    assert [] == buffer.window(-5, 0)
    assert [] == buffer.window(1, 0)
    assert [] == buffer.window(-1, -2)
    assert [] == buffer.window(-3, 0)


def test_window_index_fill_value() -> None:
    """Test fill_value functionality of window function."""
    # Init with dummy data of size 3 and fill with [0, nan, 2]
    buffer = OrderedRingBuffer([4711.0] * 3, ONE_SECOND)
    buffer.update(Sample(dt(0), Quantity(0)))
    buffer.update(Sample(dt(1), None))
    buffer.update(Sample(dt(2), Quantity(2)))

    # Test fill_value for explicitly set gaps
    assert [0.0, np.nan, 2.0] == buffer.window(0, None)
    assert [0.0, np.nan, 2.0] == buffer.window(0, None, fill_value=None)
    assert [0.0, 1.0, 2.0] == buffer.window(0, None, fill_value=1)

    # initial nan is ignored (optional implementation decision)
    buffer.update(Sample(dt(3), Quantity(3)))  # -> [nan, 2, 3]
    assert [2.0, 3.0] == buffer.window(0, None)
    assert [2.0, 3.0] == buffer.window(0, None, fill_value=1)

    # Test fill_value for gaps implicitly created gaps by time jumps
    buffer.update(Sample(dt(4), Quantity(4)))
    buffer.update(Sample(dt(6), Quantity(6)))  # -> [4, ?, 6]
    # Default is fill missing values with NaNs
    assert [4.0, np.nan, 6.0] == buffer.window(0, None)
    assert [4.0, np.nan, 6.0] == buffer.window(0, None, fill_value=np.nan)
    assert [4.0, 5.0, 6.0] == buffer.window(0, None, fill_value=5)
    # If missing values not filled, outdated values can be returned unexpectedly
    assert [4.0, 2, 6.0] == buffer.window(0, None, fill_value=None)

    # Some edge cases
    buffer.update(Sample(dt(7), Quantity(7)))  # -> [?, 6, 7]
    assert [6.0, 7.0] == buffer.window(0, None)
    buffer.update(Sample(dt(8), Quantity(np.nan)))  # -> [6, 7, nan]
    assert [6.0, 7.0, np.nan] == buffer.window(0, None)
    buffer.update(Sample(dt(9), None))  # -> [7, nan, nan]
    assert [7.0, np.nan, np.nan] == buffer.window(0, None)


def test_window_fail() -> None:
    """Test the window function with invalid arguments."""
    buffer = get_orb([0.0, 1.0, 2.0, 3.0, 4.0])
    # Go crazy with the indices
    with pytest.raises(IndexError):
        buffer.window(dt(1), 3)
    with pytest.raises(IndexError):
        buffer.window(1, dt(3))
    with pytest.raises(IndexError):
        buffer.window(None, dt(2))
    with pytest.raises(IndexError):
        buffer.window(dt(2), None)
    # Invalid argument combination
    with pytest.raises(ValueError):
        buffer.window(0, 1, force_copy=False, fill_value=0)


def test_wrapped_buffer_window() -> None:
    """Test the wrapped buffer window function."""
    wbw = OrderedRingBuffer._wrapped_buffer_window  # pylint: disable=protected-access

    #
    # Tests for list buffer
    #
    buffer = [0.0, 1.0, 2.0, 3.0, 4.0]
    # start = end
    assert [0, 1, 2, 3, 4] == wbw(buffer, 0, 0, force_copy=False)
    assert [4, 0, 1, 2, 3] == wbw(buffer, 4, 4, force_copy=False)
    # start < end
    assert [0] == wbw(buffer, 0, 1, force_copy=False)
    assert [0, 1, 2, 3, 4] == wbw(buffer, 0, 5, force_copy=False)
    # start > end, end = 0
    assert [4] == wbw(buffer, 4, 0, force_copy=False)
    # start > end, end > 0
    assert [4, 0, 1] == wbw(buffer, 4, 2, force_copy=False)

    # Lists are always shallow copies
    res_copy = wbw(buffer, 0, 5, force_copy=False)
    assert [0, 1, 2, 3, 4] == res_copy
    buffer[0] = 9
    assert [0, 1, 2, 3, 4] == res_copy

    #
    # Tests for array buffer
    #
    buffer = np.array([0, 1, 2, 3, 4])  # type: ignore
    # start = end
    assert [0, 1, 2, 3, 4] == list(wbw(buffer, 0, 0, force_copy=False))
    assert [4, 0, 1, 2, 3] == list(wbw(buffer, 4, 4, force_copy=False))
    # start < end
    assert [0] == list(wbw(buffer, 0, 1, force_copy=False))
    assert [0, 1, 2, 3, 4] == list(wbw(buffer, 0, 5, force_copy=False))
    # start > end, end = 0
    assert [4] == list(wbw(buffer, 4, 0, force_copy=False))
    # start > end, end > 0
    assert [4, 0, 1] == list(wbw(buffer, 4, 2, force_copy=False))

    # Get a view and a copy before modifying the buffer
    res1_view = wbw(buffer, 3, 5, force_copy=False)
    res1_copy = wbw(buffer, 3, 5, force_copy=True)
    res2_view = wbw(buffer, 3, 0, force_copy=False)
    res2_copy = wbw(buffer, 3, 0, force_copy=True)
    res3_copy = wbw(buffer, 4, 1, force_copy=False)
    assert [3, 4] == list(res1_view)
    assert [3, 4] == list(res1_copy)
    assert [3, 4] == list(res2_view)
    assert [3, 4] == list(res2_copy)
    assert [4, 0] == list(res3_copy)

    # Modify the buffer and check that only the view is updated
    buffer[4] = 9
    assert [3, 9] == list(res1_view)
    assert [3, 4] == list(res1_copy)
    assert [3, 9] == list(res2_view)
    assert [3, 4] == list(res2_copy)
    assert [4, 0] == list(res3_copy)


def test_get_timestamp() -> None:
    """Test the get_timestamp function."""
    buffer = OrderedRingBuffer(
        np.empty(shape=5, dtype=float),
        sampling_period=timedelta(seconds=1),
    )
    for i in range(5):
        buffer.update(Sample(dt(i), Quantity(i)))
    assert dt(4) == buffer.get_timestamp(-1)
    assert dt(0) == buffer.get_timestamp(-5)
    assert dt(-1) == buffer.get_timestamp(-6)
    assert dt(0) == buffer.get_timestamp(0)
    assert dt(5) == buffer.get_timestamp(5)
    assert dt(6) == buffer.get_timestamp(6)

    for i in range(10, 15):
        buffer.update(Sample(dt(i), Quantity(i)))
    assert dt(14) == buffer.get_timestamp(-1)
    assert dt(10) == buffer.get_timestamp(-5)
    assert dt(9) == buffer.get_timestamp(-6)
    assert dt(10) == buffer.get_timestamp(0)
    assert dt(15) == buffer.get_timestamp(5)
    assert dt(16) == buffer.get_timestamp(6)



================================================
FILE: tests/timeseries/test_ringbuffer_serialization.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests for the `SerializableRingBuffer` class."""


import random
from datetime import datetime, timedelta, timezone
from typing import Any

import numpy as np
import pytest
from frequenz.quantities import Quantity

import frequenz.sdk.timeseries._ringbuffer as rb
from frequenz.sdk.timeseries import Sample

FIVE_MINUTES = timedelta(minutes=5)
_29_DAYS = 60 * 24 * 29
ONE_MINUTE = timedelta(minutes=1)


def load_dump_test(dumped: rb.OrderedRingBuffer[Any], path: str) -> None:
    """Test ordered ring buffer."""
    size = dumped.maxlen

    random.seed(0)

    # Fill with data so we have something to compare
    # Avoiding .update() because it takes very long for 40k entries
    for i in range(size):
        dumped._buffer[i] = i  # pylint: disable=protected-access

    # But use update a bit so the timestamp and gaps are initialized
    for i in range(0, size, 100):
        dumped.update(
            Sample(
                datetime.fromtimestamp(
                    200 + i * FIVE_MINUTES.total_seconds(), tz=timezone.utc
                ),
                Quantity(i),
            )
        )

    rb.dump(dumped, path)

    # Load old data
    loaded = rb.load(path)
    assert loaded is not None

    np.testing.assert_equal(dumped[:], loaded[:])

    # pylint: disable=protected-access
    assert dumped._timestamp_oldest == loaded._timestamp_oldest
    assert dumped._timestamp_newest == loaded._timestamp_newest
    assert len(dumped._gaps) == len(loaded._gaps)
    assert dumped._gaps == loaded._gaps
    assert dumped._sampling_period == loaded._sampling_period
    assert dumped._time_index_alignment == loaded._time_index_alignment


def test_load_dump_short(tmp_path_factory: pytest.TempPathFactory) -> None:
    """Short test to perform loading & dumping."""
    tmpdir = tmp_path_factory.mktemp("load_dump")

    load_dump_test(
        rb.OrderedRingBuffer(
            [0.0] * int(24 * FIVE_MINUTES.total_seconds()),
            FIVE_MINUTES,
            datetime(2, 2, 2, tzinfo=timezone.utc),
        ),
        f"{tmpdir}/test_list.bin",
    )

    load_dump_test(
        rb.OrderedRingBuffer(
            np.empty(shape=(24 * int(FIVE_MINUTES.total_seconds()),), dtype=np.float64),
            FIVE_MINUTES,
            datetime(2, 2, 2, tzinfo=timezone.utc),
        ),
        f"{tmpdir}/test_array.bin",
    )


def test_load_dump(tmp_path_factory: pytest.TempPathFactory) -> None:
    """Test to load/dump 29 days of 1-minute samples."""
    tmpdir = tmp_path_factory.mktemp("load_dump")

    load_dump_test(
        rb.OrderedRingBuffer(
            [0.0] * _29_DAYS,
            ONE_MINUTE,
            datetime(2, 2, 2, tzinfo=timezone.utc),
        ),
        f"{tmpdir}/test_list_29.bin",
    )

    load_dump_test(
        rb.OrderedRingBuffer(
            np.empty(shape=(_29_DAYS,), dtype=np.float64),
            ONE_MINUTE,
            datetime(2, 2, 2, tzinfo=timezone.utc),
        ),
        f"{tmpdir}/test_array_29.bin",
    )



================================================
FILE: tests/timeseries/test_voltage_streamer.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests for fetching and streaming the phase-to-neutral voltage."""


import asyncio

from pytest_mock import MockerFixture

from frequenz.sdk import microgrid

from .mock_microgrid import MockMicrogrid

# pylint: disable=protected-access


async def test_voltage_1(mocker: MockerFixture) -> None:
    """Test the phase-to-neutral voltage with a grid side meter."""
    mockgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
    mockgrid.add_batteries(1, no_meter=True)
    mockgrid.add_batteries(1, no_meter=False)

    async with mockgrid:
        voltage = microgrid.voltage_per_phase()
        voltage_recv = voltage.new_receiver()

        assert voltage._task is not None
        # Wait for voltage requests to be sent, one request per phase.
        await asyncio.sleep(0)
        await asyncio.sleep(0)
        await asyncio.sleep(0)

        for count in range(10):
            volt_delta = 1 if count % 2 == 0 else -1
            volt_phases: list[float | None] = [
                220.0 * volt_delta,
                219.8 * volt_delta,
                220.2 * volt_delta,
            ]

            await mockgrid.mock_resampler.send_meter_voltage([volt_phases, volt_phases])

            val = await voltage_recv.receive()
            assert val is not None
            assert val.value_p1 and val.value_p2 and val.value_p3
            assert val.value_p1.as_volts() == volt_phases[0]
            assert val.value_p2.as_volts() == volt_phases[1]
            assert val.value_p3.as_volts() == volt_phases[2]


async def test_voltage_2(mocker: MockerFixture) -> None:
    """Test the phase-to-neutral voltage without a grid side meter."""
    mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
    mockgrid.add_batteries(1, no_meter=False)
    mockgrid.add_batteries(1, no_meter=True)

    async with mockgrid:
        voltage = microgrid.voltage_per_phase()
        voltage_recv = voltage.new_receiver()

        assert voltage._task is not None
        # Wait for voltage requests to be sent, one request per phase.
        await asyncio.sleep(0)
        await asyncio.sleep(0)
        await asyncio.sleep(0)

        for count in range(10):
            volt_delta = 1 if count % 2 == 0 else -1
            volt_phases: list[float | None] = [
                220.0 * volt_delta,
                219.8 * volt_delta,
                220.2 * volt_delta,
            ]

            await mockgrid.mock_resampler.send_meter_voltage([volt_phases])

            val = await voltage_recv.receive()
            assert val is not None
            assert val.value_p1 and val.value_p2 and val.value_p3
            assert val.value_p1.as_volts() == volt_phases[0]
            assert val.value_p2.as_volts() == volt_phases[1]
            assert val.value_p3.as_volts() == volt_phases[2]


async def test_voltage_3(mocker: MockerFixture) -> None:
    """Test the phase-to-neutral voltage with None values."""
    mockgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
    mockgrid.add_batteries(2, no_meter=False)

    async with mockgrid:
        voltage = microgrid.voltage_per_phase()
        voltage_recv = voltage.new_receiver()

        assert voltage._task is not None
        # Wait for voltage requests to be sent, one request per phase.
        await asyncio.sleep(0)
        await asyncio.sleep(0)
        await asyncio.sleep(0)

        for count in range(10):
            volt_delta = 1 if count % 2 == 0 else -1
            volt_phases: list[float | None] = [
                220.0 * volt_delta,
                219.8 * volt_delta,
                220.2 * volt_delta,
            ]

            await mockgrid.mock_resampler.send_meter_voltage(
                [volt_phases, [None, None, None], [None, 219.8, 220.2]]
            )

            val = await voltage_recv.receive()
            assert val is not None
            assert val.value_p1 and val.value_p2 and val.value_p3
            assert val.value_p1.as_volts() == volt_phases[0]
            assert val.value_p2.as_volts() == volt_phases[1]
            assert val.value_p3.as_volts() == volt_phases[2]



================================================
FILE: tests/timeseries/_battery_pool/__init__.py
================================================
[Empty file]


================================================
FILE: tests/timeseries/_battery_pool/test_battery_pool_control_methods.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Test the battery pool control methods."""

import asyncio
import dataclasses
import typing
from datetime import datetime, timedelta, timezone
from unittest.mock import AsyncMock, MagicMock

import async_solipsism
import pytest
from frequenz.channels import LatestValueCache, Sender
from frequenz.quantities import Power
from pytest_mock import MockerFixture

from frequenz.sdk import microgrid, timeseries
from frequenz.sdk.microgrid import _power_distributing
from frequenz.sdk.microgrid._power_distributing import ComponentPoolStatus
from frequenz.sdk.microgrid._power_distributing._component_pool_status_tracker import (
    ComponentPoolStatusTracker,
)
from frequenz.sdk.timeseries import ResamplerConfig2
from frequenz.sdk.timeseries.battery_pool.messages import BatteryPoolReport

from ...utils.component_data_streamer import MockComponentDataStreamer
from ...utils.component_data_wrapper import BatteryDataWrapper, InverterDataWrapper
from ..mock_microgrid import MockMicrogrid


@pytest.fixture
def event_loop_policy() -> async_solipsism.EventLoopPolicy:
    """Event loop policy."""
    return async_solipsism.EventLoopPolicy()


@dataclasses.dataclass(frozen=True)
class Mocks:
    """Mocks for the tests."""

    microgrid: MockMicrogrid
    """A mock microgrid instance."""

    streamer: MockComponentDataStreamer
    """A mock component data streamer."""

    battery_status_sender: Sender[ComponentPoolStatus]
    """Sender for sending status of the batteries."""


# pylint doesn't understand fixtures. It thinks it is redefined name.
# pylint: disable=redefined-outer-name


@pytest.fixture
async def mocks(mocker: MockerFixture) -> typing.AsyncIterator[Mocks]:
    """Fixture for the mocks."""
    mockgrid = MockMicrogrid()
    mockgrid.add_batteries(4)
    await mockgrid.start(mocker)

    # pylint: disable=protected-access
    if microgrid._data_pipeline._DATA_PIPELINE is not None:
        microgrid._data_pipeline._DATA_PIPELINE = None
    await microgrid._data_pipeline.initialize(
        ResamplerConfig2(resampling_period=timedelta(seconds=0.1))
    )
    streamer = MockComponentDataStreamer(mockgrid.mock_client)

    dp = microgrid._data_pipeline._DATA_PIPELINE
    assert dp is not None

    try:
        yield Mocks(
            mockgrid,
            streamer,
            dp._battery_power_wrapper.status_channel.new_sender(),
        )
    finally:
        _ = await asyncio.gather(
            *[
                dp._stop(),
                streamer.stop(),
                mockgrid.cleanup(),
            ]
        )


class TestBatteryPoolControl:
    """Test the battery pool control methods."""

    async def _patch_battery_pool_status(
        self, mocks: Mocks, mocker: MockerFixture, battery_ids: list[int] | None = None
    ) -> None:
        """Patch the battery pool status.

        If `battery_ids` is not None, the mock will always return `battery_ids`.
        Otherwise, it will return the requested batteries.
        """
        if battery_ids:
            mock = MagicMock(spec=ComponentPoolStatusTracker)
            mock.get_working_components.return_value = battery_ids
            mocker.patch(
                "frequenz.sdk.microgrid._power_distributing._component_managers"
                "._battery_manager.ComponentPoolStatusTracker",
                return_value=mock,
            )
        else:
            mock = MagicMock(spec=ComponentPoolStatusTracker)
            mock.get_working_components.side_effect = set
            mocker.patch(
                "frequenz.sdk.microgrid._power_distributing._component_managers"
                "._battery_manager.ComponentPoolStatusTracker",
                return_value=mock,
            )
        await mocks.battery_status_sender.send(
            ComponentPoolStatus(
                working=set(mocks.microgrid.battery_ids), uncertain=set()
            )
        )

    async def _init_data_for_batteries(
        self, mocks: Mocks, *, exclusion_bounds: tuple[float, float] | None = None
    ) -> None:
        excl_lower = exclusion_bounds[0] if exclusion_bounds else 0.0
        excl_upper = exclusion_bounds[1] if exclusion_bounds else 0.0
        now = datetime.now(tz=timezone.utc)
        for battery_id in mocks.microgrid.battery_ids:
            mocks.streamer.start_streaming(
                BatteryDataWrapper(
                    battery_id,
                    now,
                    soc=50.0,
                    soc_lower_bound=10.0,
                    soc_upper_bound=90.0,
                    power_exclusion_lower_bound=excl_lower,
                    power_exclusion_upper_bound=excl_upper,
                    power_inclusion_lower_bound=-1000.0,
                    power_inclusion_upper_bound=1000.0,
                    capacity=2000.0,
                ),
                0.05,
            )

    async def _init_data_for_inverters(self, mocks: Mocks) -> None:
        now = datetime.now(tz=timezone.utc)
        for inv_id in mocks.microgrid.battery_inverter_ids:
            mocks.streamer.start_streaming(
                InverterDataWrapper(
                    inv_id,
                    now,
                    active_power_exclusion_lower_bound=0.0,
                    active_power_exclusion_upper_bound=0.0,
                    active_power_inclusion_lower_bound=-1000.0,
                    active_power_inclusion_upper_bound=1000.0,
                ),
                0.05,
            )

    def _assert_report(  # pylint: disable=too-many-arguments
        self,
        report: BatteryPoolReport,
        *,
        power: float | None,
        lower: float,
        upper: float,
        dist_result: _power_distributing.Result | None = None,
        expected_result_pred: (
            typing.Callable[[_power_distributing.Result], bool] | None
        ) = None,
    ) -> None:
        assert report.target_power == (
            Power.from_watts(power) if power is not None else None
        )
        assert report.bounds is not None
        assert report.bounds.lower == Power.from_watts(lower)
        assert report.bounds.upper == Power.from_watts(upper)
        if expected_result_pred is not None:
            assert dist_result is not None
            assert expected_result_pred(dist_result)

    async def test_case_1(
        self,
        mocks: Mocks,
        mocker: MockerFixture,
    ) -> None:
        """Test case 1.

        - single battery pool with all batteries.
        - all batteries are working, then one battery stops working.
        """
        set_power = typing.cast(
            AsyncMock, microgrid.connection_manager.get().api_client.set_power
        )

        await self._patch_battery_pool_status(mocks, mocker)
        await self._init_data_for_batteries(mocks)
        await self._init_data_for_inverters(mocks)

        battery_pool = microgrid.new_battery_pool(priority=5)

        bounds_rx = battery_pool.power_status.new_receiver()
        latest_dist_result = LatestValueCache(
            battery_pool.power_distribution_results.new_receiver()
        )

        self._assert_report(
            await bounds_rx.receive(), power=None, lower=-4000.0, upper=4000.0
        )

        await battery_pool.propose_power(Power.from_watts(1000.0))

        self._assert_report(
            await bounds_rx.receive(), power=1000.0, lower=-4000.0, upper=4000.0
        )

        await asyncio.sleep(0.0)  # Wait for the power to be distributed.
        assert set_power.call_count == 4
        assert sorted(set_power.call_args_list) == [
            mocker.call(inv_id, 250.0)
            for inv_id in mocks.microgrid.battery_inverter_ids
        ]
        self._assert_report(
            await bounds_rx.receive(),
            power=1000.0,
            lower=-4000.0,
            upper=4000.0,
            dist_result=latest_dist_result.get(),
            expected_result_pred=lambda result: isinstance(
                result, _power_distributing.Success
            ),
        )

        set_power.reset_mock()

        # First battery stops working (aka set_power never returns for it, times out).
        async def side_effect(inv_id: int, _: float) -> None:
            if inv_id == mocks.microgrid.battery_inverter_ids[0]:
                await asyncio.sleep(1000.0)

        set_power.side_effect = side_effect
        await battery_pool.propose_power(Power.from_watts(100.0))
        self._assert_report(
            await bounds_rx.receive(),
            power=100.0,
            lower=-4000.0,
            upper=4000.0,
            dist_result=latest_dist_result.get(),
            expected_result_pred=lambda result: isinstance(
                result, _power_distributing.Success
            ),
        )
        await asyncio.sleep(0.0)  # Wait for the power to be distributed.
        assert set_power.call_count == 4
        assert sorted(set_power.call_args_list) == [
            mocker.call(inv_id, 25.0) for inv_id in mocks.microgrid.battery_inverter_ids
        ]
        set_power.reset_mock()
        self._assert_report(
            await bounds_rx.receive(),
            power=100.0,
            lower=-4000.0,
            upper=4000.0,
            dist_result=latest_dist_result.get(),
            expected_result_pred=lambda result: isinstance(
                result, _power_distributing.PartialFailure
            )
            and result.failed_components == {mocks.microgrid.battery_ids[0]},
        )

        # There should be an automatic retry.
        set_power.side_effect = None
        await asyncio.sleep(0.0)  # Wait for the power to be distributed.
        assert set_power.call_count == 4
        assert sorted(set_power.call_args_list) == [
            mocker.call(inv_id, 25.0) for inv_id in mocks.microgrid.battery_inverter_ids
        ]
        self._assert_report(
            await bounds_rx.receive(),
            power=100.0,
            lower=-4000.0,
            upper=4000.0,
            dist_result=latest_dist_result.get(),
            expected_result_pred=lambda result: isinstance(
                result, _power_distributing.Success
            ),
        )

    async def test_case_2(self, mocks: Mocks, mocker: MockerFixture) -> None:
        """Test case 2.

        - two battery pools with different batteries.
        - all batteries are working.
        """
        set_power = typing.cast(
            AsyncMock, microgrid.connection_manager.get().api_client.set_power
        )

        await self._patch_battery_pool_status(mocks, mocker)
        await self._init_data_for_batteries(mocks)
        await self._init_data_for_inverters(mocks)

        battery_pool_1 = microgrid.new_battery_pool(
            priority=5, component_ids=set(mocks.microgrid.battery_ids[:2])
        )
        bounds_1_rx = battery_pool_1.power_status.new_receiver()
        battery_pool_2 = microgrid.new_battery_pool(
            priority=5, component_ids=set(mocks.microgrid.battery_ids[2:])
        )
        bounds_2_rx = battery_pool_2.power_status.new_receiver()
        latest_dist_result_2 = LatestValueCache(
            battery_pool_2.power_distribution_results.new_receiver()
        )

        self._assert_report(
            await bounds_1_rx.receive(), power=None, lower=-2000.0, upper=2000.0
        )
        self._assert_report(
            await bounds_2_rx.receive(), power=None, lower=-2000.0, upper=2000.0
        )
        await battery_pool_1.propose_power(Power.from_watts(1000.0))
        self._assert_report(
            await bounds_1_rx.receive(), power=1000.0, lower=-2000.0, upper=2000.0
        )
        await asyncio.sleep(0.0)  # Wait for the power to be distributed.
        assert set_power.call_count == 2
        assert sorted(set_power.call_args_list) == [
            mocker.call(inv_id, 500.0)
            for inv_id in mocks.microgrid.battery_inverter_ids[:2]
        ]
        set_power.reset_mock()

        await battery_pool_2.propose_power(Power.from_watts(1000.0))

        bounds = await bounds_2_rx.receive()
        if not latest_dist_result_2.has_value():
            bounds = await bounds_2_rx.receive()
        self._assert_report(bounds, power=1000.0, lower=-2000.0, upper=2000.0)
        await asyncio.sleep(0.0)  # Wait for the power to be distributed.
        assert set_power.call_count == 2
        assert sorted(set_power.call_args_list) == [
            mocker.call(inv_id, 500.0)
            for inv_id in mocks.microgrid.battery_inverter_ids[2:]
        ]

    async def test_case_3(self, mocks: Mocks, mocker: MockerFixture) -> None:
        """Test case 3.

        - two battery pools with same batteries, but different priorities.
        - all batteries are working.
        """
        set_power = typing.cast(
            AsyncMock, microgrid.connection_manager.get().api_client.set_power
        )

        await self._patch_battery_pool_status(mocks, mocker)
        await self._init_data_for_batteries(mocks)
        await self._init_data_for_inverters(mocks)

        battery_pool_1 = microgrid.new_battery_pool(priority=2)
        bounds_1_rx = battery_pool_1.power_status.new_receiver()
        battery_pool_2 = microgrid.new_battery_pool(priority=1)
        bounds_2_rx = battery_pool_2.power_status.new_receiver()
        latest_dist_result_2 = LatestValueCache(
            battery_pool_2.power_distribution_results.new_receiver()
        )

        self._assert_report(
            await bounds_1_rx.receive(), power=None, lower=-4000.0, upper=4000.0
        )
        self._assert_report(
            await bounds_2_rx.receive(), power=None, lower=-4000.0, upper=4000.0
        )
        await battery_pool_1.propose_power(
            Power.from_watts(-1000.0),
            bounds=timeseries.Bounds(Power.from_watts(-1000.0), Power.from_watts(0.0)),
        )
        self._assert_report(
            await bounds_1_rx.receive(), power=-1000.0, lower=-4000.0, upper=4000.0
        )
        self._assert_report(
            await bounds_2_rx.receive(), power=-1000.0, lower=0.0, upper=1000.0
        )
        await asyncio.sleep(0.0)  # Wait for the power to be distributed.
        assert set_power.call_count == 4
        assert sorted(set_power.call_args_list) == [
            mocker.call(inv_id, -250.0)
            for inv_id in mocks.microgrid.battery_inverter_ids
        ]
        set_power.reset_mock()

        await battery_pool_2.propose_power(
            Power.from_watts(200.0),
            bounds=timeseries.Bounds(Power.from_watts(0.0), Power.from_watts(1000.0)),
        )
        self._assert_report(
            await bounds_1_rx.receive(), power=-800.0, lower=-4000.0, upper=4000.0
        )
        bounds = await bounds_2_rx.receive()
        if not latest_dist_result_2.has_value():
            bounds = await bounds_2_rx.receive()
        self._assert_report(bounds, power=-800.0, lower=0.0, upper=1000.0)
        await asyncio.sleep(0.0)  # Wait for the power to be distributed.
        assert set_power.call_count == 4
        assert sorted(set_power.call_args_list) == [
            mocker.call(inv_id, -200.0)
            for inv_id in mocks.microgrid.battery_inverter_ids
        ]

    async def test_case_4(self, mocks: Mocks, mocker: MockerFixture) -> None:
        """Test case 4.

        - single battery pool with all batteries.
        - all batteries are working, but have exclusion bounds.
        """
        set_power = typing.cast(
            AsyncMock, microgrid.connection_manager.get().api_client.set_power
        )
        await self._patch_battery_pool_status(mocks, mocker)
        await self._init_data_for_batteries(mocks, exclusion_bounds=(-100.0, 100.0))
        await self._init_data_for_inverters(mocks)

        battery_pool = microgrid.new_battery_pool(priority=5)
        bounds_rx = battery_pool.power_status.new_receiver()
        latest_dist_result = LatestValueCache(
            battery_pool.power_distribution_results.new_receiver()
        )

        self._assert_report(
            await bounds_rx.receive(), power=None, lower=-4000.0, upper=4000.0
        )

        await battery_pool.propose_power(Power.from_watts(1000.0))

        self._assert_report(
            await bounds_rx.receive(), power=1000.0, lower=-4000.0, upper=4000.0
        )
        await asyncio.sleep(0.0)  # Wait for the power to be distributed.
        assert set_power.call_count == 4
        assert sorted(set_power.call_args_list) == [
            mocker.call(inv_id, 250.0)
            for inv_id in mocks.microgrid.battery_inverter_ids
        ]
        self._assert_report(
            await bounds_rx.receive(),
            power=1000.0,
            lower=-4000.0,
            upper=4000.0,
            dist_result=latest_dist_result.get(),
            expected_result_pred=lambda result: isinstance(
                result, _power_distributing.Success
            ),
        )

        set_power.reset_mock()

        # Non-zero power but within the exclusion bounds should get adjusted to nearest
        # available power.
        await battery_pool.propose_power(Power.from_watts(50.0))

        self._assert_report(
            await bounds_rx.receive(), power=400.0, lower=-4000.0, upper=4000.0
        )
        await asyncio.sleep(0.0)  # Wait for the power to be distributed.
        assert set_power.call_count == 4
        assert sorted(set_power.call_args_list) == [
            mocker.call(inv_id, 100.0)
            for inv_id in mocks.microgrid.battery_inverter_ids
        ]
        self._assert_report(
            await bounds_rx.receive(),
            power=400.0,
            lower=-4000.0,
            upper=4000.0,
            dist_result=latest_dist_result.get(),
            expected_result_pred=lambda result: isinstance(
                result, _power_distributing.Success
            ),
        )

        set_power.reset_mock()

        # Zero power should be allowed, even if there are exclusion bounds.
        await battery_pool.propose_power(Power.from_watts(0.0))

        self._assert_report(
            await bounds_rx.receive(), power=0.0, lower=-4000.0, upper=4000.0
        )
        await asyncio.sleep(0.0)  # Wait for the power to be distributed.
        assert set_power.call_count == 4
        assert sorted(set_power.call_args_list) == [
            mocker.call(inv_id, 0.0) for inv_id in mocks.microgrid.battery_inverter_ids
        ]
        self._assert_report(
            await bounds_rx.receive(),
            power=0.0,
            lower=-4000.0,
            upper=4000.0,
            dist_result=latest_dist_result.get(),
            expected_result_pred=lambda result: isinstance(
                result, _power_distributing.Success
            ),
        )

        set_power.reset_mock()

        # Non-zero power but within the exclusion bounds should get adjusted to nearest
        # available power.
        await battery_pool.propose_power(Power.from_watts(-150.0))

        self._assert_report(
            await bounds_rx.receive(), power=-400.0, lower=-4000.0, upper=4000.0
        )
        await asyncio.sleep(0.0)  # Wait for the power to be distributed.
        assert set_power.call_count == 4
        assert sorted(set_power.call_args_list) == [
            mocker.call(inv_id, -100.0)
            for inv_id in mocks.microgrid.battery_inverter_ids
        ]
        self._assert_report(
            await bounds_rx.receive(),
            power=-400.0,
            lower=-4000.0,
            upper=4000.0,
            dist_result=latest_dist_result.get(),
            expected_result_pred=lambda result: isinstance(
                result, _power_distributing.Success
            ),
        )

        # Resetting the power should lead to default (zero) power getting set for all
        # the batteries.
        set_power.reset_mock()
        await battery_pool.propose_power(None)
        self._assert_report(
            await bounds_rx.receive(), power=None, lower=-4000.0, upper=4000.0
        )
        await asyncio.sleep(0.0)
        assert set_power.call_count == 4
        assert sorted(set_power.call_args_list) == [
            mocker.call(inv_id, 0.0) for inv_id in mocks.microgrid.battery_inverter_ids
        ]
        self._assert_report(
            await bounds_rx.receive(),
            power=None,
            lower=-4000.0,
            upper=4000.0,
            dist_result=latest_dist_result.get(),
            expected_result_pred=lambda result: isinstance(
                result, _power_distributing.Success
            ),
        )



================================================
FILE: tests/timeseries/_ev_charger_pool/__init__.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Test the EV charger pool control methods."""



================================================
FILE: tests/timeseries/_ev_charger_pool/test_ev_charger_pool.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests for the `EVChargerPool`."""


from frequenz.quantities import Power
from pytest_mock import MockerFixture

from frequenz.sdk import microgrid
from tests.timeseries.mock_microgrid import MockMicrogrid


class TestEVChargerPool:
    """Tests for the `EVChargerPool`."""

    async def test_ev_power(  # pylint: disable=too-many-locals
        self,
        mocker: MockerFixture,
    ) -> None:
        """Test the ev power formula."""
        mockgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
        mockgrid.add_ev_chargers(3)

        async with mockgrid:
            ev_pool = microgrid.new_ev_charger_pool(priority=5)
            power_receiver = ev_pool.power.new_receiver()

            await mockgrid.mock_resampler.send_evc_power([2.0, 4.0, 10.0])
            assert (await power_receiver.receive()).value == Power.from_watts(16.0)

            await mockgrid.mock_resampler.send_evc_power([2.0, 4.0, -10.0])
            assert (await power_receiver.receive()).value == Power.from_watts(-4.0)



================================================
FILE: tests/timeseries/_ev_charger_pool/test_ev_charger_pool_control_methods.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Test the EV charger pool control methods."""

import asyncio
import typing
from datetime import datetime, timedelta, timezone
from unittest.mock import AsyncMock, MagicMock

import async_solipsism
import pytest
import time_machine
from frequenz.channels import Receiver
from frequenz.client.microgrid import EVChargerCableState, EVChargerComponentState
from frequenz.quantities import Power, Voltage
from pytest_mock import MockerFixture

from frequenz.sdk import microgrid
from frequenz.sdk.microgrid import _power_distributing
from frequenz.sdk.microgrid._data_pipeline import _DataPipeline
from frequenz.sdk.microgrid._power_distributing import ComponentPoolStatus
from frequenz.sdk.microgrid._power_distributing._component_pool_status_tracker import (
    ComponentPoolStatusTracker,
)
from frequenz.sdk.timeseries import ResamplerConfig2, Sample3Phase
from frequenz.sdk.timeseries.ev_charger_pool import EVChargerPoolReport

from ...microgrid.fixtures import _Mocks
from ...utils.component_data_streamer import MockComponentDataStreamer
from ...utils.component_data_wrapper import EvChargerDataWrapper, MeterDataWrapper
from ..mock_microgrid import MockMicrogrid

# pylint: disable=protected-access


@pytest.fixture
def event_loop_policy() -> async_solipsism.EventLoopPolicy:
    """Event loop policy."""
    return async_solipsism.EventLoopPolicy()


@pytest.fixture
async def mocks(mocker: MockerFixture) -> typing.AsyncIterator[_Mocks]:
    """Create the mocks."""
    mockgrid = MockMicrogrid(grid_meter=True)
    mockgrid.add_ev_chargers(4)
    await mockgrid.start(mocker)

    # pylint: disable=protected-access
    if microgrid._data_pipeline._DATA_PIPELINE is not None:
        microgrid._data_pipeline._DATA_PIPELINE = None
    await microgrid._data_pipeline.initialize(
        ResamplerConfig2(resampling_period=timedelta(seconds=0.1))
    )
    streamer = MockComponentDataStreamer(mockgrid.mock_client)

    dp = typing.cast(_DataPipeline, microgrid._data_pipeline._DATA_PIPELINE)

    try:
        yield _Mocks(
            mockgrid,
            streamer,
            dp._ev_power_wrapper.status_channel.new_sender(),
        )
    finally:
        _ = await asyncio.gather(
            *[
                dp._stop(),
                streamer.stop(),
                mockgrid.cleanup(),
            ]
        )


class TestEVChargerPoolControl:
    """Test the EV charger pool control methods."""

    async def _patch_ev_pool_status(
        self,
        mocks: _Mocks,
        mocker: MockerFixture,
        component_ids: list[int] | None = None,
    ) -> None:
        """Patch the EV charger pool status.

        If `component_ids` is not None, the mock will always return `component_ids`.
        Otherwise, it will return the requested components.
        """
        if component_ids:
            mock = MagicMock(spec=ComponentPoolStatusTracker)
            mock.get_working_components.return_value = component_ids
            mocker.patch(
                "frequenz.sdk.microgrid._power_distributing._component_managers"
                "._ev_charger_manager._ev_charger_manager.ComponentPoolStatusTracker",
                return_value=mock,
            )
        else:
            mock = MagicMock(spec=ComponentPoolStatusTracker)
            mock.get_working_components.side_effect = set
            mocker.patch(
                "frequenz.sdk.microgrid._power_distributing._component_managers"
                "._ev_charger_manager._ev_charger_manager.ComponentPoolStatusTracker",
                return_value=mock,
            )
        await mocks.component_status_sender.send(
            ComponentPoolStatus(working=set(mocks.microgrid.evc_ids), uncertain=set())
        )

    async def _patch_power_distributing_actor(
        self,
        mocker: MockerFixture,
    ) -> None:
        mocker.patch(
            "frequenz.sdk.microgrid._data_pipeline._DATA_PIPELINE._ev_power_wrapper"
            "._power_distributing_actor._component_manager._voltage_cache.get",
            return_value=Sample3Phase(
                timestamp=datetime.now(tz=timezone.utc),
                value_p1=Voltage.from_volts(220.0),
                value_p2=Voltage.from_volts(220.0),
                value_p3=Voltage.from_volts(220.0),
            ),
        )

    async def _init_ev_chargers(self, mocks: _Mocks) -> None:
        now = datetime.now(tz=timezone.utc)
        for evc_id in mocks.microgrid.evc_ids:
            mocks.streamer.start_streaming(
                EvChargerDataWrapper(
                    evc_id,
                    now,
                    cable_state=EVChargerCableState.EV_PLUGGED,
                    component_state=EVChargerComponentState.READY,
                    active_power=0.0,
                    active_power_inclusion_lower_bound=0.0,
                    active_power_inclusion_upper_bound=16.0 * 230.0 * 3,
                    voltage_per_phase=(230.0, 230.0, 230.0),
                ),
                0.05,
            )

        for meter_id in mocks.microgrid.meter_ids:
            mocks.streamer.start_streaming(
                MeterDataWrapper(
                    meter_id,
                    now,
                    voltage_per_phase=(230.0, 230.0, 230.0),
                ),
                0.05,
            )

    async def _recv_reports_until(
        self,
        bounds_rx: Receiver[EVChargerPoolReport],
        check: typing.Callable[[EVChargerPoolReport], bool],
    ) -> EVChargerPoolReport | None:
        """Receive reports until the given condition is met."""
        max_reports = 10
        ctr = 0
        latest_report: EVChargerPoolReport | None = None
        while ctr < max_reports:
            ctr += 1
            latest_report = await bounds_rx.receive()
            if check(latest_report):
                break

        return latest_report

    def _assert_report(  # pylint: disable=too-many-arguments
        self,
        report: EVChargerPoolReport | None,
        *,
        power: float | None,
        lower: float,
        upper: float,
        dist_result: _power_distributing.Result | None = None,
        expected_result_pred: (
            typing.Callable[[_power_distributing.Result], bool] | None
        ) = None,
    ) -> None:
        assert report is not None and report.target_power == (
            Power.from_watts(power) if power is not None else None
        )
        assert report.bounds is not None
        assert report.bounds.lower == Power.from_watts(lower)
        assert report.bounds.upper == Power.from_watts(upper)
        if expected_result_pred is not None:
            assert dist_result is not None
            assert expected_result_pred(dist_result)

    async def test_setting_power(
        self,
        mocks: _Mocks,
        mocker: MockerFixture,
    ) -> None:
        """Test setting power."""
        traveller = time_machine.travel(datetime(2012, 12, 12))
        mock_time = traveller.start()

        set_power = typing.cast(
            AsyncMock, microgrid.connection_manager.get().api_client.set_power
        )
        await self._init_ev_chargers(mocks)
        ev_charger_pool = microgrid.new_ev_charger_pool(priority=5)
        await self._patch_ev_pool_status(mocks, mocker)
        await self._patch_power_distributing_actor(mocker)

        bounds_rx = ev_charger_pool.power_status.new_receiver()
        latest_report = await self._recv_reports_until(
            bounds_rx,
            lambda x: x.bounds is not None and x.bounds.upper.as_watts() == 44160.0,
        )

        self._assert_report(latest_report, power=None, lower=0.0, upper=44160.0)

        # Check that chargers are initialized to Power.zero()
        assert set_power.call_count == 4
        assert all(x.args[1] == 0.0 for x in set_power.call_args_list)

        set_power.reset_mock()
        await ev_charger_pool.propose_power(Power.from_watts(40000.0))
        # ignore one report because it is not always immediately updated.
        self._assert_report(
            await bounds_rx.receive(), power=40000.0, lower=0.0, upper=44160.0
        )
        mock_time.shift(timedelta(seconds=60))
        await asyncio.sleep(0.15)

        # Components are set initial power
        assert set_power.call_count == 4
        assert all(x.args[1] == 6600.0 for x in set_power.call_args_list)

        # All available power is allocated. 3 chargers are set to 11040.0
        # and the last one is set to 6880.0
        set_power.reset_mock()
        mock_time.shift(timedelta(seconds=60))
        await asyncio.sleep(0.15)
        assert set_power.call_count == 4

        evs_11040 = [x.args for x in set_power.call_args_list if x.args[1] == 11040.0]
        assert 3 == len(evs_11040)
        evs_6680 = [x.args for x in set_power.call_args_list if x.args[1] == 6880.0]
        assert 1 == len(evs_6680)

        # Throttle the power
        set_power.reset_mock()
        await ev_charger_pool.propose_power(Power.from_watts(32000.0))
        await bounds_rx.receive()
        await asyncio.sleep(0.02)
        assert set_power.call_count == 1

        stopped_evs = [x.args for x in set_power.call_args_list if x.args[1] == 0.0]
        assert 1 == len(stopped_evs)
        assert stopped_evs[0][0] in [evc[0] for evc in evs_11040]

        traveller.stop()



================================================
FILE: tests/timeseries/_formula_engine/__init__.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Formula engine tests."""



================================================
FILE: tests/timeseries/_formula_engine/test_formula_composition.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tests for formula composition."""


import math
from contextlib import AsyncExitStack

import pytest
from frequenz.client.microgrid import ComponentMetricId
from frequenz.quantities import Power
from pytest_mock import MockerFixture

from frequenz.sdk import microgrid
from frequenz.sdk.timeseries import Sample

from ..mock_microgrid import MockMicrogrid
from .utils import get_resampled_stream


class TestFormulaComposition:
    """Tests for formula composition."""

    async def test_formula_composition(  # pylint: disable=too-many-locals
        self,
        mocker: MockerFixture,
    ) -> None:
        """Test the composition of formulas."""
        mockgrid = MockMicrogrid(grid_meter=False, num_namespaces=2, mocker=mocker)
        mockgrid.add_consumer_meters()
        mockgrid.add_batteries(3)
        mockgrid.add_solar_inverters(2)

        async with mockgrid, AsyncExitStack() as stack:
            logical_meter = microgrid.logical_meter()
            stack.push_async_callback(logical_meter.stop)

            battery_pool = microgrid.new_battery_pool(priority=5)
            stack.push_async_callback(battery_pool.stop)

            pv_pool = microgrid.new_pv_pool(priority=5)
            stack.push_async_callback(pv_pool.stop)

            grid = microgrid.grid()
            stack.push_async_callback(grid.stop)

            grid_meter_recv = get_resampled_stream(
                grid._formula_pool._namespace,  # pylint: disable=protected-access
                mockgrid.meter_ids[0],
                ComponentMetricId.ACTIVE_POWER,
                Power.from_watts,
            )
            grid_power_recv = grid.power.new_receiver()
            battery_power_recv = battery_pool.power.new_receiver()
            pv_power_recv = pv_pool.power.new_receiver()

            engine = (pv_pool.power + battery_pool.power).build("inv_power")
            stack.push_async_callback(engine.stop)
            inv_calc_recv = engine.new_receiver()

            await mockgrid.mock_resampler.send_bat_inverter_power([10.0, 12.0, 14.0])
            await mockgrid.mock_resampler.send_meter_power(
                [100.0, 10.0, 12.0, 14.0, -100.0, -200.0]
            )
            await mockgrid.mock_resampler.send_pv_inverter_power([-100.0, -200.0])

            grid_pow = await grid_power_recv.receive()
            pv_pow = await pv_power_recv.receive()
            bat_pow = await battery_power_recv.receive()
            main_pow = await grid_meter_recv.receive()
            inv_calc_pow = await inv_calc_recv.receive()

            assert (
                grid_pow is not None
                and grid_pow.value is not None
                and math.isclose(grid_pow.value.base_value, -164.0)
            )  # 100 + 10 + 12 + 14 + -100 + -200
            assert (
                bat_pow is not None
                and bat_pow.value is not None
                and math.isclose(bat_pow.value.base_value, 36.0)
            )  # 10 + 12 + 14
            assert (
                pv_pow is not None
                and pv_pow.value is not None
                and math.isclose(pv_pow.value.base_value, -300.0)
            )  # -100 + -200
            assert (
                inv_calc_pow is not None
                and inv_calc_pow.value is not None
                and math.isclose(inv_calc_pow.value.base_value, -264.0)  # -300 + 36
            )
            assert (
                main_pow is not None
                and main_pow.value is not None
                and math.isclose(main_pow.value.base_value, 100.0)
            )

            assert math.isclose(
                inv_calc_pow.value.base_value,
                pv_pow.value.base_value + bat_pow.value.base_value,
            )
            assert math.isclose(
                grid_pow.value.base_value,
                inv_calc_pow.value.base_value + main_pow.value.base_value,
            )

    async def test_formula_composition_missing_pv(self, mocker: MockerFixture) -> None:
        """Test the composition of formulas with missing PV power data."""
        mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
        mockgrid.add_batteries(3)

        count = 0
        async with mockgrid, AsyncExitStack() as stack:
            battery_pool = microgrid.new_battery_pool(priority=5)
            stack.push_async_callback(battery_pool.stop)

            pv_pool = microgrid.new_pv_pool(priority=5)
            stack.push_async_callback(pv_pool.stop)

            logical_meter = microgrid.logical_meter()
            stack.push_async_callback(logical_meter.stop)

            battery_power_recv = battery_pool.power.new_receiver()
            pv_power_recv = pv_pool.power.new_receiver()
            engine = (pv_pool.power + battery_pool.power).build("inv_power")
            stack.push_async_callback(engine.stop)

            inv_calc_recv = engine.new_receiver()

            for _ in range(10):
                await mockgrid.mock_resampler.send_meter_power(
                    [10.0 + count, 12.0 + count, 14.0 + count]
                )
                await mockgrid.mock_resampler.send_non_existing_component_value()

                bat_pow = await battery_power_recv.receive()
                pv_pow = await pv_power_recv.receive()
                inv_pow = await inv_calc_recv.receive()

                assert inv_pow == bat_pow
                assert (
                    pv_pow.timestamp == inv_pow.timestamp
                    and pv_pow.value == Power.from_watts(0.0)
                )
                count += 1

        assert count == 10

    async def test_formula_composition_missing_bat(self, mocker: MockerFixture) -> None:
        """Test the composition of formulas with missing battery power data."""
        mockgrid = MockMicrogrid(grid_meter=False, mocker=mocker)
        mockgrid.add_solar_inverters(2, no_meter=True)

        count = 0
        async with mockgrid, AsyncExitStack() as stack:
            battery_pool = microgrid.new_battery_pool(priority=5)
            stack.push_async_callback(battery_pool.stop)

            pv_pool = microgrid.new_pv_pool(priority=5)
            stack.push_async_callback(pv_pool.stop)

            logical_meter = microgrid.logical_meter()
            stack.push_async_callback(logical_meter.stop)

            battery_power_recv = battery_pool.power.new_receiver()
            pv_power_recv = pv_pool.power.new_receiver()
            engine = (pv_pool.power + battery_pool.power).build("inv_power")
            stack.push_async_callback(engine.stop)

            inv_calc_recv = engine.new_receiver()

            for _ in range(10):
                await mockgrid.mock_resampler.send_pv_inverter_power(
                    [12.0 + count, 14.0 + count]
                )
                await mockgrid.mock_resampler.send_non_existing_component_value()
                bat_pow = await battery_power_recv.receive()
                pv_pow = await pv_power_recv.receive()
                inv_pow = await inv_calc_recv.receive()

                assert inv_pow == pv_pow
                assert (
                    bat_pow.timestamp == inv_pow.timestamp
                    and bat_pow.value == Power.from_watts(0.0)
                )
                count += 1

        assert count == 10

    async def test_formula_composition_min_max(self, mocker: MockerFixture) -> None:
        """Test the composition of formulas with the min and max."""
        mockgrid = MockMicrogrid(grid_meter=True, mocker=mocker)
        mockgrid.add_chps(1)

        async with mockgrid, AsyncExitStack() as stack:
            logical_meter = microgrid.logical_meter()
            stack.push_async_callback(logical_meter.stop)

            grid = microgrid.grid()
            stack.push_async_callback(grid.stop)

            engine_min = grid.power.min(logical_meter.chp_power).build("grid_power_min")
            stack.push_async_callback(engine_min.stop)
            engine_min_rx = engine_min.new_receiver()

            engine_max = grid.power.max(logical_meter.chp_power).build("grid_power_max")
            stack.push_async_callback(engine_max.stop)
            engine_max_rx = engine_max.new_receiver()

            await mockgrid.mock_resampler.send_meter_power([100.0, 200.0])

            # Test min
            min_pow = await engine_min_rx.receive()
            assert (
                min_pow
                and min_pow.value
                and min_pow.value.isclose(Power.from_watts(100.0))
            )

            # Test max
            max_pow = await engine_max_rx.receive()
            assert (
                max_pow
                and max_pow.value
                and max_pow.value.isclose(Power.from_watts(200.0))
            )

            await mockgrid.mock_resampler.send_meter_power([-100.0, -200.0])

            # Test min
            min_pow = await engine_min_rx.receive()
            assert (
                min_pow
                and min_pow.value
                and min_pow.value.isclose(Power.from_watts(-200.0))
            )

            # Test max
            max_pow = await engine_max_rx.receive()
            assert (
                max_pow
                and max_pow.value
                and max_pow.value.isclose(Power.from_watts(-100.0))
            )

    async def test_formula_composition_min_max_const(
        self, mocker: MockerFixture
    ) -> None:
        """Test the compositing formulas and constants with the min and max functions."""
        async with (
            MockMicrogrid(grid_meter=True, mocker=mocker) as mockgrid,
            AsyncExitStack() as stack,
        ):
            logical_meter = microgrid.logical_meter()
            stack.push_async_callback(logical_meter.stop)

            grid = microgrid.grid()
            stack.push_async_callback(grid.stop)

            engine_min = grid.power.min(Power.zero()).build("grid_power_min")
            stack.push_async_callback(engine_min.stop)
            engine_min_rx = engine_min.new_receiver()

            engine_max = grid.power.max(Power.zero()).build("grid_power_max")
            stack.push_async_callback(engine_max.stop)
            engine_max_rx = engine_max.new_receiver()

            await mockgrid.mock_resampler.send_meter_power([100.0])

            # Test min
            min_pow = await engine_min_rx.receive()
            assert min_pow and min_pow.value and min_pow.value.isclose(Power.zero())

            # Test max
            max_pow = await engine_max_rx.receive()
            assert (
                max_pow
                and max_pow.value
                and max_pow.value.isclose(Power.from_watts(100.0))
            )

            await mockgrid.mock_resampler.send_meter_power([-100.0])

            # Test min
            min_pow = await engine_min_rx.receive()
            assert (
                min_pow
                and min_pow.value
                and min_pow.value.isclose(Power.from_watts(-100.0))
            )

            # Test max
            max_pow = await engine_max_rx.receive()
            assert max_pow and max_pow.value and max_pow.value.isclose(Power.zero())

    async def test_formula_composition_constant(  # pylint: disable=too-many-locals
        self,
        mocker: MockerFixture,
    ) -> None:
        """Test the composition of formulas with constant values."""
        async with (
            MockMicrogrid(grid_meter=True, mocker=mocker) as mockgrid,
            AsyncExitStack() as stack,
        ):
            logical_meter = microgrid.logical_meter()
            stack.push_async_callback(logical_meter.stop)
            grid = microgrid.grid()
            stack.push_async_callback(grid.stop)
            engine_add = (grid.power + Power.from_watts(50)).build(
                "grid_power_addition"
            )
            stack.push_async_callback(engine_add.stop)
            engine_sub = (grid.power - Power.from_watts(100)).build(
                "grid_power_subtraction"
            )
            stack.push_async_callback(engine_sub.stop)
            engine_mul = (grid.power * 2.0).build("grid_power_multiplication")
            stack.push_async_callback(engine_mul.stop)
            engine_div = (grid.power / 2.0).build("grid_power_division")
            stack.push_async_callback(engine_div.stop)

            engine_composite = (
                (
                    (grid.power + Power.from_watts(50.0)) / 2.0
                    + grid.power
                    - Power.from_watts(20.0)
                )
                * 2.0
            ).build("grid_power_composite")

            await mockgrid.mock_resampler.send_meter_power([100.0])

            # Test addition
            grid_power_addition: Sample[Power] = (
                await engine_add.new_receiver().receive()
            )
            assert grid_power_addition.value is not None
            assert math.isclose(
                grid_power_addition.value.as_watts(),
                150.0,
            )

            # Test subtraction
            grid_power_subtraction: Sample[Power] = (
                await engine_sub.new_receiver().receive()
            )
            assert grid_power_subtraction.value is not None
            assert math.isclose(
                grid_power_subtraction.value.as_watts(),
                0.0,
            )

            # Test multiplication
            grid_power_multiplication: Sample[Power] = (
                await engine_mul.new_receiver().receive()
            )
            assert grid_power_multiplication.value is not None
            assert math.isclose(
                grid_power_multiplication.value.as_watts(),
                200.0,
            )

            # Test division
            grid_power_division: Sample[Power] = (
                await engine_div.new_receiver().receive()
            )
            assert grid_power_division.value is not None
            assert math.isclose(
                grid_power_division.value.as_watts(),
                50.0,
            )

            # Test composite formula
            grid_power_composite: Sample[Power] = (
                await engine_composite.new_receiver().receive()
            )
            assert grid_power_composite.value is not None
            assert math.isclose(grid_power_composite.value.as_watts(), 310.0)

            # Test multiplication with a Quantity
            with pytest.raises(RuntimeError):
                engine_assert = (grid.power * Power.from_watts(2.0)).build(  # type: ignore
                    "grid_power_multiplication"
                )
                await engine_assert.new_receiver().receive()

            # Test addition with a float
            with pytest.raises(RuntimeError):
                engine_assert = (grid.power + 2.0).build(  # type: ignore
                    "grid_power_multiplication"
                )
                await engine_assert.new_receiver().receive()

    async def test_3_phase_formulas(self, mocker: MockerFixture) -> None:
        """Test 3 phase formulas current formulas and their composition."""
        mockgrid = MockMicrogrid(
            grid_meter=False, sample_rate_s=0.05, num_namespaces=2, mocker=mocker
        )
        mockgrid.add_batteries(3)
        mockgrid.add_ev_chargers(1)

        count = 0
        async with mockgrid, AsyncExitStack() as stack:
            logical_meter = microgrid.logical_meter()
            stack.push_async_callback(logical_meter.stop)

            ev_pool = microgrid.new_ev_charger_pool(priority=5)
            stack.push_async_callback(ev_pool.stop)

            grid = microgrid.grid()
            stack.push_async_callback(grid.stop)

            grid_current_recv = grid.current_per_phase.new_receiver()
            ev_current_recv = ev_pool.current_per_phase.new_receiver()

            engine = (grid.current_per_phase - ev_pool.current_per_phase).build(
                "net_current"
            )
            stack.push_async_callback(engine.stop)
            net_current_recv = engine.new_receiver()

            for _ in range(10):
                await mockgrid.mock_resampler.send_meter_current(
                    [
                        [10.0, 12.0, 14.0],
                        [10.0, 12.0, 14.0],
                        [10.0, 12.0, 14.0],
                    ]
                )
                await mockgrid.mock_resampler.send_evc_current(
                    [[10.0 + count, 12.0 + count, 14.0 + count]]
                )

                grid_amps = await grid_current_recv.receive()
                ev_amps = await ev_current_recv.receive()
                net_amps = await net_current_recv.receive()

                assert (
                    grid_amps.value_p1 is not None
                    and grid_amps.value_p1.base_value > 0.0
                )
                assert (
                    grid_amps.value_p2 is not None
                    and grid_amps.value_p2.base_value > 0.0
                )
                assert (
                    grid_amps.value_p3 is not None
                    and grid_amps.value_p3.base_value > 0.0
                )
                assert (
                    ev_amps.value_p1 is not None and ev_amps.value_p1.base_value > 0.0
                )
                assert (
                    ev_amps.value_p2 is not None and ev_amps.value_p2.base_value > 0.0
                )
                assert (
                    ev_amps.value_p3 is not None and ev_amps.value_p3.base_value > 0.0
                )
                assert (
                    net_amps.value_p1 is not None and net_amps.value_p1.base_value > 0.0
                )
                assert (
                    net_amps.value_p2 is not None and net_amps.value_p2.base_value > 0.0
                )
                assert (
                    net_amps.value_p3 is not None and net_amps.value_p3.base_value > 0.0
                )

                assert (
                    net_amps.value_p1.base_value
                    == grid_amps.value_p1.base_value - ev_amps.value_p1.base_value
                )
                assert (
                    net_amps.value_p2.base_value
                    == grid_amps.value_p2.base_value - ev_amps.value_p2.base_value
                )
                assert (
                    net_amps.value_p3.base_value
                    == grid_amps.value_p3.base_value - ev_amps.value_p3.base_value
                )
                count += 1

        assert count == 10



================================================
FILE: tests/timeseries/_formula_engine/utils.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Utils for testing formula engines."""


from collections.abc import Callable
from math import isclose

from frequenz.channels import Receiver
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentMetricId

from frequenz.sdk.microgrid import _data_pipeline
from frequenz.sdk.timeseries._base_types import QuantityT, Sample
from frequenz.sdk.timeseries.formula_engine._resampled_formula_builder import (
    ResampledFormulaBuilder,
)


def get_resampled_stream(
    namespace: str,
    comp_id: ComponentId,
    metric_id: ComponentMetricId,
    create_method: Callable[[float], QuantityT],
) -> Receiver[Sample[QuantityT]]:
    """Return the resampled data stream for the given component."""
    # Create a `FormulaBuilder` instance, just in order to reuse its
    # `_get_resampled_receiver` function implementation.

    # pylint: disable=protected-access
    builder = ResampledFormulaBuilder(
        namespace=namespace,
        formula_name="",
        channel_registry=_data_pipeline._get()._channel_registry,
        resampler_subscription_sender=_data_pipeline._get()._resampling_request_sender(),
        metric_id=metric_id,
        create_method=create_method,
    )
    # Resampled data is always `Quantity` type, so we need to convert it to the desired
    # output type.
    return builder._get_resampled_receiver(
        comp_id,
        metric_id,
    ).map(
        lambda sample: Sample(
            sample.timestamp,
            None if sample.value is None else create_method(sample.value.base_value),
        )
    )
    # pylint: enable=protected-access


def equal_float_lists(list1: list[QuantityT], list2: list[QuantityT]) -> bool:
    """Compare two float lists with `math.isclose()`."""
    return (
        len(list1) > 0
        and len(list1) == len(list2)
        and all(isclose(v1.base_value, v2.base_value) for v1, v2 in zip(list1, list2))
    )



================================================
FILE: tests/timeseries/_pv_pool/__init__.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Test the PV pool control methods."""



================================================
FILE: tests/timeseries/_pv_pool/test_pv_pool_control_methods.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Test the PV pool control methods."""

import asyncio
import typing
from datetime import datetime, timedelta, timezone
from unittest.mock import AsyncMock

import async_solipsism
import pytest
from frequenz.channels import Receiver
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import InverterComponentState
from frequenz.quantities import Power
from pytest_mock import MockerFixture

from frequenz.sdk import microgrid
from frequenz.sdk.microgrid import _power_distributing
from frequenz.sdk.microgrid._data_pipeline import _DataPipeline
from frequenz.sdk.timeseries import ResamplerConfig2
from frequenz.sdk.timeseries.pv_pool import PVPoolReport

from ...microgrid.fixtures import _Mocks
from ...utils.component_data_streamer import MockComponentDataStreamer
from ...utils.component_data_wrapper import InverterDataWrapper
from ..mock_microgrid import MockMicrogrid


@pytest.fixture
def event_loop_policy() -> async_solipsism.EventLoopPolicy:
    """Event loop policy."""
    return async_solipsism.EventLoopPolicy()


@pytest.fixture
async def mocks(mocker: MockerFixture) -> typing.AsyncIterator[_Mocks]:
    """Create the mocks."""
    mockgrid = MockMicrogrid(grid_meter=True)
    mockgrid.add_solar_inverters(4)
    await mockgrid.start(mocker)

    # pylint: disable=protected-access
    if microgrid._data_pipeline._DATA_PIPELINE is not None:
        microgrid._data_pipeline._DATA_PIPELINE = None
    await microgrid._data_pipeline.initialize(
        ResamplerConfig2(resampling_period=timedelta(seconds=0.1))
    )
    streamer = MockComponentDataStreamer(mockgrid.mock_client)

    dp = typing.cast(_DataPipeline, microgrid._data_pipeline._DATA_PIPELINE)

    try:
        yield _Mocks(
            mockgrid,
            streamer,
            dp._pv_power_wrapper.status_channel.new_sender(),
        )
    finally:
        _ = await asyncio.gather(
            *[
                dp._stop(),
                streamer.stop(),
                mockgrid.cleanup(),
            ]
        )


class TestPVPoolControl:
    """Test control methods for the PVPool."""

    async def _init_pv_inverters(self, mocks: _Mocks) -> None:
        now = datetime.now(tz=timezone.utc)
        for idx, comp_id in enumerate(mocks.microgrid.pv_inverter_ids):
            mocks.streamer.start_streaming(
                InverterDataWrapper(
                    comp_id,
                    now,
                    component_state=InverterComponentState.IDLE,
                    active_power=0.0,
                    active_power_inclusion_lower_bound=-10000.0 * (idx + 1),
                    active_power_inclusion_upper_bound=0.0,
                ),
                0.05,
            )

    async def _fail_pv_inverters(
        self, fail_ids: list[ComponentId], mocks: _Mocks
    ) -> None:
        now = datetime.now(tz=timezone.utc)
        for idx, comp_id in enumerate(mocks.microgrid.pv_inverter_ids):
            mocks.streamer.update_stream(
                InverterDataWrapper(
                    comp_id,
                    now,
                    component_state=(
                        InverterComponentState.ERROR
                        if comp_id in fail_ids
                        else InverterComponentState.IDLE
                    ),
                    active_power=0.0,
                    active_power_inclusion_lower_bound=-10000.0 * (idx + 1),
                    active_power_inclusion_upper_bound=0.0,
                ),
            )

    def _assert_report(  # pylint: disable=too-many-arguments
        self,
        report: PVPoolReport | None,
        *,
        power: float | None,
        lower: float,
        upper: float,
        dist_result: _power_distributing.Result | None = None,
        expected_result_pred: (
            typing.Callable[[_power_distributing.Result], bool] | None
        ) = None,
    ) -> None:
        assert report is not None and report.target_power == (
            Power.from_watts(power) if power is not None else None
        )
        assert report.bounds is not None
        assert report.bounds.lower == Power.from_watts(lower)
        assert report.bounds.upper == Power.from_watts(upper)
        if expected_result_pred is not None:
            assert dist_result is not None
            assert expected_result_pred(dist_result)

    async def _recv_reports_until(
        self,
        bounds_rx: Receiver[PVPoolReport],
        check: typing.Callable[[PVPoolReport], bool],
    ) -> PVPoolReport | None:
        """Receive reports until the given condition is met."""
        max_reports = 10
        ctr = 0
        latest_report: PVPoolReport | None = None
        while ctr < max_reports:
            ctr += 1
            latest_report = await bounds_rx.receive()
            if check(latest_report):
                break

        return latest_report

    async def test_setting_power(  # pylint: disable=too-many-statements
        self,
        mocks: _Mocks,
        mocker: MockerFixture,
    ) -> None:
        """Test setting power."""
        set_power = typing.cast(
            AsyncMock, microgrid.connection_manager.get().api_client.set_power
        )

        await self._init_pv_inverters(mocks)
        pv_pool = microgrid.new_pv_pool(priority=5)
        bounds_rx = pv_pool.power_status.new_receiver()
        latest_report = await self._recv_reports_until(
            bounds_rx,
            lambda x: x.bounds is not None and x.bounds.lower.as_watts() == -100000.0,
        )
        dist_results_rx = pv_pool.power_distribution_results.new_receiver()

        self._assert_report(latest_report, power=None, lower=-100000.0, upper=0.0)
        await pv_pool.propose_power(Power.from_watts(-80000.0))
        await self._recv_reports_until(
            bounds_rx,
            lambda x: x.target_power is not None
            and x.target_power.as_watts() == -80000.0,
        )
        self._assert_report(
            await bounds_rx.receive(), power=-80000.0, lower=-100000.0, upper=0.0
        )
        await asyncio.sleep(0.0)

        # Components are set initial power
        assert set_power.call_count == 4
        inv_ids = mocks.microgrid.pv_inverter_ids
        assert sorted(set_power.call_args_list, key=lambda x: x.args[0]) == [
            mocker.call(inv_ids[0], -10000.0),
            mocker.call(inv_ids[1], -20000.0),
            mocker.call(inv_ids[2], -25000.0),
            mocker.call(inv_ids[3], -25000.0),
        ]
        dist_results = await dist_results_rx.receive()
        assert isinstance(
            dist_results, _power_distributing.Success
        ), f"Expected a success, got {dist_results}"
        assert dist_results.succeeded_power == Power.from_watts(-80000.0)
        assert dist_results.excess_power == Power.zero()
        assert dist_results.succeeded_components == {
            ComponentId(8),
            ComponentId(18),
            ComponentId(28),
            ComponentId(38),
        }

        set_power.reset_mock()
        await pv_pool.propose_power(Power.from_watts(-4000.0))
        await self._recv_reports_until(
            bounds_rx,
            lambda x: x.target_power is not None
            and x.target_power.as_watts() == -4000.0,
        )
        self._assert_report(
            await bounds_rx.receive(), power=-4000.0, lower=-100000.0, upper=0.0
        )
        await asyncio.sleep(0.0)

        # Components are set initial power
        assert set_power.call_count == 4
        inv_ids = mocks.microgrid.pv_inverter_ids
        assert sorted(set_power.call_args_list, key=lambda x: x.args[0]) == [
            mocker.call(inv_ids[0], -1000.0),
            mocker.call(inv_ids[1], -1000.0),
            mocker.call(inv_ids[2], -1000.0),
            mocker.call(inv_ids[3], -1000.0),
        ]
        dist_results = await dist_results_rx.receive()
        assert isinstance(
            dist_results, _power_distributing.Success
        ), f"Expected a success, got {dist_results}"
        assert dist_results.succeeded_power == Power.from_watts(-4000.0)
        assert dist_results.excess_power == Power.zero()
        assert dist_results.succeeded_components == {
            ComponentId(8),
            ComponentId(18),
            ComponentId(28),
            ComponentId(38),
        }

        # After failing 1 inverter, bounds should go down and power shouldn't be
        # distributed to that inverter.
        await self._fail_pv_inverters([inv_ids[1]], mocks)
        await self._recv_reports_until(
            bounds_rx,
            lambda x: x.bounds is not None and x.bounds.lower.as_watts() == -80000.0,
        )
        self._assert_report(
            await bounds_rx.receive(), power=-4000.0, lower=-80000.0, upper=0.0
        )
        dist_results = await dist_results_rx.receive()
        assert isinstance(
            dist_results, _power_distributing.Success
        ), f"Expected a success, got {dist_results}"
        assert dist_results.succeeded_power == Power.from_watts(-4000.0)
        assert dist_results.excess_power == Power.zero()
        assert dist_results.succeeded_components == {
            ComponentId(8),
            ComponentId(28),
            ComponentId(38),
        }

        set_power.reset_mock()
        await pv_pool.propose_power(Power.from_watts(-70000.0))
        await self._recv_reports_until(
            bounds_rx,
            lambda x: x.target_power is not None
            and x.target_power.as_watts() == -70000.0,
        )

        self._assert_report(
            await bounds_rx.receive(), power=-70000.0, lower=-80000.0, upper=0.0
        )
        await asyncio.sleep(0.0)

        # Components are set initial power
        assert set_power.call_count == 3
        inv_ids = mocks.microgrid.pv_inverter_ids
        assert sorted(set_power.call_args_list, key=lambda x: x.args[0]) == [
            mocker.call(inv_ids[0], -10000.0),
            mocker.call(inv_ids[2], -30000.0),
            mocker.call(inv_ids[3], -30000.0),
        ]
        dist_results = await dist_results_rx.receive()
        assert isinstance(
            dist_results, _power_distributing.Success
        ), f"Expected a success, got {dist_results}"
        assert dist_results.succeeded_power == Power.from_watts(-70000.0)
        assert dist_results.excess_power == Power.zero()
        assert dist_results.succeeded_components == {
            ComponentId(8),
            ComponentId(28),
            ComponentId(38),
        }

        # After the failed inverter recovers, bounds should go back up and power
        # should be distributed to all inverters
        await self._fail_pv_inverters([], mocks)
        await self._recv_reports_until(
            bounds_rx,
            lambda x: x.bounds is not None and x.bounds.lower.as_watts() == -100000.0,
        )
        self._assert_report(
            await bounds_rx.receive(), power=-70000.0, lower=-100000.0, upper=0.0
        )
        dist_results = await dist_results_rx.receive()
        assert isinstance(
            dist_results, _power_distributing.Success
        ), f"Expected a success, got {dist_results}"
        assert dist_results.succeeded_power == Power.from_watts(-70000.0)
        assert dist_results.excess_power == Power.zero()
        assert dist_results.succeeded_components == {
            ComponentId(8),
            ComponentId(18),
            ComponentId(28),
            ComponentId(38),
        }

        set_power.reset_mock()
        await pv_pool.propose_power(Power.from_watts(-200000.0))
        await self._recv_reports_until(
            bounds_rx,
            lambda x: x.target_power is not None
            and x.target_power.as_watts() == -100000.0,
        )

        self._assert_report(
            await bounds_rx.receive(), power=-100000.0, lower=-100000.0, upper=0.0
        )
        await asyncio.sleep(0.0)

        assert set_power.call_count == 4
        inv_ids = mocks.microgrid.pv_inverter_ids
        assert sorted(set_power.call_args_list, key=lambda x: x.args[0]) == [
            mocker.call(inv_ids[0], -10000.0),
            mocker.call(inv_ids[1], -20000.0),
            mocker.call(inv_ids[2], -30000.0),
            mocker.call(inv_ids[3], -40000.0),
        ]
        dist_results = await dist_results_rx.receive()
        assert isinstance(
            dist_results, _power_distributing.Success
        ), f"Expected a success, got {dist_results}"
        assert dist_results.succeeded_power == Power.from_watts(-100000.0)
        assert dist_results.excess_power == Power.zero()
        assert dist_results.succeeded_components == {
            ComponentId(8),
            ComponentId(18),
            ComponentId(28),
            ComponentId(38),
        }

        # Setting 0 power should set all inverters to 0
        set_power.reset_mock()
        await pv_pool.propose_power(Power.zero())
        await self._recv_reports_until(
            bounds_rx,
            lambda x: x.target_power is not None and x.target_power.as_watts() == 0.0,
        )
        self._assert_report(
            await bounds_rx.receive(), power=0.0, lower=-100000.0, upper=0.0
        )
        await asyncio.sleep(0.0)

        assert set_power.call_count == 4
        inv_ids = mocks.microgrid.pv_inverter_ids
        assert sorted(set_power.call_args_list, key=lambda x: x.args[0]) == [
            mocker.call(inv_ids[0], 0.0),
            mocker.call(inv_ids[1], 0.0),
            mocker.call(inv_ids[2], 0.0),
            mocker.call(inv_ids[3], 0.0),
        ]
        dist_results = await dist_results_rx.receive()
        assert isinstance(
            dist_results, _power_distributing.Success
        ), f"Expected a success, got {dist_results}"
        assert dist_results.succeeded_power == Power.zero()
        assert dist_results.excess_power == Power.zero()
        assert dist_results.succeeded_components == {
            ComponentId(8),
            ComponentId(18),
            ComponentId(28),
            ComponentId(38),
        }

        # Resetting the power should lead to default (full) power getting set for all
        # inverters.
        set_power.reset_mock()
        await pv_pool.propose_power(None)
        report = await self._recv_reports_until(
            bounds_rx,
            lambda x: x.target_power is None,
        )
        self._assert_report(report, power=None, lower=-100000.0, upper=0.0)
        await asyncio.sleep(0.0)

        assert set_power.call_count == 4
        inv_ids = mocks.microgrid.pv_inverter_ids
        assert sorted(set_power.call_args_list, key=lambda x: x.args[0]) == [
            mocker.call(inv_ids[0], -10_000.0),
            mocker.call(inv_ids[1], -20_000.0),
            mocker.call(inv_ids[2], -30_000.0),
            mocker.call(inv_ids[3], -40_000.0),
        ]
        dist_results = await dist_results_rx.receive()
        assert isinstance(
            dist_results, _power_distributing.Success
        ), f"Expected a success, got {dist_results}"
        assert dist_results.succeeded_power == Power.from_watts(-100000.0)
        assert dist_results.excess_power == Power.zero()
        assert dist_results.succeeded_components == {
            ComponentId(8),
            ComponentId(18),
            ComponentId(28),
            ComponentId(38),
        }



================================================
FILE: tests/timeseries/_resampling/__init__.py
================================================
# License: MIT
# Copyright © 2025 Frequenz Energy-as-a-Service GmbH

"""Resampling tests."""



================================================
FILE: tests/timeseries/_resampling/wall_clock_timer/__init__.py
================================================
# License: MIT
# Copyright © 2025 Frequenz Energy-as-a-Service GmbH

"""Wall clock timer tests."""



================================================
FILE: tests/timeseries/_resampling/wall_clock_timer/conftest.py
================================================
# License: MIT
# Copyright © 2025 Frequenz Energy-as-a-Service GmbH

"""Fixtures for wall clock timer tests."""

import asyncio
from collections.abc import Callable, Iterator, Sequence
from datetime import datetime, timedelta
from unittest.mock import AsyncMock, MagicMock, patch

import pytest
from frequenz.core.datetime import UNIX_EPOCH

from frequenz.sdk.timeseries._resampling._wall_clock_timer import ClocksInfo, TickInfo

# Some of the utils do assertions and we want them to be rewritten by pytest for better
# error messages
pytest.register_assert_rewrite("tests.timeseries._resampling.wall_clock_timer.util")

# We need to import this module after registering the assert rewrite
from .util import TimeDriver  # noqa: E402


@pytest.fixture
def datetime_mock() -> Iterator[MagicMock]:
    """Mock the datetime class in the target module and set now to the UNIX epoch."""
    dt_symbol = "frequenz.sdk.timeseries._resampling._wall_clock_timer.datetime"
    dt_mock = MagicMock(name="datetime_mock", wraps=datetime, spec_set=datetime)
    dt_mock.now.return_value = UNIX_EPOCH
    with patch(dt_symbol, new=dt_mock):
        yield dt_mock


@pytest.fixture
def asyncio_sleep_mock() -> Iterator[AsyncMock]:
    """Mock asyncio.sleep in the target module for all tests."""
    asyncio_symbol = "frequenz.sdk.timeseries._resampling._wall_clock_timer.asyncio"
    mock = AsyncMock(
        name="asyncio_sleep_mock", wraps=asyncio.sleep, spec_set=asyncio.sleep
    )
    asyncio_mock = AsyncMock(name="asyncio_mock", wraps=asyncio, spec_set=asyncio)
    asyncio_mock.sleep = mock
    with patch(asyncio_symbol, new=asyncio_mock):
        yield mock


@pytest.fixture
async def time_driver(datetime_mock: MagicMock) -> TimeDriver:
    """Fixture to mock the clocks environment for testing."""
    return TimeDriver(
        datetime_mock=datetime_mock,
    )


def pytest_assertrepr_compare(op: str, left: object, right: object) -> list[str] | None:
    """Provide custom, readable error reports for TickInfo comparisons."""
    # We only care about == comparisons involving our TickInfo objects, returning None
    # makes pytest fall back to its default comparison behavior.
    if op != "==" or not isinstance(left, TickInfo) or not isinstance(right, TickInfo):
        return None

    # Helper function to format values for readability
    def format_val(val: object) -> str:
        # For our time-based types, use str() for readability instead of repr()
        if isinstance(val, (datetime, timedelta)):
            return str(val)
        # For our approx objects and others, the default repr is already good.
        return repr(val)

    errors = _compare_tick_info_objects(left, right, format_val)
    # If the comparison was actually successful (no errors), let pytest handle it
    if not errors:
        return None

    # Format the final error message
    report = ["Comparing TickInfo objects:"]
    report.append("  Differing attributes:")
    report.append(f"  {list(errors.keys())!r}")
    report.append("")

    for field, diff in errors.items():
        report.append(f"  Drill down into differing attribute '{field}':")
        # The diff can be a simple tuple of (left, right) values or a list of
        # strings for nested diffs
        match diff:
            case list():
                report.extend(f"    {line}" for line in diff)
            case (left_val, right_val):
                report.append(f"    - {format_val(left_val)}")
                report.append(f"    + {format_val(right_val)}")
            case _:
                assert False, f"Unexpected diff type: {type(diff)}"

    return report


# We need to compare the fields in TickInfo in a particular way in
# _compare_tick_info_objects. If new fields are added to the dataclass, we'll most
# likely need to add a custom comparison for those fields too. To catch this, we do
# a sanity check here, so if new fields are added we get an early warning instead of
# getting a comparison that misses those fields.
assert set(TickInfo.__dataclass_fields__.keys()) == {
    "expected_tick_time",
    "sleep_infos",
}, "TickInfo fields were added or removed, please update the _compare_tick_info_objects function."


def _compare_tick_info_objects(
    left: TickInfo, right: TickInfo, format_val: Callable[[object], str]
) -> dict[str, object]:
    """Compare two TickInfo objects and return a dictionary of differences."""
    errors: dict[str, object] = {}

    # 1. Compare top-level fields
    if left.expected_tick_time != right.expected_tick_time:
        errors["expected_tick_time"] = (
            left.expected_tick_time,
            right.expected_tick_time,
        )

    # 2. Compare the list of ClocksInfo objects
    sleeps_diff = _compare_sleep_infos_list(
        left.sleep_infos, right.sleep_infos, format_val
    )
    if sleeps_diff:
        errors["sleep_infos"] = sleeps_diff

    return errors


def _compare_sleep_infos_list(
    left: Sequence[ClocksInfo],
    right: Sequence[ClocksInfo],
    format_val: Callable[[object], str],
) -> list[str]:
    """Compare two lists of ClocksInfo objects and return a list of error strings."""
    if len(left) != len(right):
        return [
            f"List lengths differ: {len(left)} != {len(right)}",
            f"                     {left!r}",
            "                     !=",
            f"                     {right!r}",
        ]

    diffs: list[str] = []
    for i, (l_clock, r_clock) in enumerate(zip(left, right)):
        if l_clock != r_clock:
            diffs.append(f"Item at index [{i}] differs:")
            # Get detailed diffs for the fields inside the ClocksInfo object
            for field in l_clock.__dataclass_fields__:
                l_val = getattr(l_clock, field)
                r_val = getattr(r_clock, field)
                if l_val != r_val:
                    diffs.append(f"  Attribute '{field}':")
                    diffs.append(f"    - {format_val(l_val)}")
                    diffs.append(f"    + {format_val(r_val)}")
    return diffs



================================================
FILE: tests/timeseries/_resampling/wall_clock_timer/test_clocksinfo.py
================================================
# License: MIT
# Copyright © 2025 Frequenz Energy-as-a-Service GmbH

"""Tests for the `ClocksInfo` class."""

import math
import re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone

import pytest

from frequenz.sdk.timeseries._resampling._wall_clock_timer import ClocksInfo

_DEFAULT_MONOTONIC_REQUESTED_SLEEP = timedelta(seconds=1.0)
_DEFAULT_MONOTONIC_TIME = 1234.5
_DEFAULT_WALL_CLOCK_TIME = datetime(2023, 1, 1, tzinfo=timezone.utc)
_DEFAULT_MONOTONIC_ELAPSED = timedelta(seconds=1.1)
_DEFAULT_WALL_CLOCK_ELAPSED = timedelta(seconds=1.2)


@pytest.mark.parametrize(
    "elapsed",
    [timedelta(seconds=0), timedelta(seconds=-1.0)],
    ids=["zero", "negative"],
)
def test_monotonic_requested_sleep_invalid(elapsed: timedelta) -> None:
    """Test monotonic_requested_sleep with invalid values."""
    with pytest.raises(
        ValueError,
        match=r"^monotonic_requested_sleep must be strictly positive, not "
        + re.escape(repr(elapsed))
        + r"$",
    ):
        _ = ClocksInfo(
            monotonic_requested_sleep=elapsed,
            monotonic_time=_DEFAULT_MONOTONIC_TIME,
            wall_clock_time=_DEFAULT_WALL_CLOCK_TIME,
            monotonic_elapsed=_DEFAULT_MONOTONIC_ELAPSED,
            wall_clock_elapsed=_DEFAULT_WALL_CLOCK_ELAPSED,
        )


@pytest.mark.parametrize(
    "time",
    [float("-inf"), float("nan"), float("inf")],
)
def test_monotonic_time_invalid(time: float) -> None:
    """Test monotonic_time with invalid values."""
    with pytest.raises(
        ValueError,
        match=rf"^monotonic_time must be a number, not {re.escape(repr(time))}$",
    ):
        _ = ClocksInfo(
            monotonic_requested_sleep=_DEFAULT_MONOTONIC_REQUESTED_SLEEP,
            monotonic_time=time,
            wall_clock_time=_DEFAULT_WALL_CLOCK_TIME,
            monotonic_elapsed=_DEFAULT_MONOTONIC_ELAPSED,
            wall_clock_elapsed=_DEFAULT_WALL_CLOCK_ELAPSED,
        )


@pytest.mark.parametrize(
    "elapsed",
    [timedelta(seconds=0), timedelta(seconds=-1.0)],
    ids=["zero", "negative"],
)
def test_monotonic_elapsed_invalid(elapsed: timedelta) -> None:
    """Test monotonic_elapsed with invalid values."""
    with pytest.raises(
        ValueError,
        match="^monotonic_elapsed must be strictly positive, not "
        rf"{re.escape(repr(elapsed))}$",
    ):
        _ = ClocksInfo(
            monotonic_requested_sleep=_DEFAULT_MONOTONIC_REQUESTED_SLEEP,
            monotonic_time=_DEFAULT_MONOTONIC_TIME,
            wall_clock_time=_DEFAULT_WALL_CLOCK_TIME,
            monotonic_elapsed=elapsed,
            wall_clock_elapsed=_DEFAULT_WALL_CLOCK_ELAPSED,
        )


@pytest.mark.parametrize("wall_clock_factor", [float("nan"), 2.3])
def test_clocks_info_construction(wall_clock_factor: float) -> None:
    """Test that ClocksInfo can be constructed and attributes are set correctly."""
    monotonic_requested_sleep = timedelta(seconds=1.0)
    monotonic_time = 1234.5
    wall_clock_time = datetime(2023, 1, 1, tzinfo=timezone.utc)
    monotonic_elapsed = timedelta(seconds=1.1)
    wall_clock_elapsed = timedelta(seconds=1.2)

    info = ClocksInfo(
        monotonic_requested_sleep=monotonic_requested_sleep,
        monotonic_time=monotonic_time,
        wall_clock_time=wall_clock_time,
        monotonic_elapsed=monotonic_elapsed,
        wall_clock_elapsed=wall_clock_elapsed,
        wall_clock_factor=wall_clock_factor,
    )

    assert info.monotonic_requested_sleep == monotonic_requested_sleep
    assert info.monotonic_time == monotonic_time
    assert info.wall_clock_time == wall_clock_time
    assert info.monotonic_elapsed == monotonic_elapsed
    assert info.wall_clock_elapsed == wall_clock_elapsed

    # Check in particular that using nan explicitly is the same as using the default
    # We test how the default is calculated in another test
    if math.isnan(wall_clock_factor):
        assert info == ClocksInfo(
            monotonic_requested_sleep=monotonic_requested_sleep,
            monotonic_time=monotonic_time,
            wall_clock_time=wall_clock_time,
            monotonic_elapsed=monotonic_elapsed,
            wall_clock_elapsed=wall_clock_elapsed,
        )
    else:
        assert info.wall_clock_factor == wall_clock_factor


@pytest.mark.parametrize(
    "requested_sleep, monotonic_elapsed, expected_drift",
    [
        (timedelta(seconds=1.0), timedelta(seconds=1.1), timedelta(seconds=0.1)),
        (timedelta(seconds=1.0), timedelta(seconds=0.9), timedelta(seconds=-0.1)),
        (timedelta(seconds=1.0), timedelta(seconds=1.0), timedelta(seconds=0.0)),
    ],
    ids=["positive", "negative", "no_drift"],
)
def test_monotonic_drift(
    requested_sleep: timedelta,
    monotonic_elapsed: timedelta,
    expected_drift: timedelta,
) -> None:
    """Test the monotonic_drift property."""
    info = ClocksInfo(
        monotonic_requested_sleep=requested_sleep,
        monotonic_time=_DEFAULT_MONOTONIC_TIME,
        wall_clock_time=_DEFAULT_WALL_CLOCK_TIME,
        monotonic_elapsed=monotonic_elapsed,
        wall_clock_elapsed=_DEFAULT_WALL_CLOCK_ELAPSED,
    )
    assert info.monotonic_drift == pytest.approx(expected_drift)


@pytest.mark.parametrize(
    "wall_clock_elapsed, monotonic_elapsed, expected_jump",
    [
        (timedelta(seconds=1.0), timedelta(seconds=2.1), timedelta(seconds=-1.1)),
        (timedelta(seconds=1.0), timedelta(seconds=0.19), timedelta(seconds=0.81)),
        (timedelta(seconds=1.0), timedelta(seconds=1.0), timedelta(seconds=0.0)),
    ],
    ids=["positive", "negative", "no_jump"],
)
def test_wall_clock_jump(
    wall_clock_elapsed: timedelta,
    monotonic_elapsed: timedelta,
    expected_jump: timedelta,
) -> None:
    """Test the wall_clock_jump property."""
    info = ClocksInfo(
        monotonic_requested_sleep=_DEFAULT_MONOTONIC_REQUESTED_SLEEP,
        monotonic_time=_DEFAULT_MONOTONIC_TIME,
        wall_clock_time=_DEFAULT_WALL_CLOCK_TIME,
        monotonic_elapsed=monotonic_elapsed,
        wall_clock_elapsed=wall_clock_elapsed,
    )
    assert info.wall_clock_jump == pytest.approx(expected_jump)


@dataclass(kw_only=True, frozen=True)
class _TestCaseWallClockFactor:
    """Test case for wall clock factor calculation."""

    id: str
    monotonic_elapsed: timedelta
    wall_clock_elapsed: timedelta
    expected_factor: float


@pytest.mark.parametrize(
    "case",
    [
        _TestCaseWallClockFactor(
            id="wall_faster",
            monotonic_elapsed=timedelta(seconds=1.0),
            wall_clock_elapsed=timedelta(seconds=1.1),
            expected_factor=0.9090909090909091,
        ),
        _TestCaseWallClockFactor(
            id="wall_slower",
            monotonic_elapsed=timedelta(seconds=1.0),
            wall_clock_elapsed=timedelta(seconds=0.9),
            expected_factor=1.11111111111111,
        ),
        _TestCaseWallClockFactor(
            id="in_sync",
            monotonic_elapsed=timedelta(seconds=1.0),
            wall_clock_elapsed=timedelta(seconds=1.0),
            expected_factor=1.0,
        ),
        _TestCaseWallClockFactor(
            id="wall_twice_as_fast",
            monotonic_elapsed=timedelta(seconds=0.5),
            wall_clock_elapsed=timedelta(seconds=1.0),
            expected_factor=0.5,
        ),
    ],
    ids=lambda case: case.id,
)
def test_wall_clock_factor(case: _TestCaseWallClockFactor) -> None:
    """Test the calculate_wall_clock_factor method with valid inputs."""
    info = ClocksInfo(
        monotonic_requested_sleep=_DEFAULT_MONOTONIC_REQUESTED_SLEEP,
        monotonic_time=_DEFAULT_MONOTONIC_TIME,
        wall_clock_time=_DEFAULT_WALL_CLOCK_TIME,
        monotonic_elapsed=case.monotonic_elapsed,
        wall_clock_elapsed=case.wall_clock_elapsed,
    )
    assert info.wall_clock_factor == pytest.approx(case.expected_factor)
    assert info.wall_clock_to_monotonic(case.wall_clock_elapsed) == pytest.approx(
        case.monotonic_elapsed
    )


@pytest.mark.parametrize(
    "elapsed",
    [timedelta(seconds=0), timedelta(seconds=-1.0)],
    ids=["zero", "negative"],
)
def test_wall_clock_factor_invalid_wall_clock_elapsed(
    elapsed: timedelta, caplog: pytest.LogCaptureFixture
) -> None:
    """Test that a warning is logged when wall_clock_elapsed is zero."""
    expected_log = (
        "The monotonic clock advanced 0:00:01, but the wall clock "
        f"stayed still or jumped back (elapsed: {elapsed})!"
    )
    with caplog.at_level("WARNING"):
        info = ClocksInfo(
            monotonic_requested_sleep=_DEFAULT_MONOTONIC_REQUESTED_SLEEP,
            monotonic_time=_DEFAULT_MONOTONIC_TIME,
            wall_clock_time=_DEFAULT_WALL_CLOCK_TIME,
            monotonic_elapsed=timedelta(seconds=1.0),
            wall_clock_elapsed=elapsed,
        )
        assert info.wall_clock_to_monotonic(timedelta(seconds=1.0)) == timedelta(
            seconds=10.0
        )
        assert info.wall_clock_factor == 10.0

    assert expected_log in caplog.text



================================================
FILE: tests/timeseries/_resampling/wall_clock_timer/test_config.py
================================================
# License: MIT
# Copyright © 2025 Frequenz Energy-as-a-Service GmbH

"""Tests for the `WallClockTimerConfig` class."""


import math
import re
from datetime import datetime, timedelta, timezone

import pytest
from frequenz.core.datetime import UNIX_EPOCH

from frequenz.sdk.timeseries._resampling._wall_clock_timer import WallClockTimerConfig


def test_from_interval_defaults() -> None:
    """Test WallClockTimerConfig.from_interval() with only interval (all defaults)."""
    interval = timedelta(seconds=10)
    config = WallClockTimerConfig.from_interval(interval)
    assert config.align_to == UNIX_EPOCH
    assert config.async_drift_tolerance == pytest.approx(timedelta(seconds=1.0))
    assert config.wall_clock_drift_tolerance_factor == pytest.approx(0.1)
    assert config.wall_clock_jump_threshold == pytest.approx(timedelta(seconds=10.0))


def test_from_interval_all_args() -> None:
    """Test WallClockTimerConfig.from_interval() with all arguments provided."""
    interval = timedelta(seconds=5)
    align_to = datetime(2023, 1, 1, tzinfo=timezone.utc)
    async_factor = 0.2
    wall_factor = 0.3
    jump_factor = 0.4
    config = WallClockTimerConfig.from_interval(
        interval,
        align_to=align_to,
        async_drift_tolerance_factor=async_factor,
        wall_clock_drift_tolerance_factor=wall_factor,
        wall_clock_jump_threshold_factor=jump_factor,
    )
    assert config.align_to == align_to
    assert config.async_drift_tolerance == pytest.approx(timedelta(seconds=1.0))
    assert config.wall_clock_drift_tolerance_factor == pytest.approx(0.3)
    assert config.wall_clock_jump_threshold == pytest.approx(timedelta(seconds=2.0))


@pytest.mark.parametrize(
    "interval", [timedelta(seconds=0), timedelta(seconds=-1)], ids=str
)
def test_from_interval_invalid(interval: timedelta) -> None:
    """Test WallClockTimerConfig.from_interval() with invalid interval raises ValueError."""
    with pytest.raises(ValueError, match=r"^interval must be bigger than 0, not "):
        WallClockTimerConfig.from_interval(interval)


def test_trivial_defaults() -> None:
    """Test that WallClockTimerConfig can be constructed with all defaults."""
    config = WallClockTimerConfig()
    assert config.align_to == UNIX_EPOCH
    assert config.async_drift_tolerance is None
    assert config.wall_clock_drift_tolerance_factor is None
    assert config.wall_clock_jump_threshold is None


def test_all_valid_arguments() -> None:
    """Test that WallClockTimerConfig can be constructed with all valid arguments."""
    align_to = datetime(2024, 1, 1, tzinfo=timezone.utc)
    async_drift_tolerance = timedelta(seconds=5)
    wall_clock_drift_tolerance_factor = 0.5
    wall_clock_jump_threshold = timedelta(seconds=10)
    config = WallClockTimerConfig(
        align_to=align_to,
        async_drift_tolerance=async_drift_tolerance,
        wall_clock_drift_tolerance_factor=wall_clock_drift_tolerance_factor,
        wall_clock_jump_threshold=wall_clock_jump_threshold,
    )
    assert config.align_to == align_to
    assert config.async_drift_tolerance == async_drift_tolerance
    assert config.wall_clock_drift_tolerance_factor == wall_clock_drift_tolerance_factor
    assert config.wall_clock_jump_threshold == wall_clock_jump_threshold


@pytest.mark.parametrize(
    "align_to",
    [None, datetime(2020, 1, 1, tzinfo=timezone.utc)],
    ids=str,
)
def test_valid_align_to(align_to: datetime | None) -> None:
    """Test that align_to is accepted and set for valid input."""
    config = WallClockTimerConfig(align_to=align_to)
    assert config.align_to == align_to


def test_align_to_timezone_unaware() -> None:
    """Test checks on the resampling buffer."""
    with pytest.raises(
        ValueError, match=r"^align_to (.*) should be a timezone aware datetime$"
    ):
        _ = WallClockTimerConfig(align_to=datetime(2020, 1, 1, tzinfo=None))


_VALID_NUMBERS = [
    0.1,
    1.0,
    None,
]
_VALID_TIMEDELTAS = [
    *(timedelta(seconds=v) for v in _VALID_NUMBERS if v is not None),
    None,
]

_INVALID_NUMBERS = [
    -0.0001,
    0.0,
    -1,
    0,
    float("inf"),
    float("-inf"),
    float("nan"),
]

_INVALID_TIMEDELTAS = [
    *(timedelta(seconds=v) for v in _INVALID_NUMBERS if math.isfinite(v)),
]


@pytest.mark.parametrize("value", _VALID_TIMEDELTAS, ids=str)
def test_valid_async_drift_tolerance(value: timedelta | None) -> None:
    """Test that async_drift_tolerance accepts valid values."""
    config = WallClockTimerConfig(async_drift_tolerance=value)
    assert config.async_drift_tolerance == value


@pytest.mark.parametrize("value", _INVALID_TIMEDELTAS, ids=str)
def test_invalid_async_drift_tolerance(value: timedelta | None) -> None:
    """Test that strictly positive fields reject invalid values (matrix)."""
    with pytest.raises(
        ValueError,
        match=rf"^async_drift_tolerance should be positive or None, not {re.escape(repr(value))}$",
    ):
        _ = WallClockTimerConfig(async_drift_tolerance=value)


@pytest.mark.parametrize("value", _VALID_TIMEDELTAS, ids=str)
def test_valid_wall_clock_jump_threshold(
    value: timedelta | None,
) -> None:
    """Test that wall_clock_jump_threshold accepts valid values."""
    config = WallClockTimerConfig(wall_clock_jump_threshold=value)
    assert config.wall_clock_jump_threshold == value


@pytest.mark.parametrize("value", _INVALID_TIMEDELTAS, ids=str)
def test_invalid_wall_clock_jump_threshold(
    value: timedelta | None,
) -> None:
    """Test that strictly positive fields reject invalid values (matrix)."""
    with pytest.raises(
        ValueError,
        match=r"^wall_clock_jump_threshold should be positive or None, not "
        + re.escape(repr(value))
        + r"$",
    ):
        _ = WallClockTimerConfig(wall_clock_jump_threshold=value)


@pytest.mark.parametrize("value", [0.1, 1.0, 1, None])
def test_valid_wall_clock_drift_tolerance_factor(
    value: float | None,
) -> None:
    """Test that strictly positive fields accept valid values."""
    config = WallClockTimerConfig(wall_clock_drift_tolerance_factor=value)
    assert config.wall_clock_drift_tolerance_factor == value


@pytest.mark.parametrize("value", _INVALID_NUMBERS, ids=str)
def test_invalid_wall_clock_drift_tolerance_factor(
    value: float | None,
) -> None:
    """Test that strictly positive fields reject invalid values (matrix)."""
    with pytest.raises(
        ValueError,
        match=r"^wall_clock_drift_tolerance_factor should be positive or None, not "
        + re.escape(repr(value))
        + r"$",
    ):
        _ = WallClockTimerConfig(wall_clock_drift_tolerance_factor=value)



================================================
FILE: tests/timeseries/_resampling/wall_clock_timer/test_timer_basic.py
================================================
# License: MIT
# Copyright © 2025 Frequenz Energy-as-a-Service GmbH

"""Basic tests for `WallClockTimer`."""

import re
from datetime import timedelta
from unittest.mock import MagicMock

import pytest

from frequenz.sdk.timeseries._resampling._wall_clock_timer import (
    WallClockTimer,
    WallClockTimerConfig,
)

from .util import to_seconds, wall_now

pytestmark = pytest.mark.usefixtures("datetime_mock")


@pytest.mark.parametrize(
    "interval",
    [timedelta(seconds=0.0), timedelta(seconds=-0.01)],
    ids=["zero", "negative"],
)
def test_invalid_interval(interval: timedelta) -> None:
    """Test WallClockTimer with invalid intervals raises ValueError."""
    with pytest.raises(
        ValueError,
        match=rf"^interval must be positive, not {re.escape(str(interval))}$",
    ):
        _ = WallClockTimer(interval)


def test_custom_config() -> None:
    """Test WallClockTimer with a custom configuration."""
    interval = timedelta(seconds=5)
    config = MagicMock(name="config", spec=WallClockTimerConfig)

    timer = WallClockTimer(interval, config=config, auto_start=False)
    assert timer.interval == interval
    assert timer.config is config


def test_auto_start_default() -> None:
    """Test WallClockTimer uses auto_start=True by default."""
    interval = timedelta(seconds=1)
    timer = WallClockTimer(interval)
    assert timer.interval == interval
    assert timer.config == WallClockTimerConfig.from_interval(interval)
    assert timer.is_running
    assert timer.next_tick_time == wall_now() + interval


def test_auto_start_disabled() -> None:
    """Test WallClockTimer does not start when auto_start=False."""
    interval = timedelta(seconds=1)
    timer = WallClockTimer(interval, auto_start=False)
    assert timer.interval == interval
    assert timer.config == WallClockTimerConfig.from_interval(interval)
    assert not timer.is_running
    assert timer.next_tick_time is None


def test_reset() -> None:
    """Test WallClockTimer.reset() starts the timer and sets next_tick_time relative to now."""
    interval = timedelta(seconds=3)
    timer = WallClockTimer(interval, auto_start=False)
    timer.reset()
    assert timer.is_running
    assert timer.next_tick_time is not None
    assert to_seconds(timer.next_tick_time) == pytest.approx(
        to_seconds(wall_now() + interval)
    )


def test_close() -> None:
    """Test WallClockTimer.close() stops the timer and returns no next_tick_time."""
    interval = timedelta(seconds=2)
    timer = WallClockTimer(interval, auto_start=True)
    timer.close()
    assert not timer.is_running
    assert timer.next_tick_time is None


def test_str() -> None:
    """Test __str__ returns only the interval."""
    interval = timedelta(seconds=4)
    timer = WallClockTimer(interval, auto_start=False)
    assert str(timer) == f"WallClockTimer({interval})"


def test_repr() -> None:
    """Test __repr__ includes interval and running state."""
    interval = timedelta(seconds=4)
    timer = WallClockTimer(interval, auto_start=False)
    assert repr(timer) == f"WallClockTimer<interval={interval!r}, is_running=False>"
    timer.reset()
    assert (
        repr(timer) == f"WallClockTimer<interval={interval!r}, is_running=True, "
        f"next_tick_time={timer.next_tick_time!r}>"
    )



================================================
FILE: tests/timeseries/_resampling/wall_clock_timer/test_timer_ticking.py
================================================
# License: MIT
# Copyright © 2025 Frequenz Energy-as-a-Service GmbH

"""Tests for the ticking behavior of the WallClockTimer.

This module contains tests to verify that the `WallClockTimer` behaves correctly
under various clock conditions, such as clock drift and time jumps.

The core of the testing is done in the `test_ticking` function, which is a
parameterized test that runs multiple scenarios defined as `_TickTestCase`
instances. Each test case simulates a sequence of timer ticks, specifying how the
wall clock should be manipulated during each tick's sleep phase.

Key aspects of the testing approach:

- **Time Mocking**: We mock both the wall clock (`datetime.now`) and monotonic
  clock (`asyncio.sleep`) to have full control over time. The `TimeDriver`
  utility class orchestrates the time advancements and adjustments.

- **Simplified Time Representation**: All time values in the test cases (timestamps,
  deltas) are expressed in plain seconds (as floats). This is possible because
  the mocked wall clock starts at the Unix epoch (time 0). This convention
  greatly simplifies writing and debugging tests, as it's easier to reason
  about small numbers rather than complex `datetime` and `timedelta` objects,
  which can be especially cryptic when they are large or negative.

- **Scenario-based Testing**: Each `_TickTestCase` defines a complete scenario,
  like "constant forward drift" or "backward jump". For each tick within the
  scenario, a `_TickSpec` defines the expected `TickInfo` and any wall clock
  adjustments to be made. This allows for precise testing of the timer's logic
  for drift compensation and resynchronization.
"""

import logging
import re
from collections.abc import Sequence
from unittest.mock import AsyncMock, call

import async_solipsism
import pytest
from attr import dataclass

from frequenz.sdk.timeseries._resampling._wall_clock_timer import (
    ClocksInfo,
    TickInfo,
    WallClockTimer,
    WallClockTimerConfig,
)

from .util import (
    Adjustment,
    TimeDriver,
    approx_tick_info,
    approx_time,
    delta,
    matches_re,
    mono_now,
    timestamp,
    to_seconds,
    wall_now,
)

_logger = logging.getLogger(__name__)


@pytest.fixture
def event_loop_policy() -> async_solipsism.EventLoopPolicy:
    """Return an event loop policy that uses the async solipsism event loop."""
    return async_solipsism.EventLoopPolicy()


async def test_ready_called_twice_returns_immediately(time_driver: TimeDriver) -> None:
    """Test that calling ready() twice returns immediately the second time."""
    interval = delta(1.0)
    timer = WallClockTimer(
        interval, config=WallClockTimerConfig.from_interval(interval)
    )

    # The first call to ready() will start the timer and wait for the first tick.
    mono_start = mono_now()
    await time_driver.next_tick([Adjustment(0.9, 1.0)], timer.ready())
    mono_end = mono_now()
    assert mono_end == pytest.approx(mono_start + 1.0)

    # The second call should return immediately.
    mono_start = mono_now()
    assert await timer.ready()
    mono_end = mono_now()

    # Time should not have advanced.
    assert mono_end == pytest.approx(mono_start)


@dataclass(kw_only=True, frozen=True)
class _TickSpec:
    """Specification for one tick in a TickTestCase."""

    wall_clock_adjustments: Sequence[Adjustment]
    """The wall clock time adjustments to do while waiting for the next tick."""

    expected_tick_info: TickInfo
    """Expected TickInfo returned by the timer after the tick, if any."""

    expected_warnings: Sequence[str] = ()
    """The expected warning messages during the tick.

    Each string will be used as a regex pattern that should match the logged warning
    messages.
    """


@dataclass(kw_only=True, frozen=True)
class _TickTestCase:
    """A test case for the WallClockTimer ticking behavior."""

    id: str
    """The identifier for the test case, used in parameterized tests."""

    ticks: Sequence[_TickSpec]
    """The specifications for the ticks in this test case."""


# IMPORTANT: All tests are written for a timer with a 1 second interval and default
# config. If the default changes, the tests will need to be updated accordingly.
@pytest.mark.parametrize(
    "case",
    [
        # Both clocks are perfecly in sync. The timer should sleep for the full
        # interval and wake up at the expected wall clock time.
        _TickTestCase(
            id="in_sync",
            ticks=[
                # We adjust the wall clock time slightly before the timer wakes up
                # to the interval (1.0 seconds) to match the elapsed time in the
                # monotonic clock, to ensure both clocks are in sync.
                _TickSpec(  # Tick 1, 2, 3, 4, 5
                    wall_clock_adjustments=[Adjustment(0.9, 1.0 * (i + 1))],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(1.0 * (i + 1)),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(1.0 * (i + 1)),
                                monotonic_time=1.0 * (i + 1),
                                wall_clock_elapsed=delta(1.0),
                                monotonic_elapsed=delta(1.0),
                                monotonic_requested_sleep=delta(1.0),
                            )
                        ],
                    ),
                )
                for i in range(5)
            ],
        ),
        # The wall clock is slightly ahead of the monotonic clock, but with a
        # constant drift.  The first tick will be slightly off because of the drift,
        # the next tick the timer will adjust using a factor due to the drift, but
        # also account for the drift in the first tick, ending up with a
        # particularly short sleep time. Afterwards the timer should apply a factor
        # and always sleep slightly less than the interval to account for the drift.
        _TickTestCase(
            id="constant_forward_drift_within_tolerance",
            ticks=[
                # We adjust the wall clock time slightly before the timer wakes up
                # to a bit more than the interval (1.01 seconds) to simulate a wall
                # clock forward drift.
                _TickSpec(  # Tick 1
                    wall_clock_adjustments=[Adjustment(0.9, 1.01)],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(1.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(1.01),
                                monotonic_time=1.0,
                                wall_clock_elapsed=delta(1.01),
                                monotonic_elapsed=delta(1.0),
                                monotonic_requested_sleep=delta(1.0),
                            )
                        ],
                    ),
                ),
                # In the second tick, we expect the timer to detect that drift and
                # adjust the sleep time accordingly, so it will sleep slightly less
                # than the interval to account for the drift. It calculates an
                # adjustment factor of 0.99 seconds but for this tick sleeps even
                # less, because it also woke up late, so it effectively sleeps
                # 0.980198 seconds.
                # We adjust the wall clock time slightly before the timer wakes up
                # to a perfect multiple of the interval (2.0 seconds) to simulate
                # that the wall clock drift stays constant (so after the adjustments
                # the timer could wake up at the desired wall clock time of 2
                # intervals).
                _TickSpec(  # Tick 2
                    wall_clock_adjustments=[Adjustment(0.9, 2.0)],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(2.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(2.0),
                                monotonic_time=1.980198,
                                wall_clock_elapsed=delta(0.99),
                                monotonic_elapsed=delta(0.980198),
                                monotonic_requested_sleep=delta(0.980198),
                            )
                        ],
                    ),
                ),
                # For the rest of the ticks, we just expect the timer to always
                # sleep for the interval * the factor (0.99099 seconds) to account
                # for the drift, so we adjust the wall clock time to match the
                # expected wall clock time for each tick at a multiple of the
                # interval (3.0 seconds, 4.0 seconds, etc.) and verify the timer
                # reports a constant factor (sleeps always for 0.990099 seconds).
                *[
                    _TickSpec(  # Tick 3, 4, ..., 10
                        wall_clock_adjustments=[Adjustment(0.9, 3.0 + i)],
                        expected_tick_info=TickInfo(
                            expected_tick_time=timestamp(3.0 + i),
                            sleep_infos=[
                                ClocksInfo(
                                    wall_clock_time=timestamp(3.0 + i),
                                    monotonic_time=2.970297 + i * 0.990099,
                                    wall_clock_elapsed=delta(1.0),
                                    monotonic_elapsed=delta(0.990099),
                                    monotonic_requested_sleep=delta(0.990099),
                                )
                            ],
                        ),
                    )
                    for i in range(8)
                ],
            ],
        ),
        # The wall clock is slightly behind the monotonic clock, but with a
        # constant drift. The first tick will be slightly off because of the lag,
        # the next tick the timer will adjust using a factor due to the lag, but
        # also account for the lag in the first tick, ending up with a
        # particularly long sleep time. Afterwards the timer should apply a factor
        # and always sleep slightly more than the interval to account for the lag.
        _TickTestCase(
            id="constant_backward_drift_within_tolerance",
            ticks=[
                # We adjust the wall clock time slightly before the timer wakes up
                # to a bit less than the interval (0.99 seconds) to simulate a wall
                # clock backward drift.
                _TickSpec(  # Tick 1
                    wall_clock_adjustments=[
                        Adjustment(0.9, 0.99),
                        Adjustment(0.105, 1.0),
                    ],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(1.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(0.99),
                                monotonic_time=1.0,
                                wall_clock_elapsed=delta(0.99),
                                monotonic_elapsed=delta(1.0),
                                monotonic_requested_sleep=delta(1.0),
                            ),
                            ClocksInfo(
                                wall_clock_time=timestamp(1.0),
                                monotonic_time=1.010101,
                                wall_clock_elapsed=delta(0.01),
                                monotonic_elapsed=delta(0.010101),
                                monotonic_requested_sleep=delta(0.010101),
                            ),
                        ],
                    ),
                ),
                _TickSpec(  # Tick 2
                    wall_clock_adjustments=[Adjustment(0.9, 2.0)],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(2.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(2.0),
                                monotonic_time=2.020202,
                                wall_clock_elapsed=delta(1.0),
                                monotonic_elapsed=delta(1.010101),
                                monotonic_requested_sleep=delta(1.010101),
                            )
                        ],
                    ),
                ),
                *[
                    _TickSpec(  # Tick 3, 4, ..., 10
                        wall_clock_adjustments=[Adjustment(0.9, 3.0 + i)],
                        expected_tick_info=TickInfo(
                            expected_tick_time=timestamp(3.0 + i),
                            sleep_infos=[
                                ClocksInfo(
                                    wall_clock_time=timestamp(3.0 + i),
                                    monotonic_time=3.030303 + i * 1.010101,
                                    wall_clock_elapsed=delta(1.0),
                                    monotonic_elapsed=delta(1.010101),
                                    monotonic_requested_sleep=delta(1.010101),
                                )
                            ],
                        ),
                    )
                    for i in range(8)
                ],
            ],
        ),
        # The wall clock is erratic and drifts forward and backwards but always within
        # the tolerance drift tolerance.
        _TickTestCase(
            id="erratic_drift_without_jumps",
            ticks=[
                # We adjust the wall clock time slightly before the timer wakes up
                # to a bit more than the interval (1.01 seconds) to simulate a wall
                # clock forward drift.
                _TickSpec(  # Tick 1
                    wall_clock_adjustments=[Adjustment(0.9, 1.01)],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(1.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(1.01),
                                monotonic_time=1.0,
                                wall_clock_elapsed=delta(1.01),
                                monotonic_elapsed=delta(1.0),
                                monotonic_requested_sleep=delta(1.0),
                            )
                        ],
                    ),
                ),
                # The timer should have adjusted for the forward drift. Now we
                # introduce a backward drift.
                _TickSpec(  # Tick 2
                    wall_clock_adjustments=[
                        Adjustment(0.9, 1.99),
                        # So the timer will need to sleep again to compensate
                        Adjustment(0.09, 2.07),
                    ],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(2.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(1.99),
                                monotonic_time=1.980198,
                                wall_clock_elapsed=delta(0.98),
                                monotonic_elapsed=delta(0.980198),
                                monotonic_requested_sleep=delta(0.980198),
                            ),
                            ClocksInfo(
                                wall_clock_time=timestamp(2.07),
                                monotonic_time=1.9902,
                                wall_clock_elapsed=delta(0.08),
                                monotonic_elapsed=delta(0.010002),
                                monotonic_requested_sleep=delta(0.010002),
                            ),
                        ],
                    ),
                    # We get a warning this time because the last time was WAY off,
                    # it wanted to sleep for 0.01002 seconds but the wall clock
                    # advanced 0.080000 seconds, so the factor changed too much,
                    # which should trigger a warning.
                    # The total difference between the clocks is still way under the
                    # jump tolerance, so we don't get a resync.
                    expected_warnings=[
                        re.escape(
                            "The wall clock time drifted too much from the monotonic time. The "
                            "monotonic time will be adjusted to compensate for this "
                            "difference. We expected the wall clock time to have advanced "
                            "(0:00:00.080000), but the monotonic time advanced "
                            "(0:00:00.010002) [previous_factor=1.00020204"
                        )
                        + r"\d*"
                        + re.escape(
                            " current_factor=0.125025, factor_change_absolute_tolerance=0.1]."
                        ),
                    ],
                ),
                # The timer should have adjusted for the backward drift. Now we
                # introduce a cumulative backward drift.
                # The clock adjustment factor is now way off because of the last
                # small sleep that took too long, so the timer will compensate for
                # it, and sleep 0.116273 seconds when trying to sleep for 0.93
                # seconds (3.0 - 2.07).
                _TickSpec(  # Tick 3
                    wall_clock_adjustments=[
                        # Start monotonic_time=1.9902
                        Adjustment(0.11, 2.998),
                        # monotonic_time=2.1002 (next sleep end at 2.106473)
                        Adjustment(0.0064, 2.999),
                        # monotonic_time=2.1066 (next sleep end at 2.106724)
                        Adjustment(0.0002, 3.0),
                        # monotonic_time=2.1068 (next sleep end at 2.106975)
                    ],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(3.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(2.998),
                                monotonic_time=2.106473,
                                wall_clock_elapsed=delta(0.928),
                                monotonic_elapsed=delta(0.116273),
                                monotonic_requested_sleep=delta(0.116273),
                            ),
                            ClocksInfo(
                                wall_clock_time=timestamp(2.999),
                                monotonic_time=2.106724,
                                wall_clock_elapsed=delta(0.001),
                                monotonic_elapsed=delta(0.000251),
                                monotonic_requested_sleep=delta(0.000251),
                            ),
                            ClocksInfo(
                                wall_clock_time=timestamp(3.0),
                                monotonic_time=2.106975,
                                wall_clock_elapsed=delta(0.001000),
                                monotonic_elapsed=delta(0.000251),
                                monotonic_requested_sleep=delta(0.000251),
                            ),
                        ],
                    ),
                    # Again there is a big difference in the drifts of different sleeps
                    expected_warnings=[
                        re.escape(
                            "The wall clock time drifted too much from the monotonic time. The "
                            "monotonic time will be adjusted to compensate for this "
                            "difference. We expected the wall clock time to have advanced "
                            "(0:00:00.001000), but the monotonic time advanced "
                            "(0:00:00.000251) [previous_factor=0.125294"
                        )
                        + r"\d*"
                        + re.escape(
                            " current_factor=0.251, factor_change_absolute_tolerance=0.1]."
                        ),
                    ],
                ),
                # Now it goes back to a forward drift
                _TickSpec(  # Tick 4
                    wall_clock_adjustments=[Adjustment(0.2, 4.01)],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(4.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(4.01),
                                monotonic_time=2.3579749,
                                wall_clock_elapsed=delta(1.01),
                                monotonic_elapsed=delta(0.251),
                                monotonic_requested_sleep=delta(0.251),
                            )
                        ],
                    ),
                ),
                # And finally it stabilizes to a monotonic clock that is much
                # faster than the wall clock, so the sleeps are constant but much smaller
                # than the interval to make up for the drift.
                _TickSpec(  # Tick 5
                    wall_clock_adjustments=[Adjustment(0.2, 5.0)],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(5.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(5.0),
                                monotonic_time=2.604005,
                                wall_clock_elapsed=delta(0.99),
                                monotonic_elapsed=delta(0.246030),
                                monotonic_requested_sleep=delta(0.246030),
                            )
                        ],
                    ),
                ),
                *[
                    _TickSpec(  # Tick 6, 7, ..., 10
                        wall_clock_adjustments=[Adjustment(0.2, 6.0 + i)],
                        expected_tick_info=TickInfo(
                            expected_tick_time=timestamp(6.0 + i),
                            sleep_infos=[
                                ClocksInfo(
                                    wall_clock_time=timestamp(6.0 + i),
                                    monotonic_time=2.852519 + i * 0.248515,
                                    wall_clock_elapsed=delta(1.0),
                                    monotonic_elapsed=delta(0.248515),
                                    monotonic_requested_sleep=delta(0.248515),
                                )
                            ],
                        ),
                    )
                    for i in range(5)
                ],
            ],
        ),
        _TickTestCase(
            id="forward_jump",
            ticks=[
                # First tick is in sync
                _TickSpec(  # Tick 1
                    wall_clock_adjustments=[Adjustment(0.9, 1.0)],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(1.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(1.0),
                                monotonic_time=1.0,
                                wall_clock_elapsed=delta(1.0),
                                monotonic_elapsed=delta(1.0),
                                monotonic_requested_sleep=delta(1.0),
                            )
                        ],
                    ),
                ),
                # The second tick has a forward jump of 1.01 seconds, which is more
                # than the jump tolerance of 1 second, so we expect a warning and a
                # resync of the timer to the wall clock.  The clocks info will also
                # have an override for the wall clock factor to use the previous
                # one, because the factor will be either extremely low otherwise due
                # to the jump.
                _TickSpec(  # Tick 2
                    wall_clock_adjustments=[Adjustment(0.9, 3.01)],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(2.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(3.01),
                                monotonic_time=2.0,
                                wall_clock_elapsed=delta(2.01),
                                monotonic_elapsed=delta(1.0),
                                monotonic_requested_sleep=delta(1.0),
                                wall_clock_factor=1.0,
                            )
                        ],
                    ),
                    expected_warnings=[
                        re.escape(
                            "The wall clock jumped 0:00:01.010000 (1.01 seconds) in time "
                            "(threshold=0:00:01). A tick will be triggered immediately with "
                            "the `expected_tick_time` as it was before the time jump and the "
                            "timer will be resynced to the wall clock."
                        ),
                    ],
                ),
                _TickSpec(  # Tick 3
                    wall_clock_adjustments=[Adjustment(0.9, 4.0)],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(4.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(4.0),
                                monotonic_time=2.99,
                                wall_clock_elapsed=delta(0.99),
                                monotonic_elapsed=delta(0.99),
                                monotonic_requested_sleep=delta(0.99),
                            )
                        ],
                    ),
                ),
                # For the rest of the ticks, we just expect the clocks to keep being
                # in sync, so the timer will sleep for the full interval
                # (1.0 seconds) and wake up at the expected wall clock time.
                *[
                    _TickSpec(  # Tick 4, 5, ..., 10
                        wall_clock_adjustments=[Adjustment(0.9, 5.0 + i)],
                        expected_tick_info=TickInfo(
                            expected_tick_time=timestamp(5.0 + i),
                            sleep_infos=[
                                ClocksInfo(
                                    wall_clock_time=timestamp(5.0 + i),
                                    monotonic_time=3.99 + i,
                                    wall_clock_elapsed=delta(1.0),
                                    monotonic_elapsed=delta(1.0),
                                    monotonic_requested_sleep=delta(1.0),
                                )
                            ],
                        ),
                    )
                    for i in range(7)
                ],
            ],
        ),
        _TickTestCase(
            id="backward_jump",
            ticks=[
                # First tick is in sync
                _TickSpec(  # Tick 1
                    wall_clock_adjustments=[Adjustment(0.9, 1.0)],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(1.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(1.0),
                                monotonic_time=1.0,
                                wall_clock_elapsed=delta(1.0),
                                monotonic_elapsed=delta(1.0),
                                monotonic_requested_sleep=delta(1.0),
                            )
                        ],
                    ),
                ),
                # The second tick has a backward jump of 1.1 seconds, which is more
                # than the jump tolerance of 1 second, so we expect a warning and a
                # resync of the timer to the wall clock.
                _TickSpec(  # Tick 2
                    wall_clock_adjustments=[Adjustment(0.9, 0.9)],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(2.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(0.9),
                                monotonic_time=2.0,
                                wall_clock_elapsed=delta(-0.1),
                                monotonic_elapsed=delta(1.0),
                                monotonic_requested_sleep=delta(1.0),
                                wall_clock_factor=1.0,
                            )
                        ],
                    ),
                    expected_warnings=[
                        re.escape(
                            "The wall clock jumped -1 day, 23:59:58.900000 (-1.1 seconds) in "
                            "time (threshold=0:00:01). A tick will be triggered immediately "
                            "with the `expected_tick_time` as it was before the time jump "
                            "and the timer will be resynced to the wall clock.",
                        ),
                    ],
                ),
                # After the jump, the timer is resynced. The next tick is set to be
                # at 1.0, which is the next interval aligned to the epoch after 0.9
                # seconds. We make both clocks advance in sync for this tick, so
                # after this all should get back to normal.
                _TickSpec(  # Tick 3
                    wall_clock_adjustments=[Adjustment(0.09, 1.0)],
                    expected_tick_info=TickInfo(
                        expected_tick_time=timestamp(1.0),
                        sleep_infos=[
                            ClocksInfo(
                                wall_clock_time=timestamp(1.0),
                                monotonic_time=2.1,
                                wall_clock_elapsed=delta(0.1),
                                monotonic_elapsed=delta(0.1),
                                monotonic_requested_sleep=delta(0.1),
                            )
                        ],
                    ),
                ),
                # For the rest of the ticks, we just expect the clocks to keep being
                # in sync, so the timer will sleep for the full interval
                # (1.0 seconds) and wake up at the expected wall clock time.
                *[
                    _TickSpec(  # Tick 4, 5, ..., 10
                        wall_clock_adjustments=[Adjustment(0.9, 2.0 + i)],
                        expected_tick_info=TickInfo(
                            expected_tick_time=timestamp(2.0 + i),
                            sleep_infos=[
                                ClocksInfo(
                                    wall_clock_time=timestamp(2.0 + i),
                                    monotonic_time=3.1 + i,
                                    wall_clock_elapsed=delta(1.0),
                                    monotonic_elapsed=delta(1.0),
                                    monotonic_requested_sleep=delta(1.0),
                                )
                            ],
                        ),
                    )
                    for i in range(7)
                ],
            ],
        ),
    ],
    ids=lambda case: case.id,
)
async def test_ticking(
    case: _TickTestCase,
    time_driver: TimeDriver,
    caplog: pytest.LogCaptureFixture,
    asyncio_sleep_mock: AsyncMock,
) -> None:
    """Test ticking behavior of the wall clock timer."""
    # You might need to comment this out if you want to enable debug logging
    caplog.set_level(
        logging.WARNING,
        logger="frequenz.sdk.timeseries._resampling._wall_clock_timer",
    )

    # IMPORTANT: All test cases are relying on a timer with a 1 second interval and
    # default config. If the default changes, the tests will need to be updated
    # accordingly.
    interval = delta(1.0)
    timer = WallClockTimer(
        interval, config=WallClockTimerConfig.from_interval(interval)
    )

    for i, tick in enumerate(case.ticks):
        _logger.debug(
            "================== %s: tick %s/%s =============================",
            case.id,
            i + 1,
            len(case.ticks),
        )
        tick = case.ticks[i]
        expected_tick_info = tick.expected_tick_info

        mono_start = mono_now()
        actual_tick_info = await time_driver.next_tick(
            tick.wall_clock_adjustments, timer.receive()
        )
        mono_end = mono_now()
        _logger.debug(
            "After tick %s: now_wall=%s, now_mono=%s, factor=%s",
            i,
            wall_now(),
            mono_end,
            (
                actual_tick_info.latest_sleep_info.wall_clock_factor
                if actual_tick_info.latest_sleep_info
                else None
            ),
        )

        assert actual_tick_info == approx_tick_info(expected_tick_info)

        if actual_tick_info.latest_sleep_info:
            assert actual_tick_info.latest_sleep_info.wall_clock_time == approx_time(
                wall_now()
            )
        assert actual_tick_info.sleep_infos[-1].monotonic_time == pytest.approx(
            mono_end
        )

        if tick.wall_clock_adjustments:
            assert timestamp(tick.wall_clock_adjustments[-1].wall_time) == approx_time(
                wall_now()
            )
        assert sum(
            to_seconds(i.monotonic_elapsed) for i in actual_tick_info.sleep_infos
        ) == pytest.approx(mono_end - mono_start)
        assert asyncio_sleep_mock.mock_calls == [
            call(pytest.approx(to_seconds(t.monotonic_elapsed)))
            for t in expected_tick_info.sleep_infos
        ]

        filtered_logs = [
            r.message
            for r in caplog.records
            if r.levelno == logging.WARNING
            and r.name == "frequenz.sdk.timeseries._resampling._wall_clock_timer"
        ]
        assert filtered_logs == [matches_re(w) for w in tick.expected_warnings]

        asyncio_sleep_mock.reset_mock()
        caplog.clear()



================================================
FILE: tests/timeseries/_resampling/wall_clock_timer/util.py
================================================
# License: MIT
# Copyright © 2025 Frequenz Energy-as-a-Service GmbH

"""Utility functions for testing the wall clock timer."""

import asyncio
import logging
import re
from collections.abc import Coroutine, Sequence
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from typing import NamedTuple, TypeVar, assert_never, overload
from unittest.mock import MagicMock

import pytest

# This is not great, we are depending on an internal pytest API, but it is
# the most convenient way to provide a custom approx() comparison for datetime
# and timedelta.
# Other alternatives proven to be even more complex and hacky.
# It also looks like we are not the only ones doing this, see:
# https://github.com/pytest-dev/pytest/issues/8395
from _pytest.python_api import ApproxBase
from typing_extensions import override

from frequenz.sdk.timeseries import ClocksInfo, TickInfo

_logger = logging.getLogger(__name__)

_T = TypeVar("_T")


@overload
def to_seconds(value: datetime | timedelta | float) -> float: ...


@overload
def to_seconds(value: None) -> None: ...


def to_seconds(value: datetime | timedelta | float | None) -> float | None:
    """Convert a datetime, timedelta, or float to seconds."""
    match value:
        case datetime():
            return value.timestamp()
        case timedelta():
            return value.total_seconds()
        case float() | int() | None:
            return value
        case unexpected:
            assert_never(unexpected)


def timestamp(ts: datetime | float, /) -> datetime:
    """Convert a timestamp in seconds since the epoch to a UTC datetime."""
    if isinstance(ts, datetime):
        return ts
    return datetime.fromtimestamp(ts, tz=timezone.utc)


def delta(sec: float | timedelta, /) -> timedelta:
    """Create a timedelta from seconds."""
    if isinstance(sec, timedelta):
        return sec
    return timedelta(seconds=sec)


# This is needed to work with the datetime_mock fixture in conftest.py
def wall_now() -> datetime:
    """Get the current wall clock time from the mocked datetime in the target module."""
    # Disable isort formatting because it wants to put the ignore in the wrong line
    # We now also have to ignore the maximum line length (E501)
    # isort: off
    # pylint: disable-next=import-outside-toplevel
    from frequenz.sdk.timeseries._resampling._wall_clock_timer import (  # type: ignore[attr-defined] # noqa: E501
        datetime as mock_datetime,
    )

    # isort: on

    return mock_datetime.now(timezone.utc)


def mono_now() -> float:
    """Get the current monotonic time."""
    return asyncio.get_running_loop().time()


# Pylint complains about abstract-method because _yield_comparisons is not implemented
# but it is used only in the default __eq__ method, which we are re-defining, so we can
# ignore it.
class approx_time(ApproxBase):  # pylint: disable=invalid-name, abstract-method
    """Perform approximate comparisons for datetime or timedelta objects.

    Inherits from `ApproxBase` to provide a rich comparison output in pytest.
    """

    expected: datetime | timedelta
    abs: timedelta

    def __init__(
        self,
        expected: datetime | timedelta,
        *,
        abs: timedelta = timedelta(milliseconds=1),  # pylint: disable=redefined-builtin
    ) -> None:
        """Initialize this instance."""
        if abs < timedelta():
            raise ValueError(
                f"absolute tolerance must be a non-negative timedelta, not {abs}"
            )
        super().__init__(expected, abs=abs)

    def __repr__(self) -> str:
        """Return a string representation of this instance."""
        return f"{self.expected} ± {self.abs}"

    def __eq__(self, actual: object) -> bool:
        """Compare this instance with another object."""
        # We need to split the cases for datetime and timedelta for type checking
        # reasons.
        diff: timedelta
        match (self.expected, actual):
            case (datetime(), datetime()):
                diff = self.expected - actual
            case (timedelta(), timedelta()):
                diff = self.expected - actual
            case _:
                return NotImplemented

        return abs(diff) <= self.abs


# We need to rewrite most of the attributes in these classes to use approximate
# comparisons. This means if a new field is added, we need to update the
# approx_tick_info function below to handle the new fields. To catch this, we do
# a sanity check here, so if new fields are added we get an early warning instead of
# getting tests errors because of some rounding error in a newly added field.
assert set(ClocksInfo.__dataclass_fields__.keys()) == {
    "monotonic_requested_sleep",
    "monotonic_time",
    "wall_clock_time",
    "monotonic_elapsed",
    "wall_clock_elapsed",
    "wall_clock_factor",
}, "ClocksInfo fields were added or removed, please update the approx_tick_info function."
assert set(TickInfo.__dataclass_fields__.keys()) == {
    "expected_tick_time",
    "sleep_infos",
}, "TickInfo fields were added or removed, please update the approx_tick_info function."


def approx_tick_info(
    expected: TickInfo,
    *,
    abs: timedelta = timedelta(milliseconds=1),  # pylint: disable=redefined-builtin
) -> TickInfo:
    """Create a copy of a `TickInfo` object with approximate comparisons.

    Fields are replaced by approximate comparison objects (`approx_time` or
    `pytest.approx`).

    This version bypasses `__post_init__` to avoid validation `TypeError`s.

    Args:
        expected: The expected `TickInfo` object to compare against.
        abs: The absolute tolerance as a `timedelta` for all time-based
            comparisons. Defaults to 1ms.

    Returns:
        A new `TickInfo` instance ready for approximate comparison.
    """
    abs_s = abs.total_seconds()
    approx_sleeps = []
    for s_info in expected.sleep_infos:
        # HACK: Create a blank instance to bypass __init__ and __post_init__.
        # This prevents the TypeError from the validation logic.
        approx_s = object.__new__(ClocksInfo)

        # Use object.__setattr__ to assign fields to the frozen instance.
        object.__setattr__(
            approx_s,
            "monotonic_requested_sleep",
            approx_time(s_info.monotonic_requested_sleep, abs=abs),
        )
        # Use the standard pytest.approx for float values
        object.__setattr__(
            approx_s, "monotonic_time", pytest.approx(s_info.monotonic_time, abs=abs_s)
        )
        object.__setattr__(
            approx_s, "wall_clock_time", approx_time(s_info.wall_clock_time, abs=abs)
        )
        object.__setattr__(
            approx_s,
            "monotonic_elapsed",
            approx_time(s_info.monotonic_elapsed, abs=abs),
        )
        object.__setattr__(
            approx_s,
            "wall_clock_elapsed",
            approx_time(s_info.wall_clock_elapsed, abs=abs),
        )
        if s_info.wall_clock_factor is not None:
            object.__setattr__(
                approx_s,
                "wall_clock_factor",
                pytest.approx(s_info.wall_clock_factor, abs=abs_s),
            )
        approx_sleeps.append(approx_s)

    # Do the same for the top-level frozen TickInfo object
    approx_tick_info = object.__new__(TickInfo)
    object.__setattr__(
        approx_tick_info,
        "expected_tick_time",
        approx_time(expected.expected_tick_time, abs=abs),
    )
    object.__setattr__(approx_tick_info, "sleep_infos", approx_sleeps)

    return approx_tick_info


class matches_re:  # pylint: disable=invalid-name
    """Assert that a given string (or string representation) matches a regex pattern."""

    def __init__(self, pattern: str, flags: int = 0) -> None:
        """Initialize with a regex pattern and optional flags."""
        self._regex = re.compile(pattern, flags)

    @override
    def __eq__(self, other: object) -> bool:
        """Check if the string representation of `other` matches the regex pattern."""
        return bool(self._regex.match(str(other)))

    @override
    def __repr__(self) -> str:
        """Return a string representation of this instance."""
        return self._regex.pattern


class Adjustment(NamedTuple):
    """A time adjustment to be applied at a specific monotonic time."""

    mono_delta: timedelta | float
    """The monotonic time delta to sleep before adjusting the wall clock time, in seconds."""

    wall_time: datetime | float
    """The new wall clock time to set at that monotonic time, in seconds since the epoch in UTC."""


@dataclass(kw_only=True, frozen=True)
class TimeDriver:
    """A utility for driving the wall and monotonic clocks in tests.

    This class encapsulates the necessary mocks for `datetime.datetime` and
    provides methods to manipulate wall clock time during tests. This is particularly
    useful for testing components that rely on both wall clock time (which can be
    adjusted by the system) and monotonic time (which should always move forward).

    It is designed to be used as a pytest fixture, where `datetime_mock` is
    provided by another fixture.

    The main method to use in tests is `next_tick()`, which simulates time
    passing and wall clock adjustments while waiting for a timer tick.
    """

    datetime_mock: MagicMock
    """A mock for the `datetime` module."""

    loop: asyncio.AbstractEventLoop = field(default_factory=asyncio.get_event_loop)
    """The asyncio event loop."""

    mono_start: float = field(default_factory=mono_now)
    """The starting monotonic time of the test."""

    wall_start: datetime = field(default_factory=wall_now)
    """The starting wall clock time of the test."""

    def __post_init__(self) -> None:
        """Initialize the time driver by logging the start times."""
        _logger.debug(
            "Start: wall_now=%s, mono_now=%s", self.wall_start, self.mono_start
        )

    async def _shift_time(
        self,
        wall_delta: timedelta | float,
        *,
        mono_delta: timedelta | float | None = None,
    ) -> tuple[datetime, float]:
        """Advance the time by the given number of seconds.

        This advances both the wall clock and the time machine fake time.

        Args:
            wall_delta: The amount of time to advance the wall clock, in seconds or as a
                timedelta.
            mono_delta: The amount of time to advance the monotonic clock, in seconds or
                as a timedelta.  If None, it defaults to the same value as `wall_time`.

        Returns:
            A tuple containing the new wall clock time and the new monotonic time.
        """
        wall_delta = to_seconds(wall_delta)
        mono_delta = to_seconds(mono_delta)
        if mono_delta is None:
            mono_delta = wall_delta
        _logger.debug(
            "_shift_time(): wall_delta=%s, mono_delta=%s", wall_delta, mono_delta
        )

        wall_start = wall_now()
        mono_start = mono_now()

        _logger.debug(
            "_shift_time(): Before sleep: wall_now=%s, mono_now=%s",
            wall_start,
            mono_start,
        )
        await asyncio.sleep(mono_delta)
        self.datetime_mock.now.return_value = wall_now() + timedelta(seconds=wall_delta)
        _logger.debug(
            "_shift_time(): After shift: wall_now=%s, mono_now=%s",
            wall_now(),
            mono_now(),
        )

        new_wall_now = wall_now()
        new_mono_now = mono_now()
        _logger.debug(
            "NEW TIME: new_wall_now=%s, new_mono_now=%s", new_wall_now, new_mono_now
        )

        assert new_wall_now == approx_time(wall_start + delta(wall_delta))
        assert new_mono_now == pytest.approx(mono_start + mono_delta)

        return new_wall_now, new_mono_now

    def _update_wall_clock(self, new_time: datetime | float) -> None:
        """Update the wall clock time to the given datetime."""
        new_time = timestamp(new_time)
        _logger.debug(
            "_update_wall_clock(): at mono_now=%s %s -> %s (%s -> %s)",
            mono_now(),
            to_seconds(wall_now()),
            to_seconds(new_time),
            wall_now(),
            new_time,
        )
        self.datetime_mock.now.return_value = new_time
        _logger.debug(
            "_update_wall_clock(): wall clock updated to %s (%s)",
            to_seconds(wall_now()),
            wall_now(),
        )

    def _shift_wall_clock(self, shift_delta: timedelta | float) -> None:
        """Shift the wall clock by the given timedelta."""
        shift_delta = delta(shift_delta)
        new_wall_now = wall_now() + shift_delta
        self._update_wall_clock(new_wall_now)

    async def next_tick(
        self,
        next_tick_wall_times: Sequence[Adjustment],
        coro: Coroutine[None, None, _T],
    ) -> _T:
        """Wait for the next tick of the timer and return the result of the coroutine.

        This method simulates the passage of time and wall clock adjustments while a
        timer is waiting for its next tick. It runs a provided coroutine (like a
        timer's `receive()` or `ready()` method) and, while it's running, applies a
        series of time adjustments.

        This is useful for simulating wall clock jumps or drifts. The adjustments are
        applied after the timer has started waiting, ensuring the timer correctly
        observes the time change.

        Args:
            next_tick_wall_times: A sequence of `Adjustment` tuples. Each tuple
                specifies a monotonic time delta to wait before setting the wall
                clock to a new time. This simulates wall clock adjustments
                happening while the timer is sleeping.
            coro: The coroutine to run that will receive the next tick from the
                timer. Typically this will be the timer's `receive()` or `ready()`
                method.

        Returns:
            The result of the `coro` coroutine.
        """
        async with asyncio.TaskGroup() as tg:
            _logger.debug("_next_tick(): Creating timer task for receive()")
            timer_task: asyncio.Task[_T] = tg.create_task(coro)
            for adj in next_tick_wall_times:
                sleep_time = to_seconds(adj.mono_delta)
                wall_time = to_seconds(adj.wall_time)
                _logger.debug(
                    "_next_tick(): Waiting for %s seconds before setting wall clock to %s",
                    sleep_time,
                    wall_time,
                )
                await asyncio.sleep(sleep_time)
                assert not timer_task.done()
                self._update_wall_clock(adj.wall_time)
                _logger.debug(
                    "_next_tick(): After setting wall clock: now_wall=%s, now_mono=%s",
                    to_seconds(wall_now()),
                    mono_now(),
                )
            return await timer_task



================================================
FILE: tests/utils/__init__.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Utilities for testing purposes."""

from ._a_sequence import a_sequence
from .mock_microgrid_client import MockMicrogridClient

__all__ = [
    "a_sequence",
    "MockMicrogridClient",
]



================================================
FILE: tests/utils/_a_sequence.py
================================================
# License: MIT
# Copyright © 2022 Frequenz Energy-as-a-Service GmbH

"""Helper class to compare two sequences without caring about the underlying type."""


from collections.abc import Sequence
from typing import Any


# Disabling the lint because it reads easier in tests like this
class a_sequence:  # pylint: disable=invalid-name
    """Helper class to compare two sequences without caring about the underlying type.

    Examples:
        >>> from collections import deque
        >>> (1, 2) == a_sequence(1, 2)
        >>> True
        >>> deque([1, 2]) == a_sequence(1, 2)
        >>> True
        >>> deque([2, 1]) == a_sequence(1, 2)
        >>> False
        >>> 1 == a_sequence(1, 2)
        >>> False
    """

    def __init__(self, *sequence: Any) -> None:
        """Create an instance."""
        self.sequence: Sequence[Any] = sequence

    def __eq__(self, other: Any) -> bool:
        """# noqa D105 (Missing docstring in magic method)."""
        if not isinstance(other, Sequence):
            return False
        return list(self.sequence) == list(other)

    def __str__(self) -> str:
        """# noqa D105 (Missing docstring in magic method)."""
        return f"{self.__class__.__name__}({', '.join(str(s) for s in self.sequence)})"

    def __repr__(self) -> str:
        """# noqa D105 (Missing docstring in magic method)."""
        return f"{self.__class__.__name__}({', '.join(repr(s) for s in self.sequence)})"



================================================
FILE: tests/utils/component_data_streamer.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Tool for mocking streams of component data."""


import asyncio
from dataclasses import replace
from datetime import datetime, timezone

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import ComponentData

from frequenz.sdk._internal._asyncio import cancel_and_await

from .mock_microgrid_client import MockMicrogridClient


class MockComponentDataStreamer:
    """Mock streams of component data.

    This tool was create to:
    * specify what data should be send with the stream.
    * specify different sampling for each component data.
    * stop and start any stream any time.
    * modify the data that comes with the stream in runtime.
    """

    def __init__(self, mock_microgrid: MockMicrogridClient) -> None:
        """Create class instance.

        Args:
            mock_microgrid: Mock microgrid.
        """
        self._mock_microgrid = mock_microgrid
        self._component_data: dict[ComponentId, ComponentData] = {}
        self._streaming_tasks: dict[ComponentId, asyncio.Task[None]] = {}

    def start_streaming(
        self, component_data: ComponentData, sampling_rate: float
    ) -> None:
        """Start streaming this component data with given sampling rate.

        Args:
            component_data: component data to be streamed.
            sampling_rate: sampling rate

        Raises:
            RuntimeError: If component is already streaming data.
        """
        component_id = component_data.component_id
        if component_id in self._streaming_tasks:
            raise RuntimeError("Component is already streaming")

        self._component_data[component_id] = component_data
        self._streaming_tasks[component_id] = asyncio.create_task(
            self._stream_data(component_id, sampling_rate)
        )

    def get_current_component_data(self, component_id: ComponentId) -> ComponentData:
        """Get component data that are currently streamed or was streaming recently.

        Args:
            component_id: component id

        Raises:
            KeyError: If component never sent any data.

        Returns:
            Component data.
        """
        if component_id not in self._component_data:
            raise KeyError(f"Component {component_id} was never streaming data.")

        return self._component_data[component_id]

    def update_stream(self, new_component_data: ComponentData) -> None:
        """Update component stream to send new data.

        Component id is taken from the given component data/

        Args:
            new_component_data: new component data

        Raises:
            KeyError: If this component is not sending data.
        """
        cid = new_component_data.component_id
        if cid not in self._streaming_tasks:
            raise KeyError(f"Component {cid} is not streaming data")

        self._component_data[cid] = new_component_data

    async def stop_streaming(self, component_id: ComponentId) -> None:
        """Stop sending data from this component."""
        if task := self._streaming_tasks.pop(component_id, None):
            await cancel_and_await(task)

    async def stop(self) -> None:
        """Stop sending any data. This will close any pending async tasks."""
        await asyncio.gather(
            *[self.stop_streaming(cid) for cid in self._streaming_tasks]
        )

    async def _stream_data(
        self, component_id: ComponentId, sampling_rate: float
    ) -> None:
        while component_id in self._component_data:
            data = self._component_data[component_id]
            new_data = replace(data, timestamp=datetime.now(tz=timezone.utc))
            await self._mock_microgrid.send(new_data)
            await asyncio.sleep(sampling_rate)



================================================
FILE: tests/utils/component_data_wrapper.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Wrappers for the ComponentData.

This module allows the easy construction of mock ComponentData instances to be used in
unit tests. Usually, all parameters for an instance need to be defined, this module
helps by allowing users to only specify the parameters they are interested in for their
tests. The rest will be filled with default protobuf values.
This also abstracts away changes in the protobuf definition and minimizes the places
that will need to be updated in such cases.
"""
from __future__ import annotations

import math
from dataclasses import dataclass, replace
from datetime import datetime

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    BatteryComponentState,
    BatteryData,
    BatteryError,
    BatteryRelayState,
    EVChargerCableState,
    EVChargerComponentState,
    EVChargerData,
    InverterComponentState,
    InverterData,
    InverterError,
    MeterData,
)

# Disable these checks for the file as we need to pass a lot of data
# pylint: disable=too-many-arguments,too-many-positional-arguments


class BatteryDataWrapper(BatteryData):
    """Wrapper for the BatteryData with default arguments."""

    def __init__(
        self,
        component_id: ComponentId,
        timestamp: datetime,
        soc: float = math.nan,
        soc_lower_bound: float = math.nan,
        soc_upper_bound: float = math.nan,
        capacity: float = math.nan,
        power_inclusion_lower_bound: float = math.nan,
        power_exclusion_lower_bound: float = math.nan,
        power_inclusion_upper_bound: float = math.nan,
        power_exclusion_upper_bound: float = math.nan,
        temperature: float = math.nan,
        relay_state: BatteryRelayState = (BatteryRelayState.UNSPECIFIED),
        component_state: BatteryComponentState = (BatteryComponentState.UNSPECIFIED),
        errors: list[BatteryError] | None = None,
    ) -> None:
        """Initialize the BatteryDataWrapper.

        This is a wrapper for the BatteryData with default arguments. The parameters are
        documented in the BatteryData class.
        """
        super().__init__(
            component_id=component_id,
            timestamp=timestamp,
            soc=soc,
            soc_lower_bound=soc_lower_bound,
            soc_upper_bound=soc_upper_bound,
            capacity=capacity,
            power_inclusion_lower_bound=power_inclusion_lower_bound,
            power_exclusion_lower_bound=power_exclusion_lower_bound,
            power_inclusion_upper_bound=power_inclusion_upper_bound,
            power_exclusion_upper_bound=power_exclusion_upper_bound,
            temperature=temperature,
            relay_state=relay_state,
            component_state=component_state,
            errors=errors or [],
        )

    def copy_with_new_timestamp(self, new_timestamp: datetime) -> BatteryDataWrapper:
        """Copy the component data but insert new timestamp.

        Because the dataclass is frozen, we can't just replace the timestamp.
        We have to copy it.

        Args:
            new_timestamp: New timestamp.

        Returns:
            Copied component data.
        """
        return replace(self, timestamp=new_timestamp)


@dataclass(frozen=True)
class InverterDataWrapper(InverterData):
    """Wrapper for the InverterData with default arguments."""

    def __init__(  # pylint: disable=too-many-locals
        self,
        component_id: ComponentId,
        timestamp: datetime,
        active_power: float = math.nan,
        active_power_per_phase: tuple[float, float, float] = (
            math.nan,
            math.nan,
            math.nan,
        ),
        current_per_phase: tuple[float, float, float] = (math.nan, math.nan, math.nan),
        voltage_per_phase: tuple[float, float, float] = (math.nan, math.nan, math.nan),
        active_power_inclusion_lower_bound: float = math.nan,
        active_power_exclusion_lower_bound: float = math.nan,
        active_power_inclusion_upper_bound: float = math.nan,
        active_power_exclusion_upper_bound: float = math.nan,
        reactive_power: float = math.nan,
        reactive_power_per_phase: tuple[float, float, float] = (
            math.nan,
            math.nan,
            math.nan,
        ),
        frequency: float = 50.0,
        component_state: InverterComponentState = InverterComponentState.UNSPECIFIED,
        errors: list[InverterError] | None = None,
    ) -> None:
        """Initialize the InverterDataWrapper.

        This is a wrapper for the InverterData with default arguments. The parameters
        are documented in the InverterData class.
        """
        super().__init__(
            component_id=component_id,
            timestamp=timestamp,
            active_power=active_power,
            active_power_per_phase=active_power_per_phase,
            current_per_phase=current_per_phase,
            voltage_per_phase=voltage_per_phase,
            active_power_inclusion_lower_bound=active_power_inclusion_lower_bound,
            active_power_exclusion_lower_bound=active_power_exclusion_lower_bound,
            active_power_inclusion_upper_bound=active_power_inclusion_upper_bound,
            active_power_exclusion_upper_bound=active_power_exclusion_upper_bound,
            reactive_power=reactive_power,
            reactive_power_per_phase=reactive_power_per_phase,
            component_state=component_state,
            frequency=frequency,
            errors=errors or [],
        )

    def copy_with_new_timestamp(self, new_timestamp: datetime) -> InverterDataWrapper:
        """Copy the component data but insert new timestamp.

        Because the dataclass is frozen, we can't just replace the timestamp.
        We have to copy it.

        Args:
            new_timestamp: New timestamp.

        Returns:
            Copied component data.
        """
        return replace(self, timestamp=new_timestamp)


@dataclass(frozen=True)
class EvChargerDataWrapper(EVChargerData):
    """Wrapper for the EvChargerData with default arguments."""

    def __init__(  # pylint: disable=too-many-locals
        self,
        component_id: ComponentId,
        timestamp: datetime,
        active_power: float = math.nan,
        active_power_per_phase: tuple[float, float, float] = (
            math.nan,
            math.nan,
            math.nan,
        ),
        current_per_phase: tuple[float, float, float] = (math.nan, math.nan, math.nan),
        voltage_per_phase: tuple[float, float, float] = (math.nan, math.nan, math.nan),
        active_power_inclusion_lower_bound: float = math.nan,
        active_power_exclusion_lower_bound: float = math.nan,
        active_power_inclusion_upper_bound: float = math.nan,
        active_power_exclusion_upper_bound: float = math.nan,
        reactive_power: float = math.nan,
        reactive_power_per_phase: tuple[float, float, float] = (
            math.nan,
            math.nan,
            math.nan,
        ),
        frequency: float = 50.0,
        cable_state: EVChargerCableState = EVChargerCableState.UNSPECIFIED,
        component_state: EVChargerComponentState = EVChargerComponentState.UNSPECIFIED,
    ) -> None:
        """Initialize the EvChargerDataWrapper.

        This is a wrapper for the EvChargerData with default arguments. The parameters
        are documented in the EvChargerData class.
        """
        super().__init__(
            component_id=component_id,
            timestamp=timestamp,
            active_power=active_power,
            active_power_per_phase=active_power_per_phase,
            current_per_phase=current_per_phase,
            voltage_per_phase=voltage_per_phase,
            active_power_inclusion_lower_bound=active_power_inclusion_lower_bound,
            active_power_exclusion_lower_bound=active_power_exclusion_lower_bound,
            active_power_inclusion_upper_bound=active_power_inclusion_upper_bound,
            active_power_exclusion_upper_bound=active_power_exclusion_upper_bound,
            reactive_power=reactive_power,
            reactive_power_per_phase=reactive_power_per_phase,
            frequency=frequency,
            cable_state=cable_state,
            component_state=component_state,
        )

    def copy_with_new_timestamp(self, new_timestamp: datetime) -> EvChargerDataWrapper:
        """Copy the component data but insert new timestamp.

        Because the dataclass is frozen, we can't just replace the timestamp.
        We have to copy it.

        Args:
            new_timestamp: New timestamp.

        Returns:
            Copied component data.
        """
        return replace(self, timestamp=new_timestamp)


@dataclass(frozen=True)
class MeterDataWrapper(MeterData):
    """Wrapper for the MeterData with default arguments."""

    def __init__(
        self,
        component_id: ComponentId,
        timestamp: datetime,
        active_power: float = math.nan,
        active_power_per_phase: tuple[float, float, float] = (
            math.nan,
            math.nan,
            math.nan,
        ),
        reactive_power: float = math.nan,
        reactive_power_per_phase: tuple[float, float, float] = (
            math.nan,
            math.nan,
            math.nan,
        ),
        current_per_phase: tuple[float, float, float] = (math.nan, math.nan, math.nan),
        voltage_per_phase: tuple[float, float, float] = (math.nan, math.nan, math.nan),
        frequency: float = math.nan,
    ) -> None:
        """Initialize the MeterDataWrapper.

        This is a wrapper for the MeterData with default arguments. The parameters are
        documented in the MeterData class.
        """
        super().__init__(
            component_id=component_id,
            timestamp=timestamp,
            active_power=active_power,
            active_power_per_phase=active_power_per_phase,
            reactive_power=reactive_power,
            reactive_power_per_phase=reactive_power_per_phase,
            current_per_phase=current_per_phase,
            voltage_per_phase=voltage_per_phase,
            frequency=frequency,
        )

    def copy_with_new_timestamp(self, new_timestamp: datetime) -> MeterDataWrapper:
        """Copy the component data but insert new timestamp.

        Because the dataclass is frozen, we can't just replace the timestamp.
        We have to copy it.

        Args:
            new_timestamp: New timestamp.

        Returns:
            Copied component data.
        """
        return replace(self, timestamp=new_timestamp)



================================================
FILE: tests/utils/component_graph_utils.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Utils for tests that uses component graph."""


from dataclasses import dataclass

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    Component,
    ComponentCategory,
    Connection,
    InverterType,
)


@dataclass
class ComponentGraphConfig:
    """Config with information how the component graph should be created."""

    grid_side_meter: bool = True
    """True if the main meter should be by grid side."""

    batteries_num: int = 0
    """Number of batteries in the component graph.

    Each battery will have its own inverter and meter connected.
    """

    solar_inverters_num: int = 0
    """Number of pv inverters in the component graph.

    Each pv inverter will have its own pv meter connected.
    """

    ev_chargers: int = 0
    """Number of ev chargers in the component graph."""


def create_component_graph_structure(
    component_graph_config: ComponentGraphConfig,
) -> tuple[set[Component], set[Connection]]:
    """Create structure of components graph.

    Args:
        component_graph_config: config that tells what graph should have.

    Returns:
        Create set of components and set of connections between them.
    """
    grid_id = ComponentId(1)
    main_meter_id = ComponentId(2)

    components = {
        Component(grid_id, ComponentCategory.GRID),
        Component(main_meter_id, ComponentCategory.METER),
    }
    connections = {Connection(grid_id, main_meter_id)}

    junction_id = grid_id
    if component_graph_config.grid_side_meter:
        junction_id = main_meter_id

    start_idx = 3
    for _ in range(component_graph_config.batteries_num):
        meter_id = ComponentId(start_idx)
        inv_id = ComponentId(int(start_idx) + 1)
        battery_id = ComponentId(start_idx + 2)
        start_idx += 3

        components.add(Component(meter_id, ComponentCategory.METER))
        components.add(Component(battery_id, ComponentCategory.BATTERY))
        components.add(
            Component(inv_id, ComponentCategory.INVERTER, InverterType.BATTERY)
        )

        connections.add(Connection(junction_id, meter_id))
        connections.add(Connection(meter_id, inv_id))
        connections.add(Connection(inv_id, battery_id))

    for _ in range(component_graph_config.solar_inverters_num):
        meter_id = ComponentId(start_idx)
        inv_id = ComponentId(start_idx + 1)
        start_idx += 2

        components.add(Component(meter_id, ComponentCategory.METER))
        components.add(
            Component(inv_id, ComponentCategory.INVERTER, InverterType.SOLAR)
        )
        connections.add(Connection(junction_id, meter_id))
        connections.add(Connection(meter_id, inv_id))

    for _ in range(component_graph_config.ev_chargers):
        ev_id = ComponentId(start_idx)
        start_idx += 1

        components.add(Component(ev_id, ComponentCategory.EV_CHARGER))
        connections.add(Connection(junction_id, ev_id))
    return components, connections



================================================
FILE: tests/utils/graph_generator.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Generate graphs from component data structures."""

from dataclasses import replace
from typing import Any, overload

from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    Component,
    ComponentCategory,
    ComponentType,
    Connection,
    GridMetadata,
    InverterType,
)

from frequenz.sdk.microgrid.component_graph import _MicrogridComponentGraph


class GraphGenerator:
    """Utilities to generate graphs from component data structures."""

    SUFFIXES = {
        ComponentCategory.CHP: 5,
        ComponentCategory.EV_CHARGER: 6,
        ComponentCategory.METER: 7,
        ComponentCategory.INVERTER: 8,
        ComponentCategory.BATTERY: 9,
    }

    def __init__(self) -> None:
        """Create a new instance."""
        self._id_increment = 0

    def new_id(self) -> dict[ComponentCategory, ComponentId]:
        """Get the next available component id.

        Usage example:

        ```python

        gen = GraphGenerator()

        base_id = gen.id()

        # You can use the same base_id for multiple categories.
        # It will be the same id with the according suffix.
        meter_id = base_id[ComponentCategory.METER]
        bat_id   = base_id[ComponentCategory.BATTERY]
        # ...

        # If you just want to get the next available id for a category,
        # you can use:
        meter2_id = gen.id()[ComponentCategory.METER]
        ```

        Returns:
            a dict containing the next available id for each component category.
        """
        id_per_category = {
            cat: ComponentId(self._id_increment * 10 + suffix)
            for cat, suffix in self.SUFFIXES.items()
        }
        self._id_increment += 1
        return id_per_category

    def battery_with_inverter(self, battery: Component, num_inverters: int) -> Any:
        """Add a meter and inverters to the given battery.

        Args:
            battery: the battery component.
            num_inverters: the number of inverters to create.

        Returns:
            connected graph components for the given battery.
        """
        assert battery.category == ComponentCategory.BATTERY
        return self._battery_with_inverter(battery, num_inverters)

    def batteries_with_inverter(
        self, one_or_more_batteries: list[Component], num_inverters: int
    ) -> Any:
        """Add a meter and inverters to the given batteries.

        Args:
            one_or_more_batteries: the battery components.
            num_inverters: the number of inverters to create.

        Returns:
            connected graph components for the given batteries.
        """
        assert all(
            b.category == ComponentCategory.BATTERY for b in one_or_more_batteries
        )
        return self._battery_with_inverter(one_or_more_batteries, num_inverters)

    def _battery_with_inverter(
        self, one_or_more_batteries: Component | list[Component], num_inverters: int
    ) -> Any:
        """Add a meter and inverters to the given battery or batteries.

        Args:
            one_or_more_batteries: the battery component or components.
            num_inverters: the number of inverters to create.

        Returns:
            connected graph components for the given battery or batteries.
        """
        return (
            ComponentCategory.METER,
            [
                (
                    self.component(ComponentCategory.INVERTER, InverterType.BATTERY),
                    one_or_more_batteries,
                )
                for _ in range(num_inverters)
            ],
        )

    @overload
    def component(
        self, other: Component, comp_type: ComponentType | None = None
    ) -> Component:
        """Just return the given component.

        Args:
            other: the component to return.
            comp_type: the component type to set, ignored

        Returns:
            the given component.
        """

    @overload
    def component(
        self, other: ComponentCategory, comp_type: ComponentType | None = None
    ) -> Component:
        """Create a new component with the next available id for the given category.

        Args:
            other: the component category to get the id for.
            comp_type: the component type to set.

        Returns:
            the next available component id for the given category.
        """

    def component(
        self,
        other: ComponentCategory | Component,
        comp_type: ComponentType | None = None,
    ) -> Component:
        """Make or return a new component.

        Args:
            other: a component or the component category to get the id for.
            comp_type: the component type to set.

        Returns:
            the next available component id for the given category.
        """
        if isinstance(other, Component):
            return other

        assert isinstance(other, ComponentCategory)
        category = other

        return Component(self.new_id()[category], category, comp_type)

    def components(self, *component_categories: ComponentCategory) -> list[Component]:
        """Create a list of components with the next available id for each category.

        Args:
            *component_categories: the component categories

        Returns:
            the given components.
        """
        return [self.component(category) for category in component_categories]

    @staticmethod
    def grid() -> Component:
        """Get a new grid component with default id.

        Returns:
            a new grid component with default id.
        """
        return Component(
            ComponentId(1), ComponentCategory.GRID, None, GridMetadata(None)
        )

    def to_graph(self, components: Any) -> _MicrogridComponentGraph:
        """Convert a list of components to a graph.

        GRID will be added and connected as the first component.

        Input can be a graph made of tuples, lists, components or component
        categories. Inverter Types will be set automatically based on the
        successors category.

        Definition of possible inputs and input types:

        - ComponentLike: A component object or a component category.
        - AnyTuple: A tuple of any tuple type explained here.
        - AnyList: A list of AnyTuple or ComponentLike, or a mix of both.
        - tuple[ComponentLike, ComponentLike]: A tuple of two components. The first
            member is connected to the second member.
        - tuple[ComponentLike, AnyTuple]: A tuple of a ComponentLike and an AnyTuple.
            The first member is connected to the first member of the tuple.
        - tuple[ComponentLike, AnyList]: A tuple of a ComponentLike and an AnyList.
            The first member is connected to all members of the list.

        ```python
        gen = GraphGenerator()

        # Pre-create a battery component to refer to it later or to use it in multiple
        # places in the graph.
        special_bat = gen.component(ComponentCategory.BATTERY)

        graph = gen.to_graph(
            (
                ComponentCategory.METER, # grid side meter
                [ # list of components connected to grid side meter
                    (
                        ComponentCategory.METER, # Meter in front of battery->inverter
                        ( # A tuple, first is connected to parent, second is the child of the first
                            # Inverter in front of battery, type will be
                            # set to InverterType.BATTERY
                            ComponentCategory.INVERTER,
                            ComponentCategory.BATTERY,  # Battery
                        ),
                    ),
                    (
                        ComponentCategory.METER,
                        (
                            # Inverter in front of battery, type will be
                            # set to InverterType.BATTERY
                            ComponentCategory.INVERTER,
                            special_bat,  # Pre-created battery
                        ),
                    ),
                ],
            )
        )
        ```

        Args:
            components: the components to convert to a graph.

        Returns:
            a tuple containing the components and connections of the graph.
        """
        graph = self._to_graph(self.grid(), components)
        return _MicrogridComponentGraph(set(graph[0]), set(graph[1]))

    def _to_graph(
        self, parent: Component, children: Any
    ) -> tuple[list[Component], list[Connection]]:
        """Convert a list of components to a graph.

        Args:
            parent: the parent component.
            children: the children components.

        Returns:
            a tuple containing the components and connections of the graph.

        Raises:
            ValueError: if the input is invalid.
        """

        def inverter_type(category: ComponentCategory) -> InverterType | None:
            if category == ComponentCategory.BATTERY:
                return InverterType.BATTERY
            return None

        def update_inverter_type(successor: Component) -> None:
            nonlocal parent
            if parent.category == ComponentCategory.INVERTER:
                if comp_type := inverter_type(successor.category):
                    parent = replace(parent, type=comp_type)

        if isinstance(children, (Component, ComponentCategory)):
            rhs = self.component(children)
            update_inverter_type(rhs)
            return [parent, rhs], [Connection(parent.component_id, rhs.component_id)]
        if isinstance(children, tuple):
            assert len(children) == 2
            comp, con = self._to_graph(self.component(children[0]), children[1])
            update_inverter_type(comp[0])
            return [parent] + comp, con + [
                Connection(parent.component_id, comp[0].component_id)
            ]
        if isinstance(children, list):
            comp = []
            con = []
            for _component in children:
                sub_components: list[Component]
                sub_con: list[Connection]

                if isinstance(_component, tuple):
                    sub_parent = self.component(_component[0])
                    sub_children = _component[1]

                    sub_components, sub_con = self._to_graph(sub_parent, sub_children)
                else:
                    sub_components = [self.component(_component)]
                    sub_con = []

                update_inverter_type(sub_components[0])
                comp += sub_components
                con += sub_con + [
                    Connection(parent.component_id, sub_components[0].component_id)
                ]
            return [parent] + comp, con

        raise ValueError("Invalid component list")


def test_graph_generator_simple() -> None:
    """Test a simple graph."""
    gen = GraphGenerator()
    graph = gen.to_graph(
        (
            ComponentCategory.METER,  # grid side meter
            [  # list of components connected to grid side meter
                (
                    ComponentCategory.METER,  # Meter in front of battery
                    (
                        # Inverter in front of battery, type will be
                        # set to InverterType.BATTERY
                        ComponentCategory.INVERTER,
                        ComponentCategory.BATTERY,  # Battery
                    ),
                ),
                (
                    ComponentCategory.METER,
                    # Inverter, type is explicitly set to InverterType.SOLAR
                    gen.component(ComponentCategory.INVERTER, InverterType.SOLAR),
                ),
                (ComponentCategory.METER, ComponentCategory.EV_CHARGER),
                (ComponentCategory.INVERTER, ComponentCategory.BATTERY),
            ],
        )
    )

    meters = list(graph.components(component_categories={ComponentCategory.METER}))
    meters.sort(key=lambda x: x.component_id)
    assert len(meters) == 4
    assert len(graph.successors(meters[0].component_id)) == 4
    assert graph.predecessors(meters[1].component_id) == {meters[0]}
    assert graph.predecessors(meters[2].component_id) == {meters[0]}
    assert graph.predecessors(meters[3].component_id) == {meters[0]}

    inverters = list(
        graph.components(component_categories={ComponentCategory.INVERTER})
    )
    inverters.sort(key=lambda x: x.component_id)
    assert len(inverters) == 3

    assert len(graph.successors(inverters[0].component_id)) == 0
    assert inverters[0].type == InverterType.SOLAR

    assert len(graph.successors(inverters[1].component_id)) == 1
    assert inverters[1].type == InverterType.BATTERY

    assert len(graph.successors(inverters[2].component_id)) == 1
    assert inverters[2].type == InverterType.BATTERY

    assert len(graph.components(component_categories={ComponentCategory.BATTERY})) == 2
    assert (
        len(graph.components(component_categories={ComponentCategory.EV_CHARGER})) == 1
    )

    graph.validate()


def test_graph_generator_no_grid_meter() -> None:
    """Test a graph without a grid side meter and a list of components at the top."""
    gen = GraphGenerator()
    graph = gen.to_graph(
        [
            (
                ComponentCategory.INVERTER,
                ComponentCategory.BATTERY,
            ),
            (
                ComponentCategory.METER,
                (
                    ComponentCategory.INVERTER,
                    ComponentCategory.BATTERY,
                ),
            ),
        ]
    )

    meters = list(graph.components(component_categories={ComponentCategory.METER}))
    assert len(meters) == 1
    assert len(graph.successors(meters[0].component_id)) == 1

    inverters = list(
        graph.components(component_categories={ComponentCategory.INVERTER})
    )
    assert len(inverters) == 2

    assert len(graph.successors(inverters[0].component_id)) == 1
    assert len(graph.successors(inverters[1].component_id)) == 1

    assert len(graph.components(component_categories={ComponentCategory.BATTERY})) == 2

    graph.validate()



================================================
FILE: tests/utils/mock_microgrid_client.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Mock microgrid definition."""
from functools import partial
from typing import Any
from unittest.mock import AsyncMock, MagicMock

from frequenz.channels import Broadcast, Receiver
from frequenz.client.common.microgrid import MicrogridId
from frequenz.client.common.microgrid.components import ComponentId
from frequenz.client.microgrid import (
    BatteryData,
    Component,
    ComponentCategory,
    ComponentData,
    Connection,
    EVChargerData,
    InverterData,
    Location,
    MeterData,
)
from pytest_mock import MockerFixture

from frequenz.sdk._internal._constants import RECEIVER_MAX_SIZE
from frequenz.sdk.microgrid.component_graph import (
    ComponentGraph,
    _MicrogridComponentGraph,
)
from frequenz.sdk.microgrid.connection_manager import ConnectionManager


class MockMicrogridClient:
    """Class that mocks MicrogridClient behavior."""

    def __init__(
        self,
        components: set[Component],
        connections: set[Connection],
        microgrid_id: MicrogridId = MicrogridId(8),
        location: Location = Location(latitude=52.520008, longitude=13.404954),
    ):
        """Create mock microgrid with given components and connections.

        This simulates microgrid.
        Every call to `battery_data` and `inverter_data` is mocked to return
        receiver owned by this class.
        User can send data to the receiver using `self.send(..)` method.
        Messages will be send based on component id.

        Args:
            components: List of the microgrid components
            connections: List of the microgrid connections
            microgrid_id: the ID of the microgrid
            location: the location of the microgrid
        """
        self._component_graph = _MicrogridComponentGraph(components, connections)

        self._components = components

        bat_channels = self._create_battery_channels()
        inv_channels = self._create_inverter_channels()
        meter_channels = self._create_meter_channels()
        ev_charger_channels = self._create_ev_charger_channels()

        self._all_channels: dict[ComponentId, Broadcast[Any]] = {
            **bat_channels,
            **inv_channels,
            **meter_channels,
            **ev_charger_channels,
        }

        mock_api = self._create_mock_api(
            bat_channels, inv_channels, meter_channels, ev_charger_channels
        )
        kwargs: dict[str, Any] = {
            "api_client": mock_api,
            "component_graph": self._component_graph,
            "microgrid_id": microgrid_id,
            "location": location,
        }

        self._mock_microgrid = MagicMock(spec=ConnectionManager, **kwargs)
        self._battery_data_senders = {
            id: channel.new_sender() for id, channel in bat_channels.items()
        }
        self._inverter_data_senders = {
            id: channel.new_sender() for id, channel in inv_channels.items()
        }
        self._meter_data_senders = {
            id: channel.new_sender() for id, channel in meter_channels.items()
        }
        self._ev_charger_data_senders = {
            id: channel.new_sender() for id, channel in ev_charger_channels.items()
        }

    def initialize(self, mocker: MockerFixture) -> None:
        """Mock `microgrid.get` call to return this mock_microgrid.

        Args:
            mocker: mocker from the current test
        """
        # Mock _MICROGRID, so `get` method return this mocked microgrid.
        mocker.patch(
            "frequenz.sdk.microgrid.connection_manager._CONNECTION_MANAGER",
            self.mock_microgrid,
        )

    @property
    def mock_microgrid(self) -> ConnectionManager:
        """Return mock microgrid.

        This is needed to patch existing microgrid.get() method.

        Returns:
            Mock microgrid.
        """
        return self._mock_microgrid

    @property
    def component_graph(self) -> ComponentGraph:
        """Return microgrid component graph.

        Component graph is not mocked.

        Returns:
            Mock microgrid.
        """
        return self._component_graph

    # We need the noqa because the `SenderError` is raised indirectly by `send()`.
    async def send(self, data: ComponentData) -> None:  # noqa: DOC503
        """Send component data using channel.

        This simulates component sending data. Right now only battery and inverter
        are supported. More components categories can be added if needed.

        Args:
            data: Data to be send

        Raises:
            RuntimeError: if the type of data is not supported.
            SenderError: if the underlying channel was closed.
                A [ChannelClosedError][frequenz.channels.ChannelClosedError] is
                set as the cause.
        """
        cid = data.component_id
        if isinstance(data, BatteryData):
            await self._battery_data_senders[cid].send(data)
        elif isinstance(data, InverterData):
            await self._inverter_data_senders[cid].send(data)
        elif isinstance(data, MeterData):
            await self._meter_data_senders[cid].send(data)
        elif isinstance(data, EVChargerData):
            await self._ev_charger_data_senders[cid].send(data)
        else:
            raise RuntimeError(f"{type(data)} is not supported in MockMicrogridClient.")

    async def close_channel(self, cid: ComponentId) -> None:
        """Close channel for given component id.

        Args:
            cid: Component id
        """
        if cid in self._all_channels:
            await self._all_channels[cid].close()

    def _create_battery_channels(self) -> dict[ComponentId, Broadcast[BatteryData]]:
        """Create channels for the batteries.

        Returns:
            Dictionary where the key is battery id and the value is channel for this
                battery.
        """
        batteries = [
            c.component_id
            for c in self.component_graph.components(
                component_categories={ComponentCategory.BATTERY}
            )
        ]

        return {
            bid: Broadcast[BatteryData](name="battery_data_" + str(bid))
            for bid in batteries
        }

    def _create_meter_channels(self) -> dict[ComponentId, Broadcast[MeterData]]:
        """Create channels for the meters.

        Returns:
            Dictionary where the key is meter id and the value is channel for this
                meter.
        """
        meters = [
            c.component_id
            for c in self.component_graph.components(
                component_categories={ComponentCategory.METER}
            )
        ]

        return {
            cid: Broadcast[MeterData](name="meter_data_" + str(cid)) for cid in meters
        }

    def _create_inverter_channels(self) -> dict[ComponentId, Broadcast[InverterData]]:
        """Create channels for the inverters.

        Returns:
            Dictionary where the key is inverter id and the value is channel for
                this inverter.
        """
        inverters = [
            c.component_id
            for c in self.component_graph.components(
                component_categories={ComponentCategory.INVERTER}
            )
        ]

        return {
            cid: Broadcast[InverterData](name="inverter_data_" + str(cid))
            for cid in inverters
        }

    def _create_ev_charger_channels(
        self,
    ) -> dict[ComponentId, Broadcast[EVChargerData]]:
        """Create channels for the ev chargers.

        Returns:
            Dictionary where the key is the id of the ev_charger and the value is
                channel for this ev_charger.
        """
        meters = [
            c.component_id
            for c in self.component_graph.components(
                component_categories={ComponentCategory.EV_CHARGER}
            )
        ]

        return {
            cid: Broadcast[EVChargerData](name="meter_data_" + str(cid))
            for cid in meters
        }

    def _create_mock_api(
        self,
        bat_channels: dict[ComponentId, Broadcast[BatteryData]],
        inv_channels: dict[ComponentId, Broadcast[InverterData]],
        meter_channels: dict[ComponentId, Broadcast[MeterData]],
        ev_charger_channels: dict[ComponentId, Broadcast[EVChargerData]],
    ) -> MagicMock:
        """Create mock of MicrogridApiClient.

        Args:
            bat_channels: battery channels to be returned from
                MicrogridApiClient.battery_data.
            inv_channels: inverter channels to be returned from
                MicrogridApiClient.inverter_data.
            meter_channels: meter channels to be returned from
                MicrogridApiClient.meter_data.
            ev_charger_channels: ev_charger channels to be returned from
                MicrogridApiClient.ev_charger_data.

        Returns:
            Magic mock instance of MicrogridApiClient.
        """
        api = MagicMock()
        api.components = AsyncMock(return_value=self._components)
        # NOTE that has to be partial, because battery_data has id argument and takes
        # channel based on the argument.
        api.battery_data = AsyncMock(
            side_effect=partial(self._get_battery_receiver, channels=bat_channels)
        )

        api.inverter_data = AsyncMock(
            side_effect=partial(self._get_inverter_receiver, channels=inv_channels)
        )

        api.meter_data = AsyncMock(
            side_effect=partial(self._get_meter_receiver, channels=meter_channels)
        )

        api.ev_charger_data = AsyncMock(
            side_effect=partial(
                self._get_ev_charger_receiver, channels=ev_charger_channels
            )
        )

        # Can be override in the future
        api.set_power = AsyncMock(return_value=None)
        return api

    def _get_battery_receiver(
        self,
        component_id: ComponentId,
        channels: dict[ComponentId, Broadcast[BatteryData]],
        maxsize: int = RECEIVER_MAX_SIZE,
    ) -> Receiver[BatteryData]:
        """Return receiver of the broadcast channel for given component_id.

        Args:
            component_id: component_id
            channels: Broadcast channels
            maxsize: Max size of the channel

        Returns:
            Receiver from the given channels.
        """
        return channels[component_id].new_receiver(
            name="component" + str(component_id), limit=maxsize
        )

    def _get_meter_receiver(
        self,
        component_id: ComponentId,
        channels: dict[ComponentId, Broadcast[MeterData]],
        maxsize: int = RECEIVER_MAX_SIZE,
    ) -> Receiver[MeterData]:
        """Return receiver of the broadcast channel for given component_id.

        Args:
            component_id: component_id
            channels: Broadcast channels
            maxsize: Max size of the channel

        Returns:
            Receiver from the given channels.
        """
        return channels[component_id].new_receiver(
            name="component" + str(component_id), limit=maxsize
        )

    def _get_ev_charger_receiver(
        self,
        component_id: ComponentId,
        channels: dict[ComponentId, Broadcast[EVChargerData]],
        maxsize: int = RECEIVER_MAX_SIZE,
    ) -> Receiver[EVChargerData]:
        """Return receiver of the broadcast channel for given component_id.

        Args:
            component_id: component_id
            channels: Broadcast channels
            maxsize: Max size of the channel

        Returns:
            Receiver from the given channels.
        """
        return channels[component_id].new_receiver(
            name="component" + str(component_id), limit=maxsize
        )

    def _get_inverter_receiver(
        self,
        component_id: ComponentId,
        channels: dict[ComponentId, Broadcast[InverterData]],
        maxsize: int = RECEIVER_MAX_SIZE,
    ) -> Receiver[InverterData]:
        """Return receiver of the broadcast channel for given component_id.

        Args:
            component_id: component_id
            channels: Broadcast channels
            maxsize: Max size of the channel

        Returns:
            Receiver from the given channels.
        """
        return channels[component_id].new_receiver(
            name="component" + str(component_id), limit=maxsize
        )



================================================
FILE: tests/utils/receive_timeout.py
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH

"""Utility for receiving messages with timeout."""

import asyncio
from typing import TypeVar

from frequenz.channels import Receiver

T = TypeVar("T")


class Timeout:
    """Sentinel for timeout."""


async def receive_timeout(recv: Receiver[T], timeout: float = 0.1) -> T | type[Timeout]:
    """Receive message from receiver with timeout.

    Args:
        recv: Receiver to receive message from.
        timeout: Timeout in seconds.

    Returns:
        Received message or Timeout if timeout is reached.
    """
    try:
        return await asyncio.wait_for(recv.receive(), timeout=timeout)
    except asyncio.TimeoutError:
        return Timeout



================================================
FILE: .github/dependabot.yml
================================================
version: 2
updates:
  - package-ecosystem: "pip"
    directory: "/"
    schedule:
      interval: "monthly"
      day: "friday"
    labels:
      - "part:tooling"
      - "type:tech-debt"
    # Default versioning-strategy. For other versioning-strategy see:
    # https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/configuration-options-for-the-dependabot.yml-file#versioning-strategy
    versioning-strategy: auto
    # Allow up to 10 open pull requests for updates to dependency versions
    open-pull-requests-limit: 10
    # We group patch updates as they should always work.
    # We also group minor updates, as it works too for most libraries,
    # typically except libraries that don't have a stable release yet (v0.x.x
    # branch), so we make some exceptions for them.
    # Major updates and dependencies excluded by the above groups are still
    # managed, but they'll create one PR per dependency, as breakage is
    # expected, so it might need manual intervention.
    # Finally, we group some dependencies that are related to each other, and
    # usually need to be updated together.
    groups:
      patch:
        update-types:
          - "patch"
        exclude-patterns:
          # pydoclint has shipped breaking changes in patch updates often
          - "pydoclint"
      minor:
        update-types:
          - "minor"
        exclude-patterns:
          - "async-solipsism"
          - "frequenz-client-microgrid"
          - "frequenz-repo-config*"
          - "markdown-callouts"
          - "mkdocs-gen-files"
          - "mkdocs-literate-nav"
          - "mkdocstrings*"
          - "pydoclint"
          - "pytest-asyncio"
      # We group repo-config updates as it uses optional dependencies that are
      # considered different dependencies otherwise, and will create one PR for
      # each if we don't group them.
      repo-config:
        patterns:
          - "frequenz-repo-config*"
      mkdocstrings:
        patterns:
          - "mkdocstrings*"
    ignore:
      # Upgrade to time-machine 2.13.0+ breaks our tests. See:
      # https://github.com/frequenz-floss/frequenz-sdk-python/issues/832
      - dependency-name: "time-machine"
        versions: [">=2.13.0"]



  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: "monthly"
      day: "friday"
    labels:
      - "part:tooling"
      - "type:tech-debt"
    groups:
      compatible:
        update-types:
          - "minor"
          - "patch"
      artifacts:
        patterns:
          - "actions/*-artifact"



================================================
FILE: .github/keylabeler.yml
================================================
# KeywordLabeler app configuration. For more information check:
# https://github.com/ZeWaka/KeywordLabeler#readme

# Determines if we search the title (optional). Defaults to true.
matchTitle: true

# Determines if we search the body (optional). Defaults to true.
matchBody: true

# Determines if label matching is case sensitive (optional). Defaults to true.
caseSensitive: true

# Explicit keyword mappings to labels. Form of match:label. Required.
labelMappings:
  "part:actor": "part:actor"
  "part:config": "part:config"
  "part:core": "part:core"
  "part:data-pipeline": "part:data-pipeline"
  "part:docs": "part:docs"
  "part:microgrid": "part:microgrid"
  "part:power-distribution": "part:power-distribution"
  "part:tests": "part:tests"
  "part:tooling": "part:tooling"
  "part:❓": "part:❓"



================================================
FILE: .github/labeler.yml
================================================
# Configuration for the Labeler GitHub action, executed by
# .github/workflows/labeler.yml.
#
# The basic syntax is [label]: [path patterns].
#
# For more details on the configuration please see:
# https://github.com/marketplace/actions/labeler

"part:actor":
  - changed-files:
    - any-glob-to-any-file:
      - "src/frequenz/sdk/actor/**"

"part:config":
  - changed-files:
    - any-glob-to-any-file:
      - "src/frequenz/sdk/config/**"

"part:core":
  - changed-files:
    - any-glob-to-any-file:
      - "src/frequenz/sdk/_internal/**"

"part:data-pipeline":
  - changed-files:
    - any-glob-to-any-file:
      - "src/frequenz/sdk/_data_handling/**"
      - "src/frequenz/sdk/_data_ingestion/**"
      - "src/frequenz/sdk/timeseries/**"

"part:docs":
  - changed-files:
    - any-glob-to-any-file:
      - "**/*.md"
      - "docs/**"
      - "examples/**"
      - LICENSE

"part:microgrid":
  - changed-files:
    - any-glob-to-any-file:
      - "src/frequenz/sdk/microgrid/**"

"part:power-distribution":
  - changed-files:
    - any-glob-to-any-file:
      - "src/frequenz/sdk/power/**"

"part:tests":
  - changed-files:
    - any-glob-to-any-file:
      - "**/conftest.py"
      - "tests/**"

"part:tooling":
  - changed-files:
    - any-glob-to-any-file:
      - "**/*.ini"
      - "**/*.toml"
      - "**/*.yaml"
      - "**/*.yml"
      - "**/conftest.py"
      - ".editorconfig"
      - ".git*"
      - ".git*/**"
      - "docs/*.py"
      - CODEOWNERS
      - MANIFEST.in
      - noxfile.py



================================================
FILE: .github/RELEASE_NOTES.template.md
================================================
# Frequenz Python SDK Release Notes

## Summary

<!-- Here goes a general summary of what this release is about -->

## Upgrading

<!-- Here goes notes on how to upgrade from previous versions, including deprecations and what they should be replaced with -->

## New Features

<!-- Here goes the main new features and examples or instructions on how to use them -->

## Bug Fixes

<!-- Here goes notable bug fixes that are worth a special mention or explanation -->



================================================
FILE: .github/containers/nox-cross-arch/arm64-ubuntu-20.04-python.Dockerfile
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH
# This Dockerfile is used to run the tests in arm64, which is not supported by
# GitHub Actions at the moment.

FROM docker.io/library/ubuntu:20.04

ARG PYTHON_VERSION="3.11"

ENV DEBIAN_FRONTEND=noninteractive

# Install Python and curl to install pip later
RUN apt-get update -y && \
    apt-get install --no-install-recommends -y \
        software-properties-common && \
    add-apt-repository ppa:deadsnakes/ppa && \
    apt-get install --no-install-recommends -y \
        ca-certificates \
        curl \
        git \
        python${PYTHON_VERSION} \
        python${PYTHON_VERSION}-venv && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Install pip
RUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}

RUN update-alternatives --install \
        /usr/local/bin/python python /usr/bin/python${PYTHON_VERSION} 1 && \
    python -m pip install --upgrade --no-cache-dir pip

COPY entrypoint.bash /usr/bin/entrypoint.bash

ENTRYPOINT ["/usr/bin/entrypoint.bash"]



================================================
FILE: .github/containers/nox-cross-arch/entrypoint.bash
================================================
#!/bin/bash
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH
set -e

echo "System details:" $(uname -a)
echo "Machine:" $(uname -m)

exec "$@"



================================================
FILE: .github/containers/test-installation/Dockerfile
================================================
# License: MIT
# Copyright © 2023 Frequenz Energy-as-a-Service GmbH
# This Dockerfile is used to test the installation of the python package in
# multiple platforms in the CI. It is not used to build the package itself.

ARG PYTHON_VERSION="3.11"

FROM python:${PYTHON_VERSION}-slim

RUN apt-get update -y && \
    apt-get install --no-install-recommends -y \
    git && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* && \
    python -m pip install --upgrade --no-cache-dir pip

COPY dist dist
RUN pip install dist/*.whl && \
    rm -rf dist



================================================
FILE: .github/ISSUE_TEMPLATE/bug.yml
================================================
# GitHub issue form. For more information see:
# https://docs.github.com/en/communities/using-templates-to-encourage-useful-issues-and-pull-requests/syntax-for-issue-forms

name: Report something is not working properly 🐛
description:
  Use this if there is something that is not working properly.  If you are not
  sure or you need help making something work, please ask a question instead.
labels:
  - "priority:❓"
  - "type:bug"
body:
  - type: markdown
    attributes:
      value:
        Thanks for taking the time to fill out this bug report!
  - type: textarea
    id: what-happened
    attributes:
      label: What happened?
      description: Please tell us what happened that shouldn't have.
      placeholder: What happened that shouldn't have.
    validations:
      required: true
  - type: textarea
    id: what-expected
    attributes:
      label: What did you expect instead?
      description: Please tell us what did you expect to happen.
      placeholder: What did you expect to happen.
    validations:
      required: true
  - type: input
    id: version
    attributes:
      label: Affected version(s)
      description:
        Please add a comma-separated list of the versions affected by this
        issue.
      placeholder: 'Example: v0.11.0, v0.12.0'
  - type: dropdown
    id: part
    attributes:
      label: Affected part(s)
      description:
        Which parts of the repo are affected by this issue? Select all that
        apply.
      multiple: true
      options:
        - I don't know (part:❓)
        - Actors or actors utilities (decorator, etc.) (part:actor)
        - Configuration management (part:config)
        - Core components (data structures, etc.) (part:core)
        - Data pipeline (part:data-pipeline)
        - Documentation (part:docs)
        - Microgrid (API, component graph, etc.) (part:microgrid)
        - Battery power distribution (part:power-distribution)
        - Unit, integration and performance tests (part:tests)
        - Build script, CI, dependencies, etc. (part:tooling)
    validations:
      required: true
  - type: textarea
    id: extra
    attributes:
      label: Extra information
      description:
        Please write here any extra information you think it might be relevant,
        e.g., if this didn't happen before, or if you suspect where the problem
        might be.
      placeholder: Any extra information you think it might be relevant.



================================================
FILE: .github/ISSUE_TEMPLATE/config.yml
================================================
# GitHub issue template chooser. For more information see:
# https://docs.github.com/en/communities/using-templates-to-encourage-useful-issues-and-pull-requests/configuring-issue-templates-for-your-repository#configuring-the-template-chooser

blank_issues_enabled: true
contact_links:
  - name: Ask a question ❓
    url: https://github.com/frequenz-floss/frequenz-sdk-python/discussions/new?category=support
    about: Use this if you are not sure how to do something, have installation problems, etc.



================================================
FILE: .github/ISSUE_TEMPLATE/feature.yml
================================================
# GitHub issue form. For more information see:
# https://docs.github.com/en/communities/using-templates-to-encourage-useful-issues-and-pull-requests/syntax-for-issue-forms

name: Request a feature or enhancement ✨
description: Use this if something is missing or could be done better or more easily.
labels:
  - "part:❓"
  - "priority:❓"
  - "type:enhancement"
body:
  - type: markdown
    attributes:
      value:
        Thanks for taking the time to fill out this feature or enhancement
        request!
  - type: textarea
    id: whats-needed
    attributes:
      label: What's needed?
      description:
        Please tell us what's missing or what could be done better or more easily.
      placeholder: What's missing or what could be done better or more easily.
    validations:
      required: true
  - type: textarea
    id: solution
    attributes:
      label: Proposed solution
      description:
        Please tell us how you think the needs above can be fulfilled. Only
        fill this field if it wasn't described above.
      placeholder:
        How do you think the needs above can be fulfilled. Only fill this field
        if it wasn't described above.
  - type: textarea
    id: use-cases
    attributes:
      label: Use cases
      description:
        Please tell us about the main use cases you see for this new feature or
        enhancement to help us understand more.
      placeholder:
        The main use cases you see for this new feature or enhancement to help
        us understand more.
  - type: textarea
    id: alternatives
    attributes:
      label: Alternatives and workarounds
      description:
        Please tell us if you tried any alternatives or workarounds for these
        use cases and how (un)useful they were.
      placeholder:
        Any alternatives or workarounds for these use cases and how (un)useful
        they were.
  - type: textarea
    id: additional-context
    attributes:
      label: Additional context
      description:
        Please add any additional information here - screenshots, diagrams, etc.
      placeholder: Any additional information here - screenshots, diagrams, etc.



================================================
FILE: .github/workflows/ci-pr.yaml
================================================
name: Test PR

on:
  pull_request:

env:
  # Please make sure this version is included in the `matrix`, as the
  # `matrix` section can't use `env`, so it must be entered manually
  DEFAULT_PYTHON_VERSION: '3.11'
  # It would be nice to be able to also define a DEFAULT_UBUNTU_VERSION
  # but sadly `env` can't be used either in `runs-on`.

jobs:
  nox:
    name: Test with nox
    runs-on: ubuntu-24.04

    steps:
      - name: Run nox
        uses: frequenz-floss/gh-action-nox@v1.0.1
        with:
          python-version: "3.11"
          nox-session: ci_checks_max

  test-docs:
    name: Test documentation website generation
    runs-on: ubuntu-24.04
    steps:
      - name: Setup Git
        uses: frequenz-floss/gh-action-setup-git@v1.0.0

      - name: Fetch sources
        uses: actions/checkout@v5
        with:
          submodules: true

      - name: Setup Python
        uses: frequenz-floss/gh-action-setup-python-with-deps@v1.0.1
        with:
          python-version: ${{ env.DEFAULT_PYTHON_VERSION }}
          dependencies: .[dev-mkdocs]

      - name: Generate the documentation
        env:
          MIKE_VERSION: gh-${{ github.job }}
        run: |
          mike deploy $MIKE_VERSION
          mike set-default $MIKE_VERSION

      - name: Upload site
        uses: actions/upload-artifact@v4
        with:
          name: docs-site
          path: site/
          if-no-files-found: error



================================================
FILE: .github/workflows/ci.yaml
================================================
name: CI

on:
  merge_group:
  push:
    # We need to explicitly include tags because otherwise when adding
    # `branches-ignore` it will only trigger on branches.
    tags:
      - '*'
    branches-ignore:
      # Ignore pushes to merge queues.
      # We only want to test the merge commit (`merge_group` event), the hashes
      # in the push were already tested by the PR checks
      - 'gh-readonly-queue/**'
      - 'dependabot/**'
  workflow_dispatch:

env:
  # Please make sure this version is included in the `matrix`, as the
  # `matrix` section can't use `env`, so it must be entered manually
  DEFAULT_PYTHON_VERSION: '3.11'
  # It would be nice to be able to also define a DEFAULT_UBUNTU_VERSION
  # but sadly `env` can't be used either in `runs-on`.

jobs:
  nox:
    name: Test with nox
    strategy:
      fail-fast: false
      matrix:
        arch:
          - amd64
          - arm
        os:
          - ubuntu-24.04
        python:
          - "3.11"
          - "3.12"
          - "3.13"
        nox-session:
          # To speed things up a bit we use the special ci_checks_max session
          # that uses the same venv to run multiple linting sessions
          - "ci_checks_max"
          - "pytest_min"
    runs-on: ${{ matrix.os }}${{ matrix.arch != 'amd64' && format('-{0}', matrix.arch) || '' }}

    steps:
      - name: Run nox
        uses: frequenz-floss/gh-action-nox@v1.0.1
        with:
          python-version: ${{ matrix.python }}
          nox-session: ${{ matrix.nox-session }}

  # This job runs if all the `nox` matrix jobs ran and succeeded.
  # It is only used to have a single job that we can require in branch
  # protection rules, so we don't have to update the protection rules each time
  # we add or remove a job from the matrix.
  nox-all:
    # The job name should match the name of the `nox` job.
    name: Test with nox
    needs: ["nox"]
    # We skip this job only if nox was also skipped
    if: always() && needs.nox.result != 'skipped'
    runs-on: ubuntu-24.04
    env:
      DEPS_RESULT: ${{ needs.nox.result }}
    steps:
      - name: Check matrix job result
        run: test "$DEPS_RESULT" = "success"

  build:
    name: Build distribution packages
    # Since this is a pure Python package, we only need to build it once. If it
    # had any architecture specific code, we would need to build it for each
    # architecture.
    runs-on: ubuntu-24.04

    steps:
      - name: Setup Git
        uses: frequenz-floss/gh-action-setup-git@v1.0.0

      - name: Fetch sources
        uses: actions/checkout@v5
        with:
          submodules: true

      - name: Setup Python
        uses: frequenz-floss/gh-action-setup-python-with-deps@v1.0.1
        with:
          python-version: ${{ env.DEFAULT_PYTHON_VERSION }}
          dependencies: build

      - name: Build the source and binary distribution
        run: python -m build

      - name: Upload distribution files
        uses: actions/upload-artifact@v4
        with:
          name: dist-packages
          path: dist/
          if-no-files-found: error

  test-installation:
    name: Test package installation
    needs: ["build"]
    strategy:
      fail-fast: false
      matrix:
        arch:
          - amd64
          - arm
        os:
          - ubuntu-24.04
        python:
          - "3.11"
          - "3.12"
          - "3.13"
    runs-on: ${{ matrix.os }}${{ matrix.arch != 'amd64' && format('-{0}', matrix.arch) || '' }}

    steps:
      - name: Setup Git
        uses: frequenz-floss/gh-action-setup-git@v1.0.0

      - name: Print environment (debug)
        run: env

      - name: Download package
        uses: actions/download-artifact@v5
        with:
          name: dist-packages
          path: dist

      # This is necessary for the `pip` caching in the setup-python action to work
      - name: Fetch the pyproject.toml file for this action hash
        env:
          GH_TOKEN: ${{ github.token }}
          REPO: ${{ github.repository }}
          REF: ${{ github.sha }}
        run: |
          set -ux
          gh api \
              -X GET \
              -H "Accept: application/vnd.github.raw" \
              "/repos/$REPO/contents/pyproject.toml?ref=$REF" \
            > pyproject.toml

      - name: Setup Python
        uses: frequenz-floss/gh-action-setup-python-with-deps@v1.0.1
        with:
          python-version: ${{ matrix.python }}
          dependencies: dist/*.whl

      - name: Print installed packages (debug)
        run: python -m pip freeze

  # This job runs if all the `test-installation` matrix jobs ran and succeeded.
  # It is only used to have a single job that we can require in branch
  # protection rules, so we don't have to update the protection rules each time
  # we add or remove a job from the matrix.
  test-installation-all:
    # The job name should match the name of the `test-installation` job.
    name: Test package installation
    needs: ["test-installation"]
    # We skip this job only if test-installation was also skipped
    if: always() && needs.test-installation.result != 'skipped'
    runs-on: ubuntu-24.04
    env:
      DEPS_RESULT: ${{ needs.test-installation.result }}
    steps:
      - name: Check matrix job result
        run: test "$DEPS_RESULT" = "success"

  test-docs:
    name: Test documentation website generation
    if: github.event_name != 'push'
    runs-on: ubuntu-24.04
    steps:
      - name: Setup Git
        uses: frequenz-floss/gh-action-setup-git@v1.0.0

      - name: Fetch sources
        uses: actions/checkout@v5
        with:
          submodules: true

      - name: Setup Python
        uses: frequenz-floss/gh-action-setup-python-with-deps@v1.0.1
        with:
          python-version: ${{ env.DEFAULT_PYTHON_VERSION }}
          dependencies: .[dev-mkdocs]

      - name: Generate the documentation
        env:
          MIKE_VERSION: gh-${{ github.job }}
        run: |
          mike deploy $MIKE_VERSION
          mike set-default $MIKE_VERSION

      - name: Upload site
        uses: actions/upload-artifact@v4
        with:
          name: docs-site
          path: site/
          if-no-files-found: error

  publish-docs:
    name: Publish documentation website to GitHub pages
    needs: ["nox-all", "test-installation-all"]
    if: github.event_name == 'push'
    runs-on: ubuntu-24.04
    permissions:
      contents: write
    steps:
      - name: Setup Git
        uses: frequenz-floss/gh-action-setup-git@v1.0.0

      - name: Fetch sources
        uses: actions/checkout@v5
        with:
          submodules: true

      - name: Setup Python
        uses: frequenz-floss/gh-action-setup-python-with-deps@v1.0.1
        with:
          python-version: ${{ env.DEFAULT_PYTHON_VERSION }}
          dependencies: .[dev-mkdocs]

      - name: Calculate and check version
        id: mike-version
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_REPO: ${{ github.repository }}
          GIT_REF: ${{ github.ref }}
          GIT_SHA: ${{ github.sha }}
        run: |
          python -m frequenz.repo.config.cli.version.mike.info

      - name: Fetch the gh-pages branch
        if: steps.mike-version.outputs.version
        run: git fetch origin gh-pages --depth=1

      - name: Build site
        if: steps.mike-version.outputs.version
        env:
          VERSION: ${{ steps.mike-version.outputs.version }}
          TITLE: ${{ steps.mike-version.outputs.title }}
          ALIASES: ${{ steps.mike-version.outputs.aliases }}
          # This is not ideal, we need to define all these variables here
          # because we need to calculate all the repository version information
          # to be able to show the correct versions in the documentation when
          # building it.
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_REPO: ${{ github.repository }}
          GIT_REF: ${{ github.ref }}
          GIT_SHA: ${{ github.sha }}
        run: |
          mike deploy --update-aliases --title "$TITLE" "$VERSION" $ALIASES

      - name: Sort site versions
        if: steps.mike-version.outputs.version
        run: |
          git checkout gh-pages
          python -m frequenz.repo.config.cli.version.mike.sort versions.json
          git commit -a -m "Sort versions.json"

      - name: Publish site
        if: steps.mike-version.outputs.version
        run: |
          git push origin gh-pages

  create-github-release:
    name: Create GitHub release
    needs: ["publish-docs"]
    # Create a release only on tags creation
    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags/v')
    permissions:
      # We need write permissions on contents to create GitHub releases and on
      # discussions to create the release announcement in the discussion forums
      contents: write
      discussions: write
    runs-on: ubuntu-24.04
    steps:
      - name: Download distribution files
        uses: actions/download-artifact@v5
        with:
          name: dist-packages
          path: dist

      - name: Download RELEASE_NOTES.md
        run: |
          set -ux
          gh api \
              -X GET \
              -f ref=$REF \
              -H "Accept: application/vnd.github.raw" \
              "/repos/$REPOSITORY/contents/RELEASE_NOTES.md" \
            > RELEASE_NOTES.md
        env:
          REF: ${{ github.ref }}
          REPOSITORY: ${{ github.repository }}
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Create GitHub release
        run: |
          set -ux
          extra_opts=
          if echo "$REF_NAME" | grep -- -; then extra_opts=" --prerelease"; fi
          gh release create \
            -R "$REPOSITORY" \
            --notes-file RELEASE_NOTES.md \
            --generate-notes \
            $extra_opts \
            $REF_NAME \
            dist/*
        env:
          REF_NAME: ${{ github.ref_name }}
          REPOSITORY: ${{ github.repository }}
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  publish-to-pypi:
    name: Publish packages to PyPI
    needs: ["create-github-release"]
    runs-on: ubuntu-24.04
    permissions:
      # For trusted publishing. See:
      # https://blog.pypi.org/posts/2023-04-20-introducing-trusted-publishers/
      id-token: write
    steps:
      - name: Download distribution files
        uses: actions/download-artifact@v5
        with:
          name: dist-packages
          path: dist

      - name: Publish the Python distribution to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1



================================================
FILE: .github/workflows/dco-merge-queue.yml
================================================
# Based on https://github.com/hyperledger/besu/pull/5207/files
name: DCO
on:
  merge_group:

jobs:
  DCO:
    runs-on: ubuntu-latest
    if: ${{ github.actor != 'dependabot[bot]' }}
    steps:
      - run: echo "This DCO job runs on merge_queue event and doesn't check PR contents"



================================================
FILE: .github/workflows/labeler.yml
================================================
name: Pull Request Labeler

on: [pull_request_target]

jobs:
  Label:
    permissions:
      contents: read
      pull-requests: write
    runs-on: ubuntu-latest
    steps:
      - name: Labeler
        # XXX: !!! SECURITY WARNING !!!
        # pull_request_target has write access to the repo, and can read secrets. We
        # need to audit any external actions executed in this workflow and make sure no
        # checked out code is run (not even installing dependencies, as installing
        # dependencies usually can execute pre/post-install scripts). We should also
        # only use hashes to pick the action to execute (instead of tags or branches).
        # For more details read:
        # https://securitylab.github.com/research/github-actions-preventing-pwn-requests/
        uses: actions/labeler@8558fd74291d67161a8a78ce36a881fa63b766a9  # 5.0.0
        with:
          repo-token: "${{ secrets.GITHUB_TOKEN }}"
          dot: true



================================================
FILE: .github/workflows/release-notes-check.yml
================================================
name: Release Notes Check

on:
  merge_group:
  pull_request:
    types:
      # On by default if you specify no types.
      - "opened"
      - "reopened"
      - "synchronize"
      # For `skip-label` only.
      - "labeled"
      - "unlabeled"


jobs:
  check-release-notes:
    name: Check release notes are updated
    runs-on: ubuntu-latest
    permissions:
      pull-requests: read
    steps:
      - name: Check for a release notes update
        if: github.event_name == 'pull_request'
        uses: brettcannon/check-for-changed-files@871d7b8b5917a4f6f06662e2262e8ffc51dff6d1 # v1.2.1
        with:
          file-pattern: "RELEASE_NOTES.md"
          prereq-pattern: "src/**"
          skip-label: "cmd:skip-release-notes"
          failure-message: "Missing a release notes update. Please add one or apply the ${skip-label} label to the pull request"


